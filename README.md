# Conduit Podcast Transcripts

This is an archive of transcriptions generated by Whisper, Audio Hijack, and related tools meant to be used as a source of data.

The text is from the [Conduit Podcast](https://relay.fm/conduit)

## Getting Started

### Installation

This project uses `uv` for fast, reliable Python package management. To set up:

```bash
# Install uv if you don't have it
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies and create virtual environment
uv sync
```

### Environment Configuration

Load the environment variables:

```bash
# Using direnv (recommended)
direnv allow

# Or manually source the .envrc file
source .envrc
```

The `.envrc` file should contain connection strings for PostgreSQL and other configuration.

## Usage

The project provides multiple interfaces for accessing and managing transcripts:

- **CLI Tool** - `conduit` command for local operations
- **REST API** - FastAPI server for programmatic access
- **MCP Server** - Model Context Protocol server for Claude integration

### Docker Setup

Start all services with Docker Compose:

```bash
# Start PostgreSQL and API server
docker compose up -d

# View logs
docker compose logs -f

# Stop services
docker compose down
```

The API will be available at `http://localhost:8000` with interactive docs at `/docs`.

### CLI Usage (via Docker)

```bash
docker compose run --rm app python -m cli.main [command] [options]
```

### Transcription

Transcribe episodes from the Conduit website using OpenAI Whisper:

```bash
# Transcribe the latest episode
docker compose run --rm app python -m cli.main transcribe <episode_number>

# Transcribe and ingest (default)
docker compose run --rm app python -m cli.main transcribe <episode_number>

# Use specific model size (tiny, base, small, medium, large)
docker compose run --rm app python -m cli.main transcribe <episode_number> --model medium

# Configure the LLM model for RAG (Retrieval Augmented Generation)
docker compose run --rm app -e LLM_MODEL=llama3 python -m cli.main transcribe <episode_number>
```

### Data Ingestion

Load transcripts into PostgreSQL:

```bash
# Ingest all files in transcripts directory
docker compose run --rm app python -m cli.main ingest

# Ingest specific file
docker compose run --rm app python -m cli.main ingest --file transcripts/episode1.md

# Recreate tables before ingestion
docker compose run --rm app python -m cli.main ingest --reindex
```

### Search

Search through ingested transcripts:

```bash
# Text search (default)
docker compose run --rm app python -m cli.main search "search term"

# Vector semantic search
docker compose run --rm app python -m cli.main search "search phrase" --vector
```

### Management

```bash
# Check episode status
docker compose run --rm app python -m cli.main status <episode_number>

# List recent episodes
docker compose run --rm app python -m cli.main list
```

## Project Structure

- `app/` - FastAPI application
  - `api/` - REST API endpoints (search, episodes, health)
  - `mcp/` - MCP Server for Claude integration
  - `main.py` - Main FastAPI application
- `cli/` - Command-line interface
- `src/conduit_transcripts/` - Shared library code
  - `database/` - Database operations (PostgreSQL)
  - `models/` - SQLAlchemy models
  - `transcription/` - Transcription logic (Whisper, MLX)
  - `utils/` - Shared utilities
- `transcripts/` - Generated markdown files with metadata and transcriptions

## Troubleshooting

**Virtual environment issues**: Run `uv sync` and ensure you're using `uv run` or have activated the venv

**Missing environment variables**: Load `.envrc` with `direnv allow` or `source .envrc`

**Whisper model**: First run downloads the "base" model (~140MB) - requires network access

**Database connection**: Services are on Aiven; verify credentials and network access

## Technology Stack

- Python 3.13+
- OpenAI Whisper (transcription)
- LangChain (text processing)
- PostgreSQL with pgvector extension
- SQLAlchemy (ORM)
- Click (CLI framework)
- FastAPI (REST API)
- MCP (Model Context Protocol)

## Usage and License

<p xmlns:cc="http://creativecommons.org/ns#" xmlns:dct="http://purl.org/dc/terms/"><a property="dct:title" rel="cc:attributionURL" href="https://github.com/kjaymiller/conduit-transcripts">Conduit Podcast Transcripts</a> by <a rel="cc:attributionURL dct:creator" property="cc:attributionName" href="https://relay.fm/conduit">Jay Miller, Kathy Campbell, original downloads from whisper work done by Pilix</a> is licensed under <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;">Attribution-NonCommercial-ShareAlike 4.0 International<img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1"><img style="height:22px!important;margin-left:3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p>
