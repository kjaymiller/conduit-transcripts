---
description: '<p>This episode on YouTube: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://youtu.be/dVIPBxHJ1kQ">https://youtu.be/dVIPBxHJ1kQ</a></p><p>00:00
  Intro</p><p>00:15 Greets for Sonam</p><p>01:02 Importance of metric learning</p><p>3:37
  Sonam''s background: Rasa, Qdrant</p><p>4:31 What''s EmbedAnything</p><p>5:52 What
  a user gets</p><p>8:48 Do I need to know Rust?</p><p>10:18 Call-out to the community</p><p>10:35
  Multimodality</p><p>12:32 How to evaluate quality of LLM-based systems</p><p>16:38
  QA for multimodal use cases</p><p>18:17 Place for a human in the LLM craze</p><p>19:00
  Use cases for EmbedAnything</p><p>20:54 Closing theme (a longer one - enjoy!)</p><p></p><p></p><p>Show
  notes:</p><p>- GitHub: <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/StarlightSearch/EmbedAnything">https://github.com/StarlightSearch/EmbedAnything</a></p><p>-
  HuggingFace Candle: <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/huggingface/candle">https://github.com/huggingface/candle</a></p><p>-
  Sonam''s talk on Berlin Buzzwords 2024: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=YfR3kuSo-XQ">https://www.youtube.com/watch?v=YfR3kuSo-XQ</a></p><p>-
  Removing GIL from Python: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://peps.python.org/pep-0703">https://peps.python.org/pep-0703</a></p><p>-
  Blind pairs in CLIP: <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2401.06209">https://arxiv.org/abs/2401.06209</a></p><p>-
  Dark matter of intelligence: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</a></p><p>-
  Rasa chatbots: <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/RasaHQ/rasa">https://github.com/RasaHQ/rasa</a></p><p>-
  Prometheus: <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/prometheus-eval/prometheus-eval">https://github.com/prometheus-eval/prometheus-eval</a></p><p>-
  Dino: <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/facebookresearch/dino">https://github.com/facebookresearch/dino</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20240919_060938_934c8351e1fe4c81a354cd419d0a3307.png
pub_date: Thu, 19 Sep 2024 11:02:40 GMT
title: Berlin Buzzwords 2024 - Sonam Pankaj - EmbedAnything
url: https://rss.com/podcasts/vector-podcast/1663042
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 21.44, "text": " Hello
  there, vector podcast and I''m here accompanied with Sonan. Sonan you are the, I
  guess,", "tokens": [50364, 2425, 456, 11, 8062, 7367, 293, 286, 478, 510, 24202,
  365, 5185, 282, 13, 5185, 282, 291, 366, 264, 11, 286, 2041, 11, 51436], "temperature":
  0.0, "avg_logprob": -0.36618917815539304, "compression_ratio": 1.335820895522388,
  "no_speech_prob": 0.13012650609016418}, {"id": 1, "seek": 0, "start": 21.44, "end":
  26.92, "text": " visitor of the conference. Are you also giving a talk? Yes, I''m
  giving a talk tomorrow", "tokens": [51436, 28222, 295, 264, 7586, 13, 2014, 291,
  611, 2902, 257, 751, 30, 1079, 11, 286, 478, 2902, 257, 751, 4153, 51710], "temperature":
  0.0, "avg_logprob": -0.36618917815539304, "compression_ratio": 1.335820895522388,
  "no_speech_prob": 0.13012650609016418}, {"id": 2, "seek": 2692, "start": 26.92,
  "end": 33.84, "text": " on metric learning. Yeah, what''s your topic? I''m not talking
  metric learning tomorrow, but I''m", "tokens": [50364, 322, 20678, 2539, 13, 865,
  11, 437, 311, 428, 4829, 30, 286, 478, 406, 1417, 20678, 2539, 4153, 11, 457, 286,
  478, 50710], "temperature": 0.0, "avg_logprob": -0.35272098541259767, "compression_ratio":
  1.6506550218340612, "no_speech_prob": 0.43288251757621765}, {"id": 3, "seek": 2692,
  "start": 33.84, "end": 39.68, "text": " very excited about what we are building
  at and better than anything on starlight. So yeah,", "tokens": [50710, 588, 2919,
  466, 437, 321, 366, 2390, 412, 293, 1101, 813, 1340, 322, 3543, 2764, 13, 407, 1338,
  11, 51002], "temperature": 0.0, "avg_logprob": -0.35272098541259767, "compression_ratio":
  1.6506550218340612, "no_speech_prob": 0.43288251757621765}, {"id": 4, "seek": 2692,
  "start": 39.68, "end": 46.160000000000004, "text": " awesome. And is it your first
  time at the conference? Yes, it''s the first time, but that''s one of", "tokens":
  [51002, 3476, 13, 400, 307, 309, 428, 700, 565, 412, 264, 7586, 30, 1079, 11, 309,
  311, 264, 700, 565, 11, 457, 300, 311, 472, 295, 51326], "temperature": 0.0, "avg_logprob":
  -0.35272098541259767, "compression_ratio": 1.6506550218340612, "no_speech_prob":
  0.43288251757621765}, {"id": 5, "seek": 2692, "start": 46.160000000000004, "end":
  51.96, "text": " the best conferences. Awesome. Yeah, I love it. I''ve been here
  first time in 2011 and I still,", "tokens": [51326, 264, 1151, 22032, 13, 10391,
  13, 865, 11, 286, 959, 309, 13, 286, 600, 668, 510, 700, 565, 294, 10154, 293, 286,
  920, 11, 51616], "temperature": 0.0, "avg_logprob": -0.35272098541259767, "compression_ratio":
  1.6506550218340612, "no_speech_prob": 0.43288251757621765}, {"id": 6, "seek": 5196,
  "start": 51.96, "end": 56.68, "text": " I still love coming back once in a while.
  It''s really good. I can see why you want to come back", "tokens": [50364, 286,
  920, 959, 1348, 646, 1564, 294, 257, 1339, 13, 467, 311, 534, 665, 13, 286, 393,
  536, 983, 291, 528, 281, 808, 646, 50600], "temperature": 0.0, "avg_logprob": -0.20331482660202754,
  "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.04114510864019394},
  {"id": 7, "seek": 5196, "start": 56.68, "end": 65.32, "text": " again and again.
  Yeah, exactly. Yeah. Awesome. And you work mostly on what I, well, we had an", "tokens":
  [50600, 797, 293, 797, 13, 865, 11, 2293, 13, 865, 13, 10391, 13, 400, 291, 589,
  5240, 322, 437, 286, 11, 731, 11, 321, 632, 364, 51032], "temperature": 0.0, "avg_logprob":
  -0.20331482660202754, "compression_ratio": 1.6244725738396624, "no_speech_prob":
  0.04114510864019394}, {"id": 8, "seek": 5196, "start": 65.32, "end": 72.36, "text":
  " episode actually with quadrants on metric learning. I will, I will make sure to
  link it. Tell me a", "tokens": [51032, 3500, 767, 365, 10787, 10968, 322, 20678,
  2539, 13, 286, 486, 11, 286, 486, 652, 988, 281, 2113, 309, 13, 5115, 385, 257,
  51384], "temperature": 0.0, "avg_logprob": -0.20331482660202754, "compression_ratio":
  1.6244725738396624, "no_speech_prob": 0.04114510864019394}, {"id": 9, "seek": 5196,
  "start": 72.36, "end": 77.72, "text": " bit more about metric learning if you will.
  Like in a, in a, why shouldn''t everyone care that he", "tokens": [51384, 857, 544,
  466, 20678, 2539, 498, 291, 486, 13, 1743, 294, 257, 11, 294, 257, 11, 983, 4659,
  380, 1518, 1127, 300, 415, 51652], "temperature": 0.0, "avg_logprob": -0.20331482660202754,
  "compression_ratio": 1.6244725738396624, "no_speech_prob": 0.04114510864019394},
  {"id": 10, "seek": 7772, "start": 77.72, "end": 83.88, "text": " seems to think
  that they should use maybe yes. So a lot of people just think about like, you know,",
  "tokens": [50364, 2544, 281, 519, 300, 436, 820, 764, 1310, 2086, 13, 407, 257,
  688, 295, 561, 445, 519, 466, 411, 11, 291, 458, 11, 50672], "temperature": 0.0,
  "avg_logprob": -0.25106561183929443, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.05657637491822243}, {"id": 11, "seek": 7772, "start": 83.88, "end": 89.8, "text":
  " we can do a check distance and then you know, we''ll get the similarity. But the
  thing is,", "tokens": [50672, 321, 393, 360, 257, 1520, 4560, 293, 550, 291, 458,
  11, 321, 603, 483, 264, 32194, 13, 583, 264, 551, 307, 11, 50968], "temperature":
  0.0, "avg_logprob": -0.25106561183929443, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.05657637491822243}, {"id": 12, "seek": 7772, "start": 89.8,
  "end": 96.2, "text": " even though you change the distance, it won''t make any difference
  because those embeddings are", "tokens": [50968, 754, 1673, 291, 1319, 264, 4560,
  11, 309, 1582, 380, 652, 604, 2649, 570, 729, 12240, 29432, 366, 51288], "temperature":
  0.0, "avg_logprob": -0.25106561183929443, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.05657637491822243}, {"id": 13, "seek": 7772, "start": 96.2,
  "end": 102.6, "text": " already in the space. So it''s already relative. So if you''re
  doing a co-science similarity,", "tokens": [51288, 1217, 294, 264, 1901, 13, 407,
  309, 311, 1217, 4972, 13, 407, 498, 291, 434, 884, 257, 598, 12, 82, 6699, 32194,
  11, 51608], "temperature": 0.0, "avg_logprob": -0.25106561183929443, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.05657637491822243}, {"id": 14, "seek": 10260,
  "start": 102.67999999999999, "end": 108.91999999999999, "text": " which I love pizza
  and I do not love pizza, that''s your 90% similarity. Right? And", "tokens": [50368,
  597, 286, 959, 8298, 293, 286, 360, 406, 959, 8298, 11, 300, 311, 428, 4289, 4,
  32194, 13, 1779, 30, 400, 50680], "temperature": 0.0, "avg_logprob": -0.31419731398760264,
  "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.026558207347989082},
  {"id": 15, "seek": 10260, "start": 108.91999999999999, "end": 114.75999999999999,
  "text": " to the other distance will not make any sense. So the thing is with metric
  learning, you can", "tokens": [50680, 281, 264, 661, 4560, 486, 406, 652, 604, 2020,
  13, 407, 264, 551, 307, 365, 20678, 2539, 11, 291, 393, 50972], "temperature": 0.0,
  "avg_logprob": -0.31419731398760264, "compression_ratio": 1.6545454545454545, "no_speech_prob":
  0.026558207347989082}, {"id": 16, "seek": 10260, "start": 114.75999999999999, "end":
  120.6, "text": " build your own data set and then the train there when embedding
  model again for giving you", "tokens": [50972, 1322, 428, 1065, 1412, 992, 293,
  550, 264, 3847, 456, 562, 12240, 3584, 2316, 797, 337, 2902, 291, 51264], "temperature":
  0.0, "avg_logprob": -0.31419731398760264, "compression_ratio": 1.6545454545454545,
  "no_speech_prob": 0.026558207347989082}, {"id": 17, "seek": 10260, "start": 120.6,
  "end": 127.08, "text": " right. Yeah. I mean, I still try to understand it, but
  it''s basically like, like on one hand,", "tokens": [51264, 558, 13, 865, 13, 286,
  914, 11, 286, 920, 853, 281, 1223, 309, 11, 457, 309, 311, 1936, 411, 11, 411, 322,
  472, 1011, 11, 51588], "temperature": 0.0, "avg_logprob": -0.31419731398760264,
  "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.026558207347989082},
  {"id": 18, "seek": 10260, "start": 127.08, "end": 131.56, "text": " you have your
  data and then you choose the model and that model should be pre-trained for you,",
  "tokens": [51588, 291, 362, 428, 1412, 293, 550, 291, 2826, 264, 2316, 293, 300,
  2316, 820, 312, 659, 12, 17227, 2001, 337, 291, 11, 51812], "temperature": 0.0,
  "avg_logprob": -0.31419731398760264, "compression_ratio": 1.6545454545454545, "no_speech_prob":
  0.026558207347989082}, {"id": 19, "seek": 13156, "start": 132.04, "end": 137.96,
  "text": " you could also fine tune it on your data if you want. And then inherently,
  it will have its own", "tokens": [50388, 291, 727, 611, 2489, 10864, 309, 322, 428,
  1412, 498, 291, 528, 13, 400, 550, 27993, 11, 309, 486, 362, 1080, 1065, 50684],
  "temperature": 0.0, "avg_logprob": -0.157344298892551, "compression_ratio": 1.7293577981651376,
  "no_speech_prob": 0.0022104955278337}, {"id": 20, "seek": 13156, "start": 139.16,
  "end": 145.0, "text": " measure of similarity. So it''s not something you can easily
  control. Yeah. But then metric learning", "tokens": [50744, 3481, 295, 32194, 13,
  407, 309, 311, 406, 746, 291, 393, 3612, 1969, 13, 865, 13, 583, 550, 20678, 2539,
  51036], "temperature": 0.0, "avg_logprob": -0.157344298892551, "compression_ratio":
  1.7293577981651376, "no_speech_prob": 0.0022104955278337}, {"id": 21, "seek": 13156,
  "start": 145.0, "end": 150.04, "text": " opposes this by saying that you should
  be in control of your metric. Yeah. It''s all your", "tokens": [51036, 1458, 4201,
  341, 538, 1566, 300, 291, 820, 312, 294, 1969, 295, 428, 20678, 13, 865, 13, 467,
  311, 439, 428, 51288], "temperature": 0.0, "avg_logprob": -0.157344298892551, "compression_ratio":
  1.7293577981651376, "no_speech_prob": 0.0022104955278337}, {"id": 22, "seek": 13156,
  "start": 150.04, "end": 155.32, "text": " similarity measure, not just the metric
  itself, but the similarity measure, which means that", "tokens": [51288, 32194,
  3481, 11, 406, 445, 264, 20678, 2564, 11, 457, 264, 32194, 3481, 11, 597, 1355,
  300, 51552], "temperature": 0.0, "avg_logprob": -0.157344298892551, "compression_ratio":
  1.7293577981651376, "no_speech_prob": 0.0022104955278337}, {"id": 23, "seek": 15532,
  "start": 155.88, "end": 162.28, "text": " I should kind of like drop the model,
  just get my data and start training some new network,", "tokens": [50392, 286, 820,
  733, 295, 411, 3270, 264, 2316, 11, 445, 483, 452, 1412, 293, 722, 3097, 512, 777,
  3209, 11, 50712], "temperature": 0.0, "avg_logprob": -0.24773545100771147, "compression_ratio":
  1.5829787234042554, "no_speech_prob": 0.007064012344926596}, {"id": 24, "seek":
  15532, "start": 162.28, "end": 169.4, "text": " right? So that I can find the basically
  fine tuning the embedding model. What with your data?", "tokens": [50712, 558, 30,
  407, 300, 286, 393, 915, 264, 1936, 2489, 15164, 264, 12240, 3584, 2316, 13, 708,
  365, 428, 1412, 30, 51068], "temperature": 0.0, "avg_logprob": -0.24773545100771147,
  "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.007064012344926596},
  {"id": 25, "seek": 15532, "start": 169.4, "end": 176.12, "text": " So yeah, suppose
  you''re finding intense. Yes. Okay. Where does metric learning really shine?", "tokens":
  [51068, 407, 1338, 11, 7297, 291, 434, 5006, 9447, 13, 1079, 13, 1033, 13, 2305,
  775, 20678, 2539, 534, 12207, 30, 51404], "temperature": 0.0, "avg_logprob": -0.24773545100771147,
  "compression_ratio": 1.5829787234042554, "no_speech_prob": 0.007064012344926596},
  {"id": 26, "seek": 15532, "start": 177.07999999999998, "end": 183.07999999999998,
  "text": " It''s classification versus similarity again. If you are doing classification,
  you are limited", "tokens": [51452, 467, 311, 21538, 5717, 32194, 797, 13, 759,
  291, 366, 884, 21538, 11, 291, 366, 5567, 51752], "temperature": 0.0, "avg_logprob":
  -0.24773545100771147, "compression_ratio": 1.5829787234042554, "no_speech_prob":
  0.007064012344926596}, {"id": 27, "seek": 18308, "start": 183.08, "end": 190.28,
  "text": " up to certain classes, right? Suppose, yeah, particular intense. Yeah.
  It''s not scalable at", "tokens": [50364, 493, 281, 1629, 5359, 11, 558, 30, 21360,
  11, 1338, 11, 1729, 9447, 13, 865, 13, 467, 311, 406, 38481, 412, 50724], "temperature":
  0.0, "avg_logprob": -0.30882939425381745, "compression_ratio": 1.6952789699570816,
  "no_speech_prob": 0.005887746810913086}, {"id": 28, "seek": 18308, "start": 190.28,
  "end": 197.0, "text": " like a million scale, you cannot keep adding adding addicts,
  but with similarity search and metric", "tokens": [50724, 411, 257, 2459, 4373,
  11, 291, 2644, 1066, 5127, 5127, 22072, 82, 11, 457, 365, 32194, 3164, 293, 20678,
  51060], "temperature": 0.0, "avg_logprob": -0.30882939425381745, "compression_ratio":
  1.6952789699570816, "no_speech_prob": 0.005887746810913086}, {"id": 29, "seek":
  18308, "start": 197.0, "end": 203.16000000000003, "text": " learning, you can add
  any intense, very keen solution. Yeah. Yeah. So it''s not limited. Yeah. That''s
  one", "tokens": [51060, 2539, 11, 291, 393, 909, 604, 9447, 11, 588, 20297, 3827,
  13, 865, 13, 865, 13, 407, 309, 311, 406, 5567, 13, 865, 13, 663, 311, 472, 51368],
  "temperature": 0.0, "avg_logprob": -0.30882939425381745, "compression_ratio": 1.6952789699570816,
  "no_speech_prob": 0.005887746810913086}, {"id": 30, "seek": 18308, "start": 203.16000000000003,
  "end": 211.0, "text": " of the, you know, classical way to view that metric learning
  plays much, much better role at scale,", "tokens": [51368, 295, 264, 11, 291, 458,
  11, 13735, 636, 281, 1910, 300, 20678, 2539, 5749, 709, 11, 709, 1101, 3090, 412,
  4373, 11, 51760], "temperature": 0.0, "avg_logprob": -0.30882939425381745, "compression_ratio":
  1.6952789699570816, "no_speech_prob": 0.005887746810913086}, {"id": 31, "seek":
  21100, "start": 211.0, "end": 219.32, "text": " and that''s why vector database
  can scale this much. Sure. Yeah. And tell me a bit more about yourself.", "tokens":
  [50364, 293, 300, 311, 983, 8062, 8149, 393, 4373, 341, 709, 13, 4894, 13, 865,
  13, 400, 980, 385, 257, 857, 544, 466, 1803, 13, 50780], "temperature": 0.0, "avg_logprob":
  -0.22737432207380023, "compression_ratio": 1.5916666666666666, "no_speech_prob":
  0.005757349543273449}, {"id": 32, "seek": 21100, "start": 219.32, "end": 223.88,
  "text": " How did you end up in this space? Like, what was your pet? I know you
  worked at Thrasa as well,", "tokens": [50780, 1012, 630, 291, 917, 493, 294, 341,
  1901, 30, 1743, 11, 437, 390, 428, 3817, 30, 286, 458, 291, 2732, 412, 334, 3906,
  64, 382, 731, 11, 51008], "temperature": 0.0, "avg_logprob": -0.22737432207380023,
  "compression_ratio": 1.5916666666666666, "no_speech_prob": 0.005757349543273449},
  {"id": 33, "seek": 21100, "start": 223.88, "end": 229.56, "text": " which is also
  an open source project. Yes. I once looked at and but now you work for another",
  "tokens": [51008, 597, 307, 611, 364, 1269, 4009, 1716, 13, 1079, 13, 286, 1564,
  2956, 412, 293, 457, 586, 291, 589, 337, 1071, 51292], "temperature": 0.0, "avg_logprob":
  -0.22737432207380023, "compression_ratio": 1.5916666666666666, "no_speech_prob":
  0.005757349543273449}, {"id": 34, "seek": 21100, "start": 229.56, "end": 238.04,
  "text": " company like, what was your journey? And yeah. So I worked at as an AI
  researcher at Sama,", "tokens": [51292, 2237, 411, 11, 437, 390, 428, 4671, 30,
  400, 1338, 13, 407, 286, 2732, 412, 382, 364, 7318, 21751, 412, 318, 2404, 11, 51716],
  "temperature": 0.0, "avg_logprob": -0.22737432207380023, "compression_ratio": 1.5916666666666666,
  "no_speech_prob": 0.005757349543273449}, {"id": 35, "seek": 23804, "start": 238.12,
  "end": 243.16, "text": " so we were mostly in clinical trials. So, you know, Pfizer
  and the world is,", "tokens": [50368, 370, 321, 645, 5240, 294, 9115, 12450, 13,
  407, 11, 291, 458, 11, 34694, 293, 264, 1002, 307, 11, 50620], "temperature": 0.0,
  "avg_logprob": -0.29561863774838654, "compression_ratio": 1.521186440677966, "no_speech_prob":
  0.055215124040842056}, {"id": 36, "seek": 23804, "start": 243.16, "end": 249.32,
  "text": " it does this clinical trials for 10 to 12 years and we had like those
  massive data and we wanted", "tokens": [50620, 309, 775, 341, 9115, 12450, 337,
  1266, 281, 2272, 924, 293, 321, 632, 411, 729, 5994, 1412, 293, 321, 1415, 50928],
  "temperature": 0.0, "avg_logprob": -0.29561863774838654, "compression_ratio": 1.521186440677966,
  "no_speech_prob": 0.055215124040842056}, {"id": 37, "seek": 23804, "start": 249.32,
  "end": 255.0, "text": " to find out the subjects could drop out of the studies.
  I also published paper before. That''s", "tokens": [50928, 281, 915, 484, 264, 13066,
  727, 3270, 484, 295, 264, 5313, 13, 286, 611, 6572, 3035, 949, 13, 663, 311, 51212],
  "temperature": 0.0, "avg_logprob": -0.29561863774838654, "compression_ratio": 1.521186440677966,
  "no_speech_prob": 0.055215124040842056}, {"id": 38, "seek": 23804, "start": 255.0,
  "end": 262.12, "text": " well-versed in this AI research and AI area. Yeah. And
  then I joined Raza for conversation,", "tokens": [51212, 731, 12, 840, 292, 294,
  341, 7318, 2132, 293, 7318, 1859, 13, 865, 13, 400, 550, 286, 6869, 497, 12257,
  337, 3761, 11, 51568], "temperature": 0.0, "avg_logprob": -0.29561863774838654,
  "compression_ratio": 1.521186440677966, "no_speech_prob": 0.055215124040842056},
  {"id": 39, "seek": 26212, "start": 262.12, "end": 270.76, "text": " AI, I love conversation,
  AI. And then I joined four friends recently and I got into this embedding", "tokens":
  [50364, 7318, 11, 286, 959, 3761, 11, 7318, 13, 400, 550, 286, 6869, 1451, 1855,
  3938, 293, 286, 658, 666, 341, 12240, 3584, 50796], "temperature": 0.0, "avg_logprob":
  -0.36772735804727635, "compression_ratio": 1.5077720207253886, "no_speech_prob":
  0.02951665408909321}, {"id": 40, "seek": 26212, "start": 270.76, "end": 276.52,
  "text": " space. And now I have my own open source project called embedding a thing
  in which you can use", "tokens": [50796, 1901, 13, 400, 586, 286, 362, 452, 1065,
  1269, 4009, 1716, 1219, 12240, 3584, 257, 551, 294, 597, 291, 393, 764, 51084],
  "temperature": 0.0, "avg_logprob": -0.36772735804727635, "compression_ratio": 1.5077720207253886,
  "no_speech_prob": 0.02951665408909321}, {"id": 41, "seek": 26212, "start": 276.52,
  "end": 283.8, "text": " very different multi-moder sources and structure sources,
  speed, you know, you get embed it in 40", "tokens": [51084, 588, 819, 4825, 12,
  8014, 260, 7139, 293, 3877, 7139, 11, 3073, 11, 291, 458, 11, 291, 483, 12240, 309,
  294, 3356, 51448], "temperature": 0.0, "avg_logprob": -0.36772735804727635, "compression_ratio":
  1.5077720207253886, "no_speech_prob": 0.02951665408909321}, {"id": 42, "seek": 28380,
  "start": 283.8, "end": 290.92, "text": " x faster speed than any other presence
  by planes. Wow. How did you do that? That is rust.", "tokens": [50364, 2031, 4663,
  3073, 813, 604, 661, 6814, 538, 14952, 13, 3153, 13, 1012, 630, 291, 360, 300, 30,
  663, 307, 15259, 13, 50720], "temperature": 0.0, "avg_logprob": -0.3309457334753585,
  "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.02470666728913784},
  {"id": 43, "seek": 28380, "start": 291.72, "end": 297.32, "text": " It''s all available.
  It''s all open source because I have like a used supporter of open source.", "tokens":
  [50760, 467, 311, 439, 2435, 13, 467, 311, 439, 1269, 4009, 570, 286, 362, 411,
  257, 1143, 28600, 295, 1269, 4009, 13, 51040], "temperature": 0.0, "avg_logprob":
  -0.3309457334753585, "compression_ratio": 1.4789473684210526, "no_speech_prob":
  0.02470666728913784}, {"id": 44, "seek": 28380, "start": 298.84000000000003, "end":
  306.84000000000003, "text": " So what we do is we have built this cluster in rust
  from PDF while it is going towards embedding.", "tokens": [51116, 407, 437, 321,
  360, 307, 321, 362, 3094, 341, 13630, 294, 15259, 490, 17752, 1339, 309, 307, 516,
  3030, 12240, 3584, 13, 51516], "temperature": 0.0, "avg_logprob": -0.3309457334753585,
  "compression_ratio": 1.4789473684210526, "no_speech_prob": 0.02470666728913784},
  {"id": 45, "seek": 30684, "start": 306.84, "end": 314.28, "text": " So one of the
  analogy that I use most is embedding models are, yeah, they are like really,", "tokens":
  [50364, 407, 472, 295, 264, 21663, 300, 286, 764, 881, 307, 12240, 3584, 5245, 366,
  11, 1338, 11, 436, 366, 411, 534, 11, 50736], "temperature": 0.0, "avg_logprob":
  -0.3068643935183261, "compression_ratio": 1.6473214285714286, "no_speech_prob":
  0.014807434752583504}, {"id": 46, "seek": 30684, "start": 314.28, "end": 320.59999999999997,
  "text": " really cool. They are becoming faster and everything. But if you want
  to drive a Porsche,", "tokens": [50736, 534, 1627, 13, 814, 366, 5617, 4663, 293,
  1203, 13, 583, 498, 291, 528, 281, 3332, 257, 31044, 11, 51052], "temperature":
  0.0, "avg_logprob": -0.3068643935183261, "compression_ratio": 1.6473214285714286,
  "no_speech_prob": 0.014807434752583504}, {"id": 47, "seek": 30684, "start": 320.59999999999997,
  "end": 327.71999999999997, "text": " would you like to drive it on a national highway
  for a road full of quadruples? So that''s the", "tokens": [51052, 576, 291, 411,
  281, 3332, 309, 322, 257, 4048, 17205, 337, 257, 3060, 1577, 295, 10787, 894, 2622,
  30, 407, 300, 311, 264, 51408], "temperature": 0.0, "avg_logprob": -0.3068643935183261,
  "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.014807434752583504},
  {"id": 48, "seek": 30684, "start": 327.71999999999997, "end": 334.28, "text": "
  analogy being used. We are giving you a high for the price for driving your embedding
  model or", "tokens": [51408, 21663, 885, 1143, 13, 492, 366, 2902, 291, 257, 1090,
  337, 264, 3218, 337, 4840, 428, 12240, 3584, 2316, 420, 51736], "temperature": 0.0,
  "avg_logprob": -0.3068643935183261, "compression_ratio": 1.6473214285714286, "no_speech_prob":
  0.014807434752583504}, {"id": 49, "seek": 33428, "start": 334.28, "end": 341.55999999999995,
  "text": " Porsche, you know, in a very sophisticated and like, yeah, no tech depth,
  you call it by", "tokens": [50364, 31044, 11, 291, 458, 11, 294, 257, 588, 16950,
  293, 411, 11, 1338, 11, 572, 7553, 7161, 11, 291, 818, 309, 538, 50728], "temperature":
  0.0, "avg_logprob": -0.3562089134665096, "compression_ratio": 1.6105769230769231,
  "no_speech_prob": 0.0064294240437448025}, {"id": 50, "seek": 33428, "start": 341.55999999999995,
  "end": 347.88, "text": " blind for embedding. Interesting. So you are basically
  building an infrastructure", "tokens": [50728, 6865, 337, 12240, 3584, 13, 14711,
  13, 407, 291, 366, 1936, 2390, 364, 6896, 51044], "temperature": 0.0, "avg_logprob":
  -0.3562089134665096, "compression_ratio": 1.6105769230769231, "no_speech_prob":
  0.0064294240437448025}, {"id": 51, "seek": 33428, "start": 348.67999999999995, "end":
  354.59999999999997, "text": " where or infrastructure for this is a very model.
  So what as a user, what can I do on this", "tokens": [51084, 689, 420, 6896, 337,
  341, 307, 257, 588, 2316, 13, 407, 437, 382, 257, 4195, 11, 437, 393, 286, 360,
  322, 341, 51380], "temperature": 0.0, "avg_logprob": -0.3562089134665096, "compression_ratio":
  1.6105769230769231, "no_speech_prob": 0.0064294240437448025}, {"id": 52, "seek":
  33428, "start": 355.64, "end": 361.08, "text": " project? Yeah, very good question.
  So we are very production ready. Yeah.", "tokens": [51432, 1716, 30, 865, 11, 588,
  665, 1168, 13, 407, 321, 366, 588, 4265, 1919, 13, 865, 13, 51704], "temperature":
  0.0, "avg_logprob": -0.3562089134665096, "compression_ratio": 1.6105769230769231,
  "no_speech_prob": 0.0064294240437448025}, {"id": 53, "seek": 36108, "start": 362.03999999999996,
  "end": 368.12, "text": " And we do not use any kind of heavy library, right? They
  are lip torches. So if you have to embed", "tokens": [50412, 400, 321, 360, 406,
  764, 604, 733, 295, 4676, 6405, 11, 558, 30, 814, 366, 8280, 3930, 3781, 13, 407,
  498, 291, 362, 281, 12240, 50716], "temperature": 0.0, "avg_logprob": -0.40969002765157947,
  "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.201416477560997}, {"id":
  54, "seek": 36108, "start": 368.12, "end": 373.88, "text": " something, the first
  go on hugging phase, use sentence, transformers, and then you will download", "tokens":
  [50716, 746, 11, 264, 700, 352, 322, 41706, 5574, 11, 764, 8174, 11, 4088, 433,
  11, 293, 550, 291, 486, 5484, 51004], "temperature": 0.0, "avg_logprob": -0.40969002765157947,
  "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.201416477560997}, {"id":
  55, "seek": 36108, "start": 373.88, "end": 379.47999999999996, "text": " that 2.5
  TV library and stuff like which will come with lip torches and stuff like that.",
  "tokens": [51004, 300, 568, 13, 20, 3558, 6405, 293, 1507, 411, 597, 486, 808, 365,
  8280, 3930, 3781, 293, 1507, 411, 300, 13, 51284], "temperature": 0.0, "avg_logprob":
  -0.40969002765157947, "compression_ratio": 1.6590038314176245, "no_speech_prob":
  0.201416477560997}, {"id": 56, "seek": 36108, "start": 379.47999999999996, "end":
  381.96, "text": " Yeah. And we have removed all those dependents. All right.", "tokens":
  [51284, 865, 13, 400, 321, 362, 7261, 439, 729, 5672, 791, 13, 1057, 558, 13, 51408],
  "temperature": 0.0, "avg_logprob": -0.40969002765157947, "compression_ratio": 1.6590038314176245,
  "no_speech_prob": 0.201416477560997}, {"id": 57, "seek": 36108, "start": 383.0,
  "end": 388.44, "text": " That''s a good lighter. Yeah. We have liked it. Yeah. But
  of candle from the hugging phase,", "tokens": [51460, 663, 311, 257, 665, 11546,
  13, 865, 13, 492, 362, 4501, 309, 13, 865, 13, 583, 295, 17968, 490, 264, 41706,
  5574, 11, 51732], "temperature": 0.0, "avg_logprob": -0.40969002765157947, "compression_ratio":
  1.6590038314176245, "no_speech_prob": 0.201416477560997}, {"id": 58, "seek": 38844,
  "start": 388.44, "end": 393.88, "text": " because candle also uses rust and because
  we are also building rust, it''s much easier to integrate", "tokens": [50364, 570,
  17968, 611, 4960, 15259, 293, 570, 321, 366, 611, 2390, 15259, 11, 309, 311, 709,
  3571, 281, 13365, 50636], "temperature": 0.0, "avg_logprob": -0.2641873449649451,
  "compression_ratio": 1.6625, "no_speech_prob": 0.021137645468115807}, {"id": 59,
  "seek": 38844, "start": 393.88, "end": 400.36, "text": " with candle. So yeah, so
  it''s much lighter, much faster, you know, way of creating. What is candle?", "tokens":
  [50636, 365, 17968, 13, 407, 1338, 11, 370, 309, 311, 709, 11546, 11, 709, 4663,
  11, 291, 458, 11, 636, 295, 4084, 13, 708, 307, 17968, 30, 50960], "temperature":
  0.0, "avg_logprob": -0.2641873449649451, "compression_ratio": 1.6625, "no_speech_prob":
  0.021137645468115807}, {"id": 60, "seek": 38844, "start": 401.0, "end": 406.68,
  "text": " Candle is basically, basically, inference on GPU and CPU. Oh, I see. Yeah.
  Yeah. And it''s also open source.", "tokens": [50992, 20466, 306, 307, 1936, 11,
  1936, 11, 38253, 322, 18407, 293, 13199, 13, 876, 11, 286, 536, 13, 865, 13, 865,
  13, 400, 309, 311, 611, 1269, 4009, 13, 51276], "temperature": 0.0, "avg_logprob":
  -0.2641873449649451, "compression_ratio": 1.6625, "no_speech_prob": 0.021137645468115807},
  {"id": 61, "seek": 38844, "start": 406.68, "end": 412.2, "text": " Yeah, it''s also.
  Okay. So you do everything unconventionally in a rust, even though everyone", "tokens":
  [51276, 865, 11, 309, 311, 611, 13, 1033, 13, 407, 291, 360, 1203, 35847, 6411,
  379, 294, 257, 15259, 11, 754, 1673, 1518, 51552], "temperature": 0.0, "avg_logprob":
  -0.2641873449649451, "compression_ratio": 1.6625, "no_speech_prob": 0.021137645468115807},
  {"id": 62, "seek": 41220, "start": 412.2, "end": 419.96, "text": " else is doing
  it in Python. Because it''s, you know, multi-treting is like so much embedded in
  rust.", "tokens": [50364, 1646, 307, 884, 309, 294, 15329, 13, 1436, 309, 311, 11,
  291, 458, 11, 4825, 12, 3599, 783, 307, 411, 370, 709, 16741, 294, 15259, 13, 50752],
  "temperature": 0.0, "avg_logprob": -0.427581103342884, "compression_ratio": 1.738938053097345,
  "no_speech_prob": 0.03050844930112362}, {"id": 63, "seek": 41220, "start": 419.96,
  "end": 425.71999999999997, "text": " Like people will tell you that Python can also
  do multi-treting, but that''s not too multi-treting", "tokens": [50752, 1743, 561,
  486, 980, 291, 300, 15329, 393, 611, 360, 4825, 12, 3599, 783, 11, 457, 300, 311,
  406, 886, 4825, 12, 3599, 783, 51040], "temperature": 0.0, "avg_logprob": -0.427581103342884,
  "compression_ratio": 1.738938053097345, "no_speech_prob": 0.03050844930112362},
  {"id": 64, "seek": 41220, "start": 425.71999999999997, "end": 433.96, "text": "
  because the global, global, and global, and global law. Yeah. And rust tells you
  mutable log.", "tokens": [51040, 570, 264, 4338, 11, 4338, 11, 293, 4338, 11, 293,
  4338, 2101, 13, 865, 13, 400, 15259, 5112, 291, 5839, 712, 3565, 13, 51452], "temperature":
  0.0, "avg_logprob": -0.427581103342884, "compression_ratio": 1.738938053097345,
  "no_speech_prob": 0.03050844930112362}, {"id": 65, "seek": 41220, "start": 433.96,
  "end": 439.8, "text": " So you can do like achieve a tool multi-treting just with
  rust. Yeah. They promised actually to solve", "tokens": [51452, 407, 291, 393, 360,
  411, 4584, 257, 2290, 4825, 12, 3599, 783, 445, 365, 15259, 13, 865, 13, 814, 10768,
  767, 281, 5039, 51744], "temperature": 0.0, "avg_logprob": -0.427581103342884, "compression_ratio":
  1.738938053097345, "no_speech_prob": 0.03050844930112362}, {"id": 66, "seek": 43980,
  "start": 439.8, "end": 445.88, "text": " geolproblem in Python next version. Yeah.
  They already are through them. Oh, wow. I don''t know when", "tokens": [50364, 1519,
  401, 4318, 1113, 294, 15329, 958, 3037, 13, 865, 13, 814, 1217, 366, 807, 552, 13,
  876, 11, 6076, 13, 286, 500, 380, 458, 562, 50668], "temperature": 0.0, "avg_logprob":
  -0.27065517666103606, "compression_ratio": 1.5305343511450382, "no_speech_prob":
  0.007175566628575325}, {"id": 67, "seek": 43980, "start": 445.88, "end": 451.56,
  "text": " it will materialize, but. Okay. And so, okay. But if I look at it from
  the perspective, let''s say,", "tokens": [50668, 309, 486, 2527, 1125, 11, 457,
  13, 1033, 13, 400, 370, 11, 1392, 13, 583, 498, 286, 574, 412, 309, 490, 264, 4585,
  11, 718, 311, 584, 11, 50952], "temperature": 0.0, "avg_logprob": -0.27065517666103606,
  "compression_ratio": 1.5305343511450382, "no_speech_prob": 0.007175566628575325},
  {"id": 68, "seek": 43980, "start": 451.56, "end": 459.08000000000004, "text": "
  of building some product, being a chatbot or like search engine, you know, blend
  it with vector search,", "tokens": [50952, 295, 2390, 512, 1674, 11, 885, 257, 5081,
  18870, 420, 411, 3164, 2848, 11, 291, 458, 11, 10628, 309, 365, 8062, 3164, 11,
  51328], "temperature": 0.0, "avg_logprob": -0.27065517666103606, "compression_ratio":
  1.5305343511450382, "no_speech_prob": 0.007175566628575325}, {"id": 69, "seek":
  43980, "start": 459.08000000000004, "end": 466.68, "text": " or something like that.
  So what is my typical sort of like, like, pipeline, how does it look like?", "tokens":
  [51328, 420, 746, 411, 300, 13, 407, 437, 307, 452, 7476, 1333, 295, 411, 11, 411,
  11, 15517, 11, 577, 775, 309, 574, 411, 30, 51708], "temperature": 0.0, "avg_logprob":
  -0.27065517666103606, "compression_ratio": 1.5305343511450382, "no_speech_prob":
  0.007175566628575325}, {"id": 70, "seek": 46668, "start": 466.68, "end": 471.48,
  "text": " Right. So what will I do? Let''s say I have my data. And then maybe I''ve
  chosen a model,", "tokens": [50364, 1779, 13, 407, 437, 486, 286, 360, 30, 961,
  311, 584, 286, 362, 452, 1412, 13, 400, 550, 1310, 286, 600, 8614, 257, 2316, 11,
  50604], "temperature": 0.0, "avg_logprob": -0.1421315532085324, "compression_ratio":
  1.725868725868726, "no_speech_prob": 0.043925996869802475}, {"id": 71, "seek": 46668,
  "start": 471.48, "end": 478.12, "text": " but that model is okay. Maybe it''s not
  the fastest one. What should I do? Will I turn to your platform", "tokens": [50604,
  457, 300, 2316, 307, 1392, 13, 2704, 309, 311, 406, 264, 14573, 472, 13, 708, 820,
  286, 360, 30, 3099, 286, 1261, 281, 428, 3663, 50936], "temperature": 0.0, "avg_logprob":
  -0.1421315532085324, "compression_ratio": 1.725868725868726, "no_speech_prob": 0.043925996869802475},
  {"id": 72, "seek": 46668, "start": 478.12, "end": 482.12, "text": " to speed it
  up? Will I turn to your platform to do some other things as well?", "tokens": [50936,
  281, 3073, 309, 493, 30, 3099, 286, 1261, 281, 428, 3663, 281, 360, 512, 661, 721,
  382, 731, 30, 51136], "temperature": 0.0, "avg_logprob": -0.1421315532085324, "compression_ratio":
  1.725868725868726, "no_speech_prob": 0.043925996869802475}, {"id": 73, "seek": 46668,
  "start": 483.48, "end": 490.04, "text": " So we are not doing any changes in the
  model itself. We are not quantizing. Even though we can", "tokens": [51204, 407,
  321, 366, 406, 884, 604, 2962, 294, 264, 2316, 2564, 13, 492, 366, 406, 4426, 3319,
  13, 2754, 1673, 321, 393, 51532], "temperature": 0.0, "avg_logprob": -0.1421315532085324,
  "compression_ratio": 1.725868725868726, "no_speech_prob": 0.043925996869802475},
  {"id": 74, "seek": 46668, "start": 490.04, "end": 495.88, "text": " use those models,
  so candle gives you a certain list of models that you can use and", "tokens": [51532,
  764, 729, 5245, 11, 370, 17968, 2709, 291, 257, 1629, 1329, 295, 5245, 300, 291,
  393, 764, 293, 51824], "temperature": 0.0, "avg_logprob": -0.1421315532085324, "compression_ratio":
  1.725868725868726, "no_speech_prob": 0.043925996869802475}, {"id": 75, "seek": 49588,
  "start": 495.88, "end": 501.8, "text": " create with us. Yeah. Basically. So whatever
  candle supports, we support. Yeah. Whatever candle", "tokens": [50364, 1884, 365,
  505, 13, 865, 13, 8537, 13, 407, 2035, 17968, 9346, 11, 321, 1406, 13, 865, 13,
  8541, 17968, 50660], "temperature": 0.0, "avg_logprob": -0.2131784439086914, "compression_ratio":
  1.7393364928909953, "no_speech_prob": 0.009811174124479294}, {"id": 76, "seek":
  49588, "start": 501.8, "end": 505.88, "text": " doesn''t support, we cannot support
  because we are basically dependent of them. Yeah.", "tokens": [50660, 1177, 380,
  1406, 11, 321, 2644, 1406, 570, 321, 366, 1936, 12334, 295, 552, 13, 865, 13, 50864],
  "temperature": 0.0, "avg_logprob": -0.2131784439086914, "compression_ratio": 1.7393364928909953,
  "no_speech_prob": 0.009811174124479294}, {"id": 77, "seek": 49588, "start": 506.92,
  "end": 514.28, "text": " So if and we are not doing anything in the model itself,
  we are doing it on the extraction", "tokens": [50916, 407, 498, 293, 321, 366, 406,
  884, 1340, 294, 264, 2316, 2564, 11, 321, 366, 884, 309, 322, 264, 30197, 51284],
  "temperature": 0.0, "avg_logprob": -0.2131784439086914, "compression_ratio": 1.7393364928909953,
  "no_speech_prob": 0.009811174124479294}, {"id": 78, "seek": 49588, "start": 514.28,
  "end": 520.44, "text": " in parsing part of the data. Right. If you have different
  videos, different MDs, I will extract", "tokens": [51284, 294, 21156, 278, 644,
  295, 264, 1412, 13, 1779, 13, 759, 291, 362, 819, 2145, 11, 819, 22521, 82, 11,
  286, 486, 8947, 51592], "temperature": 0.0, "avg_logprob": -0.2131784439086914,
  "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.009811174124479294},
  {"id": 79, "seek": 52044, "start": 520.44, "end": 526.6800000000001, "text": " junk
  and parse them. And then build this like extra fast. Yeah. Yeah.", "tokens": [50364,
  19109, 293, 48377, 552, 13, 400, 550, 1322, 341, 411, 2857, 2370, 13, 865, 13, 865,
  13, 50676], "temperature": 0.0, "avg_logprob": -0.2846457842484261, "compression_ratio":
  1.5867768595041323, "no_speech_prob": 0.03815966844558716}, {"id": 80, "seek": 52044,
  "start": 528.2, "end": 534.9200000000001, "text": " And then let''s say if I want
  to go to production, but I also have some other components which", "tokens": [50752,
  400, 550, 718, 311, 584, 498, 286, 528, 281, 352, 281, 4265, 11, 457, 286, 611,
  362, 512, 661, 6677, 597, 51088], "temperature": 0.0, "avg_logprob": -0.2846457842484261,
  "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.03815966844558716},
  {"id": 81, "seek": 52044, "start": 534.9200000000001, "end": 539.48, "text": " maybe
  you wouldn''t integrate, right? I know my search cluster and something else.", "tokens":
  [51088, 1310, 291, 2759, 380, 13365, 11, 558, 30, 286, 458, 452, 3164, 13630, 293,
  746, 1646, 13, 51316], "temperature": 0.0, "avg_logprob": -0.2846457842484261, "compression_ratio":
  1.5867768595041323, "no_speech_prob": 0.03815966844558716}, {"id": 82, "seek": 52044,
  "start": 539.48, "end": 544.84, "text": " My services. So can I also go to production
  with your platform? Yeah.", "tokens": [51316, 1222, 3328, 13, 407, 393, 286, 611,
  352, 281, 4265, 365, 428, 3663, 30, 865, 13, 51584], "temperature": 0.0, "avg_logprob":
  -0.2846457842484261, "compression_ratio": 1.5867768595041323, "no_speech_prob":
  0.03815966844558716}, {"id": 83, "seek": 52044, "start": 545.48, "end": 549.32,
  "text": " Like, how will it look like exactly? Is it a docker? Is it the unit?",
  "tokens": [51616, 1743, 11, 577, 486, 309, 574, 411, 2293, 30, 1119, 309, 257, 360,
  9178, 30, 1119, 309, 264, 4985, 30, 51808], "temperature": 0.0, "avg_logprob": -0.2846457842484261,
  "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.03815966844558716},
  {"id": 84, "seek": 54932, "start": 549.8000000000001, "end": 554.7600000000001,
  "text": " It''s you do not need to first of all code in Rust. A lot of developers
  come out to reach me and", "tokens": [50388, 467, 311, 291, 360, 406, 643, 281,
  700, 295, 439, 3089, 294, 34952, 13, 316, 688, 295, 8849, 808, 484, 281, 2524, 385,
  293, 50636], "temperature": 0.0, "avg_logprob": -0.3261859348842076, "compression_ratio":
  1.691304347826087, "no_speech_prob": 0.05555598437786102}, {"id": 85, "seek": 54932,
  "start": 554.7600000000001, "end": 560.7600000000001, "text": " like, you know,
  they ask, do I need to know Rust to contribute to embed anything? I''m like, no,",
  "tokens": [50636, 411, 11, 291, 458, 11, 436, 1029, 11, 360, 286, 643, 281, 458,
  34952, 281, 10586, 281, 12240, 1340, 30, 286, 478, 411, 11, 572, 11, 50936], "temperature":
  0.0, "avg_logprob": -0.3261859348842076, "compression_ratio": 1.691304347826087,
  "no_speech_prob": 0.05555598437786102}, {"id": 86, "seek": 54932, "start": 560.7600000000001,
  "end": 568.5200000000001, "text": " you do not need. We have, because let''s fire.
  We have like worked like for building this wrapper", "tokens": [50936, 291, 360,
  406, 643, 13, 492, 362, 11, 570, 718, 311, 2610, 13, 492, 362, 411, 2732, 411, 337,
  2390, 341, 46906, 51324], "temperature": 0.0, "avg_logprob": -0.3261859348842076,
  "compression_ratio": 1.691304347826087, "no_speech_prob": 0.05555598437786102},
  {"id": 87, "seek": 54932, "start": 568.5200000000001, "end": 574.36, "text": " around
  Rust so that you know, you can easily create it with Python. Oh, so you have a Python
  wrapper", "tokens": [51324, 926, 34952, 370, 300, 291, 458, 11, 291, 393, 3612,
  1884, 309, 365, 15329, 13, 876, 11, 370, 291, 362, 257, 15329, 46906, 51616], "temperature":
  0.0, "avg_logprob": -0.3261859348842076, "compression_ratio": 1.691304347826087,
  "no_speech_prob": 0.05555598437786102}, {"id": 88, "seek": 57436, "start": 574.84,
  "end": 581.32, "text": " of your own. Yeah. You only need to know Python. You do
  not need to know Rust at all.", "tokens": [50388, 295, 428, 1065, 13, 865, 13, 509,
  787, 643, 281, 458, 15329, 13, 509, 360, 406, 643, 281, 458, 34952, 412, 439, 13,
  50712], "temperature": 0.0, "avg_logprob": -0.25760720481335275, "compression_ratio":
  1.538888888888889, "no_speech_prob": 0.01238335482776165}, {"id": 89, "seek": 57436,
  "start": 581.32, "end": 588.76, "text": " How interesting. Yeah. So do you have
  any like instances where companies have already built", "tokens": [50712, 1012,
  1880, 13, 865, 13, 407, 360, 291, 362, 604, 411, 14519, 689, 3431, 362, 1217, 3094,
  51084], "temperature": 0.0, "avg_logprob": -0.25760720481335275, "compression_ratio":
  1.538888888888889, "no_speech_prob": 0.01238335482776165}, {"id": 90, "seek": 57436,
  "start": 588.76, "end": 595.88, "text": " POCs with your platform? Or do you already
  have someone going to production? Yeah. So I get so many", "tokens": [51084, 22299,
  33290, 365, 428, 3663, 30, 1610, 360, 291, 1217, 362, 1580, 516, 281, 4265, 30,
  865, 13, 407, 286, 483, 370, 867, 51440], "temperature": 0.0, "avg_logprob": -0.25760720481335275,
  "compression_ratio": 1.538888888888889, "no_speech_prob": 0.01238335482776165},
  {"id": 91, "seek": 59588, "start": 595.88, "end": 604.4399999999999, "text": " requests
  on like acquisition part of things and stuff like that. But we are like, you know,",
  "tokens": [50364, 12475, 322, 411, 21668, 644, 295, 721, 293, 1507, 411, 300, 13,
  583, 321, 366, 411, 11, 291, 458, 11, 50792], "temperature": 0.0, "avg_logprob":
  -0.2676670809826219, "compression_ratio": 1.6237623762376239, "no_speech_prob":
  0.01055728830397129}, {"id": 92, "seek": 59588, "start": 604.4399999999999, "end":
  611.24, "text": " it''s we are one my whole company and we have nothing company
  project. But we have got six", "tokens": [50792, 309, 311, 321, 366, 472, 452, 1379,
  2237, 293, 321, 362, 1825, 2237, 1716, 13, 583, 321, 362, 658, 2309, 51132], "temperature":
  0.0, "avg_logprob": -0.2676670809826219, "compression_ratio": 1.6237623762376239,
  "no_speech_prob": 0.01055728830397129}, {"id": 93, "seek": 59588, "start": 611.24,
  "end": 617.56, "text": " key downloads, but we have gotten to production yet. But
  hopefully next two, three months.", "tokens": [51132, 2141, 36553, 11, 457, 321,
  362, 5768, 281, 4265, 1939, 13, 583, 4696, 958, 732, 11, 1045, 2493, 13, 51448],
  "temperature": 0.0, "avg_logprob": -0.2676670809826219, "compression_ratio": 1.6237623762376239,
  "no_speech_prob": 0.01055728830397129}, {"id": 94, "seek": 59588, "start": 617.56,
  "end": 621.96, "text": " Nice. And do you need any help from the community? Yes.",
  "tokens": [51448, 5490, 13, 400, 360, 291, 643, 604, 854, 490, 264, 1768, 30, 1079,
  13, 51668], "temperature": 0.0, "avg_logprob": -0.2676670809826219, "compression_ratio":
  1.6237623762376239, "no_speech_prob": 0.01055728830397129}, {"id": 95, "seek": 62196,
  "start": 622.9200000000001, "end": 628.9200000000001, "text": " If you''re interested
  in building the infrastructure for UnityBI and Rust,", "tokens": [50412, 759, 291,
  434, 3102, 294, 2390, 264, 6896, 337, 27913, 11291, 293, 34952, 11, 50712], "temperature":
  0.0, "avg_logprob": -0.28461087870801616, "compression_ratio": 1.5643939393939394,
  "no_speech_prob": 0.011971337720751762}, {"id": 96, "seek": 62196, "start": 628.9200000000001,
  "end": 634.6, "text": " Python. So to connect with us, we''re left to have you on
  board.", "tokens": [50712, 15329, 13, 407, 281, 1745, 365, 505, 11, 321, 434, 1411,
  281, 362, 291, 322, 3150, 13, 50996], "temperature": 0.0, "avg_logprob": -0.28461087870801616,
  "compression_ratio": 1.5643939393939394, "no_speech_prob": 0.011971337720751762},
  {"id": 97, "seek": 62196, "start": 634.6, "end": 639.72, "text": " All right. But
  let''s go back a little bit. So you also said that there is multi-modality", "tokens":
  [50996, 1057, 558, 13, 583, 718, 311, 352, 646, 257, 707, 857, 13, 407, 291, 611,
  848, 300, 456, 307, 4825, 12, 8014, 1860, 51252], "temperature": 0.0, "avg_logprob":
  -0.28461087870801616, "compression_ratio": 1.5643939393939394, "no_speech_prob":
  0.011971337720751762}, {"id": 98, "seek": 62196, "start": 639.72, "end": 644.6800000000001,
  "text": " element of it. Yeah. So I will tell you the way I see it, but please correct
  me or augment me.", "tokens": [51252, 4478, 295, 309, 13, 865, 13, 407, 286, 486,
  980, 291, 264, 636, 286, 536, 309, 11, 457, 1767, 3006, 385, 420, 29919, 385, 13,
  51500], "temperature": 0.0, "avg_logprob": -0.28461087870801616, "compression_ratio":
  1.5643939393939394, "no_speech_prob": 0.011971337720751762}, {"id": 99, "seek":
  62196, "start": 644.6800000000001, "end": 650.0400000000001, "text": " So I think
  a couple of years ago, two years ago, we gave a talk here at Berlin Bosnol. It''s",
  "tokens": [51500, 407, 286, 519, 257, 1916, 295, 924, 2057, 11, 732, 924, 2057,
  11, 321, 2729, 257, 751, 510, 412, 13848, 22264, 77, 401, 13, 467, 311, 51768],
  "temperature": 0.0, "avg_logprob": -0.28461087870801616, "compression_ratio": 1.5643939393939394,
  "no_speech_prob": 0.011971337720751762}, {"id": 100, "seek": 65004, "start": 650.04,
  "end": 657.0, "text": " basically showing a system where you can search images and
  text, whatever you want.", "tokens": [50364, 1936, 4099, 257, 1185, 689, 291, 393,
  3164, 5267, 293, 2487, 11, 2035, 291, 528, 13, 50712], "temperature": 0.0, "avg_logprob":
  -0.16903860569000245, "compression_ratio": 1.59375, "no_speech_prob": 0.0017836998449638486},
  {"id": 101, "seek": 65004, "start": 657.7199999999999, "end": 665.88, "text": "
  And if you have images that do not have textual metadata, then that''s your gateway
  into finding", "tokens": [50748, 400, 498, 291, 362, 5267, 300, 360, 406, 362, 2487,
  901, 26603, 11, 550, 300, 311, 428, 28532, 666, 5006, 51156], "temperature": 0.0,
  "avg_logprob": -0.16903860569000245, "compression_ratio": 1.59375, "no_speech_prob":
  0.0017836998449638486}, {"id": 102, "seek": 65004, "start": 665.88, "end": 671.56,
  "text": " these images because neural networks will understand and extract the content
  using plebe,", "tokens": [51156, 613, 5267, 570, 18161, 9590, 486, 1223, 293, 8947,
  264, 2701, 1228, 3362, 650, 11, 51440], "temperature": 0.0, "avg_logprob": -0.16903860569000245,
  "compression_ratio": 1.59375, "no_speech_prob": 0.0017836998449638486}, {"id": 103,
  "seek": 65004, "start": 671.56, "end": 676.28, "text": " right? Yeah. And so we
  were able to show some really interesting examples. For example,", "tokens": [51440,
  558, 30, 865, 13, 400, 370, 321, 645, 1075, 281, 855, 512, 534, 1880, 5110, 13,
  1171, 1365, 11, 51676], "temperature": 0.0, "avg_logprob": -0.16903860569000245,
  "compression_ratio": 1.59375, "no_speech_prob": 0.0017836998449638486}, {"id": 104,
  "seek": 67628, "start": 676.28, "end": 682.6, "text": " you could find in the context
  of the commerce, a long sleeveless dress,", "tokens": [50364, 291, 727, 915, 294,
  264, 4319, 295, 264, 26320, 11, 257, 938, 12931, 779, 442, 5231, 11, 50680], "temperature":
  0.0, "avg_logprob": -0.24652152683423914, "compression_ratio": 1.6395348837209303,
  "no_speech_prob": 0.005261875223368406}, {"id": 105, "seek": 67628, "start": 682.6,
  "end": 687.4, "text": " striped, whatever color and so on and so forth. And it worked.
  And even some audience members asked", "tokens": [50680, 3575, 3452, 11, 2035, 2017,
  293, 370, 322, 293, 370, 5220, 13, 400, 309, 2732, 13, 400, 754, 512, 4034, 2679,
  2351, 50920], "temperature": 0.0, "avg_logprob": -0.24652152683423914, "compression_ratio":
  1.6395348837209303, "no_speech_prob": 0.005261875223368406}, {"id": 106, "seek":
  67628, "start": 687.4, "end": 694.8399999999999, "text": " us to demo on their queries
  and it still worked. So that showed the power of multi-modality,", "tokens": [50920,
  505, 281, 10723, 322, 641, 24109, 293, 309, 920, 2732, 13, 407, 300, 4712, 264,
  1347, 295, 4825, 12, 8014, 1860, 11, 51292], "temperature": 0.0, "avg_logprob":
  -0.24652152683423914, "compression_ratio": 1.6395348837209303, "no_speech_prob":
  0.005261875223368406}, {"id": 107, "seek": 67628, "start": 694.8399999999999, "end":
  698.52, "text": " right? And we didn''t even need to fine-tune delete it. It was
  just out of the box.", "tokens": [51292, 558, 30, 400, 321, 994, 380, 754, 643,
  281, 2489, 12, 83, 2613, 12097, 309, 13, 467, 390, 445, 484, 295, 264, 2424, 13,
  51476], "temperature": 0.0, "avg_logprob": -0.24652152683423914, "compression_ratio":
  1.6395348837209303, "no_speech_prob": 0.005261875223368406}, {"id": 108, "seek":
  67628, "start": 699.24, "end": 703.72, "text": " But I guess the reason for it to
  multi-modality, what else are you thinking", "tokens": [51512, 583, 286, 2041, 264,
  1778, 337, 309, 281, 4825, 12, 8014, 1860, 11, 437, 1646, 366, 291, 1953, 51736],
  "temperature": 0.0, "avg_logprob": -0.24652152683423914, "compression_ratio": 1.6395348837209303,
  "no_speech_prob": 0.005261875223368406}, {"id": 109, "seek": 70372, "start": 704.28,
  "end": 711.8000000000001, "text": " that''s part of your blog? Great question. Even
  though images like Clare is known for multi-modality", "tokens": [50392, 300, 311,
  644, 295, 428, 6968, 30, 3769, 1168, 13, 2754, 1673, 5267, 411, 2033, 543, 307,
  2570, 337, 4825, 12, 8014, 1860, 50768], "temperature": 0.0, "avg_logprob": -0.29357982434724506,
  "compression_ratio": 1.5879828326180256, "no_speech_prob": 0.009344922378659248},
  {"id": 110, "seek": 70372, "start": 711.8000000000001, "end": 718.12, "text": "
  research, one of the best use cases of Clare is when you''re doing the need of short
  classification,", "tokens": [50768, 2132, 11, 472, 295, 264, 1151, 764, 3331, 295,
  2033, 543, 307, 562, 291, 434, 884, 264, 643, 295, 2099, 21538, 11, 51084], "temperature":
  0.0, "avg_logprob": -0.29357982434724506, "compression_ratio": 1.5879828326180256,
  "no_speech_prob": 0.009344922378659248}, {"id": 111, "seek": 70372, "start": 718.12,
  "end": 723.24, "text": " right? It doesn''t need the previous data at all, even
  if it is searching images,", "tokens": [51084, 558, 30, 467, 1177, 380, 643, 264,
  3894, 1412, 412, 439, 11, 754, 498, 309, 307, 10808, 5267, 11, 51340], "temperature":
  0.0, "avg_logprob": -0.29357982434724506, "compression_ratio": 1.5879828326180256,
  "no_speech_prob": 0.009344922378659248}, {"id": 112, "seek": 70372, "start": 723.24,
  "end": 728.36, "text": " if it is searching through text. And it''s like so powerful,
  right? So we have a different", "tokens": [51340, 498, 309, 307, 10808, 807, 2487,
  13, 400, 309, 311, 411, 370, 4005, 11, 558, 30, 407, 321, 362, 257, 819, 51596],
  "temperature": 0.0, "avg_logprob": -0.29357982434724506, "compression_ratio": 1.5879828326180256,
  "no_speech_prob": 0.009344922378659248}, {"id": 113, "seek": 72836, "start": 728.44,
  "end": 734.76, "text": " example with it. But coming to a question, we have audio,
  wanted to embed audio graphs,", "tokens": [50368, 1365, 365, 309, 13, 583, 1348,
  281, 257, 1168, 11, 321, 362, 6278, 11, 1415, 281, 12240, 6278, 24877, 11, 50684],
  "temperature": 0.0, "avg_logprob": -0.36870470279600565, "compression_ratio": 1.545945945945946,
  "no_speech_prob": 0.02618482895195484}, {"id": 114, "seek": 72836, "start": 734.76,
  "end": 742.2, "text": " et cetera. So all these are in five times. But right now,
  we are only embedding text and images.", "tokens": [50684, 1030, 11458, 13, 407,
  439, 613, 366, 294, 1732, 1413, 13, 583, 558, 586, 11, 321, 366, 787, 12240, 3584,
  2487, 293, 5267, 13, 51056], "temperature": 0.0, "avg_logprob": -0.36870470279600565,
  "compression_ratio": 1.545945945945946, "no_speech_prob": 0.02618482895195484},
  {"id": 115, "seek": 72836, "start": 743.64, "end": 749.72, "text": " And are you
  using Clip? Yeah, we''re using Clip. Clip, that''s right. You know, that one thing
  that you", "tokens": [51128, 400, 366, 291, 1228, 2033, 647, 30, 865, 11, 321, 434,
  1228, 2033, 647, 13, 2033, 647, 11, 300, 311, 558, 13, 509, 458, 11, 300, 472, 551,
  300, 291, 51432], "temperature": 0.0, "avg_logprob": -0.36870470279600565, "compression_ratio":
  1.545945945945946, "no_speech_prob": 0.02618482895195484}, {"id": 116, "seek": 74972,
  "start": 749.8000000000001, "end": 759.0, "text": " cannot get. I also wanted to
  ask you a bit if you may share your insight on evaluating this", "tokens": [50368,
  2644, 483, 13, 286, 611, 1415, 281, 1029, 291, 257, 857, 498, 291, 815, 2073, 428,
  11269, 322, 27479, 341, 50828], "temperature": 0.0, "avg_logprob": -0.3041996955871582,
  "compression_ratio": 1.543778801843318, "no_speech_prob": 0.009660112671554089},
  {"id": 117, "seek": 74972, "start": 759.0, "end": 765.88, "text": " system. So one
  of the feedbacks that I have gotten for, for IoT,", "tokens": [50828, 1185, 13,
  407, 472, 295, 264, 5824, 82, 300, 286, 362, 5768, 337, 11, 337, 30112, 11, 51172],
  "temperature": 0.0, "avg_logprob": -0.3041996955871582, "compression_ratio": 1.543778801843318,
  "no_speech_prob": 0.009660112671554089}, {"id": 118, "seek": 74972, "start": 765.88,
  "end": 771.08, "text": " between or anything, like basically, so let''s say I have
  my LLN based application,", "tokens": [51172, 1296, 420, 1340, 11, 411, 1936, 11,
  370, 718, 311, 584, 286, 362, 452, 441, 43, 45, 2361, 3861, 11, 51432], "temperature":
  0.0, "avg_logprob": -0.3041996955871582, "compression_ratio": 1.543778801843318,
  "no_speech_prob": 0.009660112671554089}, {"id": 119, "seek": 74972, "start": 771.08,
  "end": 776.28, "text": " you know, how do I evaluate it? Because one of the feedbacks
  is that sometimes it gives perfect", "tokens": [51432, 291, 458, 11, 577, 360, 286,
  13059, 309, 30, 1436, 472, 295, 264, 5824, 82, 307, 300, 2171, 309, 2709, 2176,
  51692], "temperature": 0.0, "avg_logprob": -0.3041996955871582, "compression_ratio":
  1.543778801843318, "no_speech_prob": 0.009660112671554089}, {"id": 120, "seek":
  77628, "start": 776.28, "end": 783.0799999999999, "text": " results, sometimes it
  gives awful results, right? So now there is nothing in between, right?", "tokens":
  [50364, 3542, 11, 2171, 309, 2709, 11232, 3542, 11, 558, 30, 407, 586, 456, 307,
  1825, 294, 1296, 11, 558, 30, 50704], "temperature": 0.0, "avg_logprob": -0.18651909663759428,
  "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.005421963054686785},
  {"id": 121, "seek": 77628, "start": 783.0799999999999, "end": 787.64, "text": "
  Or not barely. So how would you solve this? Of course, you do start with your metric
  learning and", "tokens": [50704, 1610, 406, 10268, 13, 407, 577, 576, 291, 5039,
  341, 30, 2720, 1164, 11, 291, 360, 722, 365, 428, 20678, 2539, 293, 50932], "temperature":
  0.0, "avg_logprob": -0.18651909663759428, "compression_ratio": 1.7194244604316546,
  "no_speech_prob": 0.005421963054686785}, {"id": 122, "seek": 77628, "start": 787.64,
  "end": 792.52, "text": " some other techniques, right? But there is still the other
  side of things when you go to production,", "tokens": [50932, 512, 661, 7512, 11,
  558, 30, 583, 456, 307, 920, 264, 661, 1252, 295, 721, 562, 291, 352, 281, 4265,
  11, 51176], "temperature": 0.0, "avg_logprob": -0.18651909663759428, "compression_ratio":
  1.7194244604316546, "no_speech_prob": 0.005421963054686785}, {"id": 123, "seek":
  77628, "start": 792.52, "end": 797.24, "text": " as you know, like in Rasa and Quadrant
  and many other companies, you care about quality.", "tokens": [51176, 382, 291,
  458, 11, 411, 294, 497, 9994, 293, 29619, 7541, 293, 867, 661, 3431, 11, 291, 1127,
  466, 3125, 13, 51412], "temperature": 0.0, "avg_logprob": -0.18651909663759428,
  "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.005421963054686785},
  {"id": 124, "seek": 77628, "start": 797.9599999999999, "end": 802.76, "text": "
  So how do you have any insight on that? Are you maybe planning to build something
  along the lines", "tokens": [51448, 407, 577, 360, 291, 362, 604, 11269, 322, 300,
  30, 2014, 291, 1310, 5038, 281, 1322, 746, 2051, 264, 3876, 51688], "temperature":
  0.0, "avg_logprob": -0.18651909663759428, "compression_ratio": 1.7194244604316546,
  "no_speech_prob": 0.005421963054686785}, {"id": 125, "seek": 80276, "start": 802.76,
  "end": 810.52, "text": " of evaluation? That''s a great question. You know, but
  great part of the great response to it is,", "tokens": [50364, 295, 13344, 30, 663,
  311, 257, 869, 1168, 13, 509, 458, 11, 457, 869, 644, 295, 264, 869, 4134, 281,
  309, 307, 11, 50752], "temperature": 0.0, "avg_logprob": -0.3141882194662994, "compression_ratio":
  1.721461187214612, "no_speech_prob": 0.032879944890737534}, {"id": 126, "seek":
  80276, "start": 810.52, "end": 817.3199999999999, "text": " so LLN is one of the
  examples of that. So, you know, LLN gives you a bright answer, but it also", "tokens":
  [50752, 370, 441, 43, 45, 307, 472, 295, 264, 5110, 295, 300, 13, 407, 11, 291,
  458, 11, 441, 43, 45, 2709, 291, 257, 4730, 1867, 11, 457, 309, 611, 51092], "temperature":
  0.0, "avg_logprob": -0.3141882194662994, "compression_ratio": 1.721461187214612,
  "no_speech_prob": 0.032879944890737534}, {"id": 127, "seek": 80276, "start": 817.3199999999999,
  "end": 823.56, "text": " gives you hallucination. But a lot of people see hallucination
  as a bug, but I see it as a feature,", "tokens": [51092, 2709, 291, 35212, 2486,
  13, 583, 257, 688, 295, 561, 536, 35212, 2486, 382, 257, 7426, 11, 457, 286, 536,
  309, 382, 257, 4111, 11, 51404], "temperature": 0.0, "avg_logprob": -0.3141882194662994,
  "compression_ratio": 1.721461187214612, "no_speech_prob": 0.032879944890737534},
  {"id": 128, "seek": 80276, "start": 823.56, "end": 831.72, "text": " because it
  won''t be able to do that creative job, but it can do with hallucinations.", "tokens":
  [51404, 570, 309, 1582, 380, 312, 1075, 281, 360, 300, 5880, 1691, 11, 457, 309,
  393, 360, 365, 35212, 10325, 13, 51812], "temperature": 0.0, "avg_logprob": -0.3141882194662994,
  "compression_ratio": 1.721461187214612, "no_speech_prob": 0.032879944890737534},
  {"id": 129, "seek": 83276, "start": 833.08, "end": 839.4, "text": " One of the,
  there are so many tools, right, to measure retrieval part like", "tokens": [50380,
  1485, 295, 264, 11, 456, 366, 370, 867, 3873, 11, 558, 11, 281, 3481, 19817, 3337,
  644, 411, 50696], "temperature": 0.0, "avg_logprob": -0.3032225010006927, "compression_ratio":
  1.6395939086294415, "no_speech_prob": 0.017986256629228592}, {"id": 130, "seek":
  83276, "start": 840.2, "end": 847.16, "text": " Braggas, Prometheus, right? And
  there are so many tools, too. But still, I think recall measure,", "tokens": [50736,
  4991, 1615, 296, 11, 2114, 649, 42209, 11, 558, 30, 400, 456, 366, 370, 867, 3873,
  11, 886, 13, 583, 920, 11, 286, 519, 9901, 3481, 11, 51084], "temperature": 0.0,
  "avg_logprob": -0.3032225010006927, "compression_ratio": 1.6395939086294415, "no_speech_prob":
  0.017986256629228592}, {"id": 131, "seek": 83276, "start": 847.16, "end": 852.04,
  "text": " what we call it, like, you know, measure how LLN recall is working,",
  "tokens": [51084, 437, 321, 818, 309, 11, 411, 11, 291, 458, 11, 3481, 577, 441,
  43, 45, 9901, 307, 1364, 11, 51328], "temperature": 0.0, "avg_logprob": -0.3032225010006927,
  "compression_ratio": 1.6395939086294415, "no_speech_prob": 0.017986256629228592},
  {"id": 132, "seek": 83276, "start": 852.04, "end": 858.2, "text": " where basically
  extracting most relevant information, not like rubbish information.", "tokens":
  [51328, 689, 1936, 49844, 881, 7340, 1589, 11, 406, 411, 29978, 1589, 13, 51636],
  "temperature": 0.0, "avg_logprob": -0.3032225010006927, "compression_ratio": 1.6395939086294415,
  "no_speech_prob": 0.017986256629228592}, {"id": 133, "seek": 85820, "start": 858.2800000000001,
  "end": 864.76, "text": " So those things are like really important, and a lot of
  research is going on, but we are more", "tokens": [50368, 407, 729, 721, 366, 411,
  534, 1021, 11, 293, 257, 688, 295, 2132, 307, 516, 322, 11, 457, 321, 366, 544,
  50692], "temperature": 0.0, "avg_logprob": -0.2634161435640775, "compression_ratio":
  1.6227272727272728, "no_speech_prob": 0.026854360476136208}, {"id": 134, "seek":
  85820, "start": 864.76, "end": 872.2, "text": " like focused on the infrastructure,
  and we are keeping it up, trying to keep it up, but yeah,", "tokens": [50692, 411,
  5178, 322, 264, 6896, 11, 293, 321, 366, 5145, 309, 493, 11, 1382, 281, 1066, 309,
  493, 11, 457, 1338, 11, 51064], "temperature": 0.0, "avg_logprob": -0.2634161435640775,
  "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.026854360476136208},
  {"id": 135, "seek": 85820, "start": 872.2, "end": 881.24, "text": " so mostly I
  would go for classical testing ways like precision, recall, yeah.", "tokens": [51064,
  370, 5240, 286, 576, 352, 337, 13735, 4997, 2098, 411, 18356, 11, 9901, 11, 1338,
  13, 51516], "temperature": 0.0, "avg_logprob": -0.2634161435640775, "compression_ratio":
  1.6227272727272728, "no_speech_prob": 0.026854360476136208}, {"id": 136, "seek":
  85820, "start": 881.24, "end": 886.2, "text": " But basically like, okay, you do
  test, and you see that sometimes once in a while it fails.", "tokens": [51516, 583,
  1936, 411, 11, 1392, 11, 291, 360, 1500, 11, 293, 291, 536, 300, 2171, 1564, 294,
  257, 1339, 309, 18199, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2634161435640775,
  "compression_ratio": 1.6227272727272728, "no_speech_prob": 0.026854360476136208},
  {"id": 137, "seek": 88620, "start": 886.84, "end": 891.96, "text": " So first of
  all, of course, catching that is important, right? People are going to production.",
  "tokens": [50396, 407, 700, 295, 439, 11, 295, 1164, 11, 16124, 300, 307, 1021,
  11, 558, 30, 3432, 366, 516, 281, 4265, 13, 50652], "temperature": 0.0, "avg_logprob":
  -0.27672962077613017, "compression_ratio": 1.5669291338582678, "no_speech_prob":
  0.012739204801619053}, {"id": 138, "seek": 88620, "start": 891.96, "end": 895.8000000000001,
  "text": " Yes, but what is your way backwards to fixing this?", "tokens": [50652,
  1079, 11, 457, 437, 307, 428, 636, 12204, 281, 19442, 341, 30, 50844], "temperature":
  0.0, "avg_logprob": -0.27672962077613017, "compression_ratio": 1.5669291338582678,
  "no_speech_prob": 0.012739204801619053}, {"id": 139, "seek": 88620, "start": 896.5200000000001,
  "end": 899.6400000000001, "text": " Thank you, Chef. Yeah, from from from finding
  that bug.", "tokens": [50880, 1044, 291, 11, 14447, 13, 865, 11, 490, 490, 490,
  5006, 300, 7426, 13, 51036], "temperature": 0.0, "avg_logprob": -0.27672962077613017,
  "compression_ratio": 1.5669291338582678, "no_speech_prob": 0.012739204801619053},
  {"id": 140, "seek": 88620, "start": 901.8000000000001, "end": 908.6800000000001,
  "text": " Okay, let me think about that. So, data set, maybe you can give some example
  where you have fixed", "tokens": [51144, 1033, 11, 718, 385, 519, 466, 300, 13,
  407, 11, 1412, 992, 11, 1310, 291, 393, 976, 512, 1365, 689, 291, 362, 6806, 51488],
  "temperature": 0.0, "avg_logprob": -0.27672962077613017, "compression_ratio": 1.5669291338582678,
  "no_speech_prob": 0.012739204801619053}, {"id": 141, "seek": 88620, "start": 908.6800000000001,
  "end": 913.72, "text": " a ratio, you know, reported by someone, not necessarily
  as part of your platform, but previously,", "tokens": [51488, 257, 8509, 11, 291,
  458, 11, 7055, 538, 1580, 11, 406, 4725, 382, 644, 295, 428, 3663, 11, 457, 8046,
  11, 51740], "temperature": 0.0, "avg_logprob": -0.27672962077613017, "compression_ratio":
  1.5669291338582678, "no_speech_prob": 0.012739204801619053}, {"id": 142, "seek":
  91372, "start": 913.72, "end": 922.0400000000001, "text": " I don''t know, what
  have I done? Yeah, so I was working with this, that''s why this talk came", "tokens":
  [50364, 286, 500, 380, 458, 11, 437, 362, 286, 1096, 30, 865, 11, 370, 286, 390,
  1364, 365, 341, 11, 300, 311, 983, 341, 751, 1361, 50780], "temperature": 0.0, "avg_logprob":
  -0.3175241661071777, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.020426224917173386},
  {"id": 143, "seek": 91372, "start": 922.0400000000001, "end": 928.0400000000001,
  "text": " into my mind, right? The negation problem, the negation problem is so
  huge. You will always find", "tokens": [50780, 666, 452, 1575, 11, 558, 30, 440,
  2485, 399, 1154, 11, 264, 2485, 399, 1154, 307, 370, 2603, 13, 509, 486, 1009, 915,
  51080], "temperature": 0.0, "avg_logprob": -0.3175241661071777, "compression_ratio":
  1.615702479338843, "no_speech_prob": 0.020426224917173386}, {"id": 144, "seek":
  91372, "start": 928.0400000000001, "end": 935.88, "text": " sentences with not every
  domain, read biomedical, read law and everything, and it still gives you the", "tokens":
  [51080, 16579, 365, 406, 633, 9274, 11, 1401, 49775, 11, 1401, 2101, 293, 1203,
  11, 293, 309, 920, 2709, 291, 264, 51472], "temperature": 0.0, "avg_logprob": -0.3175241661071777,
  "compression_ratio": 1.615702479338843, "no_speech_prob": 0.020426224917173386},
  {"id": 145, "seek": 91372, "start": 935.88, "end": 941.96, "text": " same similarity,
  even though you do not have to be a language, you know, expert to understand these.",
  "tokens": [51472, 912, 32194, 11, 754, 1673, 291, 360, 406, 362, 281, 312, 257,
  2856, 11, 291, 458, 11, 5844, 281, 1223, 613, 13, 51776], "temperature": 0.0, "avg_logprob":
  -0.3175241661071777, "compression_ratio": 1.615702479338843, "no_speech_prob": 0.020426224917173386},
  {"id": 146, "seek": 94196, "start": 941.96, "end": 947.32, "text": " Different things.
  Yeah, yeah, yeah. So different things, right? That''s that''s when the trick", "tokens":
  [50364, 20825, 721, 13, 865, 11, 1338, 11, 1338, 13, 407, 819, 721, 11, 558, 30,
  663, 311, 300, 311, 562, 264, 4282, 50632], "temperature": 0.0, "avg_logprob": -0.31341826121012367,
  "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.04503000155091286},
  {"id": 147, "seek": 94196, "start": 947.32, "end": 952.2800000000001, "text": "
  learning comes, that''s when inference started to come into mind, because inference
  is very important.", "tokens": [50632, 2539, 1487, 11, 300, 311, 562, 38253, 1409,
  281, 808, 666, 1575, 11, 570, 38253, 307, 588, 1021, 13, 50880], "temperature":
  0.0, "avg_logprob": -0.31341826121012367, "compression_ratio": 1.6926605504587156,
  "no_speech_prob": 0.04503000155091286}, {"id": 148, "seek": 94196, "start": 952.2800000000001,
  "end": 958.6800000000001, "text": " Like a lot of people have played with SNLI and
  stuff, and then they understand that to understand", "tokens": [50880, 1743, 257,
  688, 295, 561, 362, 3737, 365, 13955, 48718, 293, 1507, 11, 293, 550, 436, 1223,
  300, 281, 1223, 51200], "temperature": 0.0, "avg_logprob": -0.31341826121012367,
  "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.04503000155091286},
  {"id": 149, "seek": 94196, "start": 958.6800000000001, "end": 963.64, "text": "
  negation, you first need to understand inferences. So there''s a way to like,",
  "tokens": [51200, 2485, 399, 11, 291, 700, 643, 281, 1223, 13596, 2667, 13, 407,
  456, 311, 257, 636, 281, 411, 11, 51448], "temperature": 0.0, "avg_logprob": -0.31341826121012367,
  "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.04503000155091286},
  {"id": 150, "seek": 96364, "start": 964.6, "end": 971.64, "text": " method, right?
  Yeah, yeah, yeah, and entailment, contradiction, neutral, two sentences could be
  neutral,", "tokens": [50412, 3170, 11, 558, 30, 865, 11, 1338, 11, 1338, 11, 293,
  948, 864, 518, 11, 34937, 11, 10598, 11, 732, 16579, 727, 312, 10598, 11, 50764],
  "temperature": 0.0, "avg_logprob": -0.37822208404541013, "compression_ratio": 1.7488584474885844,
  "no_speech_prob": 0.061677590012550354}, {"id": 151, "seek": 96364, "start": 971.64,
  "end": 977.64, "text": " yeah, unrelated to each other, two centers could be contradictory,
  contradictory to each other.", "tokens": [50764, 1338, 11, 38967, 281, 1184, 661,
  11, 732, 10898, 727, 312, 49555, 11, 49555, 281, 1184, 661, 13, 51064], "temperature":
  0.0, "avg_logprob": -0.37822208404541013, "compression_ratio": 1.7488584474885844,
  "no_speech_prob": 0.061677590012550354}, {"id": 152, "seek": 96364, "start": 977.64,
  "end": 983.0, "text": " So, yeah, which means that you need a purpose of data somehow
  labeled, yes, yes, yes,", "tokens": [51064, 407, 11, 1338, 11, 597, 1355, 300, 291,
  643, 257, 4334, 295, 1412, 6063, 21335, 11, 2086, 11, 2086, 11, 2086, 11, 51332],
  "temperature": 0.0, "avg_logprob": -0.37822208404541013, "compression_ratio": 1.7488584474885844,
  "no_speech_prob": 0.061677590012550354}, {"id": 153, "seek": 96364, "start": 983.0,
  "end": 988.92, "text": " logically reasoned through, right, using an algorithm.
  Yes, so that''s that''s what SNLI is, like,", "tokens": [51332, 38887, 1778, 292,
  807, 11, 558, 11, 1228, 364, 9284, 13, 1079, 11, 370, 300, 311, 300, 311, 437, 13955,
  48718, 307, 11, 411, 11, 51628], "temperature": 0.0, "avg_logprob": -0.37822208404541013,
  "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.061677590012550354},
  {"id": 154, "seek": 98892, "start": 988.92, "end": 995.0799999999999, "text": "
  SNLI is a data set, particularly for this, yeah, particular questions, problems,
  so yeah,", "tokens": [50364, 13955, 48718, 307, 257, 1412, 992, 11, 4098, 337, 341,
  11, 1338, 11, 1729, 1651, 11, 2740, 11, 370, 1338, 11, 50672], "temperature": 0.0,
  "avg_logprob": -0.3972225755748182, "compression_ratio": 1.6106194690265487, "no_speech_prob":
  0.05291461944580078}, {"id": 155, "seek": 98892, "start": 995.0799999999999, "end":
  1000.5999999999999, "text": " if you, you''ve re-tune it with, it was fun. So that
  would be for text, and what about other", "tokens": [50672, 498, 291, 11, 291, 600,
  319, 12, 83, 2613, 309, 365, 11, 309, 390, 1019, 13, 407, 300, 576, 312, 337, 2487,
  11, 293, 437, 466, 661, 50948], "temperature": 0.0, "avg_logprob": -0.3972225755748182,
  "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.05291461944580078},
  {"id": 156, "seek": 98892, "start": 1000.5999999999999, "end": 1009.9599999999999,
  "text": " modalities? I don''t think it was, that''s what it was. Like images, I
  know sometimes a model may", "tokens": [50948, 1072, 16110, 30, 286, 500, 380, 519,
  309, 390, 11, 300, 311, 437, 309, 390, 13, 1743, 5267, 11, 286, 458, 2171, 257,
  2316, 815, 51416], "temperature": 0.0, "avg_logprob": -0.3972225755748182, "compression_ratio":
  1.6106194690265487, "no_speech_prob": 0.05291461944580078}, {"id": 157, "seek":
  98892, "start": 1009.9599999999999, "end": 1013.56, "text": " hallucinate that there
  is something in the real world, but there is nothing like that.", "tokens": [51416,
  35212, 13923, 300, 456, 307, 746, 294, 264, 957, 1002, 11, 457, 456, 307, 1825,
  411, 300, 13, 51596], "temperature": 0.0, "avg_logprob": -0.3972225755748182, "compression_ratio":
  1.6106194690265487, "no_speech_prob": 0.05291461944580078}, {"id": 158, "seek":
  101356, "start": 1014.52, "end": 1020.76, "text": " Oh, that''s one thing, but I
  guess there are many. So there are things like, there''s a study called", "tokens":
  [50412, 876, 11, 300, 311, 472, 551, 11, 457, 286, 2041, 456, 366, 867, 13, 407,
  456, 366, 721, 411, 11, 456, 311, 257, 2979, 1219, 50724], "temperature": 0.0, "avg_logprob":
  -0.3783814239501953, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.04725123196840286},
  {"id": 159, "seek": 101356, "start": 1020.76, "end": 1027.72, "text": " blind pairs
  in clip, that was done by, I''m sorry, I forgot the name, which were like,", "tokens":
  [50724, 6865, 15494, 294, 7353, 11, 300, 390, 1096, 538, 11, 286, 478, 2597, 11,
  286, 5298, 264, 1315, 11, 597, 645, 411, 11, 51072], "temperature": 0.0, "avg_logprob":
  -0.3783814239501953, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.04725123196840286},
  {"id": 160, "seek": 101356, "start": 1027.72, "end": 1035.8, "text": " people say
  find that. Yeah, so they found out, clip actually has blind pairs, like you cannot",
  "tokens": [51072, 561, 584, 915, 300, 13, 865, 11, 370, 436, 1352, 484, 11, 7353,
  767, 575, 6865, 15494, 11, 411, 291, 2644, 51476], "temperature": 0.0, "avg_logprob":
  -0.3783814239501953, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.04725123196840286},
  {"id": 161, "seek": 101356, "start": 1036.52, "end": 1043.32, "text": " segment
  things really well, like, cats sleeping on the, on the car, or something, and then",
  "tokens": [51512, 9469, 721, 534, 731, 11, 411, 11, 11111, 8296, 322, 264, 11, 322,
  264, 1032, 11, 420, 746, 11, 293, 550, 51852], "temperature": 0.0, "avg_logprob":
  -0.3783814239501953, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.04725123196840286},
  {"id": 162, "seek": 104332, "start": 1043.48, "end": 1051.08, "text": " something
  else will give you the same description or something. So there, Dino comes in. So
  Dino", "tokens": [50372, 746, 1646, 486, 976, 291, 264, 912, 3855, 420, 746, 13,
  407, 456, 11, 413, 2982, 1487, 294, 13, 407, 413, 2982, 50752], "temperature": 0.0,
  "avg_logprob": -0.40670801558584535, "compression_ratio": 1.7123287671232876, "no_speech_prob":
  0.0034034820273518562}, {"id": 163, "seek": 104332, "start": 1051.08, "end": 1057.0,
  "text": " there''s a segmentation with self-supervised learning, self-supervised
  learning, I think is the", "tokens": [50752, 456, 311, 257, 9469, 399, 365, 2698,
  12, 48172, 24420, 2539, 11, 2698, 12, 48172, 24420, 2539, 11, 286, 519, 307, 264,
  51048], "temperature": 0.0, "avg_logprob": -0.40670801558584535, "compression_ratio":
  1.7123287671232876, "no_speech_prob": 0.0034034820273518562}, {"id": 164, "seek":
  104332, "start": 1057.0, "end": 1062.12, "text": " best invention of like, for this
  AI, and now it thumbs up. Yeah, in the open source.", "tokens": [51048, 1151, 22265,
  295, 411, 11, 337, 341, 7318, 11, 293, 586, 309, 8838, 493, 13, 865, 11, 294, 264,
  1269, 4009, 13, 51304], "temperature": 0.0, "avg_logprob": -0.40670801558584535,
  "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0034034820273518562},
  {"id": 165, "seek": 104332, "start": 1062.6799999999998, "end": 1069.48, "text":
  " That''s supervised learning. Is it open source? A Dino, you say? Yeah, it is.
  Yeah, it''s from Meta,", "tokens": [51332, 663, 311, 46533, 2539, 13, 1119, 309,
  1269, 4009, 30, 316, 413, 2982, 11, 291, 584, 30, 865, 11, 309, 307, 13, 865, 11,
  309, 311, 490, 6377, 64, 11, 51672], "temperature": 0.0, "avg_logprob": -0.40670801558584535,
  "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0034034820273518562},
  {"id": 166, "seek": 106948, "start": 1069.96, "end": 1078.76, "text": " and I think
  Likun is one of, and when, you know, he has written this folder, what you call it,",
  "tokens": [50388, 293, 286, 519, 441, 1035, 409, 307, 472, 295, 11, 293, 562, 11,
  291, 458, 11, 415, 575, 3720, 341, 10820, 11, 437, 291, 818, 309, 11, 50828], "temperature":
  0.0, "avg_logprob": -0.29083048502604164, "compression_ratio": 1.738532110091743,
  "no_speech_prob": 0.05210966244339943}, {"id": 167, "seek": 106948, "start": 1078.76,
  "end": 1084.76, "text": " white paper on the dark matter of intelligence that is
  self-supervised learning. So they are doing a", "tokens": [50828, 2418, 3035, 322,
  264, 2877, 1871, 295, 7599, 300, 307, 2698, 12, 48172, 24420, 2539, 13, 407, 436,
  366, 884, 257, 51128], "temperature": 0.0, "avg_logprob": -0.29083048502604164,
  "compression_ratio": 1.738532110091743, "no_speech_prob": 0.05210966244339943},
  {"id": 168, "seek": 106948, "start": 1084.76, "end": 1090.1200000000001, "text":
  " lot of work in self-supervised learning. You know, make data, make the model learn
  from the data", "tokens": [51128, 688, 295, 589, 294, 2698, 12, 48172, 24420, 2539,
  13, 509, 458, 11, 652, 1412, 11, 652, 264, 2316, 1466, 490, 264, 1412, 51396], "temperature":
  0.0, "avg_logprob": -0.29083048502604164, "compression_ratio": 1.738532110091743,
  "no_speech_prob": 0.05210966244339943}, {"id": 169, "seek": 106948, "start": 1090.1200000000001,
  "end": 1095.88, "text": " itself. You do not need to label it. Yeah, that''s like
  in the self-supervised sort of.", "tokens": [51396, 2564, 13, 509, 360, 406, 643,
  281, 7645, 309, 13, 865, 11, 300, 311, 411, 294, 264, 2698, 12, 48172, 24420, 1333,
  295, 13, 51684], "temperature": 0.0, "avg_logprob": -0.29083048502604164, "compression_ratio":
  1.738532110091743, "no_speech_prob": 0.05210966244339943}, {"id": 170, "seek": 109588,
  "start": 1096.8400000000001, "end": 1102.68, "text": " Okay, then maybe another
  question I have is, where do you embed a human in this process? Do you ever,", "tokens":
  [50412, 1033, 11, 550, 1310, 1071, 1168, 286, 362, 307, 11, 689, 360, 291, 12240,
  257, 1952, 294, 341, 1399, 30, 1144, 291, 1562, 11, 50704], "temperature": 0.0,
  "avg_logprob": -0.28615981457280176, "compression_ratio": 1.5546875, "no_speech_prob":
  0.010997063480317593}, {"id": 171, "seek": 109588, "start": 1102.68, "end": 1109.0800000000002,
  "text": " like, I don''t know, to check quality or give feedback? Exactly. So one
  other thing in metric learning is", "tokens": [50704, 411, 11, 286, 500, 380, 458,
  11, 281, 1520, 3125, 420, 976, 5824, 30, 7587, 13, 407, 472, 661, 551, 294, 20678,
  2539, 307, 51024], "temperature": 0.0, "avg_logprob": -0.28615981457280176, "compression_ratio":
  1.5546875, "no_speech_prob": 0.010997063480317593}, {"id": 172, "seek": 109588,
  "start": 1109.0800000000002, "end": 1114.6000000000001, "text": " everyone thinks
  it''s self-supervised, or data, it will learn with the data. It doesn''t need",
  "tokens": [51024, 1518, 7309, 309, 311, 2698, 12, 48172, 24420, 11, 420, 1412, 11,
  309, 486, 1466, 365, 264, 1412, 13, 467, 1177, 380, 643, 51300], "temperature":
  0.0, "avg_logprob": -0.28615981457280176, "compression_ratio": 1.5546875, "no_speech_prob":
  0.010997063480317593}, {"id": 173, "seek": 109588, "start": 1114.6000000000001,
  "end": 1122.3600000000001, "text": " label. But when the contrastive learning happens,
  who is making that negative mining fare? It''s the", "tokens": [51300, 7645, 13,
  583, 562, 264, 8712, 488, 2539, 2314, 11, 567, 307, 1455, 300, 3671, 15512, 11994,
  30, 467, 311, 264, 51688], "temperature": 0.0, "avg_logprob": -0.28615981457280176,
  "compression_ratio": 1.5546875, "no_speech_prob": 0.010997063480317593}, {"id":
  174, "seek": 112236, "start": 1122.36, "end": 1128.12, "text": " huge, it''s the
  kind of making that negative one. Very, very crisp one, right? Exactly, where does
  it", "tokens": [50364, 2603, 11, 309, 311, 264, 733, 295, 1455, 300, 3671, 472,
  13, 4372, 11, 588, 22952, 472, 11, 558, 30, 7587, 11, 689, 775, 309, 50652], "temperature":
  0.0, "avg_logprob": -0.5678520512774707, "compression_ratio": 1.6833976833976834,
  "no_speech_prob": 0.030655188485980034}, {"id": 175, "seek": 112236, "start": 1128.12,
  "end": 1132.36, "text": " is learning? Not just random negative, but like synonymically
  negative.", "tokens": [50652, 307, 2539, 30, 1726, 445, 4974, 3671, 11, 457, 411,
  5451, 12732, 984, 3671, 13, 50864], "temperature": 0.0, "avg_logprob": -0.5678520512774707,
  "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.030655188485980034},
  {"id": 176, "seek": 112236, "start": 1132.36, "end": 1137.7199999999998, "text":
  " Thimmedically negative. Yeah. So there, they mean very common and like, you know.",
  "tokens": [50864, 334, 332, 1912, 984, 3671, 13, 865, 13, 407, 456, 11, 436, 914,
  588, 2689, 293, 411, 11, 291, 458, 13, 51132], "temperature": 0.0, "avg_logprob":
  -0.5678520512774707, "compression_ratio": 1.6833976833976834, "no_speech_prob":
  0.030655188485980034}, {"id": 177, "seek": 112236, "start": 1137.7199999999998,
  "end": 1143.3999999999999, "text": " For sure. Yeah. So, and so today, let''s say
  if someone wants to use your platform, you said,", "tokens": [51132, 1171, 988,
  13, 865, 13, 407, 11, 293, 370, 965, 11, 718, 311, 584, 498, 1580, 2738, 281, 764,
  428, 3663, 11, 291, 848, 11, 51416], "temperature": 0.0, "avg_logprob": -0.5678520512774707,
  "compression_ratio": 1.6833976833976834, "no_speech_prob": 0.030655188485980034},
  {"id": 178, "seek": 112236, "start": 1143.3999999999999, "end": 1147.24, "text":
  " embedding and embedding. Embedding thing. Embedding thing. It''s on GitHub, I''m
  guessing.", "tokens": [51416, 12240, 3584, 293, 12240, 3584, 13, 24234, 292, 3584,
  551, 13, 24234, 292, 3584, 551, 13, 467, 311, 322, 23331, 11, 286, 478, 17939, 13,
  51608], "temperature": 0.0, "avg_logprob": -0.5678520512774707, "compression_ratio":
  1.6833976833976834, "no_speech_prob": 0.030655188485980034}, {"id": 179, "seek":
  114724, "start": 1147.72, "end": 1154.1200000000001, "text": " We''ll link it. Yeah.
  And so how do you, like, part of the story that becomes successful, I guess,", "tokens":
  [50388, 492, 603, 2113, 309, 13, 865, 13, 400, 370, 577, 360, 291, 11, 411, 11,
  644, 295, 264, 1657, 300, 3643, 4406, 11, 286, 2041, 11, 50708], "temperature":
  0.0, "avg_logprob": -0.20942884502988873, "compression_ratio": 1.6213991769547325,
  "no_speech_prob": 0.03451335057616234}, {"id": 180, "seek": 114724, "start": 1154.1200000000001,
  "end": 1161.16, "text": " is that you can map out your path from use cases to your
  library, to your project, which would", "tokens": [50708, 307, 300, 291, 393, 4471,
  484, 428, 3100, 490, 764, 3331, 281, 428, 6405, 11, 281, 428, 1716, 11, 597, 576,
  51060], "temperature": 0.0, "avg_logprob": -0.20942884502988873, "compression_ratio":
  1.6213991769547325, "no_speech_prob": 0.03451335057616234}, {"id": 181, "seek":
  114724, "start": 1161.16, "end": 1168.04, "text": " probably be one of the components
  in the overall picture. So which scenarios and use cases do you see", "tokens":
  [51060, 1391, 312, 472, 295, 264, 6677, 294, 264, 4787, 3036, 13, 407, 597, 15077,
  293, 764, 3331, 360, 291, 536, 51404], "temperature": 0.0, "avg_logprob": -0.20942884502988873,
  "compression_ratio": 1.6213991769547325, "no_speech_prob": 0.03451335057616234},
  {"id": 182, "seek": 114724, "start": 1168.68, "end": 1175.4, "text": " where your
  platform can give value? Is it chat box? Is it vector search? Is it completely anything?",
  "tokens": [51436, 689, 428, 3663, 393, 976, 2158, 30, 1119, 309, 5081, 2424, 30,
  1119, 309, 8062, 3164, 30, 1119, 309, 2584, 1340, 30, 51772], "temperature": 0.0,
  "avg_logprob": -0.20942884502988873, "compression_ratio": 1.6213991769547325, "no_speech_prob":
  0.03451335057616234}, {"id": 183, "seek": 117540, "start": 1176.2, "end": 1183.24,
  "text": " In anywhere where embeddings are used, multi-model embeddings, my library
  will be like,", "tokens": [50404, 682, 4992, 689, 12240, 29432, 366, 1143, 11, 4825,
  12, 8014, 338, 12240, 29432, 11, 452, 6405, 486, 312, 411, 11, 50756], "temperature":
  0.0, "avg_logprob": -0.25034148352486746, "compression_ratio": 1.6926070038910506,
  "no_speech_prob": 0.009070082567632198}, {"id": 184, "seek": 117540, "start": 1183.24,
  "end": 1188.6000000000001, "text": " well, I want my library to be the infrastructure
  where people use different.", "tokens": [50756, 731, 11, 286, 528, 452, 6405, 281,
  312, 264, 6896, 689, 561, 764, 819, 13, 51024], "temperature": 0.0, "avg_logprob":
  -0.25034148352486746, "compression_ratio": 1.6926070038910506, "no_speech_prob":
  0.009070082567632198}, {"id": 185, "seek": 117540, "start": 1188.6000000000001,
  "end": 1190.0400000000002, "text": " Awesome. Yeah. Yeah. Yeah.", "tokens": [51024,
  10391, 13, 865, 13, 865, 13, 865, 13, 51096], "temperature": 0.0, "avg_logprob":
  -0.25034148352486746, "compression_ratio": 1.6926070038910506, "no_speech_prob":
  0.009070082567632198}, {"id": 186, "seek": 117540, "start": 1190.0400000000002,
  "end": 1194.52, "text": " Well, this sounds really cool. And I wish you all the
  best in this project.", "tokens": [51096, 1042, 11, 341, 3263, 534, 1627, 13, 400,
  286, 3172, 291, 439, 264, 1151, 294, 341, 1716, 13, 51320], "temperature": 0.0,
  "avg_logprob": -0.25034148352486746, "compression_ratio": 1.6926070038910506, "no_speech_prob":
  0.009070082567632198}, {"id": 187, "seek": 117540, "start": 1194.52, "end": 1199.72,
  "text": " Thank you. I hope that some of my listeners will go and check out and
  maybe you will even get", "tokens": [51320, 1044, 291, 13, 286, 1454, 300, 512,
  295, 452, 23274, 486, 352, 293, 1520, 484, 293, 1310, 291, 486, 754, 483, 51580],
  "temperature": 0.0, "avg_logprob": -0.25034148352486746, "compression_ratio": 1.6926070038910506,
  "no_speech_prob": 0.009070082567632198}, {"id": 188, "seek": 117540, "start": 1199.72,
  "end": 1204.2800000000002, "text": " some contributors or, you know, whoever users
  who can create the tickets.", "tokens": [51580, 512, 45627, 420, 11, 291, 458, 11,
  11387, 5022, 567, 393, 1884, 264, 12628, 13, 51808], "temperature": 0.0, "avg_logprob":
  -0.25034148352486746, "compression_ratio": 1.6926070038910506, "no_speech_prob":
  0.009070082567632198}, {"id": 189, "seek": 120428, "start": 1205.24, "end": 1210.84,
  "text": " Yeah. Yeah. I would love to see some issues. And, you know, even if you
  want to raise", "tokens": [50412, 865, 13, 865, 13, 286, 576, 959, 281, 536, 512,
  2663, 13, 400, 11, 291, 458, 11, 754, 498, 291, 528, 281, 5300, 50692], "temperature":
  0.0, "avg_logprob": -0.3032776184082031, "compression_ratio": 1.7228915662650603,
  "no_speech_prob": 0.023313026875257492}, {"id": 190, "seek": 120428, "start": 1210.84,
  "end": 1216.12, "text": " some issues, go ahead or add any feature, you can add
  it as a full request and we can take a look", "tokens": [50692, 512, 2663, 11, 352,
  2286, 420, 909, 604, 4111, 11, 291, 393, 909, 309, 382, 257, 1577, 5308, 293, 321,
  393, 747, 257, 574, 50956], "temperature": 0.0, "avg_logprob": -0.3032776184082031,
  "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.023313026875257492},
  {"id": 191, "seek": 120428, "start": 1216.12, "end": 1221.96, "text": " at it. We
  are really, really excited. And a lot of developers just feature to me, do I need
  to", "tokens": [50956, 412, 309, 13, 492, 366, 534, 11, 534, 2919, 13, 400, 257,
  688, 295, 8849, 445, 4111, 281, 385, 11, 360, 286, 643, 281, 51248], "temperature":
  0.0, "avg_logprob": -0.3032776184082031, "compression_ratio": 1.7228915662650603,
  "no_speech_prob": 0.023313026875257492}, {"id": 192, "seek": 120428, "start": 1221.96,
  "end": 1226.92, "text": " need no rest? No. Yeah. If you do not need to need no
  rest at all. Yeah.", "tokens": [51248, 643, 572, 1472, 30, 883, 13, 865, 13, 759,
  291, 360, 406, 643, 281, 643, 572, 1472, 412, 439, 13, 865, 13, 51496], "temperature":
  0.0, "avg_logprob": -0.3032776184082031, "compression_ratio": 1.7228915662650603,
  "no_speech_prob": 0.023313026875257492}, {"id": 193, "seek": 120428, "start": 1226.92,
  "end": 1231.48, "text": " So you can be, let''s say, writing the library and still
  can trigger it. Yeah.", "tokens": [51496, 407, 291, 393, 312, 11, 718, 311, 584,
  11, 3579, 264, 6405, 293, 920, 393, 7875, 309, 13, 865, 13, 51724], "temperature":
  0.0, "avg_logprob": -0.3032776184082031, "compression_ratio": 1.7228915662650603,
  "no_speech_prob": 0.023313026875257492}, {"id": 194, "seek": 123148, "start": 1231.48,
  "end": 1235.72, "text": " Oh, nice. Awesome. Maybe you can use chat GTP as well
  to convert your Python to rest,", "tokens": [50364, 876, 11, 1481, 13, 10391, 13,
  2704, 291, 393, 764, 5081, 460, 16804, 382, 731, 281, 7620, 428, 15329, 281, 1472,
  11, 50576], "temperature": 0.0, "avg_logprob": -0.31774391384299744, "compression_ratio":
  1.738396624472574, "no_speech_prob": 0.047574255615472794}, {"id": 195, "seek":
  123148, "start": 1235.72, "end": 1240.68, "text": " but that''s another story. Awesome.
  And I look forward to your presentation. I will not be", "tokens": [50576, 457,
  300, 311, 1071, 1657, 13, 10391, 13, 400, 286, 574, 2128, 281, 428, 5860, 13, 286,
  486, 406, 312, 50824], "temperature": 0.0, "avg_logprob": -0.31774391384299744,
  "compression_ratio": 1.738396624472574, "no_speech_prob": 0.047574255615472794},
  {"id": 196, "seek": 123148, "start": 1240.68, "end": 1246.52, "text": " there, but
  I will watch the recording and I will also link this episode and the recording of
  your", "tokens": [50824, 456, 11, 457, 286, 486, 1159, 264, 6613, 293, 286, 486,
  611, 2113, 341, 3500, 293, 264, 6613, 295, 428, 51116], "temperature": 0.0, "avg_logprob":
  -0.31774391384299744, "compression_ratio": 1.738396624472574, "no_speech_prob":
  0.047574255615472794}, {"id": 197, "seek": 123148, "start": 1246.52, "end": 1249.64,
  "text": " talk. Thank you. So good luck with that and thank you so much. Thank you.",
  "tokens": [51116, 751, 13, 1044, 291, 13, 407, 665, 3668, 365, 300, 293, 1309, 291,
  370, 709, 13, 1044, 291, 13, 51272], "temperature": 0.0, "avg_logprob": -0.31774391384299744,
  "compression_ratio": 1.738396624472574, "no_speech_prob": 0.047574255615472794},
  {"id": 198, "seek": 123148, "start": 1249.64, "end": 1252.52, "text": " Thank you
  so much. Enjoy it. Enjoy the content. Yeah. Thank you.", "tokens": [51272, 1044,
  291, 370, 709, 13, 15411, 309, 13, 15411, 264, 2701, 13, 865, 13, 1044, 291, 13,
  51416], "temperature": 0.0, "avg_logprob": -0.31774391384299744, "compression_ratio":
  1.738396624472574, "no_speech_prob": 0.047574255615472794}]'
---

Hello there, vector podcast and I'm here accompanied with Sonan. Sonan you are the, I guess, visitor of the conference. Are you also giving a talk? Yes, I'm giving a talk tomorrow on metric learning.
Yeah, what's your topic? I'm not talking metric learning tomorrow, but I'm very excited about what we are building at and better than anything on starlight. So yeah, awesome. And is it your first time at the conference? Yes, it's the first time, but that's one of the best conferences. Awesome.
Yeah, I love it. I've been here first time in 2011 and I still, I still love coming back once in a while. It's really good. I can see why you want to come back again and again. Yeah, exactly. Yeah. Awesome.
And you work mostly on what I, well, we had an episode actually with quadrants on metric learning. I will, I will make sure to link it. Tell me a bit more about metric learning if you will. Like in a, in a, why shouldn't everyone care that he seems to think that they should use maybe yes.
So a lot of people just think about like, you know, we can do a check distance and then you know, we'll get the similarity. But the thing is, even though you change the distance, it won't make any difference because those embeddings are already in the space. So it's already relative.
So if you're doing a co-science similarity, which I love pizza and I do not love pizza, that's your 90% similarity. Right? And to the other distance will not make any sense.
So the thing is with metric learning, you can build your own data set and then the train there when embedding model again for giving you right. Yeah.
I mean, I still try to understand it, but it's basically like, like on one hand, you have your data and then you choose the model and that model should be pre-trained for you, you could also fine tune it on your data if you want. And then inherently, it will have its own measure of similarity.
So it's not something you can easily control. Yeah. But then metric learning opposes this by saying that you should be in control of your metric. Yeah.
It's all your similarity measure, not just the metric itself, but the similarity measure, which means that I should kind of like drop the model, just get my data and start training some new network, right? So that I can find the basically fine tuning the embedding model.
What with your data? So yeah, suppose you're finding intense. Yes. Okay. Where does metric learning really shine? It's classification versus similarity again. If you are doing classification, you are limited up to certain classes, right? Suppose, yeah, particular intense. Yeah.
It's not scalable at like a million scale, you cannot keep adding adding addicts, but with similarity search and metric learning, you can add any intense, very keen solution. Yeah. Yeah. So it's not limited. Yeah.
That's one of the, you know, classical way to view that metric learning plays much, much better role at scale, and that's why vector database can scale this much. Sure. Yeah. And tell me a bit more about yourself.
How did you end up in this space? Like, what was your pet? I know you worked at Thrasa as well, which is also an open source project. Yes. I once looked at and but now you work for another company like, what was your journey? And yeah.
So I worked at as an AI researcher at Sama, so we were mostly in clinical trials. So, you know, Pfizer and the world is, it does this clinical trials for 10 to 12 years and we had like those massive data and we wanted to find out the subjects could drop out of the studies.
I also published paper before. That's well-versed in this AI research and AI area. Yeah. And then I joined Raza for conversation, AI, I love conversation, AI. And then I joined four friends recently and I got into this embedding space.
And now I have my own open source project called embedding a thing in which you can use very different multi-moder sources and structure sources, speed, you know, you get embed it in 40 x faster speed than any other presence by planes. Wow. How did you do that? That is rust. It's all available.
It's all open source because I have like a used supporter of open source. So what we do is we have built this cluster in rust from PDF while it is going towards embedding. So one of the analogy that I use most is embedding models are, yeah, they are like really, really cool.
They are becoming faster and everything. But if you want to drive a Porsche, would you like to drive it on a national highway for a road full of quadruples? So that's the analogy being used.
We are giving you a high for the price for driving your embedding model or Porsche, you know, in a very sophisticated and like, yeah, no tech depth, you call it by blind for embedding. Interesting. So you are basically building an infrastructure where or infrastructure for this is a very model.
So what as a user, what can I do on this project? Yeah, very good question. So we are very production ready. Yeah. And we do not use any kind of heavy library, right? They are lip torches.
So if you have to embed something, the first go on hugging phase, use sentence, transformers, and then you will download that 2.5 TV library and stuff like which will come with lip torches and stuff like that. Yeah. And we have removed all those dependents. All right. That's a good lighter. Yeah.
We have liked it. Yeah. But of candle from the hugging phase, because candle also uses rust and because we are also building rust, it's much easier to integrate with candle. So yeah, so it's much lighter, much faster, you know, way of creating.
What is candle? Candle is basically, basically, inference on GPU and CPU. Oh, I see. Yeah. Yeah. And it's also open source. Yeah, it's also. Okay. So you do everything unconventionally in a rust, even though everyone else is doing it in Python.
Because it's, you know, multi-treting is like so much embedded in rust. Like people will tell you that Python can also do multi-treting, but that's not too multi-treting because the global, global, and global, and global law. Yeah. And rust tells you mutable log.
So you can do like achieve a tool multi-treting just with rust. Yeah. They promised actually to solve geolproblem in Python next version. Yeah. They already are through them. Oh, wow. I don't know when it will materialize, but. Okay. And so, okay.
But if I look at it from the perspective, let's say, of building some product, being a chatbot or like search engine, you know, blend it with vector search, or something like that. So what is my typical sort of like, like, pipeline, how does it look like? Right.
So what will I do? Let's say I have my data. And then maybe I've chosen a model, but that model is okay. Maybe it's not the fastest one.
What should I do? Will I turn to your platform to speed it up? Will I turn to your platform to do some other things as well? So we are not doing any changes in the model itself. We are not quantizing.
Even though we can use those models, so candle gives you a certain list of models that you can use and create with us. Yeah. Basically. So whatever candle supports, we support. Yeah. Whatever candle doesn't support, we cannot support because we are basically dependent of them. Yeah.
So if and we are not doing anything in the model itself, we are doing it on the extraction in parsing part of the data. Right. If you have different videos, different MDs, I will extract junk and parse them. And then build this like extra fast. Yeah. Yeah.
And then let's say if I want to go to production, but I also have some other components which maybe you wouldn't integrate, right? I know my search cluster and something else. My services. So can I also go to production with your platform? Yeah.
Like, how will it look like exactly? Is it a docker? Is it the unit? It's you do not need to first of all code in Rust. A lot of developers come out to reach me and like, you know, they ask, do I need to know Rust to contribute to embed anything? I'm like, no, you do not need.
We have, because let's fire. We have like worked like for building this wrapper around Rust so that you know, you can easily create it with Python. Oh, so you have a Python wrapper of your own. Yeah. You only need to know Python. You do not need to know Rust at all. How interesting. Yeah.
So do you have any like instances where companies have already built POCs with your platform? Or do you already have someone going to production? Yeah. So I get so many requests on like acquisition part of things and stuff like that.
But we are like, you know, it's we are one my whole company and we have nothing company project. But we have got six key downloads, but we have gotten to production yet. But hopefully next two, three months. Nice. And do you need any help from the community? Yes.
If you're interested in building the infrastructure for UnityBI and Rust, Python. So to connect with us, we're left to have you on board. All right. But let's go back a little bit. So you also said that there is multi-modality element of it. Yeah.
So I will tell you the way I see it, but please correct me or augment me. So I think a couple of years ago, two years ago, we gave a talk here at Berlin Bosnol. It's basically showing a system where you can search images and text, whatever you want.
And if you have images that do not have textual metadata, then that's your gateway into finding these images because neural networks will understand and extract the content using plebe, right? Yeah. And so we were able to show some really interesting examples.
For example, you could find in the context of the commerce, a long sleeveless dress, striped, whatever color and so on and so forth. And it worked. And even some audience members asked us to demo on their queries and it still worked.
So that showed the power of multi-modality, right? And we didn't even need to fine-tune delete it. It was just out of the box. But I guess the reason for it to multi-modality, what else are you thinking that's part of your blog? Great question.
Even though images like Clare is known for multi-modality research, one of the best use cases of Clare is when you're doing the need of short classification, right? It doesn't need the previous data at all, even if it is searching images, if it is searching through text.
And it's like so powerful, right? So we have a different example with it. But coming to a question, we have audio, wanted to embed audio graphs, et cetera. So all these are in five times. But right now, we are only embedding text and images. And are you using Clip? Yeah, we're using Clip.
Clip, that's right. You know, that one thing that you cannot get. I also wanted to ask you a bit if you may share your insight on evaluating this system.
So one of the feedbacks that I have gotten for, for IoT, between or anything, like basically, so let's say I have my LLN based application, you know, how do I evaluate it? Because one of the feedbacks is that sometimes it gives perfect results, sometimes it gives awful results, right?
So now there is nothing in between, right? Or not barely.
So how would you solve this? Of course, you do start with your metric learning and some other techniques, right? But there is still the other side of things when you go to production, as you know, like in Rasa and Quadrant and many other companies, you care about quality.
So how do you have any insight on that? Are you maybe planning to build something along the lines of evaluation? That's a great question. You know, but great part of the great response to it is, so LLN is one of the examples of that.
So, you know, LLN gives you a bright answer, but it also gives you hallucination. But a lot of people see hallucination as a bug, but I see it as a feature, because it won't be able to do that creative job, but it can do with hallucinations.
One of the, there are so many tools, right, to measure retrieval part like Braggas, Prometheus, right? And there are so many tools, too.
But still, I think recall measure, what we call it, like, you know, measure how LLN recall is working, where basically extracting most relevant information, not like rubbish information.
So those things are like really important, and a lot of research is going on, but we are more like focused on the infrastructure, and we are keeping it up, trying to keep it up, but yeah, so mostly I would go for classical testing ways like precision, recall, yeah.
But basically like, okay, you do test, and you see that sometimes once in a while it fails. So first of all, of course, catching that is important, right? People are going to production. Yes, but what is your way backwards to fixing this? Thank you, Chef. Yeah, from from from finding that bug.
Okay, let me think about that.
So, data set, maybe you can give some example where you have fixed a ratio, you know, reported by someone, not necessarily as part of your platform, but previously, I don't know, what have I done? Yeah, so I was working with this, that's why this talk came into my mind, right?
The negation problem, the negation problem is so huge.
You will always find sentences with not every domain, read biomedical, read law and everything, and it still gives you the same similarity, even though you do not have to be a language, you know, expert to understand these. Different things. Yeah, yeah, yeah.
So different things, right? That's that's when the trick learning comes, that's when inference started to come into mind, because inference is very important.
Like a lot of people have played with SNLI and stuff, and then they understand that to understand negation, you first need to understand inferences.
So there's a way to like, method, right? Yeah, yeah, yeah, and entailment, contradiction, neutral, two sentences could be neutral, yeah, unrelated to each other, two centers could be contradictory, contradictory to each other.
So, yeah, which means that you need a purpose of data somehow labeled, yes, yes, yes, logically reasoned through, right, using an algorithm.
Yes, so that's that's what SNLI is, like, SNLI is a data set, particularly for this, yeah, particular questions, problems, so yeah, if you, you've re-tune it with, it was fun. So that would be for text, and what about other modalities? I don't think it was, that's what it was.
Like images, I know sometimes a model may hallucinate that there is something in the real world, but there is nothing like that. Oh, that's one thing, but I guess there are many.
So there are things like, there's a study called blind pairs in clip, that was done by, I'm sorry, I forgot the name, which were like, people say find that.
Yeah, so they found out, clip actually has blind pairs, like you cannot segment things really well, like, cats sleeping on the, on the car, or something, and then something else will give you the same description or something. So there, Dino comes in.
So Dino there's a segmentation with self-supervised learning, self-supervised learning, I think is the best invention of like, for this AI, and now it thumbs up. Yeah, in the open source. That's supervised learning. Is it open source? A Dino, you say? Yeah, it is.
Yeah, it's from Meta, and I think Likun is one of, and when, you know, he has written this folder, what you call it, white paper on the dark matter of intelligence that is self-supervised learning. So they are doing a lot of work in self-supervised learning.
You know, make data, make the model learn from the data itself. You do not need to label it. Yeah, that's like in the self-supervised sort of.
Okay, then maybe another question I have is, where do you embed a human in this process? Do you ever, like, I don't know, to check quality or give feedback? Exactly. So one other thing in metric learning is everyone thinks it's self-supervised, or data, it will learn with the data.
It doesn't need label. But when the contrastive learning happens, who is making that negative mining fare? It's the huge, it's the kind of making that negative one. Very, very crisp one, right? Exactly, where does it is learning? Not just random negative, but like synonymically negative.
Thimmedically negative. Yeah. So there, they mean very common and like, you know. For sure. Yeah. So, and so today, let's say if someone wants to use your platform, you said, embedding and embedding. Embedding thing. Embedding thing. It's on GitHub, I'm guessing. We'll link it. Yeah.
And so how do you, like, part of the story that becomes successful, I guess, is that you can map out your path from use cases to your library, to your project, which would probably be one of the components in the overall picture.
So which scenarios and use cases do you see where your platform can give value? Is it chat box? Is it vector search? Is it completely anything?
In anywhere where embeddings are used, multi-model embeddings, my library will be like, well, I want my library to be the infrastructure where people use different.
Awesome. Yeah. Yeah. Yeah. Well, this sounds really cool. And I wish you all the best in this project. Thank you. I hope that some of my listeners will go and check out and maybe you will even get some contributors or, you know, whoever users who can create the tickets. Yeah. Yeah.
I would love to see some issues. And, you know, even if you want to raise some issues, go ahead or add any feature, you can add it as a full request and we can take a look at it. We are really, really excited. And a lot of developers just feature to me, do I need to need no rest? No. Yeah.
If you do not need to need no rest at all. Yeah. So you can be, let's say, writing the library and still can trigger it. Yeah. Oh, nice. Awesome. Maybe you can use chat GTP as well to convert your Python to rest, but that's another story. Awesome. And I look forward to your presentation.
I will not be there, but I will watch the recording and I will also link this episode and the recording of your talk. Thank you. So good luck with that and thank you so much. Thank you. Thank you so much. Enjoy it. Enjoy the content. Yeah. Thank you.