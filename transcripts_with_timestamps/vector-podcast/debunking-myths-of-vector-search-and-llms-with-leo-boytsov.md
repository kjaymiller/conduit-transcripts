---
description: '<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=0s">00:00</a>
  Intro</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=91s">01:31</a>
  Leo''s story</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=599s">09:59</a>
  SPLADE: single model to solve both dense and sparse?</p><p><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=1266s">21:06</a>
  DeepImpact</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=1798s">29:58</a>
  NMSLIB: what are non-metric spaces</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2061s">34:21</a>
  How HNSW and NMSLIB joined forces</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2471s">41:11</a>
  Why FAISS did not choose NMSLIB''s algorithm</p><p><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2616s">43:36</a>
  Serendipity of discovery and the creation of industries</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2826s">47:06</a>
  Vector Search: intellectually rewarding, professionally undervalued</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=3157s">52:37</a>
  Why RDBMS Still Struggles with Scalable Vector and Free-Text Search</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=3616s">1:00:16</a>
  Leo''s recent favorite papers</p><p></p><ul><li>Leo Boytsov on LinkedIn: <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.linkedin.com/in/leonidboytsov/">https://www.linkedin.com/in/leonidboytsov/</a>
  and X: <a target="_blank" rel="noopener noreferrer nofollow" href="https://x.com/srchvrs">https://x.com/srchvrs</a></li><li>Leo
  Boytsovâ€™s paper list: <a target="_blank" rel="noopener noreferrer nofollow" href="https://scholar.google.com/citations?hl=en&amp;user=I79y2i4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">https://scholar.google.com/citations?hl=en&amp;user=I79y2i4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate</a></li></ul><p></p><p>Lots
  of papers and other material from Leo: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk">https://www.youtube.com/watch?v=gzWErcOXIKk</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20250117_030137_0d0e98d093e79861b9dc85d445adcf1e.png
pub_date: Fri, 17 Jan 2025 15:07:51 GMT
title: Debunking myths of vector search and LLMs with Leo Boytsov
url: https://rss.com/podcasts/vector-podcast/1852660
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 22.6, "text": " Hi everyone,
  Vector Podcast is back with still with season three and I''m super excited", "tokens":
  [50364, 2421, 1518, 11, 691, 20814, 29972, 307, 646, 365, 920, 365, 3196, 1045,
  293, 286, 478, 1687, 2919, 51494], "temperature": 0.0, "avg_logprob": -0.2913332939147949,
  "compression_ratio": 1.3255813953488371, "no_speech_prob": 0.21869045495986938},
  {"id": 1, "seek": 0, "start": 22.6, "end": 27.28, "text": " to be talking to my
  guests today and there is a connection with this episode between", "tokens": [51494,
  281, 312, 1417, 281, 452, 9804, 965, 293, 456, 307, 257, 4984, 365, 341, 3500, 1296,
  51728], "temperature": 0.0, "avg_logprob": -0.2913332939147949, "compression_ratio":
  1.3255813953488371, "no_speech_prob": 0.21869045495986938}, {"id": 2, "seek": 2728,
  "start": 27.28, "end": 34.28, "text": " this episode and the episode that we recorded
  with Yuri Malkov about one of the most famous", "tokens": [50364, 341, 3500, 293,
  264, 3500, 300, 321, 8287, 365, 33901, 376, 667, 5179, 466, 472, 295, 264, 881,
  4618, 50714], "temperature": 0.0, "avg_logprob": -0.3528098649876092, "compression_ratio":
  1.5443037974683544, "no_speech_prob": 0.2689688205718994}, {"id": 3, "seek": 2728,
  "start": 34.28, "end": 40.400000000000006, "text": " and popular vector search algorithm
  I can ask WL and I''m talking today with Leo Boyzov,", "tokens": [50714, 293, 3743,
  8062, 3164, 9284, 286, 393, 1029, 343, 43, 293, 286, 478, 1417, 965, 365, 19344,
  9486, 89, 5179, 11, 51020], "temperature": 0.0, "avg_logprob": -0.3528098649876092,
  "compression_ratio": 1.5443037974683544, "no_speech_prob": 0.2689688205718994},
  {"id": 4, "seek": 2728, "start": 40.400000000000006, "end": 49.36, "text": " who
  is the senior research scientist at AWS and he is also a co-author of Animesleab
  and Animesleab", "tokens": [51020, 567, 307, 264, 7965, 2132, 12662, 412, 17650,
  293, 415, 307, 611, 257, 598, 12, 34224, 295, 1107, 1532, 306, 455, 293, 1107, 1532,
  306, 455, 51468], "temperature": 0.0, "avg_logprob": -0.3528098649876092, "compression_ratio":
  1.5443037974683544, "no_speech_prob": 0.2689688205718994}, {"id": 5, "seek": 2728,
  "start": 49.36, "end": 55.44, "text": " is today used at Open Search and probably
  some other places that I actually don''t know", "tokens": [51468, 307, 965, 1143,
  412, 7238, 17180, 293, 1391, 512, 661, 3190, 300, 286, 767, 500, 380, 458, 51772],
  "temperature": 0.0, "avg_logprob": -0.3528098649876092, "compression_ratio": 1.5443037974683544,
  "no_speech_prob": 0.2689688205718994}, {"id": 6, "seek": 5544, "start": 55.44, "end":
  64.96, "text": " and I hope to learn it as well today. This is just exciting and
  I think goes without saying", "tokens": [50364, 293, 286, 1454, 281, 1466, 309,
  382, 731, 965, 13, 639, 307, 445, 4670, 293, 286, 519, 1709, 1553, 1566, 50840],
  "temperature": 0.0, "avg_logprob": -0.1196063504074559, "compression_ratio": 1.5265957446808511,
  "no_speech_prob": 0.010003809817135334}, {"id": 7, "seek": 5544, "start": 64.96,
  "end": 72.47999999999999, "text": " that the whole field stands on the work done
  by people like Leo and Yuri and others who actually", "tokens": [50840, 300, 264,
  1379, 2519, 7382, 322, 264, 589, 1096, 538, 561, 411, 19344, 293, 33901, 293, 2357,
  567, 767, 51216], "temperature": 0.0, "avg_logprob": -0.1196063504074559, "compression_ratio":
  1.5265957446808511, "no_speech_prob": 0.010003809817135334}, {"id": 8, "seek": 5544,
  "start": 72.47999999999999, "end": 79.75999999999999, "text": " develop the core
  algorithms and popularize them, improve them over time and then the story unfolds",
  "tokens": [51216, 1499, 264, 4965, 14642, 293, 3743, 1125, 552, 11, 3470, 552, 670,
  565, 293, 550, 264, 1657, 17980, 82, 51580], "temperature": 0.0, "avg_logprob":
  -0.1196063504074559, "compression_ratio": 1.5265957446808511, "no_speech_prob":
  0.010003809817135334}, {"id": 9, "seek": 7976, "start": 80.56, "end": 88.0, "text":
  " from there. Hi Leo, how you doing? Hi, thank you for introducing me, it''s a great
  pleasure", "tokens": [50404, 490, 456, 13, 2421, 19344, 11, 577, 291, 884, 30, 2421,
  11, 1309, 291, 337, 15424, 385, 11, 309, 311, 257, 869, 6834, 50776], "temperature":
  0.0, "avg_logprob": -0.21352429707845053, "compression_ratio": 1.4973544973544974,
  "no_speech_prob": 0.020247841253876686}, {"id": 10, "seek": 7976, "start": 88.0,
  "end": 93.92, "text": " to be able to podcast. Yeah, it''s my pleasure as well to
  have you. Traditionally we start with", "tokens": [50776, 281, 312, 1075, 281, 7367,
  13, 865, 11, 309, 311, 452, 6834, 382, 731, 281, 362, 291, 13, 22017, 15899, 321,
  722, 365, 51072], "temperature": 0.0, "avg_logprob": -0.21352429707845053, "compression_ratio":
  1.4973544973544974, "no_speech_prob": 0.020247841253876686}, {"id": 11, "seek":
  7976, "start": 93.92, "end": 101.84, "text": " the background. Can you say in a
  few words your background maybe how you got here and what''s your", "tokens": [51072,
  264, 3678, 13, 1664, 291, 584, 294, 257, 1326, 2283, 428, 3678, 1310, 577, 291,
  658, 510, 293, 437, 311, 428, 51468], "temperature": 0.0, "avg_logprob": -0.21352429707845053,
  "compression_ratio": 1.4973544973544974, "no_speech_prob": 0.020247841253876686},
  {"id": 12, "seek": 10184, "start": 101.84, "end": 112.80000000000001, "text": "
  story in search vector search and maybe LLM? Yeah, sure, yeah so background is pretty
  long.", "tokens": [50364, 1657, 294, 3164, 8062, 3164, 293, 1310, 441, 43, 44, 30,
  865, 11, 988, 11, 1338, 370, 3678, 307, 1238, 938, 13, 50912], "temperature": 0.0,
  "avg_logprob": -0.3075940673415725, "compression_ratio": 1.385786802030457, "no_speech_prob":
  0.006675052922219038}, {"id": 13, "seek": 10184, "start": 116.0, "end": 122.32000000000001,
  "text": " So I''ve had a rather long career, honestly. Well in my current capacity
  as you mentioned,", "tokens": [51072, 407, 286, 600, 632, 257, 2831, 938, 3988,
  11, 6095, 13, 1042, 294, 452, 2190, 6042, 382, 291, 2835, 11, 51388], "temperature":
  0.0, "avg_logprob": -0.3075940673415725, "compression_ratio": 1.385786802030457,
  "no_speech_prob": 0.006675052922219038}, {"id": 14, "seek": 10184, "start": 122.32000000000001,
  "end": 129.36, "text": " I am a scientist at WSAA labs. For one year I was working
  on co-generation about this year,", "tokens": [51388, 286, 669, 257, 12662, 412,
  343, 50, 5265, 20339, 13, 1171, 472, 1064, 286, 390, 1364, 322, 598, 12, 30372,
  466, 341, 1064, 11, 51740], "temperature": 0.0, "avg_logprob": -0.3075940673415725,
  "compression_ratio": 1.385786802030457, "no_speech_prob": 0.006675052922219038},
  {"id": 15, "seek": 12936, "start": 129.36, "end": 135.28, "text": " earlier this
  year I moved to a Q-console team, Q-console team works on question and", "tokens":
  [50364, 3071, 341, 1064, 286, 4259, 281, 257, 1249, 12, 1671, 9481, 1469, 11, 1249,
  12, 1671, 9481, 1469, 1985, 322, 1168, 293, 50660], "temperature": 0.0, "avg_logprob":
  -0.32064435389134793, "compression_ratio": 1.4736842105263157, "no_speech_prob":
  0.013303917832672596}, {"id": 16, "seek": 12936, "start": 135.28, "end": 143.12,
  "text": " switch at bots that answers questions about various AWS services. So we
  can ask like, I don''t know,", "tokens": [50660, 3679, 412, 35410, 300, 6338, 1651,
  466, 3683, 17650, 3328, 13, 407, 321, 393, 1029, 411, 11, 286, 500, 380, 458, 11,
  51052], "temperature": 0.0, "avg_logprob": -0.32064435389134793, "compression_ratio":
  1.4736842105263157, "no_speech_prob": 0.013303917832672596}, {"id": 17, "seek":
  12936, "start": 143.12, "end": 152.08, "text": " it''s like where''s my EC2 instance
  things like that and how I set up things. But I have to make a", "tokens": [51052,
  309, 311, 411, 689, 311, 452, 19081, 17, 5197, 721, 411, 300, 293, 577, 286, 992,
  493, 721, 13, 583, 286, 362, 281, 652, 257, 51500], "temperature": 0.0, "avg_logprob":
  -0.32064435389134793, "compression_ratio": 1.4736842105263157, "no_speech_prob":
  0.013303917832672596}, {"id": 18, "seek": 15208, "start": 152.08, "end": 160.64000000000001,
  "text": " disclaimer that today I do not speak on behalf of AWS and I cannot talk
  in details about my work", "tokens": [50364, 40896, 300, 965, 286, 360, 406, 1710,
  322, 9490, 295, 17650, 293, 286, 2644, 751, 294, 4365, 466, 452, 589, 50792], "temperature":
  0.0, "avg_logprob": -0.12685412710363214, "compression_ratio": 1.5833333333333333,
  "no_speech_prob": 0.007998749613761902}, {"id": 19, "seek": 15208, "start": 160.64000000000001,
  "end": 170.24, "text": " there. So as I said I had a really relatively long career.
  Yeah, so most of nearly all of my life,", "tokens": [50792, 456, 13, 407, 382, 286,
  848, 286, 632, 257, 534, 7226, 938, 3988, 13, 865, 11, 370, 881, 295, 6217, 439,
  295, 452, 993, 11, 51272], "temperature": 0.0, "avg_logprob": -0.12685412710363214,
  "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.007998749613761902},
  {"id": 20, "seek": 15208, "start": 170.24, "end": 175.92000000000002, "text": "
  I have been a computer science geek with a passion for building cool stuff and solving
  hard", "tokens": [51272, 286, 362, 668, 257, 3820, 3497, 36162, 365, 257, 5418,
  337, 2390, 1627, 1507, 293, 12606, 1152, 51556], "temperature": 0.0, "avg_logprob":
  -0.12685412710363214, "compression_ratio": 1.5833333333333333, "no_speech_prob":
  0.007998749613761902}, {"id": 21, "seek": 15208, "start": 175.92000000000002, "end":
  182.0, "text": " problems. Yet my professional career started in rather mundane
  fashion. So I started working", "tokens": [51556, 2740, 13, 10890, 452, 4843, 3988,
  1409, 294, 2831, 43497, 6700, 13, 407, 286, 1409, 1364, 51860], "temperature": 0.0,
  "avg_logprob": -0.12685412710363214, "compression_ratio": 1.5833333333333333, "no_speech_prob":
  0.007998749613761902}, {"id": 22, "seek": 18200, "start": 182.24, "end": 189.44,
  "text": " client and service of where for financial systems. This was not my favorite
  subject, but pretty much", "tokens": [50376, 6423, 293, 2643, 295, 689, 337, 4669,
  3652, 13, 639, 390, 406, 452, 2954, 3983, 11, 457, 1238, 709, 50736], "temperature":
  0.0, "avg_logprob": -0.17028651180037532, "compression_ratio": 1.5355648535564854,
  "no_speech_prob": 0.002024303423240781}, {"id": 23, "seek": 18200, "start": 189.44,
  "end": 195.52, "text": " the only one that was paid reasonably well at the time.
  So I had to do a lot of front end and", "tokens": [50736, 264, 787, 472, 300, 390,
  4835, 23551, 731, 412, 264, 565, 13, 407, 286, 632, 281, 360, 257, 688, 295, 1868,
  917, 293, 51040], "temperature": 0.0, "avg_logprob": -0.17028651180037532, "compression_ratio":
  1.5355648535564854, "no_speech_prob": 0.002024303423240781}, {"id": 24, "seek":
  18200, "start": 195.52, "end": 203.76, "text": " back end engineering using various
  SQL databases. I was not satisfied with my career, but", "tokens": [51040, 646,
  917, 7043, 1228, 3683, 19200, 22380, 13, 286, 390, 406, 11239, 365, 452, 3988, 11,
  457, 51452], "temperature": 0.0, "avg_logprob": -0.17028651180037532, "compression_ratio":
  1.5355648535564854, "no_speech_prob": 0.002024303423240781}, {"id": 25, "seek":
  18200, "start": 204.4, "end": 209.44, "text": " luckily I got really interested
  in algorithms, in particular retrieval algorithms.", "tokens": [51484, 22880, 286,
  658, 534, 3102, 294, 14642, 11, 294, 1729, 19817, 3337, 14642, 13, 51736], "temperature":
  0.0, "avg_logprob": -0.17028651180037532, "compression_ratio": 1.5355648535564854,
  "no_speech_prob": 0.002024303423240781}, {"id": 26, "seek": 20944, "start": 210.4,
  "end": 216.0, "text": " So I started working on this topic with the algorithms first
  part time, then full time.", "tokens": [50412, 407, 286, 1409, 1364, 322, 341, 4829,
  365, 264, 14642, 700, 644, 565, 11, 550, 1577, 565, 13, 50692], "temperature": 0.0,
  "avg_logprob": -0.24250221252441406, "compression_ratio": 1.6355140186915889, "no_speech_prob":
  0.005886485800147057}, {"id": 27, "seek": 20944, "start": 216.96, "end": 223.6,
  "text": " But largely as a software engineer, less as a researcher. And as a software
  engineer, work", "tokens": [50740, 583, 11611, 382, 257, 4722, 11403, 11, 1570,
  382, 257, 21751, 13, 400, 382, 257, 4722, 11403, 11, 589, 51072], "temperature":
  0.0, "avg_logprob": -0.24250221252441406, "compression_ratio": 1.6355140186915889,
  "no_speech_prob": 0.005886485800147057}, {"id": 28, "seek": 20944, "start": 223.6,
  "end": 231.12, "text": " for various companies, including two tiny startups in the
  Russian search engine and the Yandex.", "tokens": [51072, 337, 3683, 3431, 11, 3009,
  732, 5870, 28041, 294, 264, 7220, 3164, 2848, 293, 264, 398, 474, 3121, 13, 51448],
  "temperature": 0.0, "avg_logprob": -0.24250221252441406, "compression_ratio": 1.6355140186915889,
  "no_speech_prob": 0.005886485800147057}, {"id": 29, "seek": 20944, "start": 232.24,
  "end": 237.36, "text": " So later I moved to the United States and work on the search
  engine PubMed,", "tokens": [51504, 407, 1780, 286, 4259, 281, 264, 2824, 3040, 293,
  589, 322, 264, 3164, 2848, 21808, 42954, 11, 51760], "temperature": 0.0, "avg_logprob":
  -0.24250221252441406, "compression_ratio": 1.6355140186915889, "no_speech_prob":
  0.005886485800147057}, {"id": 30, "seek": 23736, "start": 237.52, "end": 244.24,
  "text": " International Center of Biotechnology, information. First again, that
  was a common topic in my career,", "tokens": [50372, 9157, 5169, 295, 13007, 43594,
  11, 1589, 13, 2386, 797, 11, 300, 390, 257, 2689, 4829, 294, 452, 3988, 11, 50708],
  "temperature": 0.0, "avg_logprob": -0.41284378715183423, "compression_ratio": 1.4919354838709677,
  "no_speech_prob": 0.005691992584615946}, {"id": 31, "seek": 23736, "start": 244.24,
  "end": 250.24, "text": " started working with I was doing a lot of front end development.
  But about the class,", "tokens": [50708, 1409, 1364, 365, 286, 390, 884, 257, 688,
  295, 1868, 917, 3250, 13, 583, 466, 264, 1508, 11, 51008], "temperature": 0.0, "avg_logprob":
  -0.41284378715183423, "compression_ratio": 1.4919354838709677, "no_speech_prob":
  0.005691992584615946}, {"id": 32, "seek": 23736, "start": 250.24, "end": 258.40000000000003,
  "text": " 40 years I worked primarily on the T-Roll, the core engine. In particular,
  I invented a pretty", "tokens": [51008, 3356, 924, 286, 2732, 10029, 322, 264, 314,
  12, 49, 1833, 11, 264, 4965, 2848, 13, 682, 1729, 11, 286, 14479, 257, 1238, 51416],
  "temperature": 0.0, "avg_logprob": -0.41284378715183423, "compression_ratio": 1.4919354838709677,
  "no_speech_prob": 0.005691992584615946}, {"id": 33, "seek": 23736, "start": 258.40000000000003,
  "end": 266.0, "text": " need to speed up weighted bull in the T-Roll. And around
  the time I also realized that", "tokens": [51416, 643, 281, 3073, 493, 32807, 4693,
  294, 264, 314, 12, 49, 1833, 13, 400, 926, 264, 565, 286, 611, 5334, 300, 51796],
  "temperature": 0.0, "avg_logprob": -0.41284378715183423, "compression_ratio": 1.4919354838709677,
  "no_speech_prob": 0.005691992584615946}, {"id": 34, "seek": 26600, "start": 266.72,
  "end": 273.6, "text": " it would be hard to get to the search position without a
  good degree. So that motivated me to", "tokens": [50400, 309, 576, 312, 1152, 281,
  483, 281, 264, 3164, 2535, 1553, 257, 665, 4314, 13, 407, 300, 14515, 385, 281,
  50744], "temperature": 0.0, "avg_logprob": -0.18076391623053753, "compression_ratio":
  1.4170854271356783, "no_speech_prob": 0.007606916129589081}, {"id": 35, "seek":
  26600, "start": 274.4, "end": 281.44, "text": " apply a bunch of universities and
  eventually I got accepted by Carnegie Mell, which was a huge", "tokens": [50784,
  3079, 257, 3840, 295, 11779, 293, 4728, 286, 658, 9035, 538, 47301, 376, 898, 11,
  597, 390, 257, 2603, 51136], "temperature": 0.0, "avg_logprob": -0.18076391623053753,
  "compression_ratio": 1.4170854271356783, "no_speech_prob": 0.007606916129589081},
  {"id": 36, "seek": 26600, "start": 281.44, "end": 292.08, "text": " lock. But yeah,
  so I did my PhD studies there. And during these studies, I worked on a mix of",
  "tokens": [51136, 4017, 13, 583, 1338, 11, 370, 286, 630, 452, 14476, 5313, 456,
  13, 400, 1830, 613, 5313, 11, 286, 2732, 322, 257, 2890, 295, 51668], "temperature":
  0.0, "avg_logprob": -0.18076391623053753, "compression_ratio": 1.4170854271356783,
  "no_speech_prob": 0.007606916129589081}, {"id": 37, "seek": 29208, "start": 292.08,
  "end": 299.12, "text": " machine learning and algorithm algorithms without any deep
  learning. So the vector search or", "tokens": [50364, 3479, 2539, 293, 9284, 14642,
  1553, 604, 2452, 2539, 13, 407, 264, 8062, 3164, 420, 50716], "temperature": 0.0,
  "avg_logprob": -0.32173025608062744, "compression_ratio": 1.6127167630057804, "no_speech_prob":
  0.001641846145503223}, {"id": 38, "seek": 29208, "start": 299.12, "end": 306.56,
  "text": " rather similarity search was a part of my graduate studies. So yeah, I
  didn''t use any deep learning", "tokens": [50716, 2831, 32194, 3164, 390, 257, 644,
  295, 452, 8080, 5313, 13, 407, 1338, 11, 286, 994, 380, 764, 604, 2452, 2539, 51088],
  "temperature": 0.0, "avg_logprob": -0.32173025608062744, "compression_ratio": 1.6127167630057804,
  "no_speech_prob": 0.001641846145503223}, {"id": 39, "seek": 29208, "start": 306.56,
  "end": 312.56, "text": " though. It was a mix of classical machine learning, VortoVex
  style neural networks and", "tokens": [51088, 1673, 13, 467, 390, 257, 2890, 295,
  13735, 3479, 2539, 11, 691, 477, 78, 53, 3121, 3758, 18161, 9590, 293, 51388], "temperature":
  0.0, "avg_logprob": -0.32173025608062744, "compression_ratio": 1.6127167630057804,
  "no_speech_prob": 0.001641846145503223}, {"id": 40, "seek": 31256, "start": 313.28000000000003,
  "end": 320.0, "text": " digital. So what is an interesting part of that story is
  that my advisor, Eric Nyberg,", "tokens": [50400, 4562, 13, 407, 437, 307, 364,
  1880, 644, 295, 300, 1657, 307, 300, 452, 19161, 11, 9336, 29214, 6873, 11, 50736],
  "temperature": 0.0, "avg_logprob": -0.3530677448619496, "compression_ratio": 1.4977578475336324,
  "no_speech_prob": 0.004985547158867121}, {"id": 41, "seek": 31256, "start": 320.64,
  "end": 325.92, "text": " he worked on question answering. And together with his
  theory and his", "tokens": [50768, 415, 2732, 322, 1168, 13430, 13, 400, 1214, 365,
  702, 5261, 293, 702, 51032], "temperature": 0.0, "avg_logprob": -0.3530677448619496,
  "compression_ratio": 1.4977578475336324, "no_speech_prob": 0.004985547158867121},
  {"id": 42, "seek": 31256, "start": 325.92, "end": 332.64, "text": " participated
  in development of IBM Watson, that''s an amazing trivia playing system that", "tokens":
  [51032, 17978, 294, 3250, 295, 23487, 25640, 11, 300, 311, 364, 2243, 48770, 2433,
  1185, 300, 51368], "temperature": 0.0, "avg_logprob": -0.3530677448619496, "compression_ratio":
  1.4977578475336324, "no_speech_prob": 0.004985547158867121}, {"id": 43, "seek":
  31256, "start": 334.32, "end": 341.2, "text": " 2011 defeated human champions. So
  that was like one reason why I chose my advisor. It was", "tokens": [51452, 10154,
  15563, 1952, 11230, 13, 407, 300, 390, 411, 472, 1778, 983, 286, 5111, 452, 19161,
  13, 467, 390, 51796], "temperature": 0.0, "avg_logprob": -0.3530677448619496, "compression_ratio":
  1.4977578475336324, "no_speech_prob": 0.004985547158867121}, {"id": 44, "seek":
  34120, "start": 341.2, "end": 347.36, "text": " like such a cool topic to choose.
  But pretty quickly I learned about the system and realized,", "tokens": [50364,
  411, 1270, 257, 1627, 4829, 281, 2826, 13, 583, 1238, 2661, 286, 3264, 466, 264,
  1185, 293, 5334, 11, 50672], "temperature": 0.0, "avg_logprob": -0.19147655528078797,
  "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0015903604216873646},
  {"id": 45, "seek": 34120, "start": 347.36, "end": 353.59999999999997, "text": "
  oh, like it''s actually really just not just but it''s largely such engine on steroids.
  So", "tokens": [50672, 1954, 11, 411, 309, 311, 767, 534, 445, 406, 445, 457, 309,
  311, 11611, 1270, 2848, 322, 45717, 13, 407, 50984], "temperature": 0.0, "avg_logprob":
  -0.19147655528078797, "compression_ratio": 1.5932203389830508, "no_speech_prob":
  0.0015903604216873646}, {"id": 46, "seek": 34120, "start": 353.59999999999997, "end":
  362.15999999999997, "text": " Retrieval, IBM Watson, I have a blog post about that
  if anybody is interested. But then Retrieval,", "tokens": [50984, 11495, 5469, 3337,
  11, 23487, 25640, 11, 286, 362, 257, 6968, 2183, 466, 300, 498, 4472, 307, 3102,
  13, 583, 550, 11495, 5469, 3337, 11, 51412], "temperature": 0.0, "avg_logprob":
  -0.19147655528078797, "compression_ratio": 1.5932203389830508, "no_speech_prob":
  0.0015903604216873646}, {"id": 47, "seek": 34120, "start": 362.15999999999997, "end":
  368.8, "text": " it''s basically really Retrieval based extractive question answering
  systems. So if you want to", "tokens": [51412, 309, 311, 1936, 534, 11495, 5469,
  3337, 2361, 8947, 488, 1168, 13430, 3652, 13, 407, 498, 291, 528, 281, 51744], "temperature":
  0.0, "avg_logprob": -0.19147655528078797, "compression_ratio": 1.5932203389830508,
  "no_speech_prob": 0.0015903604216873646}, {"id": 48, "seek": 36880, "start": 368.8,
  "end": 374.32, "text": " improve question answering, you need to improve Retrieval.
  So that''s how I got back to working", "tokens": [50364, 3470, 1168, 13430, 11,
  291, 643, 281, 3470, 11495, 5469, 3337, 13, 407, 300, 311, 577, 286, 658, 646, 281,
  1364, 50640], "temperature": 0.0, "avg_logprob": -0.23950576782226562, "compression_ratio":
  1.5127118644067796, "no_speech_prob": 0.0013318248093128204}, {"id": 49, "seek":
  36880, "start": 374.32, "end": 380.24, "text": " on quality algorithms. And again,
  I saw an opportunity and why big research question was,", "tokens": [50640, 322,
  3125, 14642, 13, 400, 797, 11, 286, 1866, 364, 2650, 293, 983, 955, 2132, 1168,
  390, 11, 50936], "temperature": 0.0, "avg_logprob": -0.23950576782226562, "compression_ratio":
  1.5127118644067796, "no_speech_prob": 0.0013318248093128204}, {"id": 50, "seek":
  36880, "start": 380.8, "end": 386.48, "text": " how can we do information Retrieval
  using more advanced techniques rather than", "tokens": [50964, 577, 393, 321, 360,
  1589, 11495, 5469, 3337, 1228, 544, 7339, 7512, 2831, 813, 51248], "temperature":
  0.0, "avg_logprob": -0.23950576782226562, "compression_ratio": 1.5127118644067796,
  "no_speech_prob": 0.0013318248093128204}, {"id": 51, "seek": 36880, "start": 386.48,
  "end": 394.08000000000004, "text": " lexical search with BIRN 25? And because before
  birth, nowadays like everybody just uses like", "tokens": [51248, 476, 87, 804,
  3164, 365, 363, 7740, 45, 3552, 30, 400, 570, 949, 3965, 11, 13434, 411, 2201, 445,
  4960, 411, 51628], "temperature": 0.0, "avg_logprob": -0.23950576782226562, "compression_ratio":
  1.5127118644067796, "no_speech_prob": 0.0013318248093128204}, {"id": 52, "seek":
  39408, "start": 394.08, "end": 402.47999999999996, "text": " word-based models or
  any like other transform based models to create dense vector embeddings and", "tokens":
  [50364, 1349, 12, 6032, 5245, 420, 604, 411, 661, 4088, 2361, 5245, 281, 1884, 18011,
  8062, 12240, 29432, 293, 50784], "temperature": 0.0, "avg_logprob": -0.16363255730990706,
  "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.0017891578609123826},
  {"id": 53, "seek": 39408, "start": 402.47999999999996, "end": 410.96, "text": "
  they are quite effective, that was not the case when like 10 years ago. So whatever
  we had there", "tokens": [50784, 436, 366, 1596, 4942, 11, 300, 390, 406, 264, 1389,
  562, 411, 1266, 924, 2057, 13, 407, 2035, 321, 632, 456, 51208], "temperature":
  0.0, "avg_logprob": -0.16363255730990706, "compression_ratio": 1.6623931623931625,
  "no_speech_prob": 0.0017891578609123826}, {"id": 54, "seek": 39408, "start": 410.96,
  "end": 416.79999999999995, "text": " was pretty ineffective Retrieval. And so my
  thought was that because the single representation was", "tokens": [51208, 390,
  1238, 48836, 11495, 5469, 3337, 13, 400, 370, 452, 1194, 390, 300, 570, 264, 2167,
  10290, 390, 51500], "temperature": 0.0, "avg_logprob": -0.16363255730990706, "compression_ratio":
  1.6623931623931625, "no_speech_prob": 0.0017891578609123826}, {"id": 55, "seek":
  39408, "start": 416.79999999999995, "end": 422.56, "text": " not effective Retrieval,
  those need to be somehow combined and assembled. So you basically don''t", "tokens":
  [51500, 406, 4942, 11495, 5469, 3337, 11, 729, 643, 281, 312, 6063, 9354, 293, 24204,
  13, 407, 291, 1936, 500, 380, 51788], "temperature": 0.0, "avg_logprob": -0.16363255730990706,
  "compression_ratio": 1.6623931623931625, "no_speech_prob": 0.0017891578609123826},
  {"id": 56, "seek": 42256, "start": 422.56, "end": 428.96, "text": " get a single
  representation. You use a combination, you use combine similarity and then you treat",
  "tokens": [50364, 483, 257, 2167, 10290, 13, 509, 764, 257, 6562, 11, 291, 764,
  10432, 32194, 293, 550, 291, 2387, 50684], "temperature": 0.0, "avg_logprob": -0.2163057105485783,
  "compression_ratio": 1.786046511627907, "no_speech_prob": 0.0010187685256823897},
  {"id": 57, "seek": 42256, "start": 428.96, "end": 435.12, "text": " this similarity
  as a black box and then you apply generic Retrieval algorithm. So this was a pretty",
  "tokens": [50684, 341, 32194, 382, 257, 2211, 2424, 293, 550, 291, 3079, 19577,
  11495, 5469, 3337, 9284, 13, 407, 341, 390, 257, 1238, 50992], "temperature": 0.0,
  "avg_logprob": -0.2163057105485783, "compression_ratio": 1.786046511627907, "no_speech_prob":
  0.0010187685256823897}, {"id": 58, "seek": 42256, "start": 436.32, "end": 442.08,
  "text": " in hindsight that was a pretty ambitious project that required working
  on both design and effective", "tokens": [51052, 294, 44357, 300, 390, 257, 1238,
  20239, 1716, 300, 4739, 1364, 322, 1293, 1715, 293, 4942, 51340], "temperature":
  0.0, "avg_logprob": -0.2163057105485783, "compression_ratio": 1.786046511627907,
  "no_speech_prob": 0.0010187685256823897}, {"id": 59, "seek": 42256, "start": 442.08,
  "end": 451.12, "text": " similarities. And Retrieval algorithms. And that''s why
  we, well one, that''s where that", "tokens": [51340, 24197, 13, 400, 11495, 5469,
  3337, 14642, 13, 400, 300, 311, 983, 321, 11, 731, 472, 11, 300, 311, 689, 300,
  51792], "temperature": 0.0, "avg_logprob": -0.2163057105485783, "compression_ratio":
  1.786046511627907, "no_speech_prob": 0.0010187685256823897}, {"id": 60, "seek":
  45112, "start": 451.12, "end": 456.64, "text": " animously library turned out to
  be very useful. It was instrumental to this work.", "tokens": [50364, 2383, 5098,
  6405, 3574, 484, 281, 312, 588, 4420, 13, 467, 390, 17388, 281, 341, 589, 13, 50640],
  "temperature": 0.0, "avg_logprob": -0.19445706927587117, "compression_ratio": 1.4864864864864864,
  "no_speech_prob": 0.0008963451255112886}, {"id": 61, "seek": 45112, "start": 457.36,
  "end": 465.68, "text": " Although it was created for somewhat unrelated people.
  Okay, so that was an overall rather bumpy,", "tokens": [50676, 5780, 309, 390, 2942,
  337, 8344, 38967, 561, 13, 1033, 11, 370, 300, 390, 364, 4787, 2831, 49400, 11,
  51092], "temperature": 0.0, "avg_logprob": -0.19445706927587117, "compression_ratio":
  1.4864864864864864, "no_speech_prob": 0.0008963451255112886}, {"id": 62, "seek":
  45112, "start": 465.68, "end": 473.92, "text": " right things didn''t work well
  initially and I got a lot of help from other people in particular", "tokens": [51092,
  558, 721, 994, 380, 589, 731, 9105, 293, 286, 658, 257, 688, 295, 854, 490, 661,
  561, 294, 1729, 51504], "temperature": 0.0, "avg_logprob": -0.19445706927587117,
  "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.0008963451255112886},
  {"id": 63, "seek": 47392, "start": 473.92, "end": 480.40000000000003, "text": "
  from my author, David Norva, who proposed an amazing improvement for one of the
  algorithms", "tokens": [50364, 490, 452, 3793, 11, 4389, 6966, 2757, 11, 567, 10348,
  364, 2243, 10444, 337, 472, 295, 264, 14642, 50688], "temperature": 0.0, "avg_logprob":
  -0.3238578464673913, "compression_ratio": 1.4422110552763818, "no_speech_prob":
  0.004585818853229284}, {"id": 64, "seek": 47392, "start": 481.04, "end": 491.44,
  "text": " in a thermosleep. Yeah, and so we published and opened after my graduation.
  And when I was writing", "tokens": [50720, 294, 257, 8810, 329, 7927, 13, 865, 11,
  293, 370, 321, 6572, 293, 5625, 934, 452, 15652, 13, 400, 562, 286, 390, 3579, 51240],
  "temperature": 0.0, "avg_logprob": -0.3238578464673913, "compression_ratio": 1.4422110552763818,
  "no_speech_prob": 0.004585818853229284}, {"id": 65, "seek": 47392, "start": 491.44,
  "end": 498.8, "text": " my thesis, yeah, I was found like a bunch of issues with
  my previous approaches and realized that", "tokens": [51240, 452, 22288, 11, 1338,
  11, 286, 390, 1352, 411, 257, 3840, 295, 2663, 365, 452, 3894, 11587, 293, 5334,
  300, 51608], "temperature": 0.0, "avg_logprob": -0.3238578464673913, "compression_ratio":
  1.4422110552763818, "no_speech_prob": 0.004585818853229284}, {"id": 66, "seek":
  49880, "start": 498.96000000000004, "end": 506.56, "text": " I could also use like
  a H&SW like algorithms which were not like core part of my thesis work.", "tokens":
  [50372, 286, 727, 611, 764, 411, 257, 389, 5, 50, 54, 411, 14642, 597, 645, 406,
  411, 4965, 644, 295, 452, 22288, 589, 13, 50752], "temperature": 0.0, "avg_logprob":
  -0.20676604679652621, "compression_ratio": 1.4946808510638299, "no_speech_prob":
  0.0005867545260116458}, {"id": 67, "seek": 49880, "start": 507.44, "end": 513.12,
  "text": " And I got even stronger results, but that was like a little bit too late
  to publish and use otherwise.", "tokens": [50796, 400, 286, 658, 754, 7249, 3542,
  11, 457, 300, 390, 411, 257, 707, 857, 886, 3469, 281, 11374, 293, 764, 5911, 13,
  51080], "temperature": 0.0, "avg_logprob": -0.20676604679652621, "compression_ratio":
  1.4946808510638299, "no_speech_prob": 0.0005867545260116458}, {"id": 68, "seek":
  49880, "start": 514.8, "end": 521.76, "text": " Moreover, that the similarity that
  I used was a sort of a face palm realization that", "tokens": [51164, 19838, 11,
  300, 264, 32194, 300, 286, 1143, 390, 257, 1333, 295, 257, 1851, 17018, 25138, 300,
  51512], "temperature": 0.0, "avg_logprob": -0.20676604679652621, "compression_ratio":
  1.4946808510638299, "no_speech_prob": 0.0005867545260116458}, {"id": 69, "seek":
  52176, "start": 522.08, "end": 529.6, "text": " that similarity that I used, like
  Retrieval, completely as a black box. And it worked with", "tokens": [50380, 300,
  32194, 300, 286, 1143, 11, 411, 11495, 5469, 3337, 11, 2584, 382, 257, 2211, 2424,
  13, 400, 309, 2732, 365, 50756], "temperature": 0.0, "avg_logprob": -0.31423912925281744,
  "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0018524241168051958},
  {"id": 70, "seek": 52176, "start": 529.6, "end": 534.8, "text": " most effective.
  It was more effective than B125 on the collections that I used. But I didn''t",
  "tokens": [50756, 881, 4942, 13, 467, 390, 544, 4942, 813, 363, 16, 6074, 322, 264,
  16641, 300, 286, 1143, 13, 583, 286, 994, 380, 51016], "temperature": 0.0, "avg_logprob":
  -0.31423912925281744, "compression_ratio": 1.5940170940170941, "no_speech_prob":
  0.0018524241168051958}, {"id": 71, "seek": 52176, "start": 534.8, "end": 540.96,
  "text": " realize that this black box similarity was actually representable by another
  product between", "tokens": [51016, 4325, 300, 341, 2211, 2424, 32194, 390, 767,
  2906, 712, 538, 1071, 1674, 1296, 51324], "temperature": 0.0, "avg_logprob": -0.31423912925281744,
  "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.0018524241168051958},
  {"id": 72, "seek": 52176, "start": 540.96, "end": 548.88, "text": " two large sparse
  vectors by another former author Chris Dyer pointed this out. And if I embraced",
  "tokens": [51324, 732, 2416, 637, 11668, 18875, 538, 1071, 5819, 3793, 6688, 413,
  7224, 10932, 341, 484, 13, 400, 498, 286, 28673, 51720], "temperature": 0.0, "avg_logprob":
  -0.31423912925281744, "compression_ratio": 1.5940170940170941, "no_speech_prob":
  0.0018524241168051958}, {"id": 73, "seek": 54888, "start": 548.88, "end": 553.6,
  "text": " this sparse vector approach from the get-go, it would have been a much
  easier problem to solve", "tokens": [50364, 341, 637, 11668, 8062, 3109, 490, 264,
  483, 12, 1571, 11, 309, 576, 362, 668, 257, 709, 3571, 1154, 281, 5039, 50600],
  "temperature": 0.0, "avg_logprob": -0.19930816226535372, "compression_ratio": 1.5767634854771784,
  "no_speech_prob": 0.005145624279975891}, {"id": 74, "seek": 54888, "start": 554.48,
  "end": 560.64, "text": " from both engineering and scientific points of view even
  without work. And okay, it could have", "tokens": [50644, 490, 1293, 7043, 293,
  8134, 2793, 295, 1910, 754, 1553, 589, 13, 400, 1392, 11, 309, 727, 362, 50952],
  "temperature": 0.0, "avg_logprob": -0.19930816226535372, "compression_ratio": 1.5767634854771784,
  "no_speech_prob": 0.005145624279975891}, {"id": 75, "seek": 54888, "start": 560.64,
  "end": 570.96, "text": " produced some more impact. But yeah, a little bit too late
  to dwell this now. Okay, and enough", "tokens": [50952, 7126, 512, 544, 2712, 13,
  583, 1338, 11, 257, 707, 857, 886, 3469, 281, 24355, 341, 586, 13, 1033, 11, 293,
  1547, 51468], "temperature": 0.0, "avg_logprob": -0.19930816226535372, "compression_ratio":
  1.5767634854771784, "no_speech_prob": 0.005145624279975891}, {"id": 76, "seek":
  54888, "start": 570.96, "end": 578.72, "text": " with that I graduated six years
  ago. And since then I haven''t working as a researcher scientist", "tokens": [51468,
  365, 300, 286, 13693, 2309, 924, 2057, 13, 400, 1670, 550, 286, 2378, 380, 1364,
  382, 257, 21751, 12662, 51856], "temperature": 0.0, "avg_logprob": -0.19930816226535372,
  "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.005145624279975891},
  {"id": 77, "seek": 57872, "start": 578.72, "end": 585.0400000000001, "text": " and
  engineer on deep learning in what specifically I had in training models for specific
  initial", "tokens": [50364, 293, 11403, 322, 2452, 2539, 294, 437, 4682, 286, 632,
  294, 3097, 5245, 337, 2685, 5883, 50680], "temperature": 0.0, "avg_logprob": -0.24766811692571067,
  "compression_ratio": 1.5826086956521739, "no_speech_prob": 0.0034362594597041607},
  {"id": 78, "seek": 57872, "start": 585.0400000000001, "end": 592.08, "text": " computer
  vision and Retrieval. Despite this diversity, things have come a full circle and",
  "tokens": [50680, 3820, 5201, 293, 11495, 5469, 3337, 13, 11334, 341, 8811, 11,
  721, 362, 808, 257, 1577, 6329, 293, 51032], "temperature": 0.0, "avg_logprob":
  -0.24766811692571067, "compression_ratio": 1.5826086956521739, "no_speech_prob":
  0.0034362594597041607}, {"id": 79, "seek": 57872, "start": 592.08, "end": 598.96,
  "text": " are working question as being systems once again. Yeah, that was pretty
  much involved.", "tokens": [51032, 366, 1364, 1168, 382, 885, 3652, 1564, 797, 13,
  865, 11, 300, 390, 1238, 709, 3288, 13, 51376], "temperature": 0.0, "avg_logprob":
  -0.24766811692571067, "compression_ratio": 1.5826086956521739, "no_speech_prob":
  0.0034362594597041607}, {"id": 80, "seek": 57872, "start": 598.96, "end": 606.08,
  "text": " Yeah, amazing story. Yeah, thank you for that. It''s like the story tends
  to repeat itself,", "tokens": [51376, 865, 11, 2243, 1657, 13, 865, 11, 1309, 291,
  337, 300, 13, 467, 311, 411, 264, 1657, 12258, 281, 7149, 2564, 11, 51732], "temperature":
  0.0, "avg_logprob": -0.24766811692571067, "compression_ratio": 1.5826086956521739,
  "no_speech_prob": 0.0034362594597041607}, {"id": 81, "seek": 60608, "start": 606.08,
  "end": 612.08, "text": " but at the same time, if we find the topic still exciting
  and it seems like you are still very", "tokens": [50364, 457, 412, 264, 912, 565,
  11, 498, 321, 915, 264, 4829, 920, 4670, 293, 309, 2544, 411, 291, 366, 920, 588,
  50664], "temperature": 0.0, "avg_logprob": -0.11656635006268819, "compression_ratio":
  1.5899581589958158, "no_speech_prob": 0.008717834018170834}, {"id": 82, "seek":
  60608, "start": 612.08, "end": 618.64, "text": " interested in question answering
  and improving building blocks of that, it''s kind of cool,", "tokens": [50664, 3102,
  294, 1168, 13430, 293, 11470, 2390, 8474, 295, 300, 11, 309, 311, 733, 295, 1627,
  11, 50992], "temperature": 0.0, "avg_logprob": -0.11656635006268819, "compression_ratio":
  1.5899581589958158, "no_speech_prob": 0.008717834018170834}, {"id": 83, "seek":
  60608, "start": 618.64, "end": 623.36, "text": " right? So that we are able to come
  back to some of the topics, pick them up on a different level.", "tokens": [50992,
  558, 30, 407, 300, 321, 366, 1075, 281, 808, 646, 281, 512, 295, 264, 8378, 11,
  1888, 552, 493, 322, 257, 819, 1496, 13, 51228], "temperature": 0.0, "avg_logprob":
  -0.11656635006268819, "compression_ratio": 1.5899581589958158, "no_speech_prob":
  0.008717834018170834}, {"id": 84, "seek": 60608, "start": 623.36, "end": 630.6400000000001,
  "text": " That''s amazing. And yeah, there is a lot to unpack. I almost wanted to
  ask you or the moment you", "tokens": [51228, 663, 311, 2243, 13, 400, 1338, 11,
  456, 307, 257, 688, 281, 26699, 13, 286, 1920, 1415, 281, 1029, 291, 420, 264, 1623,
  291, 51592], "temperature": 0.0, "avg_logprob": -0.11656635006268819, "compression_ratio":
  1.5899581589958158, "no_speech_prob": 0.008717834018170834}, {"id": 85, "seek":
  63064, "start": 630.64, "end": 637.6, "text": " spoke about spars and dance. I wanted
  to pick your brain on what''s it take on the model called", "tokens": [50364, 7179,
  466, 637, 685, 293, 4489, 13, 286, 1415, 281, 1888, 428, 3567, 322, 437, 311, 309,
  747, 322, 264, 2316, 1219, 50712], "temperature": 0.0, "avg_logprob": -0.1600088022523007,
  "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.02030409313738346},
  {"id": 86, "seek": 63064, "start": 637.6, "end": 642.0, "text": " split and split
  V2. I don''t know if you''re familiar with that model, but basically,", "tokens":
  [50712, 7472, 293, 7472, 691, 17, 13, 286, 500, 380, 458, 498, 291, 434, 4963, 365,
  300, 2316, 11, 457, 1936, 11, 50932], "temperature": 0.0, "avg_logprob": -0.1600088022523007,
  "compression_ratio": 1.6549295774647887, "no_speech_prob": 0.02030409313738346},
  {"id": 87, "seek": 63064, "start": 643.1999999999999, "end": 648.3199999999999,
  "text": " you know, there is always this discussion should we take lexical search,
  combine it with dance", "tokens": [50992, 291, 458, 11, 456, 307, 1009, 341, 5017,
  820, 321, 747, 476, 87, 804, 3164, 11, 10432, 309, 365, 4489, 51248], "temperature":
  0.0, "avg_logprob": -0.1600088022523007, "compression_ratio": 1.6549295774647887,
  "no_speech_prob": 0.02030409313738346}, {"id": 88, "seek": 63064, "start": 648.3199999999999,
  "end": 654.3199999999999, "text": " search and then do some kind of hybrid formal
  on top and then how do we even learn the parameters", "tokens": [51248, 3164, 293,
  550, 360, 512, 733, 295, 13051, 9860, 322, 1192, 293, 550, 577, 360, 321, 754, 1466,
  264, 9834, 51548], "temperature": 0.0, "avg_logprob": -0.1600088022523007, "compression_ratio":
  1.6549295774647887, "no_speech_prob": 0.02030409313738346}, {"id": 89, "seek": 63064,
  "start": 654.3199999999999, "end": 660.16, "text": " of that model, right? Depending
  on the domain. But then there is a drastic sort of approach. Let''s", "tokens":
  [51548, 295, 300, 2316, 11, 558, 30, 22539, 322, 264, 9274, 13, 583, 550, 456, 307,
  257, 36821, 1333, 295, 3109, 13, 961, 311, 51840], "temperature": 0.0, "avg_logprob":
  -0.1600088022523007, "compression_ratio": 1.6549295774647887, "no_speech_prob":
  0.02030409313738346}, {"id": 90, "seek": 66016, "start": 660.16, "end": 665.8399999999999,
  "text": " not do that. Let''s just take a complete model which can handle both and
  then you can also", "tokens": [50364, 406, 360, 300, 13, 961, 311, 445, 747, 257,
  3566, 2316, 597, 393, 4813, 1293, 293, 550, 291, 393, 611, 50648], "temperature":
  0.0, "avg_logprob": -0.14631761010013408, "compression_ratio": 1.5106382978723405,
  "no_speech_prob": 0.0028053484857082367}, {"id": 91, "seek": 66016, "start": 666.48,
  "end": 674.16, "text": " support what the dance search doesn''t support like exact
  phrase searches. What''s your general", "tokens": [50680, 1406, 437, 264, 4489,
  3164, 1177, 380, 1406, 411, 1900, 9535, 26701, 13, 708, 311, 428, 2674, 51064],
  "temperature": 0.0, "avg_logprob": -0.14631761010013408, "compression_ratio": 1.5106382978723405,
  "no_speech_prob": 0.0028053484857082367}, {"id": 92, "seek": 66016, "start": 674.16,
  "end": 680.24, "text": " intuition about that? How do you think about this? Well,
  that''s a super interesting question. I have", "tokens": [51064, 24002, 466, 300,
  30, 1012, 360, 291, 519, 466, 341, 30, 1042, 11, 300, 311, 257, 1687, 1880, 1168,
  13, 286, 362, 51368], "temperature": 0.0, "avg_logprob": -0.14631761010013408, "compression_ratio":
  1.5106382978723405, "no_speech_prob": 0.0028053484857082367}, {"id": 93, "seek":
  68024, "start": 680.32, "end": 691.28, "text": " one clarifying question though.
  So, before I answer, you said that some people who want to have", "tokens": [50368,
  472, 6093, 5489, 1168, 1673, 13, 407, 11, 949, 286, 1867, 11, 291, 848, 300, 512,
  561, 567, 528, 281, 362, 50916], "temperature": 0.0, "avg_logprob": -0.199555788542095,
  "compression_ratio": 1.576271186440678, "no_speech_prob": 0.011479969136416912},
  {"id": 94, "seek": 68024, "start": 691.28, "end": 696.32, "text": " a single model
  that''s doing both. Could you elaborate a little bit on this? Well, I guess", "tokens":
  [50916, 257, 2167, 2316, 300, 311, 884, 1293, 13, 7497, 291, 20945, 257, 707, 857,
  322, 341, 30, 1042, 11, 286, 2041, 51168], "temperature": 0.0, "avg_logprob": -0.199555788542095,
  "compression_ratio": 1.576271186440678, "no_speech_prob": 0.011479969136416912},
  {"id": 95, "seek": 68024, "start": 697.28, "end": 703.36, "text": " maybe it''s
  not that they wanted, but it''s like the development when, instead of sort of,",
  "tokens": [51216, 1310, 309, 311, 406, 300, 436, 1415, 11, 457, 309, 311, 411, 264,
  3250, 562, 11, 2602, 295, 1333, 295, 11, 51520], "temperature": 0.0, "avg_logprob":
  -0.199555788542095, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.011479969136416912},
  {"id": 96, "seek": 68024, "start": 703.36, "end": 709.28, "text": " you know, combining
  these disparate sources of results, you know, one coming from lexical search,",
  "tokens": [51520, 291, 458, 11, 21928, 613, 14548, 473, 7139, 295, 3542, 11, 291,
  458, 11, 472, 1348, 490, 476, 87, 804, 3164, 11, 51816], "temperature": 0.0, "avg_logprob":
  -0.199555788542095, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.011479969136416912},
  {"id": 97, "seek": 70928, "start": 709.28, "end": 713.76, "text": " which is kind
  of like well-known BM25 driven, I guess. And then the other one is more", "tokens":
  [50364, 597, 307, 733, 295, 411, 731, 12, 6861, 15901, 6074, 9555, 11, 286, 2041,
  13, 400, 550, 264, 661, 472, 307, 544, 50588], "temperature": 0.0, "avg_logprob":
  -0.1798357866248306, "compression_ratio": 1.6182572614107884, "no_speech_prob":
  0.0022938442416489124}, {"id": 98, "seek": 70928, "start": 715.76, "end": 722.4,
  "text": " like more modern in a way that everyone wants to get exposed to dance
  search. And then you need to", "tokens": [50688, 411, 544, 4363, 294, 257, 636,
  300, 1518, 2738, 281, 483, 9495, 281, 4489, 3164, 13, 400, 550, 291, 643, 281, 51020],
  "temperature": 0.0, "avg_logprob": -0.1798357866248306, "compression_ratio": 1.6182572614107884,
  "no_speech_prob": 0.0022938442416489124}, {"id": 99, "seek": 70928, "start": 722.4,
  "end": 727.92, "text": " somehow figure out how you combine the results, right?
  So one is designed maybe for precision lexical.", "tokens": [51020, 6063, 2573,
  484, 577, 291, 10432, 264, 3542, 11, 558, 30, 407, 472, 307, 4761, 1310, 337, 18356,
  476, 87, 804, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1798357866248306,
  "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.0022938442416489124},
  {"id": 100, "seek": 70928, "start": 727.92, "end": 736.16, "text": " The other one
  is designed more for recall, right? Because the vectors are not, they don''t have
  as many", "tokens": [51296, 440, 661, 472, 307, 4761, 544, 337, 9901, 11, 558, 30,
  1436, 264, 18875, 366, 406, 11, 436, 500, 380, 362, 382, 867, 51708], "temperature":
  0.0, "avg_logprob": -0.1798357866248306, "compression_ratio": 1.6182572614107884,
  "no_speech_prob": 0.0022938442416489124}, {"id": 101, "seek": 73616, "start": 736.16,
  "end": 741.52, "text": " dimensions as these far specters. But then you still need
  to figure out, okay, how do I combine this", "tokens": [50364, 12819, 382, 613,
  1400, 6177, 433, 13, 583, 550, 291, 920, 643, 281, 2573, 484, 11, 1392, 11, 577,
  360, 286, 10432, 341, 50632], "temperature": 0.0, "avg_logprob": -0.19729736873081752,
  "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00239355256780982},
  {"id": 102, "seek": 73616, "start": 741.52, "end": 749.28, "text": " to? And usually
  people cite reciprocal rank fusion in what I hear, but there are other methods as",
  "tokens": [50632, 281, 30, 400, 2673, 561, 37771, 46948, 6181, 23100, 294, 437,
  286, 1568, 11, 457, 456, 366, 661, 7150, 382, 51020], "temperature": 0.0, "avg_logprob":
  -0.19729736873081752, "compression_ratio": 1.606425702811245, "no_speech_prob":
  0.00239355256780982}, {"id": 103, "seek": 73616, "start": 749.28, "end": 756.48,
  "text": " well, like even clustering based. But then that''s one approach. Another
  approach is just stop doing that,", "tokens": [51020, 731, 11, 411, 754, 596, 48673,
  2361, 13, 583, 550, 300, 311, 472, 3109, 13, 3996, 3109, 307, 445, 1590, 884, 300,
  11, 51380], "temperature": 0.0, "avg_logprob": -0.19729736873081752, "compression_ratio":
  1.606425702811245, "no_speech_prob": 0.00239355256780982}, {"id": 104, "seek": 73616,
  "start": 756.48, "end": 762.9599999999999, "text": " I guess. If I really understand
  what split does, and then you encode with split your data once,", "tokens": [51380,
  286, 2041, 13, 759, 286, 534, 1223, 437, 7472, 775, 11, 293, 550, 291, 2058, 1429,
  365, 7472, 428, 1412, 1564, 11, 51704], "temperature": 0.0, "avg_logprob": -0.19729736873081752,
  "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00239355256780982},
  {"id": 105, "seek": 76296, "start": 762.96, "end": 769.6, "text": " and you retrieve,
  you know, you use its capabilities to also retrieve exact phrases, right? So,",
  "tokens": [50364, 293, 291, 30254, 11, 291, 458, 11, 291, 764, 1080, 10862, 281,
  611, 30254, 1900, 20312, 11, 558, 30, 407, 11, 50696], "temperature": 0.0, "avg_logprob":
  -0.176083307999831, "compression_ratio": 1.4411764705882353, "no_speech_prob": 0.005018287338316441},
  {"id": 106, "seek": 76296, "start": 769.6, "end": 777.44, "text": " effectively,
  ideally, you don''t need the lexical matching engine anymore, but maybe I''m completely",
  "tokens": [50696, 8659, 11, 22915, 11, 291, 500, 380, 643, 264, 476, 87, 804, 14324,
  2848, 3602, 11, 457, 1310, 286, 478, 2584, 51088], "temperature": 0.0, "avg_logprob":
  -0.176083307999831, "compression_ratio": 1.4411764705882353, "no_speech_prob": 0.005018287338316441},
  {"id": 107, "seek": 76296, "start": 777.44, "end": 785.6800000000001, "text": "
  wrong. I''m just, I wanted to hear your opinion on that. Okay, well, let''s get
  it. Using your words,", "tokens": [51088, 2085, 13, 286, 478, 445, 11, 286, 1415,
  281, 1568, 428, 4800, 322, 300, 13, 1033, 11, 731, 11, 718, 311, 483, 309, 13, 11142,
  428, 2283, 11, 51500], "temperature": 0.0, "avg_logprob": -0.176083307999831, "compression_ratio":
  1.4411764705882353, "no_speech_prob": 0.005018287338316441}, {"id": 108, "seek":
  78568, "start": 785.68, "end": 793.76, "text": " it''s a lot on back here. I''m
  still not quite sure what you mean by having like a single model.", "tokens": [50364,
  309, 311, 257, 688, 322, 646, 510, 13, 286, 478, 920, 406, 1596, 988, 437, 291,
  914, 538, 1419, 411, 257, 2167, 2316, 13, 50768], "temperature": 0.0, "avg_logprob":
  -0.2128382682800293, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.010937400162220001},
  {"id": 109, "seek": 78568, "start": 794.4, "end": 801.3599999999999, "text": " Although,
  maybe I love me try to maybe start answering questions and you can drop me and",
  "tokens": [50800, 5780, 11, 1310, 286, 959, 385, 853, 281, 1310, 722, 13430, 1651,
  293, 291, 393, 3270, 385, 293, 51148], "temperature": 0.0, "avg_logprob": -0.2128382682800293,
  "compression_ratio": 1.455497382198953, "no_speech_prob": 0.010937400162220001},
  {"id": 110, "seek": 78568, "start": 802.7199999999999, "end": 811.52, "text": "
  guide me into the other direction if needed. So first of all, we have what''s interesting
  about", "tokens": [51216, 5934, 385, 666, 264, 661, 3513, 498, 2978, 13, 407, 700,
  295, 439, 11, 321, 362, 437, 311, 1880, 466, 51656], "temperature": 0.0, "avg_logprob":
  -0.2128382682800293, "compression_ratio": 1.455497382198953, "no_speech_prob": 0.010937400162220001},
  {"id": 111, "seek": 81152, "start": 811.52, "end": 818.0799999999999, "text": "
  Nashville language is that, and that''s very different from computer vision domain,
  is that we", "tokens": [50364, 36370, 2856, 307, 300, 11, 293, 300, 311, 588, 819,
  490, 3820, 5201, 9274, 11, 307, 300, 321, 50692], "temperature": 0.0, "avg_logprob":
  -0.2879610061645508, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.01184320542961359},
  {"id": 112, "seek": 81152, "start": 818.0799999999999, "end": 826.24, "text": "
  usually represent, we can we have multiple ways to represent text. So in computer
  vision,", "tokens": [50692, 2673, 2906, 11, 321, 393, 321, 362, 3866, 2098, 281,
  2906, 2487, 13, 407, 294, 3820, 5201, 11, 51100], "temperature": 0.0, "avg_logprob":
  -0.2879610061645508, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.01184320542961359},
  {"id": 113, "seek": 81152, "start": 827.28, "end": 834.56, "text": " usually it''s
  just like each image is a traditional representative of actors that was the", "tokens":
  [51152, 2673, 309, 311, 445, 411, 1184, 3256, 307, 257, 5164, 12424, 295, 10037,
  300, 390, 264, 51516], "temperature": 0.0, "avg_logprob": -0.2879610061645508, "compression_ratio":
  1.619047619047619, "no_speech_prob": 0.01184320542961359}, {"id": 114, "seek": 83456,
  "start": 834.56, "end": 842.88, "text": " commodity theme. But in the in a national
  language processing, we started with the so-called", "tokens": [50364, 29125, 6314,
  13, 583, 294, 264, 294, 257, 4048, 2856, 9007, 11, 321, 1409, 365, 264, 370, 12,
  11880, 50780], "temperature": 0.0, "avg_logprob": -0.2968230708952873, "compression_ratio":
  1.540983606557377, "no_speech_prob": 0.0029008244164288044}, {"id": 115, "seek":
  83456, "start": 842.88, "end": 850.0799999999999, "text": " bag of words representations
  where a document was represented by basically a sparse vector where", "tokens":
  [50780, 3411, 295, 2283, 33358, 689, 257, 4166, 390, 10379, 538, 1936, 257, 637,
  11668, 8062, 689, 51140], "temperature": 0.0, "avg_logprob": -0.2968230708952873,
  "compression_ratio": 1.540983606557377, "no_speech_prob": 0.0029008244164288044},
  {"id": 116, "seek": 83456, "start": 850.0799999999999, "end": 857.1999999999999,
  "text": " you will have either zeros and ones, which means the specific terms present
  or not, or maybe", "tokens": [51140, 291, 486, 362, 2139, 35193, 293, 2306, 11,
  597, 1355, 264, 2685, 2115, 1974, 420, 406, 11, 420, 1310, 51496], "temperature":
  0.0, "avg_logprob": -0.2968230708952873, "compression_ratio": 1.540983606557377,
  "no_speech_prob": 0.0029008244164288044}, {"id": 117, "seek": 85720, "start": 857.2,
  "end": 864.4000000000001, "text": " weights, not just zeros and ones, but weights.
  But then, with development of deep learning, and I", "tokens": [50364, 17443, 11,
  406, 445, 35193, 293, 2306, 11, 457, 17443, 13, 583, 550, 11, 365, 3250, 295, 2452,
  2539, 11, 293, 286, 50724], "temperature": 0.0, "avg_logprob": -0.21123206798846905,
  "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.001722217071801424},
  {"id": 118, "seek": 85720, "start": 864.4000000000001, "end": 875.12, "text": "
  actually started a little bit earlier with people, people learned how to represent
  text using fixed", "tokens": [50724, 767, 1409, 257, 707, 857, 3071, 365, 561, 11,
  561, 3264, 577, 281, 2906, 2487, 1228, 6806, 51260], "temperature": 0.0, "avg_logprob":
  -0.21123206798846905, "compression_ratio": 1.5157894736842106, "no_speech_prob":
  0.001722217071801424}, {"id": 119, "seek": 85720, "start": 875.12, "end": 882.48,
  "text": " size vectors. And that was like using principle component analysis. And
  this is not a very", "tokens": [51260, 2744, 18875, 13, 400, 300, 390, 411, 1228,
  8665, 6542, 5215, 13, 400, 341, 307, 406, 257, 588, 51628], "temperature": 0.0,
  "avg_logprob": -0.21123206798846905, "compression_ratio": 1.5157894736842106, "no_speech_prob":
  0.001722217071801424}, {"id": 120, "seek": 88248, "start": 882.48, "end": 890.24,
  "text": " natural representation for text and it didn''t work really well initially.
  But now we''re having good", "tokens": [50364, 3303, 10290, 337, 2487, 293, 309,
  994, 380, 589, 534, 731, 9105, 13, 583, 586, 321, 434, 1419, 665, 50752], "temperature":
  0.0, "avg_logprob": -0.31740800957930715, "compression_ratio": 1.6527196652719665,
  "no_speech_prob": 0.002557026222348213}, {"id": 121, "seek": 88248, "start": 890.24,
  "end": 896.32, "text": " results. So we have like two representations and there
  are different approaches to combine those,", "tokens": [50752, 3542, 13, 407, 321,
  362, 411, 732, 33358, 293, 456, 366, 819, 11587, 281, 10432, 729, 11, 51056], "temperature":
  0.0, "avg_logprob": -0.31740800957930715, "compression_ratio": 1.6527196652719665,
  "no_speech_prob": 0.002557026222348213}, {"id": 122, "seek": 88248, "start": 896.32,
  "end": 903.6800000000001, "text": " of course. One is just if you want to do the
  T-wall, you can indeed just do the lexical base search,", "tokens": [51056, 295,
  1164, 13, 1485, 307, 445, 498, 291, 528, 281, 360, 264, 314, 12, 16256, 11, 291,
  393, 6451, 445, 360, 264, 476, 87, 804, 3096, 3164, 11, 51424], "temperature": 0.0,
  "avg_logprob": -0.31740800957930715, "compression_ratio": 1.6527196652719665, "no_speech_prob":
  0.002557026222348213}, {"id": 123, "seek": 88248, "start": 904.96, "end": 911.28,
  "text": " you can do a kidney or a snabestation vector representations, and then
  you can somehow merge the", "tokens": [51488, 291, 393, 360, 257, 19000, 420, 257,
  2406, 455, 377, 399, 8062, 33358, 11, 293, 550, 291, 393, 6063, 22183, 264, 51804],
  "temperature": 0.0, "avg_logprob": -0.31740800957930715, "compression_ratio": 1.6527196652719665,
  "no_speech_prob": 0.002557026222348213}, {"id": 124, "seek": 91128, "start": 911.28,
  "end": 918.0, "text": " results. You can use ranker. But you don''t have to, and
  that''s the so-called hybrid search, but the", "tokens": [50364, 3542, 13, 509,
  393, 764, 6181, 260, 13, 583, 291, 500, 380, 362, 281, 11, 293, 300, 311, 264, 370,
  12, 11880, 13051, 3164, 11, 457, 264, 50700], "temperature": 0.0, "avg_logprob":
  -0.14561008920474927, "compression_ratio": 1.6778242677824269, "no_speech_prob":
  0.0011995760723948479}, {"id": 125, "seek": 91128, "start": 918.0, "end": 924.48,
  "text": " hybrid search can exist in different versions. So if you want to combine
  it sort of in a single model,", "tokens": [50700, 13051, 3164, 393, 2514, 294, 819,
  9606, 13, 407, 498, 291, 528, 281, 10432, 309, 1333, 295, 294, 257, 2167, 2316,
  11, 51024], "temperature": 0.0, "avg_logprob": -0.14561008920474927, "compression_ratio":
  1.6778242677824269, "no_speech_prob": 0.0011995760723948479}, {"id": 126, "seek":
  91128, "start": 924.48, "end": 933.52, "text": " why don''t you represent each document
  using both sparse and dense vector? And when you''re computing", "tokens": [51024,
  983, 500, 380, 291, 2906, 1184, 4166, 1228, 1293, 637, 11668, 293, 18011, 8062,
  30, 400, 562, 291, 434, 15866, 51476], "temperature": 0.0, "avg_logprob": -0.14561008920474927,
  "compression_ratio": 1.6778242677824269, "no_speech_prob": 0.0011995760723948479},
  {"id": 127, "seek": 91128, "start": 933.52, "end": 939.6, "text": " the similarity,
  you can compute the similarity between sparse parts, between dense parts, and then",
  "tokens": [51476, 264, 32194, 11, 291, 393, 14722, 264, 32194, 1296, 637, 11668,
  3166, 11, 1296, 18011, 3166, 11, 293, 550, 51780], "temperature": 0.0, "avg_logprob":
  -0.14561008920474927, "compression_ratio": 1.6778242677824269, "no_speech_prob":
  0.0011995760723948479}, {"id": 128, "seek": 93960, "start": 939.9200000000001, "end":
  946.72, "text": " combine them somehow. For example, using a weight. And that''s
  in fact what I was trying to do in my thesis", "tokens": [50380, 10432, 552, 6063,
  13, 1171, 1365, 11, 1228, 257, 3364, 13, 400, 300, 311, 294, 1186, 437, 286, 390,
  1382, 281, 360, 294, 452, 22288, 50720], "temperature": 0.0, "avg_logprob": -0.14781245900623835,
  "compression_ratio": 1.5210526315789474, "no_speech_prob": 0.0011417664354667068},
  {"id": 129, "seek": 93960, "start": 946.72, "end": 955.9200000000001, "text": "
  as well, because I was doing, my similarities was basically an ensemble of several
  similarities", "tokens": [50720, 382, 731, 11, 570, 286, 390, 884, 11, 452, 24197,
  390, 1936, 364, 19492, 295, 2940, 24197, 51180], "temperature": 0.0, "avg_logprob":
  -0.14781245900623835, "compression_ratio": 1.5210526315789474, "no_speech_prob":
  0.0011417664354667068}, {"id": 130, "seek": 93960, "start": 955.9200000000001, "end":
  962.48, "text": " course for at least two representations. And that could work.
  There''s of course modern", "tokens": [51180, 1164, 337, 412, 1935, 732, 33358,
  13, 400, 300, 727, 589, 13, 821, 311, 295, 1164, 4363, 51508], "temperature": 0.0,
  "avg_logprob": -0.14781245900623835, "compression_ratio": 1.5210526315789474, "no_speech_prob":
  0.0011417664354667068}, {"id": 131, "seek": 96248, "start": 962.48, "end": 967.2,
  "text": " instantiations of this, and there''s a paper, I think both are by some",
  "tokens": [50364, 9836, 72, 763, 295, 341, 11, 293, 456, 311, 257, 3035, 11, 286,
  519, 1293, 366, 538, 512, 50600], "temperature": 0.0, "avg_logprob": -0.26171720595586867,
  "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.003796835197135806},
  {"id": 132, "seek": 96248, "start": 968.4, "end": 976.72, "text": " glue people,
  where they did exactly like this, they combined splaid and some dense vector", "tokens":
  [50660, 8998, 561, 11, 689, 436, 630, 2293, 411, 341, 11, 436, 9354, 637, 875, 327,
  293, 512, 18011, 8062, 51076], "temperature": 0.0, "avg_logprob": -0.26171720595586867,
  "compression_ratio": 1.6261261261261262, "no_speech_prob": 0.003796835197135806},
  {"id": 133, "seek": 96248, "start": 976.72, "end": 983.04, "text": " embeddings.
  And that can work apparently a little bit better than, or sometimes maybe a lot
  better", "tokens": [51076, 12240, 29432, 13, 400, 300, 393, 589, 7970, 257, 707,
  857, 1101, 813, 11, 420, 2171, 1310, 257, 688, 1101, 51392], "temperature": 0.0,
  "avg_logprob": -0.26171720595586867, "compression_ratio": 1.6261261261261262, "no_speech_prob":
  0.003796835197135806}, {"id": 134, "seek": 96248, "start": 984.0, "end": 992.4,
  "text": " than basic representations, like each representation specifically. So
  with both approaches, of course,", "tokens": [51440, 813, 3875, 33358, 11, 411,
  1184, 10290, 4682, 13, 407, 365, 1293, 11587, 11, 295, 1164, 11, 51860], "temperature":
  0.0, "avg_logprob": -0.26171720595586867, "compression_ratio": 1.6261261261261262,
  "no_speech_prob": 0.003796835197135806}, {"id": 135, "seek": 99240, "start": 993.36,
  "end": 1000.8, "text": " there are issues that you mentioned. So I don''t know what
  the best approach there, and I don''t", "tokens": [50412, 456, 366, 2663, 300, 291,
  2835, 13, 407, 286, 500, 380, 458, 437, 264, 1151, 3109, 456, 11, 293, 286, 500,
  380, 50784], "temperature": 0.0, "avg_logprob": -0.12821337154933385, "compression_ratio":
  1.608695652173913, "no_speech_prob": 0.0010765603510662913}, {"id": 136, "seek":
  99240, "start": 1000.8, "end": 1006.88, "text": " have a crystal ball regarding
  what''s the best path forward. But with dense representation,", "tokens": [50784,
  362, 257, 13662, 2594, 8595, 437, 311, 264, 1151, 3100, 2128, 13, 583, 365, 18011,
  10290, 11, 51088], "temperature": 0.0, "avg_logprob": -0.12821337154933385, "compression_ratio":
  1.608695652173913, "no_speech_prob": 0.0010765603510662913}, {"id": 137, "seek":
  99240, "start": 1006.88, "end": 1013.28, "text": " the clearly the problem is that
  you have to pack everything into the fixed size vector.", "tokens": [51088, 264,
  4448, 264, 1154, 307, 300, 291, 362, 281, 2844, 1203, 666, 264, 6806, 2744, 8062,
  13, 51408], "temperature": 0.0, "avg_logprob": -0.12821337154933385, "compression_ratio":
  1.608695652173913, "no_speech_prob": 0.0010765603510662913}, {"id": 138, "seek":
  99240, "start": 1014.0, "end": 1021.12, "text": " And as your document is getting
  bigger, you basically the vector size, the amount of information", "tokens": [51444,
  400, 382, 428, 4166, 307, 1242, 3801, 11, 291, 1936, 264, 8062, 2744, 11, 264, 2372,
  295, 1589, 51800], "temperature": 0.0, "avg_logprob": -0.12821337154933385, "compression_ratio":
  1.608695652173913, "no_speech_prob": 0.0010765603510662913}, {"id": 139, "seek":
  102112, "start": 1021.2, "end": 1025.92, "text": " you can store is the same, but
  your document increases in size. So you would", "tokens": [50368, 291, 393, 3531,
  307, 264, 912, 11, 457, 428, 4166, 8637, 294, 2744, 13, 407, 291, 576, 50604], "temperature":
  0.0, "avg_logprob": -0.17627362073478053, "compression_ratio": 1.4915254237288136,
  "no_speech_prob": 0.0008666721405461431}, {"id": 140, "seek": 102112, "start": 1028.32,
  "end": 1037.52, "text": " possibly expect some deterioration in quality. But another
  reason why you can see deteriorating", "tokens": [50724, 6264, 2066, 512, 26431,
  399, 294, 3125, 13, 583, 1071, 1778, 983, 291, 393, 536, 26431, 990, 51184], "temperature":
  0.0, "avg_logprob": -0.17627362073478053, "compression_ratio": 1.4915254237288136,
  "no_speech_prob": 0.0008666721405461431}, {"id": 141, "seek": 102112, "start": 1037.52,
  "end": 1046.0, "text": " results just because some like you have fixed representations,
  the number of words is huge.", "tokens": [51184, 3542, 445, 570, 512, 411, 291,
  362, 6806, 33358, 11, 264, 1230, 295, 2283, 307, 2603, 13, 51608], "temperature":
  0.0, "avg_logprob": -0.17627362073478053, "compression_ratio": 1.4915254237288136,
  "no_speech_prob": 0.0008666721405461431}, {"id": 142, "seek": 104600, "start": 1046.96,
  "end": 1054.4, "text": " And like in regular person knows like around like educated
  person knows about 30,000 words,", "tokens": [50412, 400, 411, 294, 3890, 954, 3255,
  411, 926, 411, 15872, 954, 3255, 466, 2217, 11, 1360, 2283, 11, 50784], "temperature":
  0.0, "avg_logprob": -0.26194129519992404, "compression_ratio": 1.76036866359447,
  "no_speech_prob": 0.022743871435523033}, {"id": 143, "seek": 104600, "start": 1054.88,
  "end": 1062.8, "text": " but in reality, like internet has millions of words, right?
  And the words are not just only words,", "tokens": [50808, 457, 294, 4103, 11, 411,
  4705, 575, 6803, 295, 2283, 11, 558, 30, 400, 264, 2283, 366, 406, 445, 787, 2283,
  11, 51204], "temperature": 0.0, "avg_logprob": -0.26194129519992404, "compression_ratio":
  1.76036866359447, "no_speech_prob": 0.022743871435523033}, {"id": 144, "seek": 104600,
  "start": 1062.8, "end": 1069.12, "text": " there are things like product identifiers,
  right? If you want to, and sometimes people will do", "tokens": [51204, 456, 366,
  721, 411, 1674, 2473, 23463, 11, 558, 30, 759, 291, 528, 281, 11, 293, 2171, 561,
  486, 360, 51520], "temperature": 0.0, "avg_logprob": -0.26194129519992404, "compression_ratio":
  1.76036866359447, "no_speech_prob": 0.022743871435523033}, {"id": 145, "seek": 104600,
  "start": 1069.12, "end": 1074.8, "text": " products, they will search something
  they want to buy, and they would you know copy paste those,", "tokens": [51520,
  3383, 11, 436, 486, 3164, 746, 436, 528, 281, 2256, 11, 293, 436, 576, 291, 458,
  5055, 9163, 729, 11, 51804], "temperature": 0.0, "avg_logprob": -0.26194129519992404,
  "compression_ratio": 1.76036866359447, "no_speech_prob": 0.022743871435523033},
  {"id": 146, "seek": 107480, "start": 1074.8, "end": 1083.28, "text": " or type them
  in, and then they got squished in the in that dense vector. So it cannot be precise.",
  "tokens": [50364, 420, 2010, 552, 294, 11, 293, 550, 436, 658, 2339, 4729, 294,
  264, 294, 300, 18011, 8062, 13, 407, 309, 2644, 312, 13600, 13, 50788], "temperature":
  0.0, "avg_logprob": -0.40208232402801514, "compression_ratio": 1.4912280701754386,
  "no_speech_prob": 0.0035374897997826338}, {"id": 147, "seek": 107480, "start": 1084.24,
  "end": 1090.6399999999999, "text": " There is an interesting paper by author by
  Neil Srymer''s,", "tokens": [50836, 821, 307, 364, 1880, 3035, 538, 3793, 538, 18615,
  318, 627, 936, 311, 11, 51156], "temperature": 0.0, "avg_logprob": -0.40208232402801514,
  "compression_ratio": 1.4912280701754386, "no_speech_prob": 0.0035374897997826338},
  {"id": 148, "seek": 107480, "start": 1091.52, "end": 1100.1599999999999, "text":
  " sentence board author, where he has a, in like some experimental and even theoretical
  evidence that", "tokens": [51200, 8174, 3150, 3793, 11, 689, 415, 575, 257, 11,
  294, 411, 512, 17069, 293, 754, 20864, 4467, 300, 51632], "temperature": 0.0, "avg_logprob":
  -0.40208232402801514, "compression_ratio": 1.4912280701754386, "no_speech_prob":
  0.0035374897997826338}, {"id": 149, "seek": 110016, "start": 1100.5600000000002,
  "end": 1108.0, "text": " as the collection size increases so the dense vector search
  can deteriorate just because there", "tokens": [50384, 382, 264, 5765, 2744, 8637,
  370, 264, 18011, 8062, 3164, 393, 26431, 473, 445, 570, 456, 50756], "temperature":
  0.0, "avg_logprob": -0.2601868414109753, "compression_ratio": 1.5495867768595042,
  "no_speech_prob": 0.006780822295695543}, {"id": 150, "seek": 110016, "start": 1108.0,
  "end": 1112.96, "text": " would be some false positives and measures due to you
  know the excruciating a lot of information", "tokens": [50756, 576, 312, 512, 7908,
  35127, 293, 8000, 3462, 281, 291, 458, 264, 1624, 894, 537, 990, 257, 688, 295,
  1589, 51004], "temperature": 0.0, "avg_logprob": -0.2601868414109753, "compression_ratio":
  1.5495867768595042, "no_speech_prob": 0.006780822295695543}, {"id": 151, "seek":
  110016, "start": 1112.96, "end": 1121.1200000000001, "text": " together and they
  fix size directly. So yeah, I mean, it''s quite possible, but I haven''t seen",
  "tokens": [51004, 1214, 293, 436, 3191, 2744, 3838, 13, 407, 1338, 11, 286, 914,
  11, 309, 311, 1596, 1944, 11, 457, 286, 2378, 380, 1612, 51412], "temperature":
  0.0, "avg_logprob": -0.2601868414109753, "compression_ratio": 1.5495867768595042,
  "no_speech_prob": 0.006780822295695543}, {"id": 152, "seek": 110016, "start": 1121.1200000000001,
  "end": 1126.16, "text": " like a fall off of this work, so I don''t know how much
  of a problem it isn''t in practice.", "tokens": [51412, 411, 257, 2100, 766, 295,
  341, 589, 11, 370, 286, 500, 380, 458, 577, 709, 295, 257, 1154, 309, 1943, 380,
  294, 3124, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2601868414109753, "compression_ratio":
  1.5495867768595042, "no_speech_prob": 0.006780822295695543}, {"id": 153, "seek":
  112616, "start": 1126.96, "end": 1132.64, "text": " And coming back to the sparse
  representations, so yeah, they could potentially", "tokens": [50404, 400, 1348,
  646, 281, 264, 637, 11668, 33358, 11, 370, 1338, 11, 436, 727, 7263, 50688], "temperature":
  0.0, "avg_logprob": -0.2595607951536017, "compression_ratio": 1.6503067484662577,
  "no_speech_prob": 0.00905586127191782}, {"id": 154, "seek": 112616, "start": 1133.2,
  "end": 1140.3200000000002, "text": " use all this issue, but not necessarily with
  displayed like models. Well, the problem with", "tokens": [50716, 764, 439, 341,
  2734, 11, 457, 406, 4725, 365, 16372, 411, 5245, 13, 1042, 11, 264, 1154, 365, 51072],
  "temperature": 0.0, "avg_logprob": -0.2595607951536017, "compression_ratio": 1.6503067484662577,
  "no_speech_prob": 0.00905586127191782}, {"id": 155, "seek": 112616, "start": 1140.3200000000002,
  "end": 1148.88, "text": " display is that displayed models, they create those sparse
  representations using the, not the words", "tokens": [51072, 4674, 307, 300, 16372,
  5245, 11, 436, 1884, 729, 637, 11668, 33358, 1228, 264, 11, 406, 264, 2283, 51500],
  "temperature": 0.0, "avg_logprob": -0.2595607951536017, "compression_ratio": 1.6503067484662577,
  "no_speech_prob": 0.00905586127191782}, {"id": 156, "seek": 114888, "start": 1148.88,
  "end": 1158.48, "text": " themselves, they''re using sub word talking. So as a reminder
  with like models like with transform", "tokens": [50364, 2969, 11, 436, 434, 1228,
  1422, 1349, 1417, 13, 407, 382, 257, 13548, 365, 411, 5245, 411, 365, 4088, 50844],
  "temperature": 0.0, "avg_logprob": -0.22559534419666638, "compression_ratio": 1.5988700564971752,
  "no_speech_prob": 0.003961000591516495}, {"id": 157, "seek": 114888, "start": 1158.48,
  "end": 1169.0400000000002, "text": " models, they create this sort of new sort of
  vocabulary that has some complete words, but most", "tokens": [50844, 5245, 11,
  436, 1884, 341, 1333, 295, 777, 1333, 295, 19864, 300, 575, 512, 3566, 2283, 11,
  457, 881, 51372], "temperature": 0.0, "avg_logprob": -0.22559534419666638, "compression_ratio":
  1.5988700564971752, "no_speech_prob": 0.003961000591516495}, {"id": 158, "seek":
  114888, "start": 1169.0400000000002, "end": 1175.8400000000001, "text": " words
  are incomplete. So like they have like extract prefix, suffixes, parts of the words,",
  "tokens": [51372, 2283, 366, 31709, 13, 407, 411, 436, 362, 411, 8947, 46969, 11,
  3889, 36005, 11, 3166, 295, 264, 2283, 11, 51712], "temperature": 0.0, "avg_logprob":
  -0.22559534419666638, "compression_ratio": 1.5988700564971752, "no_speech_prob":
  0.003961000591516495}, {"id": 159, "seek": 117584, "start": 1175.84, "end": 1180.72,
  "text": " and this is your new vocabulary and the difference between these new vocabulary
  and the", "tokens": [50364, 293, 341, 307, 428, 777, 19864, 293, 264, 2649, 1296,
  613, 777, 19864, 293, 264, 50608], "temperature": 0.0, "avg_logprob": -0.1801014833672102,
  "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.005211897660046816},
  {"id": 160, "seek": 117584, "start": 1180.72, "end": 1187.04, "text": " actual vocabulary
  that people use or use on the internet is that it''s limited to, it can have like",
  "tokens": [50608, 3539, 19864, 300, 561, 764, 420, 764, 322, 264, 4705, 307, 300,
  309, 311, 5567, 281, 11, 309, 393, 362, 411, 50924], "temperature": 0.0, "avg_logprob":
  -0.1801014833672102, "compression_ratio": 1.7339449541284404, "no_speech_prob":
  0.005211897660046816}, {"id": 161, "seek": 117584, "start": 1187.04, "end": 1195.28,
  "text": " 50,000 talking, maybe 200 talking and some of the advanced modeling models,
  but we really have like", "tokens": [50924, 2625, 11, 1360, 1417, 11, 1310, 2331,
  1417, 293, 512, 295, 264, 7339, 15983, 5245, 11, 457, 321, 534, 362, 411, 51336],
  "temperature": 0.0, "avg_logprob": -0.1801014833672102, "compression_ratio": 1.7339449541284404,
  "no_speech_prob": 0.005211897660046816}, {"id": 162, "seek": 117584, "start": 1195.28,
  "end": 1201.28, "text": " millions and millions of words. So of course, that would
  also lead to some deterioration in", "tokens": [51336, 6803, 293, 6803, 295, 2283,
  13, 407, 295, 1164, 11, 300, 576, 611, 1477, 281, 512, 26431, 399, 294, 51636],
  "temperature": 0.0, "avg_logprob": -0.1801014833672102, "compression_ratio": 1.7339449541284404,
  "no_speech_prob": 0.005211897660046816}, {"id": 163, "seek": 120128, "start": 1201.36,
  "end": 1212.32, "text": " quality false positives, and especially if you try to
  represent, represent long documents", "tokens": [50368, 3125, 7908, 35127, 11, 293,
  2318, 498, 291, 853, 281, 2906, 11, 2906, 938, 8512, 50916], "temperature": 0.0,
  "avg_logprob": -0.19237678115432327, "compression_ratio": 1.543956043956044, "no_speech_prob":
  0.0014350195415318012}, {"id": 164, "seek": 120128, "start": 1212.32, "end": 1222.6399999999999,
  "text": " using this fixed size vector. So it''s sort of sparse in more, it''s more
  sparse in some ways,", "tokens": [50916, 1228, 341, 6806, 2744, 8062, 13, 407, 309,
  311, 1333, 295, 637, 11668, 294, 544, 11, 309, 311, 544, 637, 11668, 294, 512, 2098,
  11, 51432], "temperature": 0.0, "avg_logprob": -0.19237678115432327, "compression_ratio":
  1.543956043956044, "no_speech_prob": 0.0014350195415318012}, {"id": 165, "seek":
  120128, "start": 1222.6399999999999, "end": 1230.32, "text": " but it''s still fixed
  size vector. Doesn''t make sense. Yeah, it does. I mean, it''s very insightful,",
  "tokens": [51432, 457, 309, 311, 920, 6806, 2744, 8062, 13, 12955, 380, 652, 2020,
  13, 865, 11, 309, 775, 13, 286, 914, 11, 309, 311, 588, 46401, 11, 51816], "temperature":
  0.0, "avg_logprob": -0.19237678115432327, "compression_ratio": 1.543956043956044,
  "no_speech_prob": 0.0014350195415318012}, {"id": 166, "seek": 123032, "start": 1230.32,
  "end": 1238.24, "text": " what you said that like basically to make my question
  much more succinct, I could ask,", "tokens": [50364, 437, 291, 848, 300, 411, 1936,
  281, 652, 452, 1168, 709, 544, 21578, 5460, 11, 286, 727, 1029, 11, 50760], "temperature":
  0.0, "avg_logprob": -0.2425368813907399, "compression_ratio": 1.5480225988700564,
  "no_speech_prob": 0.002814227482303977}, {"id": 167, "seek": 123032, "start": 1238.8799999999999,
  "end": 1246.08, "text": " you could we just use splaid for everything? And like
  instead of, you know, combining different", "tokens": [50792, 291, 727, 321, 445,
  764, 637, 875, 327, 337, 1203, 30, 400, 411, 2602, 295, 11, 291, 458, 11, 21928,
  819, 51152], "temperature": 0.0, "avg_logprob": -0.2425368813907399, "compression_ratio":
  1.5480225988700564, "no_speech_prob": 0.002814227482303977}, {"id": 168, "seek":
  123032, "start": 1246.08, "end": 1251.4399999999998, "text": " approaches, just
  use splaid, but you basically answered it really eloquently. You said that", "tokens":
  [51152, 11587, 11, 445, 764, 637, 875, 327, 11, 457, 291, 1936, 10103, 309, 534,
  38682, 47519, 13, 509, 848, 300, 51420], "temperature": 0.0, "avg_logprob": -0.2425368813907399,
  "compression_ratio": 1.5480225988700564, "no_speech_prob": 0.002814227482303977},
  {"id": 169, "seek": 125144, "start": 1251.44, "end": 1258.56, "text": " splaid itself
  has limitations, right? For example, that would not allow us to properly embed",
  "tokens": [50364, 637, 875, 327, 2564, 575, 15705, 11, 558, 30, 1171, 1365, 11,
  300, 576, 406, 2089, 505, 281, 6108, 12240, 50720], "temperature": 0.0, "avg_logprob":
  -0.16277454745384953, "compression_ratio": 1.4791666666666667, "no_speech_prob":
  0.010628963820636272}, {"id": 170, "seek": 125144, "start": 1260.16, "end": 1265.04,
  "text": " all variety of the language and then obviously dealing with longer documents
  is another issue.", "tokens": [50800, 439, 5673, 295, 264, 2856, 293, 550, 2745,
  6260, 365, 2854, 8512, 307, 1071, 2734, 13, 51044], "temperature": 0.0, "avg_logprob":
  -0.16277454745384953, "compression_ratio": 1.4791666666666667, "no_speech_prob":
  0.010628963820636272}, {"id": 171, "seek": 125144, "start": 1267.1200000000001,
  "end": 1278.88, "text": " There is an interesting extension to this, so I was just
  recently listening to a presentation on", "tokens": [51148, 821, 307, 364, 1880,
  10320, 281, 341, 11, 370, 286, 390, 445, 3938, 4764, 281, 257, 5860, 322, 51736],
  "temperature": 0.0, "avg_logprob": -0.16277454745384953, "compression_ratio": 1.4791666666666667,
  "no_speech_prob": 0.010628963820636272}, {"id": 172, "seek": 127888, "start": 1278.88,
  "end": 1286.8000000000002, "text": " the extendable splaid where they extend the
  vocabulary of splaid by eddy entities. That''s one", "tokens": [50364, 264, 10101,
  712, 637, 875, 327, 689, 436, 10101, 264, 19864, 295, 637, 875, 327, 538, 1257,
  3173, 16667, 13, 663, 311, 472, 50760], "temperature": 0.0, "avg_logprob": -0.21571285005599733,
  "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.002434792695567012},
  {"id": 173, "seek": 127888, "start": 1286.8000000000002, "end": 1294.3200000000002,
  "text": " interesting direction of work, but another interesting direction is like
  the so-called like", "tokens": [50760, 1880, 3513, 295, 589, 11, 457, 1071, 1880,
  3513, 307, 411, 264, 370, 12, 11880, 411, 51136], "temperature": 0.0, "avg_logprob":
  -0.21571285005599733, "compression_ratio": 1.6506024096385543, "no_speech_prob":
  0.002434792695567012}, {"id": 174, "seek": 127888, "start": 1294.3200000000002,
  "end": 1303.92, "text": " deep impact models where they take a document and they
  do document expansion using like,", "tokens": [51136, 2452, 2712, 5245, 689, 436,
  747, 257, 4166, 293, 436, 360, 4166, 11260, 1228, 411, 11, 51616], "temperature":
  0.0, "avg_logprob": -0.21571285005599733, "compression_ratio": 1.6506024096385543,
  "no_speech_prob": 0.002434792695567012}, {"id": 175, "seek": 130392, "start": 1303.92,
  "end": 1310.88, "text": " you know, the doctor query style models. And then they
  for each talking, I think,", "tokens": [50364, 291, 458, 11, 264, 4631, 14581, 3758,
  5245, 13, 400, 550, 436, 337, 1184, 1417, 11, 286, 519, 11, 50712], "temperature":
  0.0, "avg_logprob": -0.28142738342285156, "compression_ratio": 1.562874251497006,
  "no_speech_prob": 0.0037546674720942974}, {"id": 176, "seek": 130392, "start": 1311.76,
  "end": 1318.48, "text": " in the document they are learning a weight. And so this
  is like a little bit more", "tokens": [50756, 294, 264, 4166, 436, 366, 2539, 257,
  3364, 13, 400, 370, 341, 307, 411, 257, 707, 857, 544, 51092], "temperature": 0.0,
  "avg_logprob": -0.28142738342285156, "compression_ratio": 1.562874251497006, "no_speech_prob":
  0.0037546674720942974}, {"id": 177, "seek": 130392, "start": 1319.92, "end": 1329.52,
  "text": " less limited, I think. But in the end, I think it''s whenever we, yeah,
  so basically if like to be", "tokens": [51164, 1570, 5567, 11, 286, 519, 13, 583,
  294, 264, 917, 11, 286, 519, 309, 311, 5699, 321, 11, 1338, 11, 370, 1936, 498,
  411, 281, 312, 51644], "temperature": 0.0, "avg_logprob": -0.28142738342285156,
  "compression_ratio": 1.562874251497006, "no_speech_prob": 0.0037546674720942974},
  {"id": 178, "seek": 132952, "start": 1329.6, "end": 1336.96, "text": " able to handle
  those like rare, we need lexical representation to handle, you know, bigger", "tokens":
  [50368, 1075, 281, 4813, 729, 411, 5892, 11, 321, 643, 476, 87, 804, 10290, 281,
  4813, 11, 291, 458, 11, 3801, 50736], "temperature": 0.0, "avg_logprob": -0.15265502532323202,
  "compression_ratio": 1.6, "no_speech_prob": 0.0022845545317977667}, {"id": 179,
  "seek": 132952, "start": 1336.96, "end": 1341.92, "text": " vocabularies. And it''s
  probably hard to model with just fixed size vectors.", "tokens": [50736, 2329, 455,
  1040, 530, 13, 400, 309, 311, 1391, 1152, 281, 2316, 365, 445, 6806, 2744, 18875,
  13, 50984], "temperature": 0.0, "avg_logprob": -0.15265502532323202, "compression_ratio":
  1.6, "no_speech_prob": 0.0022845545317977667}, {"id": 180, "seek": 132952, "start":
  1342.8799999999999, "end": 1349.92, "text": " Yeah, it makes a lot of sense. At
  the same time, we also know that, well, it depends on how you", "tokens": [51032,
  865, 11, 309, 1669, 257, 688, 295, 2020, 13, 1711, 264, 912, 565, 11, 321, 611,
  458, 300, 11, 731, 11, 309, 5946, 322, 577, 291, 51384], "temperature": 0.0, "avg_logprob":
  -0.15265502532323202, "compression_ratio": 1.6, "no_speech_prob": 0.0022845545317977667},
  {"id": 181, "seek": 132952, "start": 1349.92, "end": 1357.52, "text": " model this,
  but lexical approach, like vanilla lexical approach would miss semantic links, right,",
  "tokens": [51384, 2316, 341, 11, 457, 476, 87, 804, 3109, 11, 411, 17528, 476, 87,
  804, 3109, 576, 1713, 47982, 6123, 11, 558, 11, 51764], "temperature": 0.0, "avg_logprob":
  -0.15265502532323202, "compression_ratio": 1.6, "no_speech_prob": 0.0022845545317977667},
  {"id": 182, "seek": 135752, "start": 1357.52, "end": 1362.8799999999999, "text":
  " and sort of understanding of larger context, because all it does is that it kind
  of looks through", "tokens": [50364, 293, 1333, 295, 3701, 295, 4833, 4319, 11,
  570, 439, 309, 775, 307, 300, 309, 733, 295, 1542, 807, 50632], "temperature": 0.0,
  "avg_logprob": -0.16803340911865233, "compression_ratio": 1.696113074204947, "no_speech_prob":
  0.010459821671247482}, {"id": 183, "seek": 135752, "start": 1362.8799999999999,
  "end": 1368.16, "text": " the VM 25 model at the words. And sometimes it just pays
  attention to some words, but doesn''t", "tokens": [50632, 264, 18038, 3552, 2316,
  412, 264, 2283, 13, 400, 2171, 309, 445, 10604, 3202, 281, 512, 2283, 11, 457, 1177,
  380, 50896], "temperature": 0.0, "avg_logprob": -0.16803340911865233, "compression_ratio":
  1.696113074204947, "no_speech_prob": 0.010459821671247482}, {"id": 184, "seek":
  135752, "start": 1368.16, "end": 1374.24, "text": " pay attention to other words.
  And it may miss the main point of the query, right? But of course,", "tokens": [50896,
  1689, 3202, 281, 661, 2283, 13, 400, 309, 815, 1713, 264, 2135, 935, 295, 264, 14581,
  11, 558, 30, 583, 295, 1164, 11, 51200], "temperature": 0.0, "avg_logprob": -0.16803340911865233,
  "compression_ratio": 1.696113074204947, "no_speech_prob": 0.010459821671247482},
  {"id": 185, "seek": 135752, "start": 1374.24, "end": 1380.72, "text": " this model
  still worked for a new work that Yandex, you know, it best, this model''s worked",
  "tokens": [51200, 341, 2316, 920, 2732, 337, 257, 777, 589, 300, 398, 474, 3121,
  11, 291, 458, 11, 309, 1151, 11, 341, 2316, 311, 2732, 51524], "temperature": 0.0,
  "avg_logprob": -0.16803340911865233, "compression_ratio": 1.696113074204947, "no_speech_prob":
  0.010459821671247482}, {"id": 186, "seek": 135752, "start": 1380.72, "end": 1387.04,
  "text": " previously, probably by virtue of you training the users that, hey, don''t
  give me the full sentence,", "tokens": [51524, 8046, 11, 1391, 538, 20816, 295,
  291, 3097, 264, 5022, 300, 11, 4177, 11, 500, 380, 976, 385, 264, 1577, 8174, 11,
  51840], "temperature": 0.0, "avg_logprob": -0.16803340911865233, "compression_ratio":
  1.696113074204947, "no_speech_prob": 0.010459821671247482}, {"id": 187, "seek":
  138704, "start": 1387.04, "end": 1392.6399999999999, "text": " just give me like,
  you know, specific words, like chopped list of words that I need to look up.", "tokens":
  [50364, 445, 976, 385, 411, 11, 291, 458, 11, 2685, 2283, 11, 411, 16497, 1329,
  295, 2283, 300, 286, 643, 281, 574, 493, 13, 50644], "temperature": 0.0, "avg_logprob":
  -0.11994230045991786, "compression_ratio": 1.6695652173913043, "no_speech_prob":
  0.0012812362983822823}, {"id": 188, "seek": 138704, "start": 1392.6399999999999,
  "end": 1399.36, "text": " And that''s how I guess inverted index worked out. And
  of course, you need to have on top of that,", "tokens": [50644, 400, 300, 311, 577,
  286, 2041, 38969, 8186, 2732, 484, 13, 400, 295, 1164, 11, 291, 643, 281, 362, 322,
  1192, 295, 300, 11, 50980], "temperature": 0.0, "avg_logprob": -0.11994230045991786,
  "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.0012812362983822823},
  {"id": 189, "seek": 138704, "start": 1399.36, "end": 1405.36, "text": " you need
  to have very smart reranking strategy to pull up the documents that are really relevant,",
  "tokens": [50980, 291, 643, 281, 362, 588, 4069, 319, 20479, 278, 5206, 281, 2235,
  493, 264, 8512, 300, 366, 534, 7340, 11, 51280], "temperature": 0.0, "avg_logprob":
  -0.11994230045991786, "compression_ratio": 1.6695652173913043, "no_speech_prob":
  0.0012812362983822823}, {"id": 190, "seek": 138704, "start": 1405.36, "end": 1412.56,
  "text": " right? But I guess today we have we have this new, well, I keep calling
  it new, but it''s not", "tokens": [51280, 558, 30, 583, 286, 2041, 965, 321, 362,
  321, 362, 341, 777, 11, 731, 11, 286, 1066, 5141, 309, 777, 11, 457, 309, 311, 406,
  51640], "temperature": 0.0, "avg_logprob": -0.11994230045991786, "compression_ratio":
  1.6695652173913043, "no_speech_prob": 0.0012812362983822823}, {"id": 191, "seek":
  141256, "start": 1412.56, "end": 1419.2, "text": " maybe necessarily that new, but
  it''s still fairly fresh development of dense,", "tokens": [50364, 1310, 4725, 300,
  777, 11, 457, 309, 311, 920, 6457, 4451, 3250, 295, 18011, 11, 50696], "temperature":
  0.0, "avg_logprob": -0.15776309967041016, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.003212756710126996}, {"id": 192, "seek": 141256, "start": 1419.2,
  "end": 1424.8, "text": " dense retrieval that not many companies, I think, have
  been boarded in the products yet.", "tokens": [50696, 18011, 19817, 3337, 300, 406,
  867, 3431, 11, 286, 519, 11, 362, 668, 3150, 292, 294, 264, 3383, 1939, 13, 50976],
  "temperature": 0.0, "avg_logprob": -0.15776309967041016, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.003212756710126996}, {"id": 193, "seek": 141256, "start": 1425.9199999999998,
  "end": 1431.04, "text": " But it''s a very interesting direction, and still you
  need to combine the two worlds, right?", "tokens": [51032, 583, 309, 311, 257, 588,
  1880, 3513, 11, 293, 920, 291, 643, 281, 10432, 264, 732, 13401, 11, 558, 30, 51288],
  "temperature": 0.0, "avg_logprob": -0.15776309967041016, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.003212756710126996}, {"id": 194, "seek": 141256, "start": 1431.04,
  "end": 1438.72, "text": " So it sounds like from what you said, the only way to
  get better quality is to combine this", "tokens": [51288, 407, 309, 3263, 411, 490,
  437, 291, 848, 11, 264, 787, 636, 281, 483, 1101, 3125, 307, 281, 10432, 341, 51672],
  "temperature": 0.0, "avg_logprob": -0.15776309967041016, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.003212756710126996}, {"id": 195, "seek": 143872, "start": 1438.72,
  "end": 1444.48, "text": " approaches rather than try to develop one single holistic
  model to handle everything.", "tokens": [50364, 11587, 2831, 813, 853, 281, 1499,
  472, 2167, 30334, 2316, 281, 4813, 1203, 13, 50652], "temperature": 0.0, "avg_logprob":
  -0.19412146175608916, "compression_ratio": 1.5963302752293578, "no_speech_prob":
  0.013916502706706524}, {"id": 196, "seek": 143872, "start": 1447.2, "end": 1452.56,
  "text": " Oh, I, yeah, it''s a great question. I actually don''t know what''s the
  best part forward is.", "tokens": [50788, 876, 11, 286, 11, 1338, 11, 309, 311,
  257, 869, 1168, 13, 286, 767, 500, 380, 458, 437, 311, 264, 1151, 644, 2128, 307,
  13, 51056], "temperature": 0.0, "avg_logprob": -0.19412146175608916, "compression_ratio":
  1.5963302752293578, "no_speech_prob": 0.013916502706706524}, {"id": 197, "seek":
  143872, "start": 1453.28, "end": 1459.1200000000001, "text": " So I highlighted
  the, the deficiencies and advantages of different approaches.", "tokens": [51092,
  407, 286, 17173, 264, 11, 264, 19248, 31294, 293, 14906, 295, 819, 11587, 13, 51384],
  "temperature": 0.0, "avg_logprob": -0.19412146175608916, "compression_ratio": 1.5963302752293578,
  "no_speech_prob": 0.013916502706706524}, {"id": 198, "seek": 143872, "start": 1461.2,
  "end": 1466.08, "text": " But I also want to comment on the deep impact model, the
  deep impact model, I think the way,", "tokens": [51488, 583, 286, 611, 528, 281,
  2871, 322, 264, 2452, 2712, 2316, 11, 264, 2452, 2712, 2316, 11, 286, 519, 264,
  636, 11, 51732], "temperature": 0.0, "avg_logprob": -0.19412146175608916, "compression_ratio":
  1.5963302752293578, "no_speech_prob": 0.013916502706706524}, {"id": 199, "seek":
  146608, "start": 1466.08, "end": 1474.96, "text": " maybe I described it, it was,
  it sounded like it is like a BM25 model, but it''s actually not.", "tokens": [50364,
  1310, 286, 7619, 309, 11, 309, 390, 11, 309, 17714, 411, 309, 307, 411, 257, 15901,
  6074, 2316, 11, 457, 309, 311, 767, 406, 13, 50808], "temperature": 0.0, "avg_logprob":
  -0.22637695141052933, "compression_ratio": 1.6728971962616823, "no_speech_prob":
  0.003180060302838683}, {"id": 200, "seek": 146608, "start": 1474.96, "end": 1480.48,
  "text": " So maybe we should have, like we''re talking about sparse representations,
  like learned sparse", "tokens": [50808, 407, 1310, 321, 820, 362, 11, 411, 321,
  434, 1417, 466, 637, 11668, 33358, 11, 411, 3264, 637, 11668, 51084], "temperature":
  0.0, "avg_logprob": -0.22637695141052933, "compression_ratio": 1.6728971962616823,
  "no_speech_prob": 0.003180060302838683}, {"id": 201, "seek": 146608, "start": 1480.48,
  "end": 1485.76, "text": " representations, because it''s a bigger topic and it''s
  much bigger topic than most people", "tokens": [51084, 33358, 11, 570, 309, 311,
  257, 3801, 4829, 293, 309, 311, 709, 3801, 4829, 813, 881, 561, 51348], "temperature":
  0.0, "avg_logprob": -0.22637695141052933, "compression_ratio": 1.6728971962616823,
  "no_speech_prob": 0.003180060302838683}, {"id": 202, "seek": 146608, "start": 1486.6399999999999,
  "end": 1492.32, "text": " realize sometimes. So people know BM25, people know dense
  vectors, and these are,", "tokens": [51392, 4325, 2171, 13, 407, 561, 458, 15901,
  6074, 11, 561, 458, 18011, 18875, 11, 293, 613, 366, 11, 51676], "temperature":
  0.0, "avg_logprob": -0.22637695141052933, "compression_ratio": 1.6728971962616823,
  "no_speech_prob": 0.003180060302838683}, {"id": 203, "seek": 149232, "start": 1493.28,
  "end": 1498.96, "text": " these simple things, but there is a lot in between. So
  first of all, what you can do, and that''s what", "tokens": [50412, 613, 2199, 721,
  11, 457, 456, 307, 257, 688, 294, 1296, 13, 407, 700, 295, 439, 11, 437, 291, 393,
  360, 11, 293, 300, 311, 437, 50696], "temperature": 0.0, "avg_logprob": -0.15300087881560373,
  "compression_ratio": 1.7210300429184548, "no_speech_prob": 0.007591988891363144},
  {"id": 204, "seek": 149232, "start": 1498.96, "end": 1505.9199999999998, "text":
  " people did, and even the doctor query is the most famous way to do so, but it
  was actually not even", "tokens": [50696, 561, 630, 11, 293, 754, 264, 4631, 14581,
  307, 264, 881, 4618, 636, 281, 360, 370, 11, 457, 309, 390, 767, 406, 754, 51044],
  "temperature": 0.0, "avg_logprob": -0.15300087881560373, "compression_ratio": 1.7210300429184548,
  "no_speech_prob": 0.007591988891363144}, {"id": 205, "seek": 149232, "start": 1505.9199999999998,
  "end": 1512.32, "text": " a single group of people who proposed this. So what can
  you do? We can take a model, a deep learning", "tokens": [51044, 257, 2167, 1594,
  295, 561, 567, 10348, 341, 13, 407, 437, 393, 291, 360, 30, 492, 393, 747, 257,
  2316, 11, 257, 2452, 2539, 51364], "temperature": 0.0, "avg_logprob": -0.15300087881560373,
  "compression_ratio": 1.7210300429184548, "no_speech_prob": 0.007591988891363144},
  {"id": 206, "seek": 149232, "start": 1512.32, "end": 1518.48, "text": " model, contextualized
  model, maybe not necessarily contextualized, but contextualized models, they", "tokens":
  [51364, 2316, 11, 35526, 1602, 2316, 11, 1310, 406, 4725, 35526, 1602, 11, 457,
  35526, 1602, 5245, 11, 436, 51672], "temperature": 0.0, "avg_logprob": -0.15300087881560373,
  "compression_ratio": 1.7210300429184548, "no_speech_prob": 0.007591988891363144},
  {"id": 207, "seek": 151848, "start": 1519.1200000000001, "end": 1524.4, "text":
  " do better job because they look at the model as a whole, the document as a whole,
  they don''t like", "tokens": [50396, 360, 1101, 1691, 570, 436, 574, 412, 264, 2316,
  382, 257, 1379, 11, 264, 4166, 382, 257, 1379, 11, 436, 500, 380, 411, 50660], "temperature":
  0.0, "avg_logprob": -0.1810528594668549, "compression_ratio": 1.7702702702702702,
  "no_speech_prob": 0.004379728808999062}, {"id": 208, "seek": 151848, "start": 1524.4,
  "end": 1531.3600000000001, "text": " look like a devidue chunks of document, right?
  So they kind of can understand what the total meaning", "tokens": [50660, 574, 411,
  257, 1905, 327, 622, 24004, 295, 4166, 11, 558, 30, 407, 436, 733, 295, 393, 1223,
  437, 264, 3217, 3620, 51008], "temperature": 0.0, "avg_logprob": -0.1810528594668549,
  "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.004379728808999062},
  {"id": 209, "seek": 151848, "start": 1531.3600000000001, "end": 1540.4, "text":
  " of the document. And then they, they propose new keywords on new terms. So some
  like synonyms,", "tokens": [51008, 295, 264, 4166, 13, 400, 550, 436, 11, 436, 17421,
  777, 21009, 322, 777, 2115, 13, 407, 512, 411, 5451, 2526, 2592, 11, 51460], "temperature":
  0.0, "avg_logprob": -0.1810528594668549, "compression_ratio": 1.7702702702702702,
  "no_speech_prob": 0.004379728808999062}, {"id": 210, "seek": 151848, "start": 1541.28,
  "end": 1546.88, "text": " synonyms that could have been in this document, but they
  are not. And if you add these documents to", "tokens": [51504, 5451, 2526, 2592,
  300, 727, 362, 668, 294, 341, 4166, 11, 457, 436, 366, 406, 13, 400, 498, 291, 909,
  613, 8512, 281, 51784], "temperature": 0.0, "avg_logprob": -0.1810528594668549,
  "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.004379728808999062},
  {"id": 211, "seek": 154688, "start": 1546.88, "end": 1556.4, "text": " the new,
  if you add, sorry pardon me, if you add these terms to the document, then this missing",
  "tokens": [50364, 264, 777, 11, 498, 291, 909, 11, 2597, 22440, 385, 11, 498, 291,
  909, 613, 2115, 281, 264, 4166, 11, 550, 341, 5361, 50840], "temperature": 0.0,
  "avg_logprob": -0.18886938610592405, "compression_ratio": 1.748502994011976, "no_speech_prob":
  0.0018137841252610087}, {"id": 212, "seek": 154688, "start": 1556.4, "end": 1564.64,
  "text": " synonyms are there. You can index this document. So basically this is
  document expansion. And you", "tokens": [50840, 5451, 2526, 2592, 366, 456, 13,
  509, 393, 8186, 341, 4166, 13, 407, 1936, 341, 307, 4166, 11260, 13, 400, 291, 51252],
  "temperature": 0.0, "avg_logprob": -0.18886938610592405, "compression_ratio": 1.748502994011976,
  "no_speech_prob": 0.0018137841252610087}, {"id": 213, "seek": 154688, "start": 1564.64,
  "end": 1573.3600000000001, "text": " can do document expansion. And that helps resolve
  that lexical mismatch, mitigate lexical mismatch", "tokens": [51252, 393, 360, 4166,
  11260, 13, 400, 300, 3665, 14151, 300, 476, 87, 804, 23220, 852, 11, 27336, 476,
  87, 804, 23220, 852, 51688], "temperature": 0.0, "avg_logprob": -0.18886938610592405,
  "compression_ratio": 1.748502994011976, "no_speech_prob": 0.0018137841252610087},
  {"id": 214, "seek": 157336, "start": 1573.36, "end": 1579.1999999999998, "text":
  " between query and documents. And I claim it''s easier to do this expansion.",
  "tokens": [50364, 1296, 14581, 293, 8512, 13, 400, 286, 3932, 309, 311, 3571, 281,
  360, 341, 11260, 13, 50656], "temperature": 0.0, "avg_logprob": -0.18294605928308824,
  "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.002375661861151457},
  {"id": 215, "seek": 157336, "start": 1579.84, "end": 1585.6799999999998, "text":
  " That there are like of course approaches that do query expansion, basically adding
  synonyms at", "tokens": [50688, 663, 456, 366, 411, 295, 1164, 11587, 300, 360,
  14581, 11260, 11, 1936, 5127, 5451, 2526, 2592, 412, 50980], "temperature": 0.0,
  "avg_logprob": -0.18294605928308824, "compression_ratio": 1.6409090909090909, "no_speech_prob":
  0.002375661861151457}, {"id": 216, "seek": 157336, "start": 1585.6799999999998,
  "end": 1591.6, "text": " the query stage. But why claim is that it''s much harder
  to do this accurately because there is", "tokens": [50980, 264, 14581, 3233, 13,
  583, 983, 3932, 307, 300, 309, 311, 709, 6081, 281, 360, 341, 20095, 570, 456, 307,
  51276], "temperature": 0.0, "avg_logprob": -0.18294605928308824, "compression_ratio":
  1.6409090909090909, "no_speech_prob": 0.002375661861151457}, {"id": 217, "seek":
  157336, "start": 1591.6, "end": 1601.76, "text": " much less context. So this is
  one, you know, this is one direction of fixing things and creating", "tokens": [51276,
  709, 1570, 4319, 13, 407, 341, 307, 472, 11, 291, 458, 11, 341, 307, 472, 3513,
  295, 19442, 721, 293, 4084, 51784], "temperature": 0.0, "avg_logprob": -0.18294605928308824,
  "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.002375661861151457},
  {"id": 218, "seek": 160176, "start": 1601.76, "end": 1607.12, "text": " sparse representations.
  Like there is a split model. What does this play? What does this play", "tokens":
  [50364, 637, 11668, 33358, 13, 1743, 456, 307, 257, 7472, 2316, 13, 708, 775, 341,
  862, 30, 708, 775, 341, 862, 50632], "temperature": 0.0, "avg_logprob": -0.27197414197419817,
  "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.004775272682309151},
  {"id": 219, "seek": 160176, "start": 1607.12, "end": 1613.68, "text": " model? It''s
  completely sort of it''s doing something completely different. It looks at the document.",
  "tokens": [50632, 2316, 30, 467, 311, 2584, 1333, 295, 309, 311, 884, 746, 2584,
  819, 13, 467, 1542, 412, 264, 4166, 13, 50960], "temperature": 0.0, "avg_logprob":
  -0.27197414197419817, "compression_ratio": 1.7636363636363637, "no_speech_prob":
  0.004775272682309151}, {"id": 220, "seek": 160176, "start": 1613.68, "end": 1621.6,
  "text": " And there is a vocabulary, like bird tokens. And for each token, it gives
  you a weight. It looks at", "tokens": [50960, 400, 456, 307, 257, 19864, 11, 411,
  5255, 22667, 13, 400, 337, 1184, 14862, 11, 309, 2709, 291, 257, 3364, 13, 467,
  1542, 412, 51356], "temperature": 0.0, "avg_logprob": -0.27197414197419817, "compression_ratio":
  1.7636363636363637, "no_speech_prob": 0.004775272682309151}, {"id": 221, "seek":
  160176, "start": 1621.6, "end": 1626.8, "text": " the document sort of understand
  this meaning that says, all like this is like, this is a word,", "tokens": [51356,
  264, 4166, 1333, 295, 1223, 341, 3620, 300, 1619, 11, 439, 411, 341, 307, 411, 11,
  341, 307, 257, 1349, 11, 51616], "temperature": 0.0, "avg_logprob": -0.27197414197419817,
  "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.004775272682309151},
  {"id": 222, "seek": 162680, "start": 1626.8, "end": 1631.68, "text": " a prefix
  or word. It should have this weight. And that''s how you get a sparse representation.",
  "tokens": [50364, 257, 46969, 420, 1349, 13, 467, 820, 362, 341, 3364, 13, 400,
  300, 311, 577, 291, 483, 257, 637, 11668, 10290, 13, 50608], "temperature": 0.0,
  "avg_logprob": -0.18384494384129843, "compression_ratio": 1.6065573770491803, "no_speech_prob":
  0.001982129644602537}, {"id": 223, "seek": 162680, "start": 1632.56, "end": 1638.56,
  "text": " But with deep impact, you''re doing something slightly different. So you
  take a document and you do", "tokens": [50652, 583, 365, 2452, 2712, 11, 291, 434,
  884, 746, 4748, 819, 13, 407, 291, 747, 257, 4166, 293, 291, 360, 50952], "temperature":
  0.0, "avg_logprob": -0.18384494384129843, "compression_ratio": 1.6065573770491803,
  "no_speech_prob": 0.001982129644602537}, {"id": 224, "seek": 162680, "start": 1638.56,
  "end": 1645.6, "text": " this document expansion. So you add words like synonyms.
  But then you don''t index this document using", "tokens": [50952, 341, 4166, 11260,
  13, 407, 291, 909, 2283, 411, 5451, 2526, 2592, 13, 583, 550, 291, 500, 380, 8186,
  341, 4166, 1228, 51304], "temperature": 0.0, "avg_logprob": -0.18384494384129843,
  "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.001982129644602537},
  {"id": 225, "seek": 162680, "start": 1645.6, "end": 1653.52, "text": " build 25.
  Why? Because build 25 is clearly old style and it doesn''t take like context to
  account.", "tokens": [51304, 1322, 3552, 13, 1545, 30, 1436, 1322, 3552, 307, 4448,
  1331, 3758, 293, 309, 1177, 380, 747, 411, 4319, 281, 2696, 13, 51700], "temperature":
  0.0, "avg_logprob": -0.18384494384129843, "compression_ratio": 1.6065573770491803,
  "no_speech_prob": 0.001982129644602537}, {"id": 226, "seek": 165352, "start": 1653.52,
  "end": 1660.08, "text": " So instead of that, you train a transform model that would
  give you weight for each", "tokens": [50364, 407, 2602, 295, 300, 11, 291, 3847,
  257, 4088, 2316, 300, 576, 976, 291, 3364, 337, 1184, 50692], "temperature": 0.0,
  "avg_logprob": -0.24819218028675427, "compression_ratio": 1.6782178217821782, "no_speech_prob":
  0.002493984531611204}, {"id": 227, "seek": 165352, "start": 1661.04, "end": 1667.44,
  "text": " term in the document, in the expanded document. And then you use this
  for it.", "tokens": [50740, 1433, 294, 264, 4166, 11, 294, 264, 14342, 4166, 13,
  400, 550, 291, 764, 341, 337, 309, 13, 51060], "temperature": 0.0, "avg_logprob":
  -0.24819218028675427, "compression_ratio": 1.6782178217821782, "no_speech_prob":
  0.002493984531611204}, {"id": 228, "seek": 165352, "start": 1669.92, "end": 1675.28,
  "text": " Oh, that''s very easy. And that''s called deep and that''s called deep
  impact models.", "tokens": [51184, 876, 11, 300, 311, 588, 1858, 13, 400, 300, 311,
  1219, 2452, 293, 300, 311, 1219, 2452, 2712, 5245, 13, 51452], "temperature": 0.0,
  "avg_logprob": -0.24819218028675427, "compression_ratio": 1.6782178217821782, "no_speech_prob":
  0.002493984531611204}, {"id": 229, "seek": 165352, "start": 1675.28, "end": 1681.52,
  "text": " Yes. Yeah. We should link that I guess there is a paper for that as well
  and should be able to", "tokens": [51452, 1079, 13, 865, 13, 492, 820, 2113, 300,
  286, 2041, 456, 307, 257, 3035, 337, 300, 382, 731, 293, 820, 312, 1075, 281, 51764],
  "temperature": 0.0, "avg_logprob": -0.24819218028675427, "compression_ratio": 1.6782178217821782,
  "no_speech_prob": 0.002493984531611204}, {"id": 230, "seek": 168152, "start": 1681.52,
  "end": 1688.32, "text": " link that. Yeah. That''s very interesting. And it''s also
  interesting that what you mentioned about", "tokens": [50364, 2113, 300, 13, 865,
  13, 663, 311, 588, 1880, 13, 400, 309, 311, 611, 1880, 300, 437, 291, 2835, 466,
  50704], "temperature": 0.0, "avg_logprob": -0.31791125768902656, "compression_ratio":
  1.5982532751091703, "no_speech_prob": 0.003316299756988883}, {"id": 231, "seek":
  168152, "start": 1688.32, "end": 1695.12, "text": " the dance model sort of not
  able to capture everything that you want them to capture.", "tokens": [50704, 264,
  4489, 2316, 1333, 295, 406, 1075, 281, 7983, 1203, 300, 291, 528, 552, 281, 7983,
  13, 51044], "temperature": 0.0, "avg_logprob": -0.31791125768902656, "compression_ratio":
  1.5982532751091703, "no_speech_prob": 0.003316299756988883}, {"id": 232, "seek":
  168152, "start": 1695.68, "end": 1701.28, "text": " And yet, this becomes a building
  block in the application phase, like for example, in", "tokens": [51072, 400, 1939,
  11, 341, 3643, 257, 2390, 3461, 294, 264, 3861, 5574, 11, 411, 337, 1365, 11, 294,
  51352], "temperature": 0.0, "avg_logprob": -0.31791125768902656, "compression_ratio":
  1.5982532751091703, "no_speech_prob": 0.003316299756988883}, {"id": 233, "seek":
  168152, "start": 1701.28, "end": 1707.52, "text": " Rage or a Givalogmented Generation
  because effectively, the only method that I heard so far off,", "tokens": [51352,
  497, 609, 420, 257, 460, 3576, 664, 14684, 23898, 570, 8659, 11, 264, 787, 3170,
  300, 286, 2198, 370, 1400, 766, 11, 51664], "temperature": 0.0, "avg_logprob": -0.31791125768902656,
  "compression_ratio": 1.5982532751091703, "no_speech_prob": 0.003316299756988883},
  {"id": 234, "seek": 170752, "start": 1707.52, "end": 1714.4, "text": " which is
  circulating a lot is just chunk it up. You chunk all documents up and then you hope
  that", "tokens": [50364, 597, 307, 39749, 257, 688, 307, 445, 16635, 309, 493, 13,
  509, 16635, 439, 8512, 493, 293, 550, 291, 1454, 300, 50708], "temperature": 0.0,
  "avg_logprob": -0.16514055493851781, "compression_ratio": 1.553763440860215, "no_speech_prob":
  0.0061403014697134495}, {"id": 235, "seek": 170752, "start": 1716.24, "end": 1722.32,
  "text": " the chunk size is less or about the same as capacity of the model, right?
  Because otherwise,", "tokens": [50800, 264, 16635, 2744, 307, 1570, 420, 466, 264,
  912, 382, 6042, 295, 264, 2316, 11, 558, 30, 1436, 5911, 11, 51104], "temperature":
  0.0, "avg_logprob": -0.16514055493851781, "compression_ratio": 1.553763440860215,
  "no_speech_prob": 0.0061403014697134495}, {"id": 236, "seek": 170752, "start": 1722.32,
  "end": 1729.76, "text": " it will chop off the end and you will lose the part of
  the meaning. Or you also apply some methods", "tokens": [51104, 309, 486, 7931,
  766, 264, 917, 293, 291, 486, 3624, 264, 644, 295, 264, 3620, 13, 1610, 291, 611,
  3079, 512, 7150, 51476], "temperature": 0.0, "avg_logprob": -0.16514055493851781,
  "compression_ratio": 1.553763440860215, "no_speech_prob": 0.0061403014697134495},
  {"id": 237, "seek": 172976, "start": 1729.76, "end": 1737.68, "text": " like some
  level of overlap, right? So you can then index a few more chunks in the same entity",
  "tokens": [50364, 411, 512, 1496, 295, 19959, 11, 558, 30, 407, 291, 393, 550, 8186,
  257, 1326, 544, 24004, 294, 264, 912, 13977, 50760], "temperature": 0.0, "avg_logprob":
  -0.1615531041071965, "compression_ratio": 1.6491228070175439, "no_speech_prob":
  0.0025198108050972223}, {"id": 238, "seek": 172976, "start": 1737.68, "end": 1747.12,
  "text": " and then try to query. And then interestingly, you can generate questions
  out of chunks", "tokens": [50760, 293, 550, 853, 281, 14581, 13, 400, 550, 25873,
  11, 291, 393, 8460, 1651, 484, 295, 24004, 51232], "temperature": 0.0, "avg_logprob":
  -0.1615531041071965, "compression_ratio": 1.6491228070175439, "no_speech_prob":
  0.0025198108050972223}, {"id": 239, "seek": 172976, "start": 1747.44, "end": 1752.72,
  "text": " that these chunks might be able to answer. And then you search those questions
  instead of the chunks", "tokens": [51248, 300, 613, 24004, 1062, 312, 1075, 281,
  1867, 13, 400, 550, 291, 3164, 729, 1651, 2602, 295, 264, 24004, 51512], "temperature":
  0.0, "avg_logprob": -0.1615531041071965, "compression_ratio": 1.6491228070175439,
  "no_speech_prob": 0.0025198108050972223}, {"id": 240, "seek": 175272, "start": 1752.72,
  "end": 1759.68, "text": " themselves, right? So which comes back to what you said
  about Dr. Query, I guess. So it''s very", "tokens": [50364, 2969, 11, 558, 30, 407,
  597, 1487, 646, 281, 437, 291, 848, 466, 2491, 13, 2326, 2109, 11, 286, 2041, 13,
  407, 309, 311, 588, 50712], "temperature": 0.0, "avg_logprob": -0.2050280006982947,
  "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.023092610761523247},
  {"id": 241, "seek": 175272, "start": 1759.68, "end": 1768.88, "text": " interesting
  that like we are sort of like standing on a set of building blocks that themselves
  should", "tokens": [50712, 1880, 300, 411, 321, 366, 1333, 295, 411, 4877, 322,
  257, 992, 295, 2390, 8474, 300, 2969, 820, 51172], "temperature": 0.0, "avg_logprob":
  -0.2050280006982947, "compression_ratio": 1.6282051282051282, "no_speech_prob":
  0.023092610761523247}, {"id": 242, "seek": 175272, "start": 1769.6000000000001,
  "end": 1775.92, "text": " be optimized and optimized and optimized. But I guess
  we already in the phase globally when", "tokens": [51208, 312, 26941, 293, 26941,
  293, 26941, 13, 583, 286, 2041, 321, 1217, 294, 264, 5574, 18958, 562, 51524], "temperature":
  0.0, "avg_logprob": -0.2050280006982947, "compression_ratio": 1.6282051282051282,
  "no_speech_prob": 0.023092610761523247}, {"id": 243, "seek": 175272, "start": 1775.92,
  "end": 1782.0, "text": " everyone is trying to derive value from LLMs and Rags and
  everything, right? And yet, we can", "tokens": [51524, 1518, 307, 1382, 281, 28446,
  2158, 490, 441, 43, 26386, 293, 497, 12109, 293, 1203, 11, 558, 30, 400, 1939, 11,
  321, 393, 51828], "temperature": 0.0, "avg_logprob": -0.2050280006982947, "compression_ratio":
  1.6282051282051282, "no_speech_prob": 0.023092610761523247}, {"id": 244, "seek":
  178200, "start": 1782.0, "end": 1788.0, "text": " stumble upon some really tricky
  situations. Like you explained. Oh, it looks like we have a lot.", "tokens": [50364,
  41302, 3564, 512, 534, 12414, 6851, 13, 1743, 291, 8825, 13, 876, 11, 309, 1542,
  411, 321, 362, 257, 688, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2663711201060902,
  "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0013222168199717999},
  {"id": 245, "seek": 178200, "start": 1788.56, "end": 1796.24, "text": " Yeah, it
  looks like we have still a lot of research topics. Yeah. A lot of answering questions.",
  "tokens": [50692, 865, 11, 309, 1542, 411, 321, 362, 920, 257, 688, 295, 2132, 8378,
  13, 865, 13, 316, 688, 295, 13430, 1651, 13, 51076], "temperature": 0.0, "avg_logprob":
  -0.2663711201060902, "compression_ratio": 1.5508021390374331, "no_speech_prob":
  0.0013222168199717999}, {"id": 246, "seek": 178200, "start": 1797.2, "end": 1805.2,
  "text": " Yeah. I wanted to a little bit digress from here to the work you''ve done
  at NMS Lip and I want to", "tokens": [51124, 865, 13, 286, 1415, 281, 257, 707,
  857, 2528, 735, 490, 510, 281, 264, 589, 291, 600, 1096, 412, 426, 10288, 27475,
  293, 286, 528, 281, 51524], "temperature": 0.0, "avg_logprob": -0.2663711201060902,
  "compression_ratio": 1.5508021390374331, "no_speech_prob": 0.0013222168199717999},
  {"id": 247, "seek": 180520, "start": 1805.2, "end": 1813.68, "text": " read it from
  your from the GitHub repository. It''s a non-metrics space library. And I did spend
  some", "tokens": [50364, 1401, 309, 490, 428, 490, 264, 23331, 25841, 13, 467, 311,
  257, 2107, 12, 5537, 10716, 1901, 6405, 13, 400, 286, 630, 3496, 512, 50788], "temperature":
  0.0, "avg_logprob": -0.19532004992167154, "compression_ratio": 1.5294117647058822,
  "no_speech_prob": 0.004680975805968046}, {"id": 248, "seek": 180520, "start": 1813.68,
  "end": 1824.0800000000002, "text": " time in my rework life, you know, when I was
  studying mathematics and we did study a bunch of,", "tokens": [50788, 565, 294,
  452, 48376, 993, 11, 291, 458, 11, 562, 286, 390, 7601, 18666, 293, 321, 630, 2979,
  257, 3840, 295, 11, 51308], "temperature": 0.0, "avg_logprob": -0.19532004992167154,
  "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.004680975805968046},
  {"id": 249, "seek": 180520, "start": 1824.0800000000002, "end": 1830.88, "text":
  " you know, metric spaces. I have never realized I would never really like imagine
  that this", "tokens": [51308, 291, 458, 11, 20678, 7673, 13, 286, 362, 1128, 5334,
  286, 576, 1128, 534, 411, 3811, 300, 341, 51648], "temperature": 0.0, "avg_logprob":
  -0.19532004992167154, "compression_ratio": 1.5294117647058822, "no_speech_prob":
  0.004680975805968046}, {"id": 250, "seek": 183088, "start": 1831.2800000000002,
  "end": 1841.1200000000001, "text": " highly theoretical stuff would now connect
  so deeply to practice and it''s amazing. But can you tell me", "tokens": [50384,
  5405, 20864, 1507, 576, 586, 1745, 370, 8760, 281, 3124, 293, 309, 311, 2243, 13,
  583, 393, 291, 980, 385, 50876], "temperature": 0.0, "avg_logprob": -0.25581018924713134,
  "compression_ratio": 1.4759615384615385, "no_speech_prob": 0.0033354482147842646},
  {"id": 251, "seek": 183088, "start": 1841.1200000000001, "end": 1849.8400000000001,
  "text": " why it''s non-metrics space library? Isn''t it so that the whole idea
  of, you know, vector searches that", "tokens": [50876, 983, 309, 311, 2107, 12,
  5537, 10716, 1901, 6405, 30, 6998, 380, 309, 370, 300, 264, 1379, 1558, 295, 11,
  291, 458, 11, 8062, 26701, 300, 51312], "temperature": 0.0, "avg_logprob": -0.25581018924713134,
  "compression_ratio": 1.4759615384615385, "no_speech_prob": 0.0033354482147842646},
  {"id": 252, "seek": 183088, "start": 1849.8400000000001, "end": 1859.7600000000002,
  "text": " we choose some metric, Cousin or dot product or whatever it is. And we
  are, and that''s how we express", "tokens": [51312, 321, 2826, 512, 20678, 11, 383,
  563, 259, 420, 5893, 1674, 420, 2035, 309, 307, 13, 400, 321, 366, 11, 293, 300,
  311, 577, 321, 5109, 51808], "temperature": 0.0, "avg_logprob": -0.25581018924713134,
  "compression_ratio": 1.4759615384615385, "no_speech_prob": 0.0033354482147842646},
  {"id": 253, "seek": 185976, "start": 1859.84, "end": 1871.68, "text": " the semantics
  similarity. Great question. So the reason why it is we decided to not limit ourselves",
  "tokens": [50368, 264, 4361, 45298, 32194, 13, 3769, 1168, 13, 407, 264, 1778, 983,
  309, 307, 321, 3047, 281, 406, 4948, 4175, 50960], "temperature": 0.0, "avg_logprob":
  -0.1907815103945525, "compression_ratio": 1.4492753623188406, "no_speech_prob":
  0.0024719147477298975}, {"id": 254, "seek": 185976, "start": 1871.68, "end": 1880.24,
  "text": " for to metric search because we felt and that''s also a feeling of other
  people is that metric search", "tokens": [50960, 337, 281, 20678, 3164, 570, 321,
  2762, 293, 300, 311, 611, 257, 2633, 295, 661, 561, 307, 300, 20678, 3164, 51388],
  "temperature": 0.0, "avg_logprob": -0.1907815103945525, "compression_ratio": 1.4492753623188406,
  "no_speech_prob": 0.0024719147477298975}, {"id": 255, "seek": 188024, "start": 1880.32,
  "end": 1891.36, "text": " is is limiting. So it''s not expressive enough. It turned
  out to be true to some degree but not as much", "tokens": [50368, 307, 307, 22083,
  13, 407, 309, 311, 406, 40189, 1547, 13, 467, 3574, 484, 281, 312, 2074, 281, 512,
  4314, 457, 406, 382, 709, 50920], "temperature": 0.0, "avg_logprob": -0.23412871655122733,
  "compression_ratio": 1.515, "no_speech_prob": 0.008462520316243172}, {"id": 256,
  "seek": 188024, "start": 1891.36, "end": 1899.84, "text": " as we hoped. And indeed,
  in many cases, so and why we''re doing so, the representation learning was not",
  "tokens": [50920, 382, 321, 19737, 13, 400, 6451, 11, 294, 867, 3331, 11, 370, 293,
  983, 321, 434, 884, 370, 11, 264, 10290, 2539, 390, 406, 51344], "temperature":
  0.0, "avg_logprob": -0.23412871655122733, "compression_ratio": 1.515, "no_speech_prob":
  0.008462520316243172}, {"id": 257, "seek": 188024, "start": 1900.72, "end": 1909.84,
  "text": " as developed as it is now. So we felt like, you know, we need to be able
  to, people will engineer", "tokens": [51388, 382, 4743, 382, 309, 307, 586, 13,
  407, 321, 2762, 411, 11, 291, 458, 11, 321, 643, 281, 312, 1075, 281, 11, 561, 486,
  11403, 51844], "temperature": 0.0, "avg_logprob": -0.23412871655122733, "compression_ratio":
  1.515, "no_speech_prob": 0.008462520316243172}, {"id": 258, "seek": 190984, "start":
  1909.84, "end": 1915.76, "text": " those complex similarities and we need to support
  individual using this complex similarity.", "tokens": [50364, 729, 3997, 24197,
  293, 321, 643, 281, 1406, 2609, 1228, 341, 3997, 32194, 13, 50660], "temperature":
  0.0, "avg_logprob": -0.20895021192489133, "compression_ratio": 1.5706521739130435,
  "no_speech_prob": 0.0023419680073857307}, {"id": 259, "seek": 190984, "start": 1915.76,
  "end": 1924.6399999999999, "text": " This did not happen. But what I think happened
  and that''s I want to connect this to my statement", "tokens": [50660, 639, 630,
  406, 1051, 13, 583, 437, 286, 519, 2011, 293, 300, 311, 286, 528, 281, 1745, 341,
  281, 452, 5629, 51104], "temperature": 0.0, "avg_logprob": -0.20895021192489133,
  "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.0023419680073857307},
  {"id": 260, "seek": 190984, "start": 1924.6399999999999, "end": 1930.8, "text":
  " that in the end of my graduate studies or rather after defending a thesis, somebody
  pointed out that", "tokens": [51104, 300, 294, 264, 917, 295, 452, 8080, 5313, 420,
  2831, 934, 21377, 257, 22288, 11, 2618, 10932, 484, 300, 51412], "temperature":
  0.0, "avg_logprob": -0.20895021192489133, "compression_ratio": 1.5706521739130435,
  "no_speech_prob": 0.0023419680073857307}, {"id": 261, "seek": 193080, "start": 1931.76,
  "end": 1939.68, "text": " the similarities that we were using were basically representable
  as the sparse similar product between", "tokens": [50412, 264, 24197, 300, 321,
  645, 1228, 645, 1936, 2906, 712, 382, 264, 637, 11668, 2531, 1674, 1296, 50808],
  "temperature": 0.0, "avg_logprob": -0.23663894832134247, "compression_ratio": 1.576271186440678,
  "no_speech_prob": 0.003317478811368346}, {"id": 262, "seek": 193080, "start": 1940.48,
  "end": 1947.44, "text": " two huge vectors. So it''s some sort of it becomes similar
  to either deep impact or split.", "tokens": [50848, 732, 2603, 18875, 13, 407, 309,
  311, 512, 1333, 295, 309, 3643, 2531, 281, 2139, 2452, 2712, 420, 7472, 13, 51196],
  "temperature": 0.0, "avg_logprob": -0.23663894832134247, "compression_ratio": 1.576271186440678,
  "no_speech_prob": 0.003317478811368346}, {"id": 263, "seek": 193080, "start": 1948.6399999999999,
  "end": 1955.52, "text": " And in fact, so the similarity is the maximum product.
  It''s not the cosine similarity.", "tokens": [51256, 400, 294, 1186, 11, 370, 264,
  32194, 307, 264, 6674, 1674, 13, 467, 311, 406, 264, 23565, 32194, 13, 51600], "temperature":
  0.0, "avg_logprob": -0.23663894832134247, "compression_ratio": 1.576271186440678,
  "no_speech_prob": 0.003317478811368346}, {"id": 264, "seek": 195552, "start": 1956.4,
  "end": 1964.48, "text": " And the the search like the search procedure is called
  maximum inner product search. So basically,", "tokens": [50408, 400, 264, 264, 3164,
  411, 264, 3164, 10747, 307, 1219, 6674, 7284, 1674, 3164, 13, 407, 1936, 11, 50812],
  "temperature": 0.0, "avg_logprob": -0.2344076156616211, "compression_ratio": 1.7378048780487805,
  "no_speech_prob": 0.005871497560292482}, {"id": 265, "seek": 195552, "start": 1964.48,
  "end": 1972.16, "text": " you want to retrieve documents that have the maximum inner
  product between query and the document.", "tokens": [50812, 291, 528, 281, 30254,
  8512, 300, 362, 264, 6674, 7284, 1674, 1296, 14581, 293, 264, 4166, 13, 51196],
  "temperature": 0.0, "avg_logprob": -0.2344076156616211, "compression_ratio": 1.7378048780487805,
  "no_speech_prob": 0.005871497560292482}, {"id": 266, "seek": 195552, "start": 1972.16,
  "end": 1980.48, "text": " And the and this is not this is a symmetric similarity
  measure in some sense symmetric,", "tokens": [51196, 400, 264, 293, 341, 307, 406,
  341, 307, 257, 32330, 32194, 3481, 294, 512, 2020, 32330, 11, 51612], "temperature":
  0.0, "avg_logprob": -0.2344076156616211, "compression_ratio": 1.7378048780487805,
  "no_speech_prob": 0.005871497560292482}, {"id": 267, "seek": 198048, "start": 1980.48,
  "end": 1990.32, "text": " but it is not it is it is not a metric and it''s not easily
  reducible to the to the cosine similarity", "tokens": [50364, 457, 309, 307, 406,
  309, 307, 309, 307, 406, 257, 20678, 293, 309, 311, 406, 3612, 2783, 32128, 281,
  264, 281, 264, 23565, 32194, 50856], "temperature": 0.0, "avg_logprob": -0.2870780944824219,
  "compression_ratio": 1.7625, "no_speech_prob": 0.00257792416960001}, {"id": 268,
  "seek": 198048, "start": 1990.32, "end": 1996.0, "text": " and to the creature searching
  using a science similarity is actually fully equivalent to searching", "tokens":
  [50856, 293, 281, 264, 12797, 10808, 1228, 257, 3497, 32194, 307, 767, 4498, 10344,
  281, 10808, 51140], "temperature": 0.0, "avg_logprob": -0.2870780944824219, "compression_ratio":
  1.7625, "no_speech_prob": 0.00257792416960001}, {"id": 269, "seek": 198048, "start":
  1996.0, "end": 2002.32, "text": " using the Euclidean distance for the inner product
  you can reduce this search to a", "tokens": [51140, 1228, 264, 462, 1311, 31264,
  282, 4560, 337, 264, 7284, 1674, 291, 393, 5407, 341, 3164, 281, 257, 51456], "temperature":
  0.0, "avg_logprob": -0.2870780944824219, "compression_ratio": 1.7625, "no_speech_prob":
  0.00257792416960001}, {"id": 270, "seek": 200232, "start": 2003.2, "end": 2009.9199999999998,
  "text": " cosine similarity and Euclidean distance, but turns out that this reduction
  affects efficiency.", "tokens": [50408, 23565, 32194, 293, 462, 1311, 31264, 282,
  4560, 11, 457, 4523, 484, 300, 341, 11004, 11807, 10493, 13, 50744], "temperature":
  0.0, "avg_logprob": -0.2968607935412177, "compression_ratio": 1.6946902654867257,
  "no_speech_prob": 0.0021964393090456724}, {"id": 271, "seek": 200232, "start": 2010.96,
  "end": 2017.9199999999998, "text": " And then somewhat like bigger topic for a discussion,
  but what happened is that people say at at", "tokens": [50796, 400, 550, 8344, 411,
  3801, 4829, 337, 257, 5017, 11, 457, 437, 2011, 307, 300, 561, 584, 412, 412, 51144],
  "temperature": 0.0, "avg_logprob": -0.2968607935412177, "compression_ratio": 1.6946902654867257,
  "no_speech_prob": 0.0021964393090456724}, {"id": 272, "seek": 200232, "start": 2017.9199999999998,
  "end": 2023.04, "text": " Lucene, who are maintaining Lucene, they were adding support
  for the maximum inner product.", "tokens": [51144, 9593, 1450, 11, 567, 366, 14916,
  9593, 1450, 11, 436, 645, 5127, 1406, 337, 264, 6674, 7284, 1674, 13, 51400], "temperature":
  0.0, "avg_logprob": -0.2968607935412177, "compression_ratio": 1.6946902654867257,
  "no_speech_prob": 0.0021964393090456724}, {"id": 273, "seek": 200232, "start": 2023.76,
  "end": 2029.6, "text": " And Vespa did this and they did this through this trick
  to reducing of of maximum inner product to", "tokens": [51436, 400, 691, 279, 4306,
  630, 341, 293, 436, 630, 341, 807, 341, 4282, 281, 12245, 295, 295, 6674, 7284,
  1674, 281, 51728], "temperature": 0.0, "avg_logprob": -0.2968607935412177, "compression_ratio":
  1.6946902654867257, "no_speech_prob": 0.0021964393090456724}, {"id": 274, "seek":
  202960, "start": 2029.6, "end": 2036.24, "text": " cosine similarity and of two.
  And I argued that there is research showing that this is suboptimal", "tokens":
  [50364, 23565, 32194, 293, 295, 732, 13, 400, 286, 20219, 300, 456, 307, 2132, 4099,
  300, 341, 307, 1422, 5747, 10650, 50696], "temperature": 0.0, "avg_logprob": -0.21691173315048218,
  "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.0031810421496629715},
  {"id": 275, "seek": 202960, "start": 2036.24, "end": 2043.52, "text": " and there
  was a discussion in this as a result they basically didn''t do this. So so a long
  story short,", "tokens": [50696, 293, 456, 390, 257, 5017, 294, 341, 382, 257, 1874,
  436, 1936, 994, 380, 360, 341, 13, 407, 370, 257, 938, 1657, 2099, 11, 51060], "temperature":
  0.0, "avg_logprob": -0.21691173315048218, "compression_ratio": 1.6890756302521008,
  "no_speech_prob": 0.0031810421496629715}, {"id": 276, "seek": 202960, "start": 2044.32,
  "end": 2051.7599999999998, "text": " I think a lot of things are so non-metrics
  similarity search as in general turn out to be not so useful,", "tokens": [51100,
  286, 519, 257, 688, 295, 721, 366, 370, 2107, 12, 5537, 10716, 32194, 3164, 382,
  294, 2674, 1261, 484, 281, 312, 406, 370, 4420, 11, 51472], "temperature": 0.0,
  "avg_logprob": -0.21691173315048218, "compression_ratio": 1.6890756302521008, "no_speech_prob":
  0.0031810421496629715}, {"id": 277, "seek": 202960, "start": 2051.7599999999998,
  "end": 2058.4, "text": " but there are some instances like maximum inner product
  search where we do have things that are", "tokens": [51472, 457, 456, 366, 512,
  14519, 411, 6674, 7284, 1674, 3164, 689, 321, 360, 362, 721, 300, 366, 51804], "temperature":
  0.0, "avg_logprob": -0.21691173315048218, "compression_ratio": 1.6890756302521008,
  "no_speech_prob": 0.0031810421496629715}, {"id": 278, "seek": 205840, "start": 2058.48,
  "end": 2069.6, "text": " non-metric entities widely used. Yeah, that''s amazing,
  but I think I hope that equally as I''m learning,", "tokens": [50368, 2107, 12,
  5537, 1341, 16667, 13371, 1143, 13, 865, 11, 300, 311, 2243, 11, 457, 286, 519,
  286, 1454, 300, 12309, 382, 286, 478, 2539, 11, 50924], "temperature": 0.0, "avg_logprob":
  -0.2104255579694917, "compression_ratio": 1.5487179487179488, "no_speech_prob":
  0.005779239349067211}, {"id": 279, "seek": 205840, "start": 2069.6, "end": 2077.44,
  "text": " I hope our listeners are also learning on this because often times when
  you plunge into a new field,", "tokens": [50924, 286, 1454, 527, 23274, 366, 611,
  2539, 322, 341, 570, 2049, 1413, 562, 291, 499, 27588, 666, 257, 777, 2519, 11,
  51316], "temperature": 0.0, "avg_logprob": -0.2104255579694917, "compression_ratio":
  1.5487179487179488, "no_speech_prob": 0.005779239349067211}, {"id": 280, "seek":
  205840, "start": 2077.44, "end": 2084.56, "text": " let''s say, then search, all
  you see is what is being popularized and you know you may go down the", "tokens":
  [51316, 718, 311, 584, 11, 550, 3164, 11, 439, 291, 536, 307, 437, 307, 885, 3743,
  1602, 293, 291, 458, 291, 815, 352, 760, 264, 51672], "temperature": 0.0, "avg_logprob":
  -0.2104255579694917, "compression_ratio": 1.5487179487179488, "no_speech_prob":
  0.005779239349067211}, {"id": 281, "seek": 208456, "start": 2084.56, "end": 2092.32,
  "text": " rabbit hole. So I''m really excited and thankful that you are able to
  share and much wider perspective", "tokens": [50364, 19509, 5458, 13, 407, 286,
  478, 534, 2919, 293, 13611, 300, 291, 366, 1075, 281, 2073, 293, 709, 11842, 4585,
  50752], "temperature": 0.0, "avg_logprob": -0.2165426390511649, "compression_ratio":
  1.4591836734693877, "no_speech_prob": 0.006146696396172047}, {"id": 282, "seek":
  208456, "start": 2092.32, "end": 2101.7599999999998, "text": " over things. And
  then also most interestingly, you work and you say you''re a co-author of", "tokens":
  [50752, 670, 721, 13, 400, 550, 611, 881, 25873, 11, 291, 589, 293, 291, 584, 291,
  434, 257, 598, 12, 34224, 295, 51224], "temperature": 0.0, "avg_logprob": -0.2165426390511649,
  "compression_ratio": 1.4591836734693877, "no_speech_prob": 0.006146696396172047},
  {"id": 283, "seek": 208456, "start": 2101.7599999999998, "end": 2110.48, "text":
  " an MSLIP besides other authors. Your collective work is also now used at like
  for open search,", "tokens": [51224, 364, 7395, 43, 9139, 11868, 661, 16552, 13,
  2260, 12590, 589, 307, 611, 586, 1143, 412, 411, 337, 1269, 3164, 11, 51660], "temperature":
  0.0, "avg_logprob": -0.2165426390511649, "compression_ratio": 1.4591836734693877,
  "no_speech_prob": 0.006146696396172047}, {"id": 284, "seek": 211048, "start": 2111.2,
  "end": 2120.08, "text": " engine, which I believe I also had a chance to test at
  some point and like it''s a C++ library that", "tokens": [50400, 2848, 11, 597,
  286, 1697, 286, 611, 632, 257, 2931, 281, 1500, 412, 512, 935, 293, 411, 309, 311,
  257, 383, 25472, 6405, 300, 50844], "temperature": 0.0, "avg_logprob": -0.19844889353556805,
  "compression_ratio": 1.4656862745098038, "no_speech_prob": 0.006503316108137369},
  {"id": 285, "seek": 211048, "start": 2120.08, "end": 2131.2, "text": " is then somehow
  loaded of GVM and basically then searches is performed using H&SW. Can you tell
  me", "tokens": [50844, 307, 550, 6063, 13210, 295, 460, 53, 44, 293, 1936, 550,
  26701, 307, 10332, 1228, 389, 5, 50, 54, 13, 1664, 291, 980, 385, 51400], "temperature":
  0.0, "avg_logprob": -0.19844889353556805, "compression_ratio": 1.4656862745098038,
  "no_speech_prob": 0.006503316108137369}, {"id": 286, "seek": 211048, "start": 2131.2,
  "end": 2138.96, "text": " a bit about that like that story of how did you end up
  you know connecting these to an MSLIP and H&SW", "tokens": [51400, 257, 857, 466,
  300, 411, 300, 1657, 295, 577, 630, 291, 917, 493, 291, 458, 11015, 613, 281, 364,
  7395, 43, 9139, 293, 389, 5, 50, 54, 51788], "temperature": 0.0, "avg_logprob":
  -0.19844889353556805, "compression_ratio": 1.4656862745098038, "no_speech_prob":
  0.006503316108137369}, {"id": 287, "seek": 213896, "start": 2139.84, "end": 2146.96,
  "text": " and here I will probably link to the episode theory that it''s quite popular
  today.", "tokens": [50408, 293, 510, 286, 486, 1391, 2113, 281, 264, 3500, 5261,
  300, 309, 311, 1596, 3743, 965, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22032879292964935,
  "compression_ratio": 1.3823529411764706, "no_speech_prob": 0.0036538366694003344},
  {"id": 288, "seek": 213896, "start": 2149.12, "end": 2156.96, "text": " Yeah, well
  first of all I have to say that I mean it was popularity like close to 100%", "tokens":
  [50872, 865, 11, 731, 700, 295, 439, 286, 362, 281, 584, 300, 286, 914, 309, 390,
  19301, 411, 1998, 281, 2319, 4, 51264], "temperature": 0.0, "avg_logprob": -0.22032879292964935,
  "compression_ratio": 1.3823529411764706, "no_speech_prob": 0.0036538366694003344},
  {"id": 289, "seek": 213896, "start": 2156.96, "end": 2164.08, "text": " of an MSLIP
  is certainly due to the development of H&SW which was", "tokens": [51264, 295, 364,
  7395, 43, 9139, 307, 3297, 3462, 281, 264, 3250, 295, 389, 5, 50, 54, 597, 390,
  51620], "temperature": 0.0, "avg_logprob": -0.22032879292964935, "compression_ratio":
  1.3823529411764706, "no_speech_prob": 0.0036538366694003344}, {"id": 290, "seek":
  216408, "start": 2164.08, "end": 2177.04, "text": " Eurisk creation not mine and
  we affected it in only very minor ways because I think the", "tokens": [50364, 462,
  374, 7797, 8016, 406, 3892, 293, 321, 8028, 309, 294, 787, 588, 6696, 2098, 570,
  286, 519, 264, 51012], "temperature": 0.0, "avg_logprob": -0.5131012549767128, "compression_ratio":
  1.4505494505494505, "no_speech_prob": 0.008249538950622082}, {"id": 291, "seek":
  216408, "start": 2177.04, "end": 2187.6, "text": " I mean we provided the platform
  and yeah so I think one trick that you reboard which I borrowed", "tokens": [51012,
  286, 914, 321, 5649, 264, 3663, 293, 1338, 370, 286, 519, 472, 4282, 300, 291, 319,
  3787, 597, 286, 26805, 51540], "temperature": 0.0, "avg_logprob": -0.5131012549767128,
  "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.008249538950622082},
  {"id": 292, "seek": 216408, "start": 2187.6, "end": 2193.92, "text": " from KGRAF
  was the efficient algorithm from him or she checking but that was it.", "tokens":
  [51540, 490, 591, 38, 3750, 37, 390, 264, 7148, 9284, 490, 796, 420, 750, 8568,
  457, 300, 390, 309, 13, 51856], "temperature": 0.0, "avg_logprob": -0.5131012549767128,
  "compression_ratio": 1.4505494505494505, "no_speech_prob": 0.008249538950622082},
  {"id": 293, "seek": 219408, "start": 2194.64, "end": 2200.64, "text": " So the end
  of the sleep but end of the sleep it was like creation of several people and it
  was like", "tokens": [50392, 407, 264, 917, 295, 264, 2817, 457, 917, 295, 264,
  2817, 309, 390, 411, 8016, 295, 2940, 561, 293, 309, 390, 411, 50692], "temperature":
  0.0, "avg_logprob": -0.3100432796754699, "compression_ratio": 1.5930232558139534,
  "no_speech_prob": 0.0034390168730169535}, {"id": 294, "seek": 219408, "start": 2200.64,
  "end": 2206.88, "text": " has like a rather wild story so it was never planned in
  the sort of random how we developed.", "tokens": [50692, 575, 411, 257, 2831, 4868,
  1657, 370, 309, 390, 1128, 8589, 294, 264, 1333, 295, 4974, 577, 321, 4743, 13,
  51004], "temperature": 0.0, "avg_logprob": -0.3100432796754699, "compression_ratio":
  1.5930232558139534, "no_speech_prob": 0.0034390168730169535}, {"id": 295, "seek":
  219408, "start": 2208.88, "end": 2217.2799999999997, "text": " So in 2012 I attended
  the conference where I met Billik Nidhan who was working on", "tokens": [51104,
  407, 294, 9125, 286, 15990, 264, 7586, 689, 286, 1131, 5477, 1035, 426, 327, 3451,
  567, 390, 1364, 322, 51524], "temperature": 0.0, "avg_logprob": -0.3100432796754699,
  "compression_ratio": 1.5930232558139534, "no_speech_prob": 0.0034390168730169535},
  {"id": 296, "seek": 221728, "start": 2217.28, "end": 2225.6800000000003, "text":
  " and he was doing his PhD on similarity research and we found that like a lot of",
  "tokens": [50364, 293, 415, 390, 884, 702, 14476, 322, 32194, 2132, 293, 321, 1352,
  300, 411, 257, 688, 295, 50784], "temperature": 0.0, "avg_logprob": -0.36457420349121095,
  "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.006099394988268614},
  {"id": 297, "seek": 221728, "start": 2226.6400000000003, "end": 2232.0800000000004,
  "text": " like we shared some interesting particularly in the written algorithms
  and we decided to do some", "tokens": [50832, 411, 321, 5507, 512, 1880, 4098, 294,
  264, 3720, 14642, 293, 321, 3047, 281, 360, 512, 51104], "temperature": 0.0, "avg_logprob":
  -0.36457420349121095, "compression_ratio": 1.6203703703703705, "no_speech_prob":
  0.006099394988268614}, {"id": 298, "seek": 221728, "start": 2232.0800000000004,
  "end": 2239.0400000000004, "text": " joint project together and then my initial
  interest was how do I support not it was somewhat", "tokens": [51104, 7225, 1716,
  1214, 293, 550, 452, 5883, 1179, 390, 577, 360, 286, 1406, 406, 309, 390, 8344,
  51452], "temperature": 0.0, "avg_logprob": -0.36457420349121095, "compression_ratio":
  1.6203703703703705, "no_speech_prob": 0.006099394988268614}, {"id": 299, "seek":
  221728, "start": 2239.0400000000004, "end": 2244.7200000000003, "text": " academic
  topic no metric search is as I explained before it''s still like largely", "tokens":
  [51452, 7778, 4829, 572, 20678, 3164, 307, 382, 286, 8825, 949, 309, 311, 920, 411,
  11611, 51736], "temperature": 0.0, "avg_logprob": -0.36457420349121095, "compression_ratio":
  1.6203703703703705, "no_speech_prob": 0.006099394988268614}, {"id": 300, "seek":
  224472, "start": 2245.4399999999996, "end": 2251.04, "text": " more like academic
  interest because a lot of things are really metric or at most", "tokens": [50400,
  544, 411, 7778, 1179, 570, 257, 688, 295, 721, 366, 534, 20678, 420, 412, 881, 50680],
  "temperature": 0.0, "avg_logprob": -0.38040171350751606, "compression_ratio": 1.606060606060606,
  "no_speech_prob": 0.004586182534694672}, {"id": 301, "seek": 224472, "start": 2251.9199999999996,
  "end": 2256.24, "text": " maximum winter product search which is sort of almost
  a scientific narrative almost metric.", "tokens": [50724, 6674, 6355, 1674, 3164,
  597, 307, 1333, 295, 1920, 257, 8134, 9977, 1920, 20678, 13, 50940], "temperature":
  0.0, "avg_logprob": -0.38040171350751606, "compression_ratio": 1.606060606060606,
  "no_speech_prob": 0.004586182534694672}, {"id": 302, "seek": 224472, "start": 2256.9599999999996,
  "end": 2266.3999999999996, "text": " And yeah so that was basically purely for academic
  interest and I connected it to the to the", "tokens": [50976, 400, 1338, 370, 300,
  390, 1936, 17491, 337, 7778, 1179, 293, 286, 4582, 309, 281, 264, 281, 264, 51448],
  "temperature": 0.0, "avg_logprob": -0.38040171350751606, "compression_ratio": 1.606060606060606,
  "no_speech_prob": 0.004586182534694672}, {"id": 303, "seek": 226640, "start": 2266.4,
  "end": 2273.36, "text": " machine learning because I saw an opportunity to use machine
  learning to support", "tokens": [50364, 3479, 2539, 570, 286, 1866, 364, 2650, 281,
  764, 3479, 2539, 281, 1406, 50712], "temperature": 0.0, "avg_logprob": -0.3465174992879232,
  "compression_ratio": 1.5941176470588236, "no_speech_prob": 0.00044519922812469304},
  {"id": 304, "seek": 226640, "start": 2274.64, "end": 2280.56, "text": " generical
  algorithms that would do a k-nearest new research with non-metrics simulator such
  as", "tokens": [50776, 1337, 804, 14642, 300, 576, 360, 257, 350, 12, 716, 17363,
  777, 2132, 365, 2107, 12, 5537, 10716, 32974, 1270, 382, 51072], "temperature":
  0.0, "avg_logprob": -0.3465174992879232, "compression_ratio": 1.5941176470588236,
  "no_speech_prob": 0.00044519922812469304}, {"id": 305, "seek": 226640, "start":
  2280.56, "end": 2288.96, "text": " scale divergence. Yeah so we did it as a machine
  learning course project and we published paper", "tokens": [51072, 4373, 47387,
  13, 865, 370, 321, 630, 309, 382, 257, 3479, 2539, 1164, 1716, 293, 321, 6572, 3035,
  51492], "temperature": 0.0, "avg_logprob": -0.3465174992879232, "compression_ratio":
  1.5941176470588236, "no_speech_prob": 0.00044519922812469304}, {"id": 306, "seek":
  228896, "start": 2289.28, "end": 2299.52, "text": " at Noribs and it could have
  stopped at this point but then I also like that conference I got like I", "tokens":
  [50380, 412, 6966, 897, 82, 293, 309, 727, 362, 5936, 412, 341, 935, 457, 550, 286,
  611, 411, 300, 7586, 286, 658, 411, 286, 50892], "temperature": 0.0, "avg_logprob":
  -0.4482315761942259, "compression_ratio": 1.518918918918919, "no_speech_prob": 0.006563352886587381},
  {"id": 307, "seek": 228896, "start": 2299.52, "end": 2308.56, "text": " met other
  URIs that conference or just treated that I met with some of with Yuri Adir", "tokens":
  [50892, 1131, 661, 624, 5577, 82, 300, 7586, 420, 445, 8668, 300, 286, 1131, 365,
  512, 295, 365, 33901, 1999, 347, 51344], "temperature": 0.0, "avg_logprob": -0.4482315761942259,
  "compression_ratio": 1.518918918918919, "no_speech_prob": 0.006563352886587381},
  {"id": 308, "seek": 228896, "start": 2309.28, "end": 2316.4, "text": " Quothar,
  Alexander and they worked both work at Merrill Habs company where they developed
  small", "tokens": [51380, 2326, 900, 289, 11, 14845, 293, 436, 2732, 1293, 589,
  412, 6124, 37480, 14225, 82, 2237, 689, 436, 4743, 1359, 51736], "temperature":
  0.0, "avg_logprob": -0.4482315761942259, "compression_ratio": 1.518918918918919,
  "no_speech_prob": 0.006563352886587381}, {"id": 309, "seek": 231640, "start": 2316.4,
  "end": 2322.2400000000002, "text": " world graph approach and that was like a general
  version and so Alexander was really interested to", "tokens": [50364, 1002, 4295,
  3109, 293, 300, 390, 411, 257, 2674, 3037, 293, 370, 14845, 390, 534, 3102, 281,
  50656], "temperature": 0.0, "avg_logprob": -0.2563207944234212, "compression_ratio":
  1.7254901960784315, "no_speech_prob": 0.0029968791641294956}, {"id": 310, "seek":
  231640, "start": 2322.2400000000002, "end": 2327.84, "text": " prove that whatever
  algorithms that we have in Emma Sleep and we were tackling generic", "tokens": [50656,
  7081, 300, 2035, 14642, 300, 321, 362, 294, 17124, 19383, 293, 321, 645, 34415,
  19577, 50936], "temperature": 0.0, "avg_logprob": -0.2563207944234212, "compression_ratio":
  1.7254901960784315, "no_speech_prob": 0.0029968791641294956}, {"id": 311, "seek":
  231640, "start": 2329.04, "end": 2334.48, "text": " search in generic spaces for
  generic similarities and he was eager to prove that", "tokens": [50996, 3164, 294,
  19577, 7673, 337, 19577, 24197, 293, 415, 390, 18259, 281, 7081, 300, 51268], "temperature":
  0.0, "avg_logprob": -0.2563207944234212, "compression_ratio": 1.7254901960784315,
  "no_speech_prob": 0.0029968791641294956}, {"id": 312, "seek": 231640, "start": 2335.84,
  "end": 2343.52, "text": " the graph based algorithm I actually truly generic and
  this is why he and his student", "tokens": [51336, 264, 4295, 2361, 9284, 286, 767,
  4908, 19577, 293, 341, 307, 983, 415, 293, 702, 3107, 51720], "temperature": 0.0,
  "avg_logprob": -0.2563207944234212, "compression_ratio": 1.7254901960784315, "no_speech_prob":
  0.0029968791641294956}, {"id": 313, "seek": 234352, "start": 2344.32, "end": 2351.28,
  "text": " that you know created the first version of a small world graph in Emma
  Sleep so basically contributed", "tokens": [50404, 300, 291, 458, 2942, 264, 700,
  3037, 295, 257, 1359, 1002, 4295, 294, 17124, 19383, 370, 1936, 18434, 50752], "temperature":
  0.0, "avg_logprob": -0.23774796062045628, "compression_ratio": 1.6236559139784945,
  "no_speech_prob": 0.0020354397129267454}, {"id": 314, "seek": 234352, "start": 2351.28,
  "end": 2359.2, "text": " this version and that was a really super slow I spread
  it up by both 10x and that was the version", "tokens": [50752, 341, 3037, 293, 300,
  390, 257, 534, 1687, 2964, 286, 3974, 309, 493, 538, 1293, 1266, 87, 293, 300, 390,
  264, 3037, 51148], "temperature": 0.0, "avg_logprob": -0.23774796062045628, "compression_ratio":
  1.6236559139784945, "no_speech_prob": 0.0020354397129267454}, {"id": 315, "seek":
  234352, "start": 2359.2, "end": 2368.4, "text": " that we used to win the first
  in Benchmarks so it was pretty good but it has issues and one issue that", "tokens":
  [51148, 300, 321, 1143, 281, 1942, 264, 700, 294, 3964, 339, 37307, 370, 309, 390,
  1238, 665, 457, 309, 575, 2663, 293, 472, 2734, 300, 51608], "temperature": 0.0,
  "avg_logprob": -0.23774796062045628, "compression_ratio": 1.6236559139784945, "no_speech_prob":
  0.0020354397129267454}, {"id": 316, "seek": 236840, "start": 2368.56, "end": 2378.2400000000002,
  "text": " was fixed thanks to Yuri sharing with me some early version with H&SW
  and I looked at the code it", "tokens": [50372, 390, 6806, 3231, 281, 33901, 5414,
  365, 385, 512, 2440, 3037, 365, 389, 5, 50, 54, 293, 286, 2956, 412, 264, 3089,
  309, 50856], "temperature": 0.0, "avg_logprob": -0.19855050898309964, "compression_ratio":
  1.5852272727272727, "no_speech_prob": 0.003960041329264641}, {"id": 317, "seek":
  236840, "start": 2378.2400000000002, "end": 2384.96, "text": " was not as like fast
  version that he created later but already there was fixing something", "tokens":
  [50856, 390, 406, 382, 411, 2370, 3037, 300, 415, 2942, 1780, 457, 1217, 456, 390,
  19442, 746, 51192], "temperature": 0.0, "avg_logprob": -0.19855050898309964, "compression_ratio":
  1.5852272727272727, "no_speech_prob": 0.003960041329264641}, {"id": 318, "seek":
  236840, "start": 2385.6800000000003, "end": 2390.48, "text": " and maybe he didn''t
  realize he showed me that piece of code and I realized oh like there is", "tokens":
  [51228, 293, 1310, 415, 994, 380, 4325, 415, 4712, 385, 300, 2522, 295, 3089, 293,
  286, 5334, 1954, 411, 456, 307, 51468], "temperature": 0.0, "avg_logprob": -0.19855050898309964,
  "compression_ratio": 1.5852272727272727, "no_speech_prob": 0.003960041329264641},
  {"id": 319, "seek": 239048, "start": 2390.48, "end": 2399.12, "text": " actually
  still an issue in SW graph so SW both SW graph was improved and Yuri like", "tokens":
  [50364, 767, 920, 364, 2734, 294, 318, 54, 4295, 370, 318, 54, 1293, 318, 54, 4295,
  390, 9689, 293, 33901, 411, 50796], "temperature": 0.0, "avg_logprob": -0.3491799174875453,
  "compression_ratio": 1.6, "no_speech_prob": 0.004256589338183403}, {"id": 320, "seek":
  239048, "start": 2399.12, "end": 2407.76, "text": " contribution is W2 and Emma
  Sleep so it greatly it was like a huge contribution like big step forward", "tokens":
  [50796, 13150, 307, 343, 17, 293, 17124, 19383, 370, 309, 14147, 309, 390, 411,
  257, 2603, 13150, 411, 955, 1823, 2128, 51228], "temperature": 0.0, "avg_logprob":
  -0.3491799174875453, "compression_ratio": 1.6, "no_speech_prob": 0.004256589338183403},
  {"id": 321, "seek": 239048, "start": 2408.96, "end": 2419.52, "text": " it won the
  second NNBG Mark competition they proved SW graph was I think the second the", "tokens":
  [51288, 309, 1582, 264, 1150, 426, 45, 33, 38, 3934, 6211, 436, 14617, 318, 54,
  4295, 390, 286, 519, 264, 1150, 264, 51816], "temperature": 0.0, "avg_logprob":
  -0.3491799174875453, "compression_ratio": 1.6, "no_speech_prob": 0.004256589338183403},
  {"id": 322, "seek": 241952, "start": 2419.52, "end": 2424.88, "text": " second algorithm
  I have a screenshot of this somewhere which I sometimes included to my job talks",
  "tokens": [50364, 1150, 9284, 286, 362, 257, 27712, 295, 341, 4079, 597, 286, 2171,
  5556, 281, 452, 1691, 6686, 50632], "temperature": 0.0, "avg_logprob": -0.1631788526262556,
  "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.00043284616549499333},
  {"id": 323, "seek": 241952, "start": 2427.12, "end": 2435.04, "text": " and the
  H&SW also influenced face because they realized oh like they actually knew about",
  "tokens": [50744, 293, 264, 389, 5, 50, 54, 611, 15269, 1851, 570, 436, 5334, 1954,
  411, 436, 767, 2586, 466, 51140], "temperature": 0.0, "avg_logprob": -0.1631788526262556,
  "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.00043284616549499333},
  {"id": 324, "seek": 241952, "start": 2435.04, "end": 2443.2, "text": " K graph and
  knew about the graph based retrieval but there was one important reason why they
  didn''t", "tokens": [51140, 591, 4295, 293, 2586, 466, 264, 4295, 2361, 19817, 3337,
  457, 456, 390, 472, 1021, 1778, 983, 436, 994, 380, 51548], "temperature": 0.0,
  "avg_logprob": -0.1631788526262556, "compression_ratio": 1.5132275132275133, "no_speech_prob":
  0.00043284616549499333}, {"id": 325, "seek": 244320, "start": 2443.68, "end": 2450.24,
  "text": " you can ask me why but anyway so it influenced face and a lot of other
  people and of course", "tokens": [50388, 291, 393, 1029, 385, 983, 457, 4033, 370,
  309, 15269, 1851, 293, 257, 688, 295, 661, 561, 293, 295, 1164, 50716], "temperature":
  0.0, "avg_logprob": -0.24033731489039178, "compression_ratio": 1.622093023255814,
  "no_speech_prob": 0.006005355156958103}, {"id": 326, "seek": 244320, "start": 2450.7999999999997,
  "end": 2458.8799999999997, "text": " yeah that that Yuri created that was Yuri''s
  work like a huge impact in the rest of his history", "tokens": [50744, 1338, 300,
  300, 33901, 2942, 300, 390, 33901, 311, 589, 411, 257, 2603, 2712, 294, 264, 1472,
  295, 702, 2503, 51148], "temperature": 0.0, "avg_logprob": -0.24033731489039178,
  "compression_ratio": 1.622093023255814, "no_speech_prob": 0.006005355156958103},
  {"id": 327, "seek": 244320, "start": 2458.8799999999997, "end": 2465.12, "text":
  " but I think Yuri shouldn''t complain no he has a great career first he has great
  career first", "tokens": [51148, 457, 286, 519, 33901, 4659, 380, 11024, 572, 415,
  575, 257, 869, 3988, 700, 415, 575, 869, 3988, 700, 51460], "temperature": 0.0,
  "avg_logprob": -0.24033731489039178, "compression_ratio": 1.622093023255814, "no_speech_prob":
  0.006005355156958103}, {"id": 328, "seek": 246512, "start": 2465.68, "end": 2473.8399999999997,
  "text": " Twitter and now it''s at OpenAI so yeah it''s a magic story just a close
  of the wolf white", "tokens": [50392, 5794, 293, 586, 309, 311, 412, 7238, 48698,
  370, 1338, 309, 311, 257, 5585, 1657, 445, 257, 1998, 295, 264, 19216, 2418, 50800],
  "temperature": 0.0, "avg_logprob": -0.33124428901119507, "compression_ratio": 1.5392670157068062,
  "no_speech_prob": 0.005152097903192043}, {"id": 329, "seek": 246512, "start": 2473.8399999999997,
  "end": 2481.7599999999998, "text": " it face did not implement the approach you
  had this is this is a really interesting thing because", "tokens": [50800, 309,
  1851, 630, 406, 4445, 264, 3109, 291, 632, 341, 307, 341, 307, 257, 534, 1880, 551,
  570, 51196], "temperature": 0.0, "avg_logprob": -0.33124428901119507, "compression_ratio":
  1.5392670157068062, "no_speech_prob": 0.005152097903192043}, {"id": 330, "seek":
  246512, "start": 2482.24, "end": 2490.48, "text": " that''s one of my favorite pieces
  in this story well turns out that the the graph based retrieval algorithms", "tokens":
  [51220, 300, 311, 472, 295, 452, 2954, 3755, 294, 341, 1657, 731, 4523, 484, 300,
  264, 264, 4295, 2361, 19817, 3337, 14642, 51632], "temperature": 0.0, "avg_logprob":
  -0.33124428901119507, "compression_ratio": 1.5392670157068062, "no_speech_prob":
  0.005152097903192043}, {"id": 331, "seek": 249048, "start": 2490.48, "end": 2497.44,
  "text": " they had long history so a lot of this was rediscovered on the the pruning
  heuristics and like", "tokens": [50364, 436, 632, 938, 2503, 370, 257, 688, 295,
  341, 390, 2182, 40080, 292, 322, 264, 264, 582, 37726, 415, 374, 6006, 293, 411,
  50712], "temperature": 0.0, "avg_logprob": -0.17607196432645203, "compression_ratio":
  1.5126582278481013, "no_speech_prob": 0.0015562961343675852}, {"id": 332, "seek":
  249048, "start": 2497.44, "end": 2506.32, "text": " the basic algorithm they go
  back to papers in 80s and 90s but people did not use it", "tokens": [50712, 264,
  3875, 9284, 436, 352, 646, 281, 10577, 294, 4688, 82, 293, 4289, 82, 457, 561, 630,
  406, 764, 309, 51156], "temperature": 0.0, "avg_logprob": -0.17607196432645203,
  "compression_ratio": 1.5126582278481013, "no_speech_prob": 0.0015562961343675852},
  {"id": 333, "seek": 249048, "start": 2507.36, "end": 2514.08, "text": " and one
  hurdle was the inability to efficiently create those", "tokens": [51208, 293, 472,
  47423, 390, 264, 33162, 281, 19621, 1884, 729, 51544], "temperature": 0.0, "avg_logprob":
  -0.17607196432645203, "compression_ratio": 1.5126582278481013, "no_speech_prob":
  0.0015562961343675852}, {"id": 334, "seek": 251408, "start": 2514.4, "end": 2521.52,
  "text": " K K nearest neighbor graphs and K nearest neighbor graph it''s a simple
  concept you have data point", "tokens": [50380, 591, 591, 23831, 5987, 24877, 293,
  591, 23831, 5987, 4295, 309, 311, 257, 2199, 3410, 291, 362, 1412, 935, 50736],
  "temperature": 0.0, "avg_logprob": -0.25811036053825825, "compression_ratio": 1.9203980099502487,
  "no_speech_prob": 0.0023210092913359404}, {"id": 335, "seek": 251408, "start": 2521.52,
  "end": 2526.96, "text": " and you have you need to find some data points that are
  nearest neighbor of these data points and", "tokens": [50736, 293, 291, 362, 291,
  643, 281, 915, 512, 1412, 2793, 300, 366, 23831, 5987, 295, 613, 1412, 2793, 293,
  51008], "temperature": 0.0, "avg_logprob": -0.25811036053825825, "compression_ratio":
  1.9203980099502487, "no_speech_prob": 0.0023210092913359404}, {"id": 336, "seek":
  251408, "start": 2526.96, "end": 2534.72, "text": " then you connect to them module
  or some post modification of this graph but how like you know if", "tokens": [51008,
  550, 291, 1745, 281, 552, 10088, 420, 512, 2183, 26747, 295, 341, 4295, 457, 577,
  411, 291, 458, 498, 51396], "temperature": 0.0, "avg_logprob": -0.25811036053825825,
  "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0023210092913359404},
  {"id": 337, "seek": 251408, "start": 2534.72, "end": 2542.16, "text": " you have
  end points it is in the brute force approach and squared computation how can you
  do", "tokens": [51396, 291, 362, 917, 2793, 309, 307, 294, 264, 47909, 3464, 3109,
  293, 8889, 24903, 577, 393, 291, 360, 51768], "temperature": 0.0, "avg_logprob":
  -0.25811036053825825, "compression_ratio": 1.9203980099502487, "no_speech_prob":
  0.0023210092913359404}, {"id": 338, "seek": 254216, "start": 2542.16, "end": 2548.72,
  "text": " this efficiently how can you approximate this so the way how it was done
  before people were", "tokens": [50364, 341, 19621, 577, 393, 291, 30874, 341, 370,
  264, 636, 577, 309, 390, 1096, 949, 561, 645, 50692], "temperature": 0.0, "avg_logprob":
  -0.22595163293786952, "compression_ratio": 1.7951219512195122, "no_speech_prob":
  0.0006250280421227217}, {"id": 339, "seek": 254216, "start": 2548.72, "end": 2556.08,
  "text": " coming up with fancy algorithms how to how to approximate a disappointment
  and those fancy", "tokens": [50692, 1348, 493, 365, 10247, 14642, 577, 281, 577,
  281, 30874, 257, 28175, 293, 729, 10247, 51060], "temperature": 0.0, "avg_logprob":
  -0.22595163293786952, "compression_ratio": 1.7951219512195122, "no_speech_prob":
  0.0006250280421227217}, {"id": 340, "seek": 254216, "start": 2556.08, "end": 2561.2799999999997,
  "text": " algorithms would not particularly scalable and K graph I think is not
  particularly scalable we", "tokens": [51060, 14642, 576, 406, 4098, 38481, 293,
  591, 4295, 286, 519, 307, 406, 4098, 38481, 321, 51320], "temperature": 0.0, "avg_logprob":
  -0.22595163293786952, "compression_ratio": 1.7951219512195122, "no_speech_prob":
  0.0006250280421227217}, {"id": 341, "seek": 254216, "start": 2562.0, "end": 2569.52,
  "text": " played with it we actually incorporated K graph in primitation into enemy
  sleep and it was", "tokens": [51356, 3737, 365, 309, 321, 767, 21654, 591, 4295,
  294, 2886, 4614, 666, 5945, 2817, 293, 309, 390, 51732], "temperature": 0.0, "avg_logprob":
  -0.22595163293786952, "compression_ratio": 1.7951219512195122, "no_speech_prob":
  0.0006250280421227217}, {"id": 342, "seek": 256952, "start": 2569.52, "end": 2574.96,
  "text": " indeed hard to create large graphs because it''s a nitty-fragurithium
  and yeah it''s not it''s not", "tokens": [50364, 6451, 1152, 281, 1884, 2416, 24877,
  570, 309, 311, 257, 297, 10016, 12, 69, 3731, 374, 355, 2197, 293, 1338, 309, 311,
  406, 309, 311, 406, 50636], "temperature": 0.0, "avg_logprob": -0.47156973142881653,
  "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0009183312067762017},
  {"id": 343, "seek": 256952, "start": 2574.96, "end": 2584.88, "text": " very scalable
  but what what Yuri and Kothar did for as small world graph and it was before", "tokens":
  [50636, 588, 38481, 457, 437, 437, 33901, 293, 591, 900, 289, 630, 337, 382, 1359,
  1002, 4295, 293, 309, 390, 949, 51132], "temperature": 0.0, "avg_logprob": -0.47156973142881653,
  "compression_ratio": 1.5494505494505495, "no_speech_prob": 0.0009183312067762017},
  {"id": 344, "seek": 256952, "start": 2584.88, "end": 2591.92, "text": " she was
  done we were while they were at Merrillab''s company they realized that they can
  combine", "tokens": [51132, 750, 390, 1096, 321, 645, 1339, 436, 645, 412, 6124,
  37480, 455, 311, 2237, 436, 5334, 300, 436, 393, 10432, 51484], "temperature": 0.0,
  "avg_logprob": -0.47156973142881653, "compression_ratio": 1.5494505494505495, "no_speech_prob":
  0.0009183312067762017}, {"id": 345, "seek": 259192, "start": 2592.32, "end": 2599.12,
  "text": " the interval and creation of the graph and they can do it efficiently
  like in what like using", "tokens": [50384, 264, 15035, 293, 8016, 295, 264, 4295,
  293, 436, 393, 360, 309, 19621, 411, 294, 437, 411, 1228, 50724], "temperature":
  0.0, "avg_logprob": -0.27516734809206245, "compression_ratio": 1.5898876404494382,
  "no_speech_prob": 0.0024735662154853344}, {"id": 346, "seek": 259192, "start": 2599.12,
  "end": 2607.6800000000003, "text": " modern terminology embarrassingly parallel
  fashion and that was I think one key missing block that", "tokens": [50724, 4363,
  27575, 9187, 12163, 8952, 6700, 293, 300, 390, 286, 519, 472, 2141, 5361, 3461,
  300, 51152], "temperature": 0.0, "avg_logprob": -0.27516734809206245, "compression_ratio":
  1.5898876404494382, "no_speech_prob": 0.0024735662154853344}, {"id": 347, "seek":
  259192, "start": 2607.6800000000003, "end": 2617.84, "text": " prevented graph based
  algorithms to become practical yeah that makes sense yeah it doesn''t", "tokens":
  [51152, 27314, 4295, 2361, 14642, 281, 1813, 8496, 1338, 300, 1669, 2020, 1338,
  309, 1177, 380, 51660], "temperature": 0.0, "avg_logprob": -0.27516734809206245,
  "compression_ratio": 1.5898876404494382, "no_speech_prob": 0.0024735662154853344},
  {"id": 348, "seek": 261784, "start": 2618.8, "end": 2624.48, "text": " like what
  what excites me in this story that you shared is that how serendipitous the", "tokens":
  [50412, 411, 437, 437, 1624, 3324, 385, 294, 341, 1657, 300, 291, 5507, 307, 300,
  577, 816, 521, 647, 39831, 264, 50696], "temperature": 0.0, "avg_logprob": -0.14853202595430262,
  "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0038029756397008896},
  {"id": 349, "seek": 261784, "start": 2625.04, "end": 2633.2000000000003, "text":
  " this like the discovery process is right and like something that feels like random
  leads to I", "tokens": [50724, 341, 411, 264, 12114, 1399, 307, 558, 293, 411, 746,
  300, 3417, 411, 4974, 6689, 281, 286, 51132], "temperature": 0.0, "avg_logprob":
  -0.14853202595430262, "compression_ratio": 1.7534883720930232, "no_speech_prob":
  0.0038029756397008896}, {"id": 350, "seek": 261784, "start": 2633.2000000000003,
  "end": 2639.1200000000003, "text": " don''t know creation of the industry right
  you could you could largely say that the new industry of", "tokens": [51132, 500,
  380, 458, 8016, 295, 264, 3518, 558, 291, 727, 291, 727, 11611, 584, 300, 264, 777,
  3518, 295, 51428], "temperature": 0.0, "avg_logprob": -0.14853202595430262, "compression_ratio":
  1.7534883720930232, "no_speech_prob": 0.0038029756397008896}, {"id": 351, "seek":
  261784, "start": 2639.1200000000003, "end": 2646.4, "text": " you know vector databases
  and vector search and now rag on top of that is created because you guys", "tokens":
  [51428, 291, 458, 8062, 22380, 293, 8062, 3164, 293, 586, 17539, 322, 1192, 295,
  300, 307, 2942, 570, 291, 1074, 51792], "temperature": 0.0, "avg_logprob": -0.14853202595430262,
  "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0038029756397008896},
  {"id": 352, "seek": 264640, "start": 2647.36, "end": 2654.1600000000003, "text":
  " worked on practical implementations of something that was also that that stood
  on no", "tokens": [50412, 2732, 322, 8496, 4445, 763, 295, 746, 300, 390, 611, 300,
  300, 9371, 322, 572, 50752], "temperature": 0.0, "avg_logprob": -0.14655622243881225,
  "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.00035575864603742957},
  {"id": 353, "seek": 264640, "start": 2654.1600000000003, "end": 2659.36, "text":
  " shoulders of you know some of the inventions and research done before that and
  so it''s kind", "tokens": [50752, 10245, 295, 291, 458, 512, 295, 264, 43748, 293,
  2132, 1096, 949, 300, 293, 370, 309, 311, 733, 51012], "temperature": 0.0, "avg_logprob":
  -0.14655622243881225, "compression_ratio": 1.6787330316742082, "no_speech_prob":
  0.00035575864603742957}, {"id": 354, "seek": 264640, "start": 2659.36, "end": 2665.28,
  "text": " of like natural progression but I mean it''s amazing how you know it''s
  just on the verge of you", "tokens": [51012, 295, 411, 3303, 18733, 457, 286, 914,
  309, 311, 2243, 577, 291, 458, 309, 311, 445, 322, 264, 37164, 295, 291, 51308],
  "temperature": 0.0, "avg_logprob": -0.14655622243881225, "compression_ratio": 1.6787330316742082,
  "no_speech_prob": 0.00035575864603742957}, {"id": 355, "seek": 264640, "start":
  2665.28, "end": 2672.0, "text": " not meeting someone at a conference could basically
  need to possibly not creating an industry right", "tokens": [51308, 406, 3440, 1580,
  412, 257, 7586, 727, 1936, 643, 281, 6264, 406, 4084, 364, 3518, 558, 51644], "temperature":
  0.0, "avg_logprob": -0.14655622243881225, "compression_ratio": 1.6787330316742082,
  "no_speech_prob": 0.00035575864603742957}, {"id": 356, "seek": 267200, "start":
  2672.96, "end": 2682.48, "text": " quite possibly I think well thank you for the
  kind words and of course it''s not because of us the", "tokens": [50412, 1596, 6264,
  286, 519, 731, 1309, 291, 337, 264, 733, 2283, 293, 295, 1164, 309, 311, 406, 570,
  295, 505, 264, 50888], "temperature": 0.0, "avg_logprob": -0.34741485820097084,
  "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0017407051054760814},
  {"id": 357, "seek": 267200, "start": 2684.0, "end": 2692.88, "text": " if not for
  us I think other people who have done this I yeah so I with them but anyways so",
  "tokens": [50964, 498, 406, 337, 505, 286, 519, 661, 561, 567, 362, 1096, 341, 286,
  1338, 370, 286, 365, 552, 457, 13448, 370, 51408], "temperature": 0.0, "avg_logprob":
  -0.34741485820097084, "compression_ratio": 1.6264367816091954, "no_speech_prob":
  0.0017407051054760814}, {"id": 358, "seek": 267200, "start": 2694.0, "end": 2700.8,
  "text": " I think we did useful work and clearly people are using a tremendous double
  you lot and people", "tokens": [51464, 286, 519, 321, 630, 4420, 589, 293, 4448,
  561, 366, 1228, 257, 10048, 3834, 291, 688, 293, 561, 51804], "temperature": 0.0,
  "avg_logprob": -0.34741485820097084, "compression_ratio": 1.6264367816091954, "no_speech_prob":
  0.0017407051054760814}, {"id": 359, "seek": 270080, "start": 2700.88, "end": 2707.92,
  "text": " using it mostly even even though it hasn''t like a lot of issues but it
  was still ended up being", "tokens": [50368, 1228, 309, 5240, 754, 754, 1673, 309,
  6132, 380, 411, 257, 688, 295, 2663, 457, 309, 390, 920, 4590, 493, 885, 50720],
  "temperature": 0.0, "avg_logprob": -0.25191213123833955, "compression_ratio": 1.6149425287356323,
  "no_speech_prob": 0.0007475847960449755}, {"id": 360, "seek": 270080, "start": 2709.2000000000003,
  "end": 2715.6000000000004, "text": " used rather widely and the one reason why it
  was used so widely because people who needed", "tokens": [50784, 1143, 2831, 13371,
  293, 264, 472, 1778, 983, 309, 390, 1143, 370, 13371, 570, 561, 567, 2978, 51104],
  "temperature": 0.0, "avg_logprob": -0.25191213123833955, "compression_ratio": 1.6149425287356323,
  "no_speech_prob": 0.0007475847960449755}, {"id": 361, "seek": 270080, "start": 2716.4,
  "end": 2724.8, "text": " a library the Python basically a library that would do
  Kenyar''s new research and it would do it", "tokens": [51144, 257, 6405, 264, 15329,
  1936, 257, 6405, 300, 576, 360, 8273, 88, 289, 311, 777, 2132, 293, 309, 576, 360,
  309, 51564], "temperature": 0.0, "avg_logprob": -0.25191213123833955, "compression_ratio":
  1.6149425287356323, "no_speech_prob": 0.0007475847960449755}, {"id": 362, "seek":
  272480, "start": 2724.8, "end": 2730.96, "text": " from Python and people often
  take these little things for granted but say initially", "tokens": [50364, 490,
  15329, 293, 561, 2049, 747, 613, 707, 721, 337, 12344, 457, 584, 9105, 50672], "temperature":
  0.0, "avg_logprob": -0.20270160918540142, "compression_ratio": 1.8434782608695652,
  "no_speech_prob": 0.002656804397702217}, {"id": 363, "seek": 272480, "start": 2730.96,
  "end": 2736.6400000000003, "text": " I honestly didn''t have Python bindings and
  to participate and then benchmarks and have something", "tokens": [50672, 286, 6095,
  994, 380, 362, 15329, 14786, 1109, 293, 281, 8197, 293, 550, 43751, 293, 362, 746,
  50956], "temperature": 0.0, "avg_logprob": -0.20270160918540142, "compression_ratio":
  1.8434782608695652, "no_speech_prob": 0.002656804397702217}, {"id": 364, "seek":
  272480, "start": 2736.6400000000003, "end": 2741.2000000000003, "text": " useful
  you would need to have Python bindings this Python bindings were written by Billek",
  "tokens": [50956, 4420, 291, 576, 643, 281, 362, 15329, 14786, 1109, 341, 15329,
  14786, 1109, 645, 3720, 538, 5477, 916, 51184], "temperature": 0.0, "avg_logprob":
  -0.20270160918540142, "compression_ratio": 1.8434782608695652, "no_speech_prob":
  0.002656804397702217}, {"id": 365, "seek": 272480, "start": 2742.48, "end": 2746.6400000000003,
  "text": " I didn''t I didn''t create those bindings so he created those bindings
  the first version", "tokens": [51248, 286, 994, 380, 286, 994, 380, 1884, 729, 14786,
  1109, 370, 415, 2942, 729, 14786, 1109, 264, 700, 3037, 51456], "temperature": 0.0,
  "avg_logprob": -0.20270160918540142, "compression_ratio": 1.8434782608695652, "no_speech_prob":
  0.002656804397702217}, {"id": 366, "seek": 272480, "start": 2747.28, "end": 2751.28,
  "text": " so there you go that library was possible to use and at the moment", "tokens":
  [51488, 370, 456, 291, 352, 300, 6405, 390, 1944, 281, 764, 293, 412, 264, 1623,
  51688], "temperature": 0.0, "avg_logprob": -0.20270160918540142, "compression_ratio":
  1.8434782608695652, "no_speech_prob": 0.002656804397702217}, {"id": 367, "seek":
  275128, "start": 2752.2400000000002, "end": 2761.2000000000003, "text": " there
  were at the moment the it was not such a big choice of libraries to do Kenyar''s
  search", "tokens": [50412, 456, 645, 412, 264, 1623, 264, 309, 390, 406, 1270, 257,
  955, 3922, 295, 15148, 281, 360, 8273, 88, 289, 311, 3164, 50860], "temperature":
  0.0, "avg_logprob": -0.22021304766337077, "compression_ratio": 1.618421052631579,
  "no_speech_prob": 0.0025101350620388985}, {"id": 368, "seek": 275128, "start": 2762.0,
  "end": 2766.6400000000003, "text": " so in terms of the competition there was anoy
  which was slower", "tokens": [50900, 370, 294, 2115, 295, 264, 6211, 456, 390, 364,
  939, 597, 390, 14009, 51132], "temperature": 0.0, "avg_logprob": -0.22021304766337077,
  "compression_ratio": 1.618421052631579, "no_speech_prob": 0.0025101350620388985},
  {"id": 369, "seek": 275128, "start": 2768.0, "end": 2775.6000000000004, "text":
  " noticeably slower there was another library flan which used similar algorithms
  to anoy but", "tokens": [51200, 3449, 1188, 14009, 456, 390, 1071, 6405, 932, 282,
  597, 1143, 2531, 14642, 281, 364, 939, 457, 51580], "temperature": 0.0, "avg_logprob":
  -0.22021304766337077, "compression_ratio": 1.618421052631579, "no_speech_prob":
  0.0025101350620388985}, {"id": 370, "seek": 277560, "start": 2776.24, "end": 2783.2,
  "text": " it was less optimized and it was also slower through three wall there
  was a keg graph but it was not", "tokens": [50396, 309, 390, 1570, 26941, 293, 309,
  390, 611, 14009, 807, 1045, 2929, 456, 390, 257, 803, 70, 4295, 457, 309, 390, 406,
  50744], "temperature": 0.0, "avg_logprob": -0.274062842130661, "compression_ratio":
  1.6646341463414633, "no_speech_prob": 0.0036516953259706497}, {"id": 371, "seek":
  277560, "start": 2783.2, "end": 2792.3199999999997, "text": " like so easily usable
  and yeah basically that was it and later came face but it came it was only", "tokens":
  [50744, 411, 370, 3612, 29975, 293, 1338, 1936, 300, 390, 309, 293, 1780, 1361,
  1851, 457, 309, 1361, 309, 390, 787, 51200], "temperature": 0.0, "avg_logprob":
  -0.274062842130661, "compression_ratio": 1.6646341463414633, "no_speech_prob": 0.0036516953259706497},
  {"id": 372, "seek": 277560, "start": 2792.88, "end": 2798.72, "text": " released
  I think a year a couple of years later after well definitely after", "tokens": [51228,
  4736, 286, 519, 257, 1064, 257, 1916, 295, 924, 1780, 934, 731, 2138, 934, 51520],
  "temperature": 0.0, "avg_logprob": -0.274062842130661, "compression_ratio": 1.6646341463414633,
  "no_speech_prob": 0.0036516953259706497}, {"id": 373, "seek": 279872, "start": 2799.2,
  "end": 2805.9199999999996, "text": " yeah it took several years for face to appear
  and people started using it so at some point", "tokens": [50388, 1338, 309, 1890,
  2940, 924, 337, 1851, 281, 4204, 293, 561, 1409, 1228, 309, 370, 412, 512, 935,
  50724], "temperature": 0.0, "avg_logprob": -0.25457817857915704, "compression_ratio":
  1.6741071428571428, "no_speech_prob": 0.0062931254506111145}, {"id": 374, "seek":
  279872, "start": 2805.9199999999996, "end": 2811.9199999999996, "text": " there
  was vacuum and I honestly filled it now like other approaches that are taking over
  yeah", "tokens": [50724, 456, 390, 14224, 293, 286, 6095, 6412, 309, 586, 411, 661,
  11587, 300, 366, 1940, 670, 1338, 51024], "temperature": 0.0, "avg_logprob": -0.25457817857915704,
  "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.0062931254506111145},
  {"id": 375, "seek": 279872, "start": 2813.68, "end": 2820.24, "text": " so yeah
  it in summary there wasn''t a lot of serendipity but I wouldn''t take credit for
  it", "tokens": [51112, 370, 1338, 309, 294, 12691, 456, 2067, 380, 257, 688, 295,
  816, 521, 647, 507, 457, 286, 2759, 380, 747, 5397, 337, 309, 51440], "temperature":
  0.0, "avg_logprob": -0.25457817857915704, "compression_ratio": 1.6741071428571428,
  "no_speech_prob": 0.0062931254506111145}, {"id": 376, "seek": 279872, "start": 2821.7599999999998,
  "end": 2828.16, "text": " in the industry it would have created without us for sure
  yeah yeah maybe or maybe not and it''s also", "tokens": [51516, 294, 264, 3518,
  309, 576, 362, 2942, 1553, 505, 337, 988, 1338, 1338, 1310, 420, 1310, 406, 293,
  309, 311, 611, 51836], "temperature": 0.0, "avg_logprob": -0.25457817857915704,
  "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.0062931254506111145},
  {"id": 377, "seek": 282872, "start": 2828.72, "end": 2836.24, "text": " well I think
  it''s quite a photo like a better work typical of the end turn not to", "tokens":
  [50364, 731, 286, 519, 309, 311, 1596, 257, 5052, 411, 257, 1101, 589, 7476, 295,
  264, 917, 1261, 406, 281, 50740], "temperature": 0.0, "avg_logprob": -0.2418431001551011,
  "compression_ratio": 1.7095238095238094, "no_speech_prob": 0.0012158129829913378},
  {"id": 378, "seek": 282872, "start": 2838.3199999999997, "end": 2843.2, "text":
  " recognize the impact they''re making because the moment they do recognize that",
  "tokens": [50844, 5521, 264, 2712, 436, 434, 1455, 570, 264, 1623, 436, 360, 5521,
  300, 51088], "temperature": 0.0, "avg_logprob": -0.2418431001551011, "compression_ratio":
  1.7095238095238094, "no_speech_prob": 0.0012158129829913378}, {"id": 379, "seek":
  282872, "start": 2843.2, "end": 2850.8799999999997, "text": " that''s probably end
  of story so like you need to be constantly sort of low ego and pointing at the",
  "tokens": [51088, 300, 311, 1391, 917, 295, 1657, 370, 411, 291, 643, 281, 312,
  6460, 1333, 295, 2295, 14495, 293, 12166, 412, 264, 51472], "temperature": 0.0,
  "avg_logprob": -0.2418431001551011, "compression_ratio": 1.7095238095238094, "no_speech_prob":
  0.0012158129829913378}, {"id": 380, "seek": 282872, "start": 2850.8799999999997,
  "end": 2856.24, "text": " goal and I think this is what you''re doing and that it
  feels like this is your approach but you also", "tokens": [51472, 3387, 293, 286,
  519, 341, 307, 437, 291, 434, 884, 293, 300, 309, 3417, 411, 341, 307, 428, 3109,
  457, 291, 611, 51740], "temperature": 0.0, "avg_logprob": -0.2418431001551011, "compression_ratio":
  1.7095238095238094, "no_speech_prob": 0.0012158129829913378}, {"id": 381, "seek":
  285624, "start": 2856.24, "end": 2865.12, "text": " do do quite a bit of impact
  I could ask a ton of questions obviously and I could relate also to", "tokens":
  [50364, 360, 360, 1596, 257, 857, 295, 2712, 286, 727, 1029, 257, 2952, 295, 1651,
  2745, 293, 286, 727, 10961, 611, 281, 50808], "temperature": 0.0, "avg_logprob":
  -0.13732427900487726, "compression_ratio": 1.6594827586206897, "no_speech_prob":
  0.0019651977345347404}, {"id": 382, "seek": 285624, "start": 2865.12, "end": 2872.16,
  "text": " the fact that what you explained about some of the struggles like how
  to optimize these algorithms", "tokens": [50808, 264, 1186, 300, 437, 291, 8825,
  466, 512, 295, 264, 17592, 411, 577, 281, 19719, 613, 14642, 51160], "temperature":
  0.0, "avg_logprob": -0.13732427900487726, "compression_ratio": 1.6594827586206897,
  "no_speech_prob": 0.0019651977345347404}, {"id": 383, "seek": 285624, "start": 2872.16,
  "end": 2879.3599999999997, "text": " because at some point I did embark on participating
  in billion scale and thench marks and I", "tokens": [51160, 570, 412, 512, 935,
  286, 630, 29832, 322, 13950, 294, 5218, 4373, 293, 550, 339, 10640, 293, 286, 51520],
  "temperature": 0.0, "avg_logprob": -0.13732427900487726, "compression_ratio": 1.6594827586206897,
  "no_speech_prob": 0.0019651977345347404}, {"id": 384, "seek": 285624, "start": 2879.3599999999997,
  "end": 2885.7599999999998, "text": " I think I failed miserably but at the same
  time I did have some code which worked on a small scale", "tokens": [51520, 286,
  519, 286, 7612, 17725, 1188, 457, 412, 264, 912, 565, 286, 630, 362, 512, 3089,
  597, 2732, 322, 257, 1359, 4373, 51840], "temperature": 0.0, "avg_logprob": -0.13732427900487726,
  "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.0019651977345347404},
  {"id": 385, "seek": 288576, "start": 2885.76, "end": 2892.88, "text": " and one
  of the building works there was hnsw with very simple I would say with very very
  simple", "tokens": [50364, 293, 472, 295, 264, 2390, 1985, 456, 390, 276, 3695,
  86, 365, 588, 2199, 286, 576, 584, 365, 588, 588, 2199, 50720], "temperature": 0.0,
  "avg_logprob": -0.16929261250929398, "compression_ratio": 1.784037558685446, "no_speech_prob":
  0.0008407257846556604}, {"id": 386, "seek": 288576, "start": 2892.88, "end": 2900.1600000000003,
  "text": " intuition that you just make several I think several passes through the
  data set and you try to", "tokens": [50720, 24002, 300, 291, 445, 652, 2940, 286,
  519, 2940, 11335, 807, 264, 1412, 992, 293, 291, 853, 281, 51084], "temperature":
  0.0, "avg_logprob": -0.16929261250929398, "compression_ratio": 1.784037558685446,
  "no_speech_prob": 0.0008407257846556604}, {"id": 387, "seek": 288576, "start": 2901.5200000000004,
  "end": 2908.1600000000003, "text": " bind points in space that are closer to each
  other and then you would push them to some common", "tokens": [51152, 14786, 2793,
  294, 1901, 300, 366, 4966, 281, 1184, 661, 293, 550, 291, 576, 2944, 552, 281, 512,
  2689, 51484], "temperature": 0.0, "avg_logprob": -0.16929261250929398, "compression_ratio":
  1.784037558685446, "no_speech_prob": 0.0008407257846556604}, {"id": 388, "seek":
  288576, "start": 2908.1600000000003, "end": 2912.1600000000003, "text": " bucket
  I called them shards and then you would build a nation s double index for those
  shards", "tokens": [51484, 13058, 286, 1219, 552, 402, 2287, 293, 550, 291, 576,
  1322, 257, 4790, 262, 3834, 8186, 337, 729, 402, 2287, 51684], "temperature": 0.0,
  "avg_logprob": -0.16929261250929398, "compression_ratio": 1.784037558685446, "no_speech_prob":
  0.0008407257846556604}, {"id": 389, "seek": 291216, "start": 2912.7999999999997,
  "end": 2919.2799999999997, "text": " the only thing that I couldn''t figure out
  is for those shards I still needed to have an entry point", "tokens": [50396, 264,
  787, 551, 300, 286, 2809, 380, 2573, 484, 307, 337, 729, 402, 2287, 286, 920, 2978,
  281, 362, 364, 8729, 935, 50720], "temperature": 0.0, "avg_logprob": -0.1255583555802055,
  "compression_ratio": 1.6149425287356323, "no_speech_prob": 0.004277515225112438},
  {"id": 390, "seek": 291216, "start": 2919.2799999999997, "end": 2927.2, "text":
  " to quickly sort of identify which shards I should go down you know through when
  I when I", "tokens": [50720, 281, 2661, 1333, 295, 5876, 597, 402, 2287, 286, 820,
  352, 760, 291, 458, 807, 562, 286, 562, 286, 51116], "temperature": 0.0, "avg_logprob":
  -0.1255583555802055, "compression_ratio": 1.6149425287356323, "no_speech_prob":
  0.004277515225112438}, {"id": 391, "seek": 291216, "start": 2930.16, "end": 2937.12,
  "text": " when I find similar similar documents for the query and I did attempt
  to modify hnsw code in", "tokens": [51264, 562, 286, 915, 2531, 2531, 8512, 337,
  264, 14581, 293, 286, 630, 5217, 281, 16927, 276, 3695, 86, 3089, 294, 51612], "temperature":
  0.0, "avg_logprob": -0.1255583555802055, "compression_ratio": 1.6149425287356323,
  "no_speech_prob": 0.004277515225112438}, {"id": 392, "seek": 293712, "start": 2937.12,
  "end": 2943.68, "text": " the enemy sleep you know to to like get me only first
  layer of the graph so that I can", "tokens": [50364, 264, 5945, 2817, 291, 458,
  281, 281, 411, 483, 385, 787, 700, 4583, 295, 264, 4295, 370, 300, 286, 393, 50692],
  "temperature": 0.0, "avg_logprob": -0.15658715496892514, "compression_ratio": 1.6340425531914893,
  "no_speech_prob": 0.003468903945758939}, {"id": 393, "seek": 293712, "start": 2943.68,
  "end": 2950.08, "text": " pretend that that''s my layer for entering the shards
  I just ran out of time but I see it was very", "tokens": [50692, 11865, 300, 300,
  311, 452, 4583, 337, 11104, 264, 402, 2287, 286, 445, 5872, 484, 295, 565, 457,
  286, 536, 309, 390, 588, 51012], "temperature": 0.0, "avg_logprob": -0.15658715496892514,
  "compression_ratio": 1.6340425531914893, "no_speech_prob": 0.003468903945758939},
  {"id": 394, "seek": 293712, "start": 2950.08, "end": 2958.64, "text": " exciting
  and also thanks to the organizers we had access to really beefy machines which I
  think I", "tokens": [51012, 4670, 293, 611, 3231, 281, 264, 35071, 321, 632, 2105,
  281, 534, 9256, 88, 8379, 597, 286, 519, 286, 51440], "temperature": 0.0, "avg_logprob":
  -0.15658715496892514, "compression_ratio": 1.6340425531914893, "no_speech_prob":
  0.003468903945758939}, {"id": 395, "seek": 293712, "start": 2958.64, "end": 2966.24,
  "text": " had I haven''t been giving like good use I was mostly burning the you
  know CPU capacity and memory but", "tokens": [51440, 632, 286, 2378, 380, 668, 2902,
  411, 665, 764, 286, 390, 5240, 9488, 264, 291, 458, 13199, 6042, 293, 4675, 457,
  51820], "temperature": 0.0, "avg_logprob": -0.15658715496892514, "compression_ratio":
  1.6340425531914893, "no_speech_prob": 0.003468903945758939}, {"id": 396, "seek":
  296712, "start": 2967.3599999999997, "end": 2974.56, "text": " I think it''s an
  exciting field and what what I hope is that like with the vacuum that you mentioned",
  "tokens": [50376, 286, 519, 309, 311, 364, 4670, 2519, 293, 437, 437, 286, 1454,
  307, 300, 411, 365, 264, 14224, 300, 291, 2835, 50736], "temperature": 0.0, "avg_logprob":
  -0.10567137706710632, "compression_ratio": 1.7276785714285714, "no_speech_prob":
  0.0012144262436777353}, {"id": 397, "seek": 296712, "start": 2974.56, "end": 2981.3599999999997,
  "text": " that it doesn''t happen that this torch will be carried forward and then
  someone will get excited", "tokens": [50736, 300, 309, 1177, 380, 1051, 300, 341,
  27822, 486, 312, 9094, 2128, 293, 550, 1580, 486, 483, 2919, 51076], "temperature":
  0.0, "avg_logprob": -0.10567137706710632, "compression_ratio": 1.7276785714285714,
  "no_speech_prob": 0.0012144262436777353}, {"id": 398, "seek": 296712, "start": 2981.3599999999997,
  "end": 2990.24, "text": " about and not afraid of trying new things in this space
  are you yourself still like looking at", "tokens": [51076, 466, 293, 406, 4638,
  295, 1382, 777, 721, 294, 341, 1901, 366, 291, 1803, 920, 411, 1237, 412, 51520],
  "temperature": 0.0, "avg_logprob": -0.10567137706710632, "compression_ratio": 1.7276785714285714,
  "no_speech_prob": 0.0012144262436777353}, {"id": 399, "seek": 296712, "start": 2991.3599999999997,
  "end": 2995.68, "text": " obviously you''re looking after enemy sleep but is there
  something that particularly excites you", "tokens": [51576, 2745, 291, 434, 1237,
  934, 5945, 2817, 457, 307, 456, 746, 300, 4098, 1624, 3324, 291, 51792], "temperature":
  0.0, "avg_logprob": -0.10567137706710632, "compression_ratio": 1.7276785714285714,
  "no_speech_prob": 0.0012144262436777353}, {"id": 400, "seek": 299568, "start": 2995.68,
  "end": 2999.2799999999997, "text": " in this field that you would be working on
  or you are working on?", "tokens": [50364, 294, 341, 2519, 300, 291, 576, 312, 1364,
  322, 420, 291, 366, 1364, 322, 30, 50544], "temperature": 0.0, "avg_logprob": -0.29092517495155334,
  "compression_ratio": 1.5029239766081872, "no_speech_prob": 0.0048709348775446415},
  {"id": 401, "seek": 299568, "start": 3002.48, "end": 3012.24, "text": " Yeah great
  question so first of all yeah I am not sure if if I would do any work on vector
  search", "tokens": [50704, 865, 869, 1168, 370, 700, 295, 439, 1338, 286, 669, 406,
  988, 498, 498, 286, 576, 360, 604, 589, 322, 8062, 3164, 51192], "temperature":
  0.0, "avg_logprob": -0.29092517495155334, "compression_ratio": 1.5029239766081872,
  "no_speech_prob": 0.0048709348775446415}, {"id": 402, "seek": 299568, "start": 3012.24,
  "end": 3021.6, "text": " in the I haven''t actually not maintaining an enemy sleep
  pretty well recently I''m just didn''t", "tokens": [51192, 294, 264, 286, 2378,
  380, 767, 406, 14916, 364, 5945, 2817, 1238, 731, 3938, 286, 478, 445, 994, 380,
  51660], "temperature": 0.0, "avg_logprob": -0.29092517495155334, "compression_ratio":
  1.5029239766081872, "no_speech_prob": 0.0048709348775446415}, {"id": 403, "seek":
  302160, "start": 3021.6, "end": 3029.52, "text": " have a lot of time and there
  was an issue with building the so I will still fix and support", "tokens": [50364,
  362, 257, 688, 295, 565, 293, 456, 390, 364, 2734, 365, 2390, 264, 370, 286, 486,
  920, 3191, 293, 1406, 50760], "temperature": 0.0, "avg_logprob": -0.2147812422584085,
  "compression_ratio": 1.569060773480663, "no_speech_prob": 0.003949436359107494},
  {"id": 404, "seek": 302160, "start": 3029.52, "end": 3037.2799999999997, "text":
  " later version of Python for sure I was like you know like piece my piecemeal work
  I found like say", "tokens": [50760, 1780, 3037, 295, 15329, 337, 988, 286, 390,
  411, 291, 458, 411, 2522, 452, 2522, 32914, 589, 286, 1352, 411, 584, 51148], "temperature":
  0.0, "avg_logprob": -0.2147812422584085, "compression_ratio": 1.569060773480663,
  "no_speech_prob": 0.003949436359107494}, {"id": 405, "seek": 302160, "start": 3037.2799999999997,
  "end": 3044.56, "text": " half a day to fix like Windows build something else popped
  up yeah so it is an exciting field", "tokens": [51148, 1922, 257, 786, 281, 3191,
  411, 8591, 1322, 746, 1646, 21545, 493, 1338, 370, 309, 307, 364, 4670, 2519, 51512],
  "temperature": 0.0, "avg_logprob": -0.2147812422584085, "compression_ratio": 1.569060773480663,
  "no_speech_prob": 0.003949436359107494}, {"id": 406, "seek": 304456, "start": 3045.12,
  "end": 3055.12, "text": " while it''s also become really busy and another thing
  is that I still see the focus main focus", "tokens": [50392, 1339, 309, 311, 611,
  1813, 534, 5856, 293, 1071, 551, 307, 300, 286, 920, 536, 264, 1879, 2135, 1879,
  50892], "temperature": 0.0, "avg_logprob": -0.15768599855727045, "compression_ratio":
  1.6845238095238095, "no_speech_prob": 0.0061231753788888454}, {"id": 407, "seek":
  304456, "start": 3055.12, "end": 3062.16, "text": " it''s not it''s not very appreciated
  so the like you said you said I mean there''s a really nice", "tokens": [50892,
  309, 311, 406, 309, 311, 406, 588, 17169, 370, 264, 411, 291, 848, 291, 848, 286,
  914, 456, 311, 257, 534, 1481, 51244], "temperature": 0.0, "avg_logprob": -0.15768599855727045,
  "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.0061231753788888454},
  {"id": 408, "seek": 304456, "start": 3062.16, "end": 3068.48, "text": " voice that
  all like that helped industry to be created and maybe it''s true to some degree
  it is", "tokens": [51244, 3177, 300, 439, 411, 300, 4254, 3518, 281, 312, 2942,
  293, 1310, 309, 311, 2074, 281, 512, 4314, 309, 307, 51560], "temperature": 0.0,
  "avg_logprob": -0.15768599855727045, "compression_ratio": 1.6845238095238095, "no_speech_prob":
  0.0061231753788888454}, {"id": 409, "seek": 306848, "start": 3069.12, "end": 3076.4,
  "text": " what yeah is it appreciated by you know your potential employer no it''s
  like zero appreciation so it", "tokens": [50396, 437, 1338, 307, 309, 17169, 538,
  291, 458, 428, 3995, 16205, 572, 309, 311, 411, 4018, 18909, 370, 309, 50760], "temperature":
  0.0, "avg_logprob": -0.17732033048357282, "compression_ratio": 1.6781609195402298,
  "no_speech_prob": 0.003773072035983205}, {"id": 410, "seek": 306848, "start": 3076.4,
  "end": 3087.52, "text": " it isn''t it''s it''s it''s it wasn''t still is somewhat
  niche topic and most people are of course", "tokens": [50760, 309, 1943, 380, 309,
  311, 309, 311, 309, 311, 309, 2067, 380, 920, 307, 8344, 19956, 4829, 293, 881,
  561, 366, 295, 1164, 51316], "temperature": 0.0, "avg_logprob": -0.17732033048357282,
  "compression_ratio": 1.6781609195402298, "no_speech_prob": 0.003773072035983205},
  {"id": 411, "seek": 306848, "start": 3087.52, "end": 3095.52, "text": " I interested
  in how do you solve intelligence in that you know broad sense of the word how do
  you", "tokens": [51316, 286, 3102, 294, 577, 360, 291, 5039, 7599, 294, 300, 291,
  458, 4152, 2020, 295, 264, 1349, 577, 360, 291, 51716], "temperature": 0.0, "avg_logprob":
  -0.17732033048357282, "compression_ratio": 1.6781609195402298, "no_speech_prob":
  0.003773072035983205}, {"id": 412, "seek": 309552, "start": 3095.52, "end": 3101.04,
  "text": " create models that can be seen and if the model like how you can combine
  them that this is like", "tokens": [50364, 1884, 5245, 300, 393, 312, 1612, 293,
  498, 264, 2316, 411, 577, 291, 393, 10432, 552, 300, 341, 307, 411, 50640], "temperature":
  0.0, "avg_logprob": -0.14303845625657302, "compression_ratio": 1.7065868263473054,
  "no_speech_prob": 0.003035679692402482}, {"id": 413, "seek": 309552, "start": 3101.04,
  "end": 3111.12, "text": " you know this new agentic ecosystem and yeah so all that
  stuff that really excites people it", "tokens": [50640, 291, 458, 341, 777, 9461,
  299, 11311, 293, 1338, 370, 439, 300, 1507, 300, 534, 1624, 3324, 561, 309, 51144],
  "temperature": 0.0, "avg_logprob": -0.14303845625657302, "compression_ratio": 1.7065868263473054,
  "no_speech_prob": 0.003035679692402482}, {"id": 414, "seek": 309552, "start": 3111.12,
  "end": 3122.08, "text": " it isn''t this you know plane of or space of large language
  models machine learning deep learning", "tokens": [51144, 309, 1943, 380, 341, 291,
  458, 5720, 295, 420, 1901, 295, 2416, 2856, 5245, 3479, 2539, 2452, 2539, 51692],
  "temperature": 0.0, "avg_logprob": -0.14303845625657302, "compression_ratio": 1.7065868263473054,
  "no_speech_prob": 0.003035679692402482}, {"id": 415, "seek": 312208, "start": 3122.08,
  "end": 3132.48, "text": " intelligence you name it yeah so that''s why I do have
  ideas I did tested some of them but", "tokens": [50364, 7599, 291, 1315, 309, 1338,
  370, 300, 311, 983, 286, 360, 362, 3487, 286, 630, 8246, 512, 295, 552, 457, 50884],
  "temperature": 0.0, "avg_logprob": -0.1587352890899216, "compression_ratio": 1.6551724137931034,
  "no_speech_prob": 0.0018897616537287831}, {"id": 416, "seek": 312208, "start": 3133.36,
  "end": 3141.12, "text": " you know things usually don''t work but yeah I don''t
  have time to think systematically about this", "tokens": [50928, 291, 458, 721,
  2673, 500, 380, 589, 457, 1338, 286, 500, 380, 362, 565, 281, 519, 39531, 466, 341,
  51316], "temperature": 0.0, "avg_logprob": -0.1587352890899216, "compression_ratio":
  1.6551724137931034, "no_speech_prob": 0.0018897616537287831}, {"id": 417, "seek":
  312208, "start": 3142.08, "end": 3151.2799999999997, "text": " issues yeah but I
  guess at the same time you did create the base for for you know for other people
  to", "tokens": [51364, 2663, 1338, 457, 286, 2041, 412, 264, 912, 565, 291, 630,
  1884, 264, 3096, 337, 337, 291, 458, 337, 661, 561, 281, 51824], "temperature":
  0.0, "avg_logprob": -0.1587352890899216, "compression_ratio": 1.6551724137931034,
  "no_speech_prob": 0.0018897616537287831}, {"id": 418, "seek": 315128, "start": 3151.28,
  "end": 3159.6800000000003, "text": " innovate and I think it''s I think it''s highly
  appreciated really I also wanted I also wanted to", "tokens": [50364, 33444, 293,
  286, 519, 309, 311, 286, 519, 309, 311, 5405, 17169, 534, 286, 611, 1415, 286, 611,
  1415, 281, 50784], "temperature": 0.0, "avg_logprob": -0.16144752502441406, "compression_ratio":
  1.6823529411764706, "no_speech_prob": 0.0018721995875239372}, {"id": 419, "seek":
  315128, "start": 3160.96, "end": 3167.36, "text": " pick up the topic that originally
  sort of interested when you interested me when when you", "tokens": [50848, 1888,
  493, 264, 4829, 300, 7993, 1333, 295, 3102, 562, 291, 3102, 385, 562, 562, 291,
  51168], "temperature": 0.0, "avg_logprob": -0.16144752502441406, "compression_ratio":
  1.6823529411764706, "no_speech_prob": 0.0018721995875239372}, {"id": 420, "seek":
  315128, "start": 3167.36, "end": 3180.32, "text": " popped up on my LinkedIn you
  know feed you you made a statement about relational databases trying to", "tokens":
  [51168, 21545, 493, 322, 452, 20657, 291, 458, 3154, 291, 291, 1027, 257, 5629,
  466, 38444, 22380, 1382, 281, 51816], "temperature": 0.0, "avg_logprob": -0.16144752502441406,
  "compression_ratio": 1.6823529411764706, "no_speech_prob": 0.0018721995875239372},
  {"id": 421, "seek": 318032, "start": 3180.32, "end": 3187.1200000000003, "text":
  " implement search feature or sort of capability and sort of like miserably failing
  and that maybe", "tokens": [50364, 4445, 3164, 4111, 420, 1333, 295, 13759, 293,
  1333, 295, 411, 17725, 1188, 18223, 293, 300, 1310, 50704], "temperature": 0.0,
  "avg_logprob": -0.08850261298092929, "compression_ratio": 1.728110599078341, "no_speech_prob":
  0.0024983815383166075}, {"id": 422, "seek": 318032, "start": 3187.1200000000003,
  "end": 3193.6800000000003, "text": " you didn''t use the word miserably it''s it''s
  my word here but I wanted to do a little bit like", "tokens": [50704, 291, 994,
  380, 764, 264, 1349, 17725, 1188, 309, 311, 309, 311, 452, 1349, 510, 457, 286,
  1415, 281, 360, 257, 707, 857, 411, 51032], "temperature": 0.0, "avg_logprob": -0.08850261298092929,
  "compression_ratio": 1.728110599078341, "no_speech_prob": 0.0024983815383166075},
  {"id": 423, "seek": 318032, "start": 3193.6800000000003, "end": 3201.1200000000003,
  "text": " expand on this like why do you think they tried to do that and also while
  they were trying that", "tokens": [51032, 5268, 322, 341, 411, 983, 360, 291, 519,
  436, 3031, 281, 360, 300, 293, 611, 1339, 436, 645, 1382, 300, 51404], "temperature":
  0.0, "avg_logprob": -0.08850261298092929, "compression_ratio": 1.728110599078341,
  "no_speech_prob": 0.0024983815383166075}, {"id": 424, "seek": 318032, "start": 3201.1200000000003,
  "end": 3209.52, "text": " what went wrong yeah great question well first of all
  I definitely wouldn''t say the word", "tokens": [51404, 437, 1437, 2085, 1338, 869,
  1168, 731, 700, 295, 439, 286, 2138, 2759, 380, 584, 264, 1349, 51824], "temperature":
  0.0, "avg_logprob": -0.08850261298092929, "compression_ratio": 1.728110599078341,
  "no_speech_prob": 0.0024983815383166075}, {"id": 425, "seek": 320952, "start": 3209.52,
  "end": 3216.08, "text": " miserably because the it it has been success to some degree
  definitely and it''s not", "tokens": [50364, 17725, 1188, 570, 264, 309, 309, 575,
  668, 2245, 281, 512, 4314, 2138, 293, 309, 311, 406, 50692], "temperature": 0.0,
  "avg_logprob": -0.10345675006057277, "compression_ratio": 1.623456790123457, "no_speech_prob":
  0.0017692841356620193}, {"id": 426, "seek": 320952, "start": 3217.2, "end": 3224.24,
  "text": " it''s not over until it''s over so the people are working with this so
  what I have been", "tokens": [50748, 309, 311, 406, 670, 1826, 309, 311, 670, 370,
  264, 561, 366, 1364, 365, 341, 370, 437, 286, 362, 668, 51100], "temperature": 0.0,
  "avg_logprob": -0.10345675006057277, "compression_ratio": 1.623456790123457, "no_speech_prob":
  0.0017692841356620193}, {"id": 427, "seek": 320952, "start": 3224.24, "end": 3231.52,
  "text": " observing for many many years and I as I said I did start my career as
  as a person working on", "tokens": [51100, 22107, 337, 867, 867, 924, 293, 286,
  382, 286, 848, 286, 630, 722, 452, 3988, 382, 382, 257, 954, 1364, 322, 51464],
  "temperature": 0.0, "avg_logprob": -0.10345675006057277, "compression_ratio": 1.623456790123457,
  "no_speech_prob": 0.0017692841356620193}, {"id": 428, "seek": 323152, "start": 3232.16,
  "end": 3246.72, "text": " databases and doing a lot of writing a lot of SQL so but
  the the typical database is a very", "tokens": [50396, 22380, 293, 884, 257, 688,
  295, 3579, 257, 688, 295, 19200, 370, 457, 264, 264, 7476, 8149, 307, 257, 588,
  51124], "temperature": 0.0, "avg_logprob": -0.23455670674641926, "compression_ratio":
  1.484375, "no_speech_prob": 0.002046620938926935}, {"id": 429, "seek": 323152, "start":
  3246.72, "end": 3255.84, "text": " different beast from what you typically need
  to do information to you so the first of all they all", "tokens": [51124, 819, 13464,
  490, 437, 291, 5850, 643, 281, 360, 1589, 281, 291, 370, 264, 700, 295, 439, 436,
  439, 51580], "temperature": 0.0, "avg_logprob": -0.23455670674641926, "compression_ratio":
  1.484375, "no_speech_prob": 0.002046620938926935}, {"id": 430, "seek": 325584, "start":
  3255.92, "end": 3263.04, "text": " like the early databases or they are oriented
  they achieve some tradeoff between the", "tokens": [50368, 411, 264, 2440, 22380,
  420, 436, 366, 21841, 436, 4584, 512, 4923, 4506, 1296, 264, 50724], "temperature":
  0.0, "avg_logprob": -0.20267138564795778, "compression_ratio": 1.6932515337423313,
  "no_speech_prob": 0.002183576114475727}, {"id": 431, "seek": 325584, "start": 3266.1600000000003,
  "end": 3273.2000000000003, "text": " so they need have got throughput in both inserts
  and updates and they need to be able to update", "tokens": [50880, 370, 436, 643,
  362, 658, 44629, 294, 1293, 49163, 293, 9205, 293, 436, 643, 281, 312, 1075, 281,
  5623, 51232], "temperature": 0.0, "avg_logprob": -0.20267138564795778, "compression_ratio":
  1.6932515337423313, "no_speech_prob": 0.002183576114475727}, {"id": 432, "seek":
  325584, "start": 3274.0, "end": 3281.1200000000003, "text": " information pretty
  quickly and also it should be pretty reasonable and they also support really", "tokens":
  [51272, 1589, 1238, 2661, 293, 611, 309, 820, 312, 1238, 10585, 293, 436, 611, 1406,
  534, 51628], "temperature": 0.0, "avg_logprob": -0.20267138564795778, "compression_ratio":
  1.6932515337423313, "no_speech_prob": 0.002183576114475727}, {"id": 433, "seek":
  328112, "start": 3281.2799999999997, "end": 3292.88, "text": " like the the data
  the data can be pretty complicated that what they call that SQL schema there can",
  "tokens": [50372, 411, 264, 264, 1412, 264, 1412, 393, 312, 1238, 6179, 300, 437,
  436, 818, 300, 19200, 34078, 456, 393, 50952], "temperature": 0.0, "avg_logprob":
  -0.19168907403945923, "compression_ratio": 1.7797619047619047, "no_speech_prob":
  0.0016946801915764809}, {"id": 434, "seek": 328112, "start": 3292.88, "end": 3298.72,
  "text": " be multiple tables and all of that needs to be supported and so of course
  there are tradeoffs to be", "tokens": [50952, 312, 3866, 8020, 293, 439, 295, 300,
  2203, 281, 312, 8104, 293, 370, 295, 1164, 456, 366, 4923, 19231, 281, 312, 51244],
  "temperature": 0.0, "avg_logprob": -0.19168907403945923, "compression_ratio": 1.7797619047619047,
  "no_speech_prob": 0.0016946801915764809}, {"id": 435, "seek": 328112, "start": 3298.72,
  "end": 3306.4, "text": " made to make it possible and again to support generality
  support efficient updates support efficient", "tokens": [51244, 1027, 281, 652,
  309, 1944, 293, 797, 281, 1406, 1337, 1860, 1406, 7148, 9205, 1406, 7148, 51628],
  "temperature": 0.0, "avg_logprob": -0.19168907403945923, "compression_ratio": 1.7797619047619047,
  "no_speech_prob": 0.0016946801915764809}, {"id": 436, "seek": 330640, "start": 3306.48,
  "end": 3313.28, "text": " inserts but at the same time if you''re doing the TV system
  a lot of of this is not necessary", "tokens": [50368, 49163, 457, 412, 264, 912,
  565, 498, 291, 434, 884, 264, 3558, 1185, 257, 688, 295, 295, 341, 307, 406, 4818,
  50708], "temperature": 0.0, "avg_logprob": -0.2388037716049746, "compression_ratio":
  1.6511627906976745, "no_speech_prob": 0.003451433964073658}, {"id": 437, "seek":
  330640, "start": 3313.84, "end": 3321.2000000000003, "text": " so say for you want
  to do like keyword-based retrieval you only need to all at high level", "tokens":
  [50736, 370, 584, 337, 291, 528, 281, 360, 411, 20428, 12, 6032, 19817, 3337, 291,
  787, 643, 281, 439, 412, 1090, 1496, 51104], "temperature": 0.0, "avg_logprob":
  -0.2388037716049746, "compression_ratio": 1.6511627906976745, "no_speech_prob":
  0.003451433964073658}, {"id": 438, "seek": 330640, "start": 3321.84, "end": 3325.6800000000003,
  "text": " is somewhat a simplification but you need to memorize them in which documents",
  "tokens": [51136, 307, 8344, 257, 6883, 3774, 457, 291, 643, 281, 27478, 552, 294,
  597, 8512, 51328], "temperature": 0.0, "avg_logprob": -0.2388037716049746, "compression_ratio":
  1.6511627906976745, "no_speech_prob": 0.003451433964073658}, {"id": 439, "seek":
  330640, "start": 3327.84, "end": 3333.04, "text": " you have which keywords and
  then you have this so called inverted index where for each keyword", "tokens": [51436,
  291, 362, 597, 21009, 293, 550, 291, 362, 341, 370, 1219, 38969, 8186, 689, 337,
  1184, 20428, 51696], "temperature": 0.0, "avg_logprob": -0.2388037716049746, "compression_ratio":
  1.6511627906976745, "no_speech_prob": 0.003451433964073658}, {"id": 440, "seek":
  333304, "start": 3333.2, "end": 3340.32, "text": " you have a list of documents
  where these keywords appear and it''s much simple structure it permits", "tokens":
  [50372, 291, 362, 257, 1329, 295, 8512, 689, 613, 21009, 4204, 293, 309, 311, 709,
  2199, 3877, 309, 30990, 50728], "temperature": 0.0, "avg_logprob": -0.1891049100207044,
  "compression_ratio": 1.7609756097560976, "no_speech_prob": 0.001181987812742591},
  {"id": 441, "seek": 333304, "start": 3340.32, "end": 3349.44, "text": " much more
  efficient compression algorithms so it''s again it''s it''s a different beast and
  and also", "tokens": [50728, 709, 544, 7148, 19355, 14642, 370, 309, 311, 797, 309,
  311, 309, 311, 257, 819, 13464, 293, 293, 611, 51184], "temperature": 0.0, "avg_logprob":
  -0.1891049100207044, "compression_ratio": 1.7609756097560976, "no_speech_prob":
  0.001181987812742591}, {"id": 442, "seek": 333304, "start": 3350.0, "end": 3354.32,
  "text": " in terms of efficiency of updates once you compress data and once you",
  "tokens": [51212, 294, 2115, 295, 10493, 295, 9205, 1564, 291, 14778, 1412, 293,
  1564, 291, 51428], "temperature": 0.0, "avg_logprob": -0.1891049100207044, "compression_ratio":
  1.7609756097560976, "no_speech_prob": 0.001181987812742591}, {"id": 443, "seek":
  333304, "start": 3355.12, "end": 3362.72, "text": " represent it in a special way
  it''s it becomes much harder to to make these incremental updates", "tokens": [51468,
  2906, 309, 294, 257, 2121, 636, 309, 311, 309, 3643, 709, 6081, 281, 281, 652, 613,
  35759, 9205, 51848], "temperature": 0.0, "avg_logprob": -0.1891049100207044, "compression_ratio":
  1.7609756097560976, "no_speech_prob": 0.001181987812742591}, {"id": 444, "seek":
  336304, "start": 3363.68, "end": 3371.36, "text": " for which those early databases
  were applied so clearly there is a disconnect it was somewhat", "tokens": [50396,
  337, 597, 729, 2440, 22380, 645, 6456, 370, 4448, 456, 307, 257, 14299, 309, 390,
  8344, 50780], "temperature": 0.0, "avg_logprob": -0.16825653334795418, "compression_ratio":
  1.6171428571428572, "no_speech_prob": 0.0012183558428660035}, {"id": 445, "seek":
  336304, "start": 3371.36, "end": 3380.72, "text": " removed with the introduction
  of so-called columnar databases but it''s still like with columnar", "tokens": [50780,
  7261, 365, 264, 9339, 295, 370, 12, 11880, 7738, 289, 22380, 457, 309, 311, 920,
  411, 365, 7738, 289, 51248], "temperature": 0.0, "avg_logprob": -0.16825653334795418,
  "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0012183558428660035},
  {"id": 446, "seek": 336304, "start": 3380.72, "end": 3389.2799999999997, "text":
  " databases I believe they actually do not favor those you know point updates anymore
  they they", "tokens": [51248, 22380, 286, 1697, 436, 767, 360, 406, 2294, 729, 291,
  458, 935, 9205, 3602, 436, 436, 51676], "temperature": 0.0, "avg_logprob": -0.16825653334795418,
  "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.0012183558428660035},
  {"id": 447, "seek": 338928, "start": 3389.36, "end": 3395.76, "text": " are best
  to be used for bulk updates and so basically once you''re doing bulk updates", "tokens":
  [50368, 366, 1151, 281, 312, 1143, 337, 16139, 9205, 293, 370, 1936, 1564, 291,
  434, 884, 16139, 9205, 50688], "temperature": 0.0, "avg_logprob": -0.20951991611056858,
  "compression_ratio": 1.809278350515464, "no_speech_prob": 0.001595189911313355},
  {"id": 448, "seek": 338928, "start": 3395.76, "end": 3403.92, "text": " yeah you''re
  sort of in this search engine area where you ask you you change things in in", "tokens":
  [50688, 1338, 291, 434, 1333, 295, 294, 341, 3164, 2848, 1859, 689, 291, 1029, 291,
  291, 1319, 721, 294, 294, 51096], "temperature": 0.0, "avg_logprob": -0.20951991611056858,
  "compression_ratio": 1.809278350515464, "no_speech_prob": 0.001595189911313355},
  {"id": 449, "seek": 338928, "start": 3404.8, "end": 3409.28, "text": " rather large
  increments changing the access in rather large increments and you don''t", "tokens":
  [51140, 2831, 2416, 1946, 1117, 4473, 264, 2105, 294, 2831, 2416, 1946, 1117, 293,
  291, 500, 380, 51364], "temperature": 0.0, "avg_logprob": -0.20951991611056858,
  "compression_ratio": 1.809278350515464, "no_speech_prob": 0.001595189911313355},
  {"id": 450, "seek": 338928, "start": 3410.96, "end": 3416.96, "text": " you don''t
  worry too much about your information is being like really up to date you can wait",
  "tokens": [51448, 291, 500, 380, 3292, 886, 709, 466, 428, 1589, 307, 885, 411,
  534, 493, 281, 4002, 291, 393, 1699, 51748], "temperature": 0.0, "avg_logprob":
  -0.20951991611056858, "compression_ratio": 1.809278350515464, "no_speech_prob":
  0.001595189911313355}, {"id": 451, "seek": 341696, "start": 3416.96, "end": 3422.88,
  "text": " maybe a day maybe a few hours but it doesn''t have like an instantaneous
  update of the database", "tokens": [50364, 1310, 257, 786, 1310, 257, 1326, 2496,
  457, 309, 1177, 380, 362, 411, 364, 45596, 5623, 295, 264, 8149, 50660], "temperature":
  0.0, "avg_logprob": -0.12978360869667746, "compression_ratio": 1.7123287671232876,
  "no_speech_prob": 0.0020212840754538774}, {"id": 452, "seek": 341696, "start": 3422.88,
  "end": 3429.36, "text": " so this is a different trade-offs so yeah um well of course
  there is a disconnect and this is why", "tokens": [50660, 370, 341, 307, 257, 819,
  4923, 12, 19231, 370, 1338, 1105, 731, 295, 1164, 456, 307, 257, 14299, 293, 341,
  307, 983, 50984], "temperature": 0.0, "avg_logprob": -0.12978360869667746, "compression_ratio":
  1.7123287671232876, "no_speech_prob": 0.0020212840754538774}, {"id": 453, "seek":
  341696, "start": 3429.36, "end": 3436.56, "text": " it''s it was always hard I believe
  to add to add like food tax indexes to regular databases", "tokens": [50984, 309,
  311, 309, 390, 1009, 1152, 286, 1697, 281, 909, 281, 909, 411, 1755, 3366, 8186,
  279, 281, 3890, 22380, 51344], "temperature": 0.0, "avg_logprob": -0.12978360869667746,
  "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0020212840754538774},
  {"id": 454, "seek": 341696, "start": 3437.92, "end": 3445.2, "text": " but another
  issue with the the disconnect is that like again the retrieval often needs like",
  "tokens": [51412, 457, 1071, 2734, 365, 264, 264, 14299, 307, 300, 411, 797, 264,
  19817, 3337, 2049, 2203, 411, 51776], "temperature": 0.0, "avg_logprob": -0.12978360869667746,
  "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.0020212840754538774},
  {"id": 455, "seek": 344520, "start": 3445.2, "end": 3452.8799999999997, "text":
  " really different set of specialized features so if you have a relational database
  system", "tokens": [50364, 534, 819, 992, 295, 19813, 4122, 370, 498, 291, 362,
  257, 38444, 8149, 1185, 50748], "temperature": 0.0, "avg_logprob": -0.14985672372286438,
  "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.000740553077775985},
  {"id": 456, "seek": 344520, "start": 3454.24, "end": 3460.72, "text": " it''s pretty
  hard to support this for example like deconization if you need to do deconization",
  "tokens": [50816, 309, 311, 1238, 1152, 281, 1406, 341, 337, 1365, 411, 979, 266,
  2144, 498, 291, 643, 281, 360, 979, 266, 2144, 51140], "temperature": 0.0, "avg_logprob":
  -0.14985672372286438, "compression_ratio": 1.6395348837209303, "no_speech_prob":
  0.000740553077775985}, {"id": 457, "seek": 344520, "start": 3461.2799999999997,
  "end": 3467.7599999999998, "text": " in multiple languages yeah so of course that''s
  part like you know the creation of those specialized", "tokens": [51168, 294, 3866,
  8650, 1338, 370, 295, 1164, 300, 311, 644, 411, 291, 458, 264, 8016, 295, 729, 19813,
  51492], "temperature": 0.0, "avg_logprob": -0.14985672372286438, "compression_ratio":
  1.6395348837209303, "no_speech_prob": 0.000740553077775985}, {"id": 458, "seek":
  346776, "start": 3467.76, "end": 3474.1600000000003, "text": " tools with a lot
  of features like you''ve seen and VESPA and databases are catching up", "tokens":
  [50364, 3873, 365, 257, 688, 295, 4122, 411, 291, 600, 1612, 293, 691, 2358, 10297,
  293, 22380, 366, 16124, 493, 50684], "temperature": 0.0, "avg_logprob": -0.20436343927493042,
  "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0016593633918091655},
  {"id": 459, "seek": 346776, "start": 3475.1200000000003, "end": 3483.6000000000004,
  "text": " but there is still a gap and you know it''s probably like going to be
  really tedious to to support", "tokens": [50732, 457, 456, 307, 920, 257, 7417,
  293, 291, 458, 309, 311, 1391, 411, 516, 281, 312, 534, 38284, 281, 281, 1406, 51156],
  "temperature": 0.0, "avg_logprob": -0.20436343927493042, "compression_ratio": 1.6333333333333333,
  "no_speech_prob": 0.0016593633918091655}, {"id": 460, "seek": 346776, "start": 3484.5600000000004,
  "end": 3487.84, "text": " yeah full set of features like you know you need to match
  VESPA", "tokens": [51204, 1338, 1577, 992, 295, 4122, 411, 291, 458, 291, 643, 281,
  2995, 691, 2358, 10297, 51368], "temperature": 0.0, "avg_logprob": -0.20436343927493042,
  "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0016593633918091655},
  {"id": 461, "seek": 346776, "start": 3489.44, "end": 3494.96, "text": " so yeah
  these are like my five cents on this stuff yeah but I''m curious to sort of a little
  bit", "tokens": [51448, 370, 1338, 613, 366, 411, 452, 1732, 14941, 322, 341, 1507,
  1338, 457, 286, 478, 6369, 281, 1333, 295, 257, 707, 857, 51724], "temperature":
  0.0, "avg_logprob": -0.20436343927493042, "compression_ratio": 1.6333333333333333,
  "no_speech_prob": 0.0016593633918091655}, {"id": 462, "seek": 349496, "start": 3494.96,
  "end": 3503.76, "text": " the understand why do you think databases are still trying
  why are they trying to", "tokens": [50364, 264, 1223, 983, 360, 291, 519, 22380,
  366, 920, 1382, 983, 366, 436, 1382, 281, 50804], "temperature": 0.0, "avg_logprob":
  -0.18421046517112039, "compression_ratio": 1.630952380952381, "no_speech_prob":
  0.0010139207588508725}, {"id": 463, "seek": 349496, "start": 3504.8, "end": 3514.0,
  "text": " encompass this seemingly disparate ways of searching right when you actually
  if basically like", "tokens": [50856, 28268, 341, 18709, 14548, 473, 2098, 295,
  10808, 558, 562, 291, 767, 498, 1936, 411, 51316], "temperature": 0.0, "avg_logprob":
  -0.18421046517112039, "compression_ratio": 1.630952380952381, "no_speech_prob":
  0.0010139207588508725}, {"id": 464, "seek": 349496, "start": 3514.0, "end": 3518.64,
  "text": " you explained if you need to have a fully blown search engine that can
  support multiple languages", "tokens": [51316, 291, 8825, 498, 291, 643, 281, 362,
  257, 4498, 16479, 3164, 2848, 300, 393, 1406, 3866, 8650, 51548], "temperature":
  0.0, "avg_logprob": -0.18421046517112039, "compression_ratio": 1.630952380952381,
  "no_speech_prob": 0.0010139207588508725}, {"id": 465, "seek": 351864, "start": 3518.72,
  "end": 3526.08, "text": " tokenization and so on you better be using the likes of
  recene VESPA and you know maybe", "tokens": [50368, 14862, 2144, 293, 370, 322,
  291, 1101, 312, 1228, 264, 5902, 295, 850, 1450, 691, 2358, 10297, 293, 291, 458,
  1310, 50736], "temperature": 0.0, "avg_logprob": -0.22827778586858435, "compression_ratio":
  1.5922330097087378, "no_speech_prob": 0.001773255062289536}, {"id": 466, "seek":
  351864, "start": 3526.08, "end": 3530.72, "text": " elastic search on top of the
  scene and so on why why are they still trying", "tokens": [50736, 17115, 3164, 322,
  1192, 295, 264, 4145, 293, 370, 322, 983, 983, 366, 436, 920, 1382, 50968], "temperature":
  0.0, "avg_logprob": -0.22827778586858435, "compression_ratio": 1.5922330097087378,
  "no_speech_prob": 0.001773255062289536}, {"id": 467, "seek": 351864, "start": 3533.12,
  "end": 3540.4, "text": " they want customers it''s of course advantageous to be
  like you know one stop shop so they come", "tokens": [51088, 436, 528, 4581, 309,
  311, 295, 1164, 5002, 563, 281, 312, 411, 291, 458, 472, 1590, 3945, 370, 436, 808,
  51452], "temperature": 0.0, "avg_logprob": -0.22827778586858435, "compression_ratio":
  1.5922330097087378, "no_speech_prob": 0.001773255062289536}, {"id": 468, "seek":
  351864, "start": 3542.08, "end": 3546.72, "text": " to specific provider and they
  have everything so I listen to a podcast", "tokens": [51536, 281, 2685, 12398, 293,
  436, 362, 1203, 370, 286, 2140, 281, 257, 7367, 51768], "temperature": 0.0, "avg_logprob":
  -0.22827778586858435, "compression_ratio": 1.5922330097087378, "no_speech_prob":
  0.001773255062289536}, {"id": 469, "seek": 354864, "start": 3549.12, "end": 3558.08,
  "text": " the roxette co-founder which was the roxette the one was acquired by OpenAI
  but I think you", "tokens": [50388, 264, 744, 87, 3007, 598, 12, 33348, 597, 390,
  264, 744, 87, 3007, 264, 472, 390, 17554, 538, 7238, 48698, 457, 286, 519, 291,
  50836], "temperature": 0.0, "avg_logprob": -0.22136606889612534, "compression_ratio":
  1.532258064516129, "no_speech_prob": 0.007909516803920269}, {"id": 470, "seek":
  354864, "start": 3558.08, "end": 3564.56, "text": " recorded that podcast before
  they were acquired so good timing and you can clearly hear that message", "tokens":
  [50836, 8287, 300, 7367, 949, 436, 645, 17554, 370, 665, 10822, 293, 291, 393, 4448,
  1568, 300, 3636, 51160], "temperature": 0.0, "avg_logprob": -0.22136606889612534,
  "compression_ratio": 1.532258064516129, "no_speech_prob": 0.007909516803920269},
  {"id": 471, "seek": 354864, "start": 3564.56, "end": 3571.3599999999997, "text":
  " all like we really want people to come and use our solution so we have hybrid
  search we have", "tokens": [51160, 439, 411, 321, 534, 528, 561, 281, 808, 293,
  764, 527, 3827, 370, 321, 362, 13051, 3164, 321, 362, 51500], "temperature": 0.0,
  "avg_logprob": -0.22136606889612534, "compression_ratio": 1.532258064516129, "no_speech_prob":
  0.007909516803920269}, {"id": 472, "seek": 357136, "start": 3571.36, "end": 3578.48,
  "text": " some support for ranking we have this and we have that yeah I can''t I
  can''t argue against", "tokens": [50364, 512, 1406, 337, 17833, 321, 362, 341, 293,
  321, 362, 300, 1338, 286, 393, 380, 286, 393, 380, 9695, 1970, 50720], "temperature":
  0.0, "avg_logprob": -0.1349770264192061, "compression_ratio": 1.7077625570776256,
  "no_speech_prob": 0.0035973640624433756}, {"id": 473, "seek": 357136, "start": 3578.48,
  "end": 3585.52, "text": " this being convenient so definitely something something
  very useful customers yeah yeah just a small", "tokens": [50720, 341, 885, 10851,
  370, 2138, 746, 746, 588, 4420, 4581, 1338, 1338, 445, 257, 1359, 51072], "temperature":
  0.0, "avg_logprob": -0.1349770264192061, "compression_ratio": 1.7077625570776256,
  "no_speech_prob": 0.0035973640624433756}, {"id": 474, "seek": 357136, "start": 3585.52,
  "end": 3591.52, "text": " correction he''s not a co-founder I think he''s well VP
  of engineering or used to be a VP of", "tokens": [51072, 19984, 415, 311, 406, 257,
  598, 12, 33348, 286, 519, 415, 311, 731, 35812, 295, 7043, 420, 1143, 281, 312,
  257, 35812, 295, 51372], "temperature": 0.0, "avg_logprob": -0.1349770264192061,
  "compression_ratio": 1.7077625570776256, "no_speech_prob": 0.0035973640624433756},
  {"id": 475, "seek": 357136, "start": 3591.52, "end": 3599.2000000000003, "text":
  " engineering in roxette but yeah I mean he''s he brings the story and I encourage
  listeners to", "tokens": [51372, 7043, 294, 744, 87, 3007, 457, 1338, 286, 914,
  415, 311, 415, 5607, 264, 1657, 293, 286, 5373, 23274, 281, 51756], "temperature":
  0.0, "avg_logprob": -0.1349770264192061, "compression_ratio": 1.7077625570776256,
  "no_speech_prob": 0.0035973640624433756}, {"id": 476, "seek": 359920, "start": 3599.2,
  "end": 3607.4399999999996, "text": " listen to the episode he brings the story of
  you know roxDB scalability issues from from Facebook and", "tokens": [50364, 2140,
  281, 264, 3500, 415, 5607, 264, 1657, 295, 291, 458, 744, 87, 27735, 15664, 2310,
  2663, 490, 490, 4384, 293, 50776], "temperature": 0.0, "avg_logprob": -0.16489293541706784,
  "compression_ratio": 1.5, "no_speech_prob": 0.0019068121910095215}, {"id": 477,
  "seek": 359920, "start": 3608.64, "end": 3618.16, "text": " how it underpins you
  know the the further journey at roxette so I feel like we could discuss", "tokens":
  [50836, 577, 309, 833, 79, 1292, 291, 458, 264, 264, 3052, 4671, 412, 744, 87, 3007,
  370, 286, 841, 411, 321, 727, 2248, 51312], "temperature": 0.0, "avg_logprob": -0.16489293541706784,
  "compression_ratio": 1.5, "no_speech_prob": 0.0019068121910095215}, {"id": 478,
  "seek": 359920, "start": 3618.16, "end": 3624.72, "text": " for five hours and I''m
  actually a big fan of Lex Friedman podcasts where some of the episodes", "tokens":
  [51312, 337, 1732, 2496, 293, 286, 478, 767, 257, 955, 3429, 295, 24086, 17605,
  1601, 24045, 689, 512, 295, 264, 9313, 51640], "temperature": 0.0, "avg_logprob":
  -0.16489293541706784, "compression_ratio": 1.5, "no_speech_prob": 0.0019068121910095215},
  {"id": 479, "seek": 362472, "start": 3625.12, "end": 3631.2, "text": " really really
  long and you can listen to them for weeks and and I think I really hope that we
  can", "tokens": [50384, 534, 534, 938, 293, 291, 393, 2140, 281, 552, 337, 3259,
  293, 293, 286, 519, 286, 534, 1454, 300, 321, 393, 50688], "temperature": 0.0, "avg_logprob":
  -0.08310505963753963, "compression_ratio": 1.6145251396648044, "no_speech_prob":
  0.007138872053474188}, {"id": 480, "seek": 362472, "start": 3631.2, "end": 3639.04,
  "text": " record with you sometime later as well as you know as you have topics
  to share but is there", "tokens": [50688, 2136, 365, 291, 15053, 1780, 382, 731,
  382, 291, 458, 382, 291, 362, 8378, 281, 2073, 457, 307, 456, 51080], "temperature":
  0.0, "avg_logprob": -0.08310505963753963, "compression_ratio": 1.6145251396648044,
  "no_speech_prob": 0.007138872053474188}, {"id": 481, "seek": 362472, "start": 3639.04,
  "end": 3645.52, "text": " something Leo that you want to share I don''t know it
  could be a paper you''ve read that particularly", "tokens": [51080, 746, 19344,
  300, 291, 528, 281, 2073, 286, 500, 380, 458, 309, 727, 312, 257, 3035, 291, 600,
  1401, 300, 4098, 51404], "temperature": 0.0, "avg_logprob": -0.08310505963753963,
  "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.007138872053474188},
  {"id": 482, "seek": 364552, "start": 3645.52, "end": 3654.24, "text": " excites
  you maybe a book or anything else that you want to say yeah I think we", "tokens":
  [50364, 1624, 3324, 291, 1310, 257, 1446, 420, 1340, 1646, 300, 291, 528, 281, 584,
  1338, 286, 519, 321, 50800], "temperature": 0.0, "avg_logprob": -0.23083412079584031,
  "compression_ratio": 1.4453781512605042, "no_speech_prob": 0.011599970981478691},
  {"id": 483, "seek": 364552, "start": 3657.6, "end": 3665.84, "text": " yeah great
  question so so I was interested a lot recently very recently I mean maybe the last",
  "tokens": [50968, 1338, 869, 1168, 370, 370, 286, 390, 3102, 257, 688, 3938, 588,
  3938, 286, 914, 1310, 264, 1036, 51380], "temperature": 0.0, "avg_logprob": -0.23083412079584031,
  "compression_ratio": 1.4453781512605042, "no_speech_prob": 0.011599970981478691},
  {"id": 484, "seek": 366584, "start": 3666.48, "end": 3676.1600000000003, "text":
  " couple of years in how LLAMS can be useful for search in one particular interesting",
  "tokens": [50396, 1916, 295, 924, 294, 577, 441, 43, 2865, 50, 393, 312, 4420, 337,
  3164, 294, 472, 1729, 1880, 50880], "temperature": 0.0, "avg_logprob": -0.24238252639770508,
  "compression_ratio": 1.372093023255814, "no_speech_prob": 0.006762172561138868},
  {"id": 485, "seek": 366584, "start": 3676.1600000000003, "end": 3685.52, "text":
  " direction is how do you use LLAMS to to train smaller models for retrieval and
  ranking for me", "tokens": [50880, 3513, 307, 577, 360, 291, 764, 441, 43, 2865,
  50, 281, 281, 3847, 4356, 5245, 337, 19817, 3337, 293, 17833, 337, 385, 51348],
  "temperature": 0.0, "avg_logprob": -0.24238252639770508, "compression_ratio": 1.372093023255814,
  "no_speech_prob": 0.006762172561138868}, {"id": 486, "seek": 368552, "start": 3685.52,
  "end": 3696.64, "text": " personally it''s a very exciting area of research yeah
  as far as distillation is concerned there", "tokens": [50364, 5665, 309, 311, 257,
  588, 4670, 1859, 295, 2132, 1338, 382, 1400, 382, 42923, 399, 307, 5922, 456, 50920],
  "temperature": 0.0, "avg_logprob": -0.17282658702922318, "compression_ratio": 1.6047904191616766,
  "no_speech_prob": 0.004426660481840372}, {"id": 487, "seek": 368552, "start": 3696.64,
  "end": 3707.7599999999998, "text": " was several interesting papers on the topic
  there was but basically the lot of of that work", "tokens": [50920, 390, 2940, 1880,
  10577, 322, 264, 4829, 456, 390, 457, 1936, 264, 688, 295, 295, 300, 589, 51476],
  "temperature": 0.0, "avg_logprob": -0.17282658702922318, "compression_ratio": 1.6047904191616766,
  "no_speech_prob": 0.004426660481840372}, {"id": 488, "seek": 368552, "start": 3707.7599999999998,
  "end": 3714.16, "text": " revolves around creation synthetic data synthetic queries
  based on the documents", "tokens": [51476, 47934, 926, 8016, 23420, 1412, 23420,
  24109, 2361, 322, 264, 8512, 51796], "temperature": 0.0, "avg_logprob": -0.17282658702922318,
  "compression_ratio": 1.6047904191616766, "no_speech_prob": 0.004426660481840372},
  {"id": 489, "seek": 371552, "start": 3715.52, "end": 3722.72, "text": " like we
  have a document that creates the queries and queries that you asked that that",
  "tokens": [50364, 411, 321, 362, 257, 4166, 300, 7829, 264, 24109, 293, 24109, 300,
  291, 2351, 300, 300, 50724], "temperature": 0.0, "avg_logprob": -0.30367846366686696,
  "compression_ratio": 1.87, "no_speech_prob": 0.0007750603253953159}, {"id": 490,
  "seek": 371552, "start": 3722.72, "end": 3727.92, "text": " pretty slash question
  and the answer is in documents we have a positive relevant document and", "tokens":
  [50724, 1238, 17330, 1168, 293, 264, 1867, 307, 294, 8512, 321, 362, 257, 3353,
  7340, 4166, 293, 50984], "temperature": 0.0, "avg_logprob": -0.30367846366686696,
  "compression_ratio": 1.87, "no_speech_prob": 0.0007750603253953159}, {"id": 491,
  "seek": 371552, "start": 3727.92, "end": 3736.24, "text": " you can sample negatives
  from from your collection and train them while but there is also a line of", "tokens":
  [50984, 291, 393, 6889, 40019, 490, 490, 428, 5765, 293, 3847, 552, 1339, 457, 456,
  307, 611, 257, 1622, 295, 51400], "temperature": 0.0, "avg_logprob": -0.30367846366686696,
  "compression_ratio": 1.87, "no_speech_prob": 0.0007750603253953159}, {"id": 492,
  "seek": 371552, "start": 3736.24, "end": 3744.16, "text": " research where they
  they would try to create both queries and documents so yeah in summary the", "tokens":
  [51400, 2132, 689, 436, 436, 576, 853, 281, 1884, 1293, 24109, 293, 8512, 370, 1338,
  294, 12691, 264, 51796], "temperature": 0.0, "avg_logprob": -0.30367846366686696,
  "compression_ratio": 1.87, "no_speech_prob": 0.0007750603253953159}, {"id": 493,
  "seek": 374416, "start": 3745.12, "end": 3753.04, "text": " that whole that whole
  not in summary but that that that line of research was particularly interesting",
  "tokens": [50412, 300, 1379, 300, 1379, 406, 294, 12691, 457, 300, 300, 300, 1622,
  295, 2132, 390, 4098, 1880, 50808], "temperature": 0.0, "avg_logprob": -0.16441614236404647,
  "compression_ratio": 1.625, "no_speech_prob": 0.001531481626443565}, {"id": 494,
  "seek": 374416, "start": 3753.04, "end": 3761.92, "text": " to me although there
  was some work before LLAMS to create synthetic queries it was not particularly",
  "tokens": [50808, 281, 385, 4878, 456, 390, 512, 589, 949, 441, 43, 2865, 50, 281,
  1884, 23420, 24109, 309, 390, 406, 4098, 51252], "temperature": 0.0, "avg_logprob":
  -0.16441614236404647, "compression_ratio": 1.625, "no_speech_prob": 0.001531481626443565},
  {"id": 495, "seek": 374416, "start": 3762.7999999999997, "end": 3770.3999999999996,
  "text": " well-used technique but one paper that stood out was the in-part paper
  from a couple of years ago", "tokens": [51296, 731, 12, 4717, 6532, 457, 472, 3035,
  300, 9371, 484, 390, 264, 294, 12, 6971, 3035, 490, 257, 1916, 295, 924, 2057, 51676],
  "temperature": 0.0, "avg_logprob": -0.16441614236404647, "compression_ratio": 1.625,
  "no_speech_prob": 0.001531481626443565}, {"id": 496, "seek": 377040, "start": 3770.4,
  "end": 3778.56, "text": " and we have reproduction of this paper in that paper had
  a a pretty quick fall-up from the", "tokens": [50364, 293, 321, 362, 33934, 295,
  341, 3035, 294, 300, 3035, 632, 257, 257, 1238, 1702, 2100, 12, 1010, 490, 264,
  50772], "temperature": 0.0, "avg_logprob": -0.2468146970195155, "compression_ratio":
  1.668639053254438, "no_speech_prob": 0.00166233885101974}, {"id": 497, "seek": 377040,
  "start": 3780.8, "end": 3788.2400000000002, "text": " there were several several
  authors from Google they called it Protigator where they showed how", "tokens":
  [50884, 456, 645, 2940, 2940, 16552, 490, 3329, 436, 1219, 309, 10019, 28895, 689,
  436, 4712, 577, 51256], "temperature": 0.0, "avg_logprob": -0.2468146970195155,
  "compression_ratio": 1.668639053254438, "no_speech_prob": 0.00166233885101974},
  {"id": 498, "seek": 377040, "start": 3788.2400000000002, "end": 3798.2400000000002,
  "text": " this technique can be improved and there was another fall-up from the
  with the same first author", "tokens": [51256, 341, 6532, 393, 312, 9689, 293, 456,
  390, 1071, 2100, 12, 1010, 490, 264, 365, 264, 912, 700, 3793, 51756], "temperature":
  0.0, "avg_logprob": -0.2468146970195155, "compression_ratio": 1.668639053254438,
  "no_speech_prob": 0.00166233885101974}, {"id": 499, "seek": 380040, "start": 3800.56,
  "end": 3809.36, "text": " now she transitioned to the mind and now they they showed
  like like oh like now we do it like", "tokens": [50372, 586, 750, 47346, 281, 264,
  1575, 293, 586, 436, 436, 4712, 411, 411, 1954, 411, 586, 321, 360, 309, 411, 50812],
  "temperature": 0.0, "avg_logprob": -0.16688385009765624, "compression_ratio": 1.7530864197530864,
  "no_speech_prob": 0.003513850038871169}, {"id": 500, "seek": 380040, "start": 3809.36,
  "end": 3817.2000000000003, "text": " somewhat better but the they they found one
  issue with the synthetic query generation approach", "tokens": [50812, 8344, 1101,
  457, 264, 436, 436, 1352, 472, 2734, 365, 264, 23420, 14581, 5125, 3109, 51204],
  "temperature": 0.0, "avg_logprob": -0.16688385009765624, "compression_ratio": 1.7530864197530864,
  "no_speech_prob": 0.003513850038871169}, {"id": 501, "seek": 380040, "start": 3817.2000000000003,
  "end": 3824.08, "text": " that not always the the document that was used to create
  the queries the most relevant document", "tokens": [51204, 300, 406, 1009, 264,
  264, 4166, 300, 390, 1143, 281, 1884, 264, 24109, 264, 881, 7340, 4166, 51548],
  "temperature": 0.0, "avg_logprob": -0.16688385009765624, "compression_ratio": 1.7530864197530864,
  "no_speech_prob": 0.003513850038871169}, {"id": 502, "seek": 382408, "start": 3824.64,
  "end": 3832.08, "text": " so you would think it sort of makes sense that if the
  question is being answered by this document it", "tokens": [50392, 370, 291, 576,
  519, 309, 1333, 295, 1669, 2020, 300, 498, 264, 1168, 307, 885, 10103, 538, 341,
  4166, 309, 50764], "temperature": 0.0, "avg_logprob": -0.20478145374971277, "compression_ratio":
  1.8803827751196172, "no_speech_prob": 0.002274239668622613}, {"id": 503, "seek":
  382408, "start": 3832.08, "end": 3836.72, "text": " is the most relevant document
  that turns out if you ask a question there can be other documents", "tokens": [50764,
  307, 264, 881, 7340, 4166, 300, 4523, 484, 498, 291, 1029, 257, 1168, 456, 393,
  312, 661, 8512, 50996], "temperature": 0.0, "avg_logprob": -0.20478145374971277,
  "compression_ratio": 1.8803827751196172, "no_speech_prob": 0.002274239668622613},
  {"id": 504, "seek": 382408, "start": 3836.72, "end": 3841.68, "text": " that that
  answer this question and they can answer that question even better and so they solve
  this", "tokens": [50996, 300, 300, 1867, 341, 1168, 293, 436, 393, 1867, 300, 1168,
  754, 1101, 293, 370, 436, 5039, 341, 51244], "temperature": 0.0, "avg_logprob":
  -0.20478145374971277, "compression_ratio": 1.8803827751196172, "no_speech_prob":
  0.002274239668622613}, {"id": 505, "seek": 382408, "start": 3841.68, "end": 3849.68,
  "text": " problem using you know a relabeling approach so that basically the due
  to the will they generate", "tokens": [51244, 1154, 1228, 291, 458, 257, 1039, 455,
  11031, 3109, 370, 300, 1936, 264, 3462, 281, 264, 486, 436, 8460, 51644], "temperature":
  0.0, "avg_logprob": -0.20478145374971277, "compression_ratio": 1.8803827751196172,
  "no_speech_prob": 0.002274239668622613}, {"id": 506, "seek": 384968, "start": 3849.68,
  "end": 3856.16, "text": " synthetic query from some document they they do it you
  and they do", "tokens": [50364, 23420, 14581, 490, 512, 4166, 436, 436, 360, 309,
  291, 293, 436, 360, 50688], "temperature": 0.0, "avg_logprob": -0.1876110277677837,
  "compression_ratio": 1.6486486486486487, "no_speech_prob": 0.0014258669689297676},
  {"id": 507, "seek": 384968, "start": 3858.56, "end": 3865.44, "text": " then they
  look at the top say 10 documents and they they use another LLM to decide whether",
  "tokens": [50808, 550, 436, 574, 412, 264, 1192, 584, 1266, 8512, 293, 436, 436,
  764, 1071, 441, 43, 44, 281, 4536, 1968, 51152], "temperature": 0.0, "avg_logprob":
  -0.1876110277677837, "compression_ratio": 1.6486486486486487, "no_speech_prob":
  0.0014258669689297676}, {"id": 508, "seek": 384968, "start": 3866.72, "end": 3872.16,
  "text": " these documents are relevant to the query or not yeah it''s also very
  interesting paper", "tokens": [51216, 613, 8512, 366, 7340, 281, 264, 14581, 420,
  406, 1338, 309, 311, 611, 588, 1880, 3035, 51488], "temperature": 0.0, "avg_logprob":
  -0.1876110277677837, "compression_ratio": 1.6486486486486487, "no_speech_prob":
  0.0014258669689297676}, {"id": 509, "seek": 387216, "start": 3873.12, "end": 3881.44,
  "text": " as well I yeah and finally the last couple of papers that I encountered
  were regarding creation of", "tokens": [50412, 382, 731, 286, 1338, 293, 2721, 264,
  1036, 1916, 295, 10577, 300, 286, 20381, 645, 8595, 8016, 295, 50828], "temperature":
  0.0, "avg_logprob": -0.19037137031555176, "compression_ratio": 1.6136363636363635,
  "no_speech_prob": 0.0033114938996732235}, {"id": 510, "seek": 387216, "start": 3882.56,
  "end": 3890.64, "text": " either creation of documents either just joined here with
  queries or based on the queries this is also", "tokens": [50884, 2139, 8016, 295,
  8512, 2139, 445, 6869, 510, 365, 24109, 420, 2361, 322, 264, 24109, 341, 307, 611,
  51288], "temperature": 0.0, "avg_logprob": -0.19037137031555176, "compression_ratio":
  1.6136363636363635, "no_speech_prob": 0.0033114938996732235}, {"id": 511, "seek":
  387216, "start": 3891.3599999999997, "end": 3897.2799999999997, "text": " very interesting
  for long yeah that''s amazing thanks for sharing and I hope we can", "tokens": [51324,
  588, 1880, 337, 938, 1338, 300, 311, 2243, 3231, 337, 5414, 293, 286, 1454, 321,
  393, 51620], "temperature": 0.0, "avg_logprob": -0.19037137031555176, "compression_ratio":
  1.6136363636363635, "no_speech_prob": 0.0033114938996732235}, {"id": 512, "seek":
  389728, "start": 3897.28, "end": 3906.2400000000002, "text": " link all of these
  papers in the in the episode you know yeah absolutely because I think one of the",
  "tokens": [50364, 2113, 439, 295, 613, 10577, 294, 264, 294, 264, 3500, 291, 458,
  1338, 3122, 570, 286, 519, 472, 295, 264, 50812], "temperature": 0.0, "avg_logprob":
  -0.2306030591328939, "compression_ratio": 1.8026905829596414, "no_speech_prob":
  0.006908038165420294}, {"id": 513, "seek": 389728, "start": 3907.28, "end": 3912.88,
  "text": " goals of this podcast is to continue to be educational resource not just
  entertainment maybe some people", "tokens": [50864, 5493, 295, 341, 7367, 307, 281,
  2354, 281, 312, 10189, 7684, 406, 445, 12393, 1310, 512, 561, 51144], "temperature":
  0.0, "avg_logprob": -0.2306030591328939, "compression_ratio": 1.8026905829596414,
  "no_speech_prob": 0.006908038165420294}, {"id": 514, "seek": 389728, "start": 3914.0,
  "end": 3918.88, "text": " potentially viewed as an entertainment entertainment and
  then good sense of word you know when you", "tokens": [51200, 7263, 19174, 382,
  364, 12393, 12393, 293, 550, 665, 2020, 295, 1349, 291, 458, 562, 291, 51444], "temperature":
  0.0, "avg_logprob": -0.2306030591328939, "compression_ratio": 1.8026905829596414,
  "no_speech_prob": 0.006908038165420294}, {"id": 515, "seek": 389728, "start": 3919.44,
  "end": 3925.92, "text": " want to sort of a little bit like break away from your
  daily routine and then listen to some of the", "tokens": [51472, 528, 281, 1333,
  295, 257, 707, 857, 411, 1821, 1314, 490, 428, 5212, 9927, 293, 550, 2140, 281,
  512, 295, 264, 51796], "temperature": 0.0, "avg_logprob": -0.2306030591328939, "compression_ratio":
  1.8026905829596414, "no_speech_prob": 0.006908038165420294}, {"id": 516, "seek":
  392592, "start": 3926.0, "end": 3931.76, "text": " insights and and we heard a lot
  of insights today from you thanks a lot for sharing Leo and I wish", "tokens": [50368,
  14310, 293, 293, 321, 2198, 257, 688, 295, 14310, 965, 490, 291, 3231, 257, 688,
  337, 5414, 19344, 293, 286, 3172, 50656], "temperature": 0.0, "avg_logprob": -0.1200817087863354,
  "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.003683835733681917},
  {"id": 517, "seek": 392592, "start": 3931.76, "end": 3937.6800000000003, "text":
  " you all the all the best in in your in your projects and your current projects
  in your future projects", "tokens": [50656, 291, 439, 264, 439, 264, 1151, 294,
  294, 428, 294, 428, 4455, 293, 428, 2190, 4455, 294, 428, 2027, 4455, 50952], "temperature":
  0.0, "avg_logprob": -0.1200817087863354, "compression_ratio": 1.7808219178082192,
  "no_speech_prob": 0.003683835733681917}, {"id": 518, "seek": 392592, "start": 3939.6,
  "end": 3947.44, "text": " and yeah I mean I would be all equally excited to talk
  to you at some point as well because it does", "tokens": [51048, 293, 1338, 286,
  914, 286, 576, 312, 439, 12309, 2919, 281, 751, 281, 291, 412, 512, 935, 382, 731,
  570, 309, 775, 51440], "temperature": 0.0, "avg_logprob": -0.1200817087863354, "compression_ratio":
  1.7808219178082192, "no_speech_prob": 0.003683835733681917}, {"id": 519, "seek":
  392592, "start": 3947.44, "end": 3953.04, "text": " feel like you have a lot more
  to say than I''m able to contain in the in a single episode", "tokens": [51440,
  841, 411, 291, 362, 257, 688, 544, 281, 584, 813, 286, 478, 1075, 281, 5304, 294,
  264, 294, 257, 2167, 3500, 51720], "temperature": 0.0, "avg_logprob": -0.1200817087863354,
  "compression_ratio": 1.7808219178082192, "no_speech_prob": 0.003683835733681917},
  {"id": 520, "seek": 395304, "start": 3953.36, "end": 3961.84, "text": " yeah it''s
  my pleasure thanks a lot for inviting me I enjoyed the podcast I enjoyed our conversation
  very", "tokens": [50380, 1338, 309, 311, 452, 6834, 3231, 257, 688, 337, 18202,
  385, 286, 4626, 264, 7367, 286, 4626, 527, 3761, 588, 50804], "temperature": 0.0,
  "avg_logprob": -0.19293814897537231, "compression_ratio": 1.1954022988505748, "no_speech_prob":
  0.005714971572160721}, {"id": 521, "seek": 396184, "start": 3961.84, "end": 3972.4,
  "text": " thank you very much Leo and good luck bye bye", "tokens": [50404, 1309,
  291, 588, 709, 19344, 293, 665, 3668, 6543, 6543, 50892], "temperature": 0.0, "avg_logprob":
  -0.4470643630394569, "compression_ratio": 0.9, "no_speech_prob": 0.02594118006527424}]'
---

 Hi everyone, Vector Podcast is back with still with season three and I'm super excited to be talking to my guests today and there is a connection with this episode between this episode and the episode that we recorded with Yuri Malkov about one of the most famous and popular vector search algorithm I can ask WL and I'm talking today with Leo Boyzov, who is the senior research scientist at AWS and he is also a co-author of Animesleab and Animesleab is today used at Open Search and probably some other places that I actually don't know and I hope to learn it as well today.
This is just exciting and I think goes without saying that the whole field stands on the work done by people like Leo and Yuri and others who actually develop the core algorithms and popularize them, improve them over time and then the story unfolds from there.
Hi Leo, how you doing? Hi, thank you for introducing me, it's a great pleasure to be able to podcast. Yeah, it's my pleasure as well to have you. Traditionally we start with the background.
Can you say in a few words your background maybe how you got here and what's your story in search vector search and maybe LLM? Yeah, sure, yeah so background is pretty long. So I've had a rather long career, honestly. Well in my current capacity as you mentioned, I am a scientist at WSAA labs.
For one year I was working on co-generation about this year, earlier this year I moved to a Q-console team, Q-console team works on question and switch at bots that answers questions about various AWS services.
So we can ask like, I don't know, it's like where's my EC2 instance things like that and how I set up things. But I have to make a disclaimer that today I do not speak on behalf of AWS and I cannot talk in details about my work there. So as I said I had a really relatively long career.
Yeah, so most of nearly all of my life, I have been a computer science geek with a passion for building cool stuff and solving hard problems. Yet my professional career started in rather mundane fashion. So I started working client and service of where for financial systems.
This was not my favorite subject, but pretty much the only one that was paid reasonably well at the time. So I had to do a lot of front end and back end engineering using various SQL databases.
I was not satisfied with my career, but luckily I got really interested in algorithms, in particular retrieval algorithms. So I started working on this topic with the algorithms first part time, then full time. But largely as a software engineer, less as a researcher.
And as a software engineer, work for various companies, including two tiny startups in the Russian search engine and the Yandex. So later I moved to the United States and work on the search engine PubMed, International Center of Biotechnology, information.
First again, that was a common topic in my career, started working with I was doing a lot of front end development. But about the class, 40 years I worked primarily on the T-Roll, the core engine. In particular, I invented a pretty need to speed up weighted bull in the T-Roll.
And around the time I also realized that it would be hard to get to the search position without a good degree. So that motivated me to apply a bunch of universities and eventually I got accepted by Carnegie Mell, which was a huge lock. But yeah, so I did my PhD studies there.
And during these studies, I worked on a mix of machine learning and algorithm algorithms without any deep learning. So the vector search or rather similarity search was a part of my graduate studies. So yeah, I didn't use any deep learning though.
It was a mix of classical machine learning, VortoVex style neural networks and digital. So what is an interesting part of that story is that my advisor, Eric Nyberg, he worked on question answering.
And together with his theory and his participated in development of IBM Watson, that's an amazing trivia playing system that 2011 defeated human champions. So that was like one reason why I chose my advisor. It was like such a cool topic to choose.
But pretty quickly I learned about the system and realized, oh, like it's actually really just not just but it's largely such engine on steroids. So Retrieval, IBM Watson, I have a blog post about that if anybody is interested.
But then Retrieval, it's basically really Retrieval based extractive question answering systems. So if you want to improve question answering, you need to improve Retrieval. So that's how I got back to working on quality algorithms.
And again, I saw an opportunity and why big research question was, how can we do information Retrieval using more advanced techniques rather than lexical search with BIRN 25?
And because before birth, nowadays like everybody just uses like word-based models or any like other transform based models to create dense vector embeddings and they are quite effective, that was not the case when like 10 years ago.
So whatever we had there was pretty ineffective Retrieval. And so my thought was that because the single representation was not effective Retrieval, those need to be somehow combined and assembled. So you basically don't get a single representation.
You use a combination, you use combine similarity and then you treat this similarity as a black box and then you apply generic Retrieval algorithm. So this was a pretty in hindsight that was a pretty ambitious project that required working on both design and effective similarities.
And Retrieval algorithms. And that's why we, well one, that's where that animously library turned out to be very useful. It was instrumental to this work. Although it was created for somewhat unrelated people.
Okay, so that was an overall rather bumpy, right things didn't work well initially and I got a lot of help from other people in particular from my author, David Norva, who proposed an amazing improvement for one of the algorithms in a thermosleep.
Yeah, and so we published and opened after my graduation. And when I was writing my thesis, yeah, I was found like a bunch of issues with my previous approaches and realized that I could also use like a H&SW like algorithms which were not like core part of my thesis work.
And I got even stronger results, but that was like a little bit too late to publish and use otherwise. Moreover, that the similarity that I used was a sort of a face palm realization that that similarity that I used, like Retrieval, completely as a black box. And it worked with most effective.
It was more effective than B125 on the collections that I used. But I didn't realize that this black box similarity was actually representable by another product between two large sparse vectors by another former author Chris Dyer pointed this out.
And if I embraced this sparse vector approach from the get-go, it would have been a much easier problem to solve from both engineering and scientific points of view even without work. And okay, it could have produced some more impact. But yeah, a little bit too late to dwell this now.
Okay, and enough with that I graduated six years ago. And since then I haven't working as a researcher scientist and engineer on deep learning in what specifically I had in training models for specific initial computer vision and Retrieval.
Despite this diversity, things have come a full circle and are working question as being systems once again. Yeah, that was pretty much involved. Yeah, amazing story. Yeah, thank you for that.
It's like the story tends to repeat itself, but at the same time, if we find the topic still exciting and it seems like you are still very interested in question answering and improving building blocks of that, it's kind of cool, right?
So that we are able to come back to some of the topics, pick them up on a different level.
That's amazing. And yeah, there is a lot to unpack. I almost wanted to ask you or the moment you spoke about spars and dance. I wanted to pick your brain on what's it take on the model called split and split V2.
I don't know if you're familiar with that model, but basically, you know, there is always this discussion should we take lexical search, combine it with dance search and then do some kind of hybrid formal on top and then how do we even learn the parameters of that model, right?
Depending on the domain.
But then there is a drastic sort of approach. Let's not do that. Let's just take a complete model which can handle both and then you can also support what the dance search doesn't support like exact phrase searches.
What's your general intuition about that? How do you think about this? Well, that's a super interesting question. I have one clarifying question though. So, before I answer, you said that some people who want to have a single model that's doing both.
Could you elaborate a little bit on this? Well, I guess maybe it's not that they wanted, but it's like the development when, instead of sort of, you know, combining these disparate sources of results, you know, one coming from lexical search, which is kind of like well-known BM25 driven, I guess.
And then the other one is more like more modern in a way that everyone wants to get exposed to dance search. And then you need to somehow figure out how you combine the results, right? So one is designed maybe for precision lexical.
The other one is designed more for recall, right? Because the vectors are not, they don't have as many dimensions as these far specters.
But then you still need to figure out, okay, how do I combine this to? And usually people cite reciprocal rank fusion in what I hear, but there are other methods as well, like even clustering based. But then that's one approach. Another approach is just stop doing that, I guess.
If I really understand what split does, and then you encode with split your data once, and you retrieve, you know, you use its capabilities to also retrieve exact phrases, right? So, effectively, ideally, you don't need the lexical matching engine anymore, but maybe I'm completely wrong.
I'm just, I wanted to hear your opinion on that. Okay, well, let's get it. Using your words, it's a lot on back here. I'm still not quite sure what you mean by having like a single model.
Although, maybe I love me try to maybe start answering questions and you can drop me and guide me into the other direction if needed.
So first of all, we have what's interesting about Nashville language is that, and that's very different from computer vision domain, is that we usually represent, we can we have multiple ways to represent text.
So in computer vision, usually it's just like each image is a traditional representative of actors that was the commodity theme.
 But in the in a national language processing, we started with the so-called bag of words representations where a document was represented by basically a sparse vector where you will have either zeros and ones, which means the specific terms present or not, or maybe weights, not just zeros and ones, but weights.
But then, with development of deep learning, and I actually started a little bit earlier with people, people learned how to represent text using fixed size vectors. And that was like using principle component analysis.
And this is not a very natural representation for text and it didn't work really well initially. But now we're having good results. So we have like two representations and there are different approaches to combine those, of course.
One is just if you want to do the T-wall, you can indeed just do the lexical base search, you can do a kidney or a snabestation vector representations, and then you can somehow merge the results. You can use ranker.
But you don't have to, and that's the so-called hybrid search, but the hybrid search can exist in different versions.
So if you want to combine it sort of in a single model, why don't you represent each document using both sparse and dense vector? And when you're computing the similarity, you can compute the similarity between sparse parts, between dense parts, and then combine them somehow.
For example, using a weight. And that's in fact what I was trying to do in my thesis as well, because I was doing, my similarities was basically an ensemble of several similarities course for at least two representations. And that could work.
There's of course modern instantiations of this, and there's a paper, I think both are by some glue people, where they did exactly like this, they combined splaid and some dense vector embeddings.
And that can work apparently a little bit better than, or sometimes maybe a lot better than basic representations, like each representation specifically. So with both approaches, of course, there are issues that you mentioned.
So I don't know what the best approach there, and I don't have a crystal ball regarding what's the best path forward. But with dense representation, the clearly the problem is that you have to pack everything into the fixed size vector.
And as your document is getting bigger, you basically the vector size, the amount of information you can store is the same, but your document increases in size. So you would possibly expect some deterioration in quality.
But another reason why you can see deteriorating results just because some like you have fixed representations, the number of words is huge.
And like in regular person knows like around like educated person knows about 30,000 words, but in reality, like internet has millions of words, right? And the words are not just only words, there are things like product identifiers, right?
If you want to, and sometimes people will do products, they will search something they want to buy, and they would you know copy paste those, or type them in, and then they got squished in the in that dense vector.
So it cannot be precise.
 There is an interesting paper by author by Neil Srymer's, sentence board author, where he has a, in like some experimental and even theoretical evidence that as the collection size increases so the dense vector search can deteriorate just because there would be some false positives and measures due to you know the excruciating a lot of information together and they fix size directly.
So yeah, I mean, it's quite possible, but I haven't seen like a fall off of this work, so I don't know how much of a problem it isn't in practice. And coming back to the sparse representations, so yeah, they could potentially use all this issue, but not necessarily with displayed like models.
Well, the problem with display is that displayed models, they create those sparse representations using the, not the words themselves, they're using sub word talking.
So as a reminder with like models like with transform models, they create this sort of new sort of vocabulary that has some complete words, but most words are incomplete.
 So like they have like extract prefix, suffixes, parts of the words, and this is your new vocabulary and the difference between these new vocabulary and the actual vocabulary that people use or use on the internet is that it's limited to, it can have like 50,000 talking, maybe 200 talking and some of the advanced modeling models, but we really have like millions and millions of words.
So of course, that would also lead to some deterioration in quality false positives, and especially if you try to represent, represent long documents using this fixed size vector. So it's sort of sparse in more, it's more sparse in some ways, but it's still fixed size vector. Doesn't make sense.
Yeah, it does.
I mean, it's very insightful, what you said that like basically to make my question much more succinct, I could ask, you could we just use splaid for everything? And like instead of, you know, combining different approaches, just use splaid, but you basically answered it really eloquently.
You said that splaid itself has limitations, right? For example, that would not allow us to properly embed all variety of the language and then obviously dealing with longer documents is another issue.
There is an interesting extension to this, so I was just recently listening to a presentation on the extendable splaid where they extend the vocabulary of splaid by eddy entities.
That's one interesting direction of work, but another interesting direction is like the so-called like deep impact models where they take a document and they do document expansion using like, you know, the doctor query style models.
And then they for each talking, I think, in the document they are learning a weight. And so this is like a little bit more less limited, I think.
But in the end, I think it's whenever we, yeah, so basically if like to be able to handle those like rare, we need lexical representation to handle, you know, bigger vocabularies. And it's probably hard to model with just fixed size vectors. Yeah, it makes a lot of sense.
At the same time, we also know that, well, it depends on how you model this, but lexical approach, like vanilla lexical approach would miss semantic links, right, and sort of understanding of larger context, because all it does is that it kind of looks through the VM 25 model at the words.
And sometimes it just pays attention to some words, but doesn't pay attention to other words.
And it may miss the main point of the query, right?
 But of course, this model still worked for a new work that Yandex, you know, it best, this model's worked previously, probably by virtue of you training the users that, hey, don't give me the full sentence, just give me like, you know, specific words, like chopped list of words that I need to look up.
And that's how I guess inverted index worked out.
And of course, you need to have on top of that, you need to have very smart reranking strategy to pull up the documents that are really relevant, right?
But I guess today we have we have this new, well, I keep calling it new, but it's not maybe necessarily that new, but it's still fairly fresh development of dense, dense retrieval that not many companies, I think, have been boarded in the products yet.
But it's a very interesting direction, and still you need to combine the two worlds, right? So it sounds like from what you said, the only way to get better quality is to combine this approaches rather than try to develop one single holistic model to handle everything.
Oh, I, yeah, it's a great question. I actually don't know what's the best part forward is. So I highlighted the, the deficiencies and advantages of different approaches.
But I also want to comment on the deep impact model, the deep impact model, I think the way, maybe I described it, it was, it sounded like it is like a BM25 model, but it's actually not.
So maybe we should have, like we're talking about sparse representations, like learned sparse representations, because it's a bigger topic and it's much bigger topic than most people realize sometimes.
So people know BM25, people know dense vectors, and these are, these simple things, but there is a lot in between.
So first of all, what you can do, and that's what people did, and even the doctor query is the most famous way to do so, but it was actually not even a single group of people who proposed this.
So what can you do?
We can take a model, a deep learning model, contextualized model, maybe not necessarily contextualized, but contextualized models, they do better job because they look at the model as a whole, the document as a whole, they don't like look like a devidue chunks of document, right?
So they kind of can understand what the total meaning of the document.
And then they, they propose new keywords on new terms. So some like synonyms, synonyms that could have been in this document, but they are not. And if you add these documents to the new, if you add, sorry pardon me, if you add these terms to the document, then this missing synonyms are there.
You can index this document. So basically this is document expansion. And you can do document expansion. And that helps resolve that lexical mismatch, mitigate lexical mismatch between query and documents. And I claim it's easier to do this expansion.
That there are like of course approaches that do query expansion, basically adding synonyms at the query stage. But why claim is that it's much harder to do this accurately because there is much less context.
So this is one, you know, this is one direction of fixing things and creating sparse representations. Like there is a split model. What does this play? What does this play model? It's completely sort of it's doing something completely different. It looks at the document.
And there is a vocabulary, like bird tokens. And for each token, it gives you a weight. It looks at the document sort of understand this meaning that says, all like this is like, this is a word, a prefix or word. It should have this weight. And that's how you get a sparse representation.
But with deep impact, you're doing something slightly different. So you take a document and you do this document expansion. So you add words like synonyms. But then you don't index this document using build 25. Why? Because build 25 is clearly old style and it doesn't take like context to account.
So instead of that, you train a transform model that would give you weight for each term in the document, in the expanded document. And then you use this for it. Oh, that's very easy. And that's called deep and that's called deep impact models. Yes. Yeah.
We should link that I guess there is a paper for that as well and should be able to link that. Yeah. That's very interesting. And it's also interesting that what you mentioned about the dance model sort of not able to capture everything that you want them to capture.
And yet, this becomes a building block in the application phase, like for example, in Rage or a Givalogmented Generation because effectively, the only method that I heard so far off, which is circulating a lot is just chunk it up.
You chunk all documents up and then you hope that the chunk size is less or about the same as capacity of the model, right? Because otherwise, it will chop off the end and you will lose the part of the meaning.
Or you also apply some methods like some level of overlap, right? So you can then index a few more chunks in the same entity and then try to query. And then interestingly, you can generate questions out of chunks that these chunks might be able to answer.
And then you search those questions instead of the chunks themselves, right? So which comes back to what you said about Dr. Query, I guess. So it's very interesting that like we are sort of like standing on a set of building blocks that themselves should be optimized and optimized and optimized.
But I guess we already in the phase globally when everyone is trying to derive value from LLMs and Rags and everything, right? And yet, we can stumble upon some really tricky situations. Like you explained. Oh, it looks like we have a lot. Yeah, it looks like we have still a lot of research topics.
Yeah. A lot of answering questions. Yeah. I wanted to a little bit digress from here to the work you've done at NMS Lip and I want to read it from your from the GitHub repository. It's a non-metrics space library.
And I did spend some time in my rework life, you know, when I was studying mathematics and we did study a bunch of, you know, metric spaces. I have never realized I would never really like imagine that this highly theoretical stuff would now connect so deeply to practice and it's amazing.
But can you tell me why it's non-metrics space library? Isn't it so that the whole idea of, you know, vector searches that we choose some metric, Cousin or dot product or whatever it is. And we are, and that's how we express the semantics similarity. Great question.
So the reason why it is we decided to not limit ourselves for to metric search because we felt and that's also a feeling of other people is that metric search is is limiting. So it's not expressive enough. It turned out to be true to some degree but not as much as we hoped.
And indeed, in many cases, so and why we're doing so, the representation learning was not as developed as it is now. So we felt like, you know, we need to be able to, people will engineer those complex similarities and we need to support individual using this complex similarity.
This did not happen.
But what I think happened and that's I want to connect this to my statement that in the end of my graduate studies or rather after defending a thesis, somebody pointed out that the similarities that we were using were basically representable as the sparse similar product between two huge vectors.
So it's some sort of it becomes similar to either deep impact or split. And in fact, so the similarity is the maximum product. It's not the cosine similarity. And the the search like the search procedure is called maximum inner product search.
So basically, you want to retrieve documents that have the maximum inner product between query and the document.
 And the and this is not this is a symmetric similarity measure in some sense symmetric, but it is not it is it is not a metric and it's not easily reducible to the to the cosine similarity and to the creature searching using a science similarity is actually fully equivalent to searching using the Euclidean distance for the inner product you can reduce this search to a cosine similarity and Euclidean distance, but turns out that this reduction affects efficiency.
And then somewhat like bigger topic for a discussion, but what happened is that people say at at Lucene, who are maintaining Lucene, they were adding support for the maximum inner product.
And Vespa did this and they did this through this trick to reducing of of maximum inner product to cosine similarity and of two. And I argued that there is research showing that this is suboptimal and there was a discussion in this as a result they basically didn't do this.
So so a long story short, I think a lot of things are so non-metrics similarity search as in general turn out to be not so useful, but there are some instances like maximum inner product search where we do have things that are non-metric entities widely used.
Yeah, that's amazing, but I think I hope that equally as I'm learning, I hope our listeners are also learning on this because often times when you plunge into a new field, let's say, then search, all you see is what is being popularized and you know you may go down the rabbit hole.
So I'm really excited and thankful that you are able to share and much wider perspective over things. And then also most interestingly, you work and you say you're a co-author of an MSLIP besides other authors.
Your collective work is also now used at like for open search, engine, which I believe I also had a chance to test at some point and like it's a C++ library that is then somehow loaded of GVM and basically then searches is performed using H&SW.
Can you tell me a bit about that like that story of how did you end up you know connecting these to an MSLIP and H&SW and here I will probably link to the episode theory that it's quite popular today.
 Yeah, well first of all I have to say that I mean it was popularity like close to 100% of an MSLIP is certainly due to the development of H&SW which was Eurisk creation not mine and we affected it in only very minor ways because I think the I mean we provided the platform and yeah so I think one trick that you reboard which I borrowed from KGRAF was the efficient algorithm from him or she checking but that was it.
So the end of the sleep but end of the sleep it was like creation of several people and it was like has like a rather wild story so it was never planned in the sort of random how we developed.
 So in 2012 I attended the conference where I met Billik Nidhan who was working on and he was doing his PhD on similarity research and we found that like a lot of like we shared some interesting particularly in the written algorithms and we decided to do some joint project together and then my initial interest was how do I support not it was somewhat academic topic no metric search is as I explained before it's still like largely more like academic interest because a lot of things are really metric or at most maximum winter product search which is sort of almost a scientific narrative almost metric.
And yeah so that was basically purely for academic interest and I connected it to the to the machine learning because I saw an opportunity to use machine learning to support generical algorithms that would do a k-nearest new research with non-metrics simulator such as scale divergence.
 Yeah so we did it as a machine learning course project and we published paper at Noribs and it could have stopped at this point but then I also like that conference I got like I met other URIs that conference or just treated that I met with some of with Yuri Adir Quothar, Alexander and they worked both work at Merrill Habs company where they developed small world graph approach and that was like a general version and so Alexander was really interested to prove that whatever algorithms that we have in Emma Sleep and we were tackling generic search in generic spaces for generic similarities and he was eager to prove that the graph based algorithm I actually truly generic and this is why he and his student that you know created the first version of a small world graph in Emma Sleep so basically contributed this version and that was a really super slow I spread it up by both 10x and that was the version that we used to win the first in Benchmarks so it was pretty good but it has issues and one issue that was fixed thanks to Yuri sharing with me some early version with H&SW and I looked at the code it was not as like fast version that he created later but already there was fixing something and maybe he didn't realize he showed me that piece of code and I realized oh like there is actually still an issue in SW graph so SW both SW graph was improved and Yuri like contribution is W2 and Emma Sleep so it greatly it was like a huge contribution like big step forward it won the second NNBG Mark competition they proved SW graph was I think the second the second algorithm I have a screenshot of this somewhere which I sometimes included to my job talks and the H&SW also influenced face because they realized oh like they actually knew about K graph and knew about the graph based retrieval but there was one important reason why they didn't you can ask me why but anyway so it influenced face and a lot of other people and of course yeah that that Yuri created that was Yuri's work like a huge impact in the rest of his history but I think Yuri shouldn't complain no he has a great career first he has great career first Twitter and now it's at OpenAI so yeah it's a magic story just a close of the wolf white it face did not implement the approach you had this is this is a really interesting thing because that's one of my favorite pieces in this story well turns out that the the graph based retrieval algorithms they had long history so a lot of this was rediscovered on the the pruning heuristics and like the basic algorithm they go back to papers in 80s and 90s but people did not use it and one hurdle was the inability to efficiently create those K K nearest neighbor graphs and K nearest neighbor graph it's a simple concept you have data point and you have you need to find some data points that are nearest neighbor of these data points and then you connect to them module or some post modification of this graph but how like you know if you have end points it is in the brute force approach and squared computation how can you do this efficiently how can you approximate this so the way how it was done before people were coming up with fancy algorithms how to how to approximate a disappointment and those fancy algorithms would not particularly scalable and K graph I think is not particularly scalable we played with it we actually incorporated K graph in primitation into enemy sleep and it was indeed hard to create large graphs because it's a nitty-fragurithium and yeah it's not it's not very scalable but what what Yuri and Kothar did for as small world graph and it was before she was done we were while they were at Merrillab's company they realized that they can combine the interval and creation of the graph and they can do it efficiently like in what like using modern terminology embarrassingly parallel fashion and that was I think one key missing block that prevented graph based algorithms to become practical yeah that makes sense yeah it doesn't like what what excites me in this story that you shared is that how serendipitous the this like the discovery process is right and like something that feels like random leads to I don't know creation of the industry right you could you could largely say that the new industry of you know vector databases and vector search and now rag on top of that is created because you guys worked on practical implementations of something that was also that that stood on no shoulders of you know some of the inventions and research done before that and so it's kind of like natural progression but I mean it's amazing how you know it's just on the verge of you not meeting someone at a conference could basically need to possibly not creating an industry right quite possibly I think well thank you for the kind words and of course it's not because of us the if not for us I think other people who have done this I yeah so I with them but anyways so I think we did useful work and clearly people are using a tremendous double you lot and people using it mostly even even though it hasn't like a lot of issues but it was still ended up being used rather widely and the one reason why it was used so widely because people who needed a library the Python basically a library that would do Kenyar's new research and it would do it from Python and people often take these little things for granted but say initially I honestly didn't have Python bindings and to participate and then benchmarks and have something useful you would need to have Python bindings this Python bindings were written by Billek I didn't I didn't create those bindings so he created those bindings the first version so there you go that library was possible to use and at the moment there were at the moment the it was not such a big choice of libraries to do Kenyar's search so in terms of the competition there was anoy which was slower noticeably slower there was another library flan which used similar algorithms to anoy but it was less optimized and it was also slower through three wall there was a keg graph but it was not like so easily usable and yeah basically that was it and later came face but it came it was only released I think a year a couple of years later after well definitely after yeah it took several years for face to appear and people started using it so at some point there was vacuum and I honestly filled it now like other approaches that are taking over yeah so yeah it in summary there wasn't a lot of serendipity but I wouldn't take credit for it in the industry it would have created without us for sure yeah yeah maybe or maybe not and it's also well I think it's quite a photo like a better work typical of the end turn not to recognize the impact they're making because the moment they do recognize that that's probably end of story so like you need to be constantly sort of low ego and pointing at the goal and I think this is what you're doing and that it feels like this is your approach but you also do do quite a bit of impact I could ask a ton of questions obviously and I could relate also to the fact that what you explained about some of the struggles like how to optimize these algorithms because at some point I did embark on participating in billion scale and thench marks and I I think I failed miserably but at the same time I did have some code which worked on a small scale and one of the building works there was hnsw with very simple I would say with very very simple intuition that you just make several I think several passes through the data set and you try to bind points in space that are closer to each other and then you would push them to some common bucket I called them shards and then you would build a nation s double index for those shards the only thing that I couldn't figure out is for those shards I still needed to have an entry point to quickly sort of identify which shards I should go down you know through when I when I when I find similar similar documents for the query and I did attempt to modify hnsw code in the enemy sleep you know to to like get me only first layer of the graph so that I can pretend that that's my layer for entering the shards I just ran out of time but I see it was very exciting and also thanks to the organizers we had access to really beefy machines which I think I had I haven't been giving like good use I was mostly burning the you know CPU capacity and memory but I think it's an exciting field and what what I hope is that like with the vacuum that you mentioned that it doesn't happen that this torch will be carried forward and then someone will get excited about and not afraid of trying new things in this space are you yourself still like looking at obviously you're looking after enemy sleep but is there something that particularly excites you in this field that you would be working on or you are working on?
 Yeah great question so first of all yeah I am not sure if if I would do any work on vector search in the I haven't actually not maintaining an enemy sleep pretty well recently I'm just didn't have a lot of time and there was an issue with building the so I will still fix and support later version of Python for sure I was like you know like piece my piecemeal work I found like say half a day to fix like Windows build something else popped up yeah so it is an exciting field while it's also become really busy and another thing is that I still see the focus main focus it's not it's not very appreciated so the like you said you said I mean there's a really nice voice that all like that helped industry to be created and maybe it's true to some degree it is what yeah is it appreciated by you know your potential employer no it's like zero appreciation so it it isn't it's it's it's it wasn't still is somewhat niche topic and most people are of course I interested in how do you solve intelligence in that you know broad sense of the word how do you create models that can be seen and if the model like how you can combine them that this is like you know this new agentic ecosystem and yeah so all that stuff that really excites people it it isn't this you know plane of or space of large language models machine learning deep learning intelligence you name it yeah so that's why I do have ideas I did tested some of them but you know things usually don't work but yeah I don't have time to think systematically about this issues yeah but I guess at the same time you did create the base for for you know for other people to innovate and I think it's I think it's highly appreciated really I also wanted I also wanted to pick up the topic that originally sort of interested when you interested me when when you popped up on my LinkedIn you know feed you you made a statement about relational databases trying to implement search feature or sort of capability and sort of like miserably failing and that maybe you didn't use the word miserably it's it's my word here but I wanted to do a little bit like expand on this like why do you think they tried to do that and also while they were trying that what went wrong yeah great question well first of all I definitely wouldn't say the word miserably because the it it has been success to some degree definitely and it's not it's not over until it's over so the people are working with this so what I have been observing for many many years and I as I said I did start my career as as a person working on databases and doing a lot of writing a lot of SQL so but the the typical database is a very different beast from what you typically need to do information to you so the first of all they all like the early databases or they are oriented they achieve some tradeoff between the so they need have got throughput in both inserts and updates and they need to be able to update information pretty quickly and also it should be pretty reasonable and they also support really like the the data the data can be pretty complicated that what they call that SQL schema there can be multiple tables and all of that needs to be supported and so of course there are tradeoffs to be made to make it possible and again to support generality support efficient updates support efficient inserts but at the same time if you're doing the TV system a lot of of this is not necessary so say for you want to do like keyword-based retrieval you only need to all at high level is somewhat a simplification but you need to memorize them in which documents you have which keywords and then you have this so called inverted index where for each keyword you have a list of documents where these keywords appear and it's much simple structure it permits much more efficient compression algorithms so it's again it's it's a different beast and and also in terms of efficiency of updates once you compress data and once you represent it in a special way it's it becomes much harder to to make these incremental updates for which those early databases were applied so clearly there is a disconnect it was somewhat removed with the introduction of so-called columnar databases but it's still like with columnar databases I believe they actually do not favor those you know point updates anymore they they are best to be used for bulk updates and so basically once you're doing bulk updates yeah you're sort of in this search engine area where you ask you you change things in in rather large increments changing the access in rather large increments and you don't you don't worry too much about your information is being like really up to date you can wait maybe a day maybe a few hours but it doesn't have like an instantaneous update of the database so this is a different trade-offs so yeah um well of course there is a disconnect and this is why it's it was always hard I believe to add to add like food tax indexes to regular databases but another issue with the the disconnect is that like again the retrieval often needs like really different set of specialized features so if you have a relational database system it's pretty hard to support this for example like deconization if you need to do deconization in multiple languages yeah so of course that's part like you know the creation of those specialized tools with a lot of features like you've seen and VESPA and databases are catching up but there is still a gap and you know it's probably like going to be really tedious to to support yeah full set of features like you know you need to match VESPA so yeah these are like my five cents on this stuff yeah but I'm curious to sort of a little bit the understand why do you think databases are still trying why are they trying to encompass this seemingly disparate ways of searching right when you actually if basically like you explained if you need to have a fully blown search engine that can support multiple languages tokenization and so on you better be using the likes of recene VESPA and you know maybe elastic search on top of the scene and so on why why are they still trying they want customers it's of course advantageous to be like you know one stop shop so they come to specific provider and they have everything so I listen to a podcast the roxette co-founder which was the roxette the one was acquired by OpenAI but I think you recorded that podcast before they were acquired so good timing and you can clearly hear that message all like we really want people to come and use our solution so we have hybrid search we have some support for ranking we have this and we have that yeah I can't I can't argue against this being convenient so definitely something something very useful customers yeah yeah just a small correction he's not a co-founder I think he's well VP of engineering or used to be a VP of engineering in roxette but yeah I mean he's he brings the story and I encourage listeners to listen to the episode he brings the story of you know roxDB scalability issues from from Facebook and how it underpins you know the the further journey at roxette so I feel like we could discuss for five hours and I'm actually a big fan of Lex Friedman podcasts where some of the episodes really really long and you can listen to them for weeks and and I think I really hope that we can record with you sometime later as well as you know as you have topics to share but is there something Leo that you want to share I don't know it could be a paper you've read that particularly excites you maybe a book or anything else that you want to say yeah I think we yeah great question so so I was interested a lot recently very recently I mean maybe the last couple of years in how LLAMS can be useful for search in one particular interesting direction is how do you use LLAMS to to train smaller models for retrieval and ranking for me personally it's a very exciting area of research yeah as far as distillation is concerned there was several interesting papers on the topic there was but basically the lot of of that work revolves around creation synthetic data synthetic queries based on the documents like we have a document that creates the queries and queries that you asked that that pretty slash question and the answer is in documents we have a positive relevant document and you can sample negatives from from your collection and train them while but there is also a line of research where they they would try to create both queries and documents so yeah in summary the that whole that whole not in summary but that that that line of research was particularly interesting to me although there was some work before LLAMS to create synthetic queries it was not particularly well-used technique but one paper that stood out was the in-part paper from a couple of years ago and we have reproduction of this paper in that paper had a a pretty quick fall-up from the there were several several authors from Google they called it Protigator where they showed how this technique can be improved and there was another fall-up from the with the same first author now she transitioned to the mind and now they they showed like like oh like now we do it like somewhat better but the they they found one issue with the synthetic query generation approach that not always the the document that was used to create the queries the most relevant document so you would think it sort of makes sense that if the question is being answered by this document it is the most relevant document that turns out if you ask a question there can be other documents that that answer this question and they can answer that question even better and so they solve this problem using you know a relabeling approach so that basically the due to the will they generate synthetic query from some document they they do it you and they do then they look at the top say 10 documents and they they use another LLM to decide whether these documents are relevant to the query or not yeah it's also very interesting paper as well I yeah and finally the last couple of papers that I encountered were regarding creation of either creation of documents either just joined here with queries or based on the queries this is also very interesting for long yeah that's amazing thanks for sharing and I hope we can link all of these papers in the in the episode you know yeah absolutely because I think one of the goals of this podcast is to continue to be educational resource not just entertainment maybe some people potentially viewed as an entertainment entertainment and then good sense of word you know when you want to sort of a little bit like break away from your daily routine and then listen to some of the insights and and we heard a lot of insights today from you thanks a lot for sharing Leo and I wish you all the all the best in in your in your projects and your current projects in your future projects and yeah I mean I would be all equally excited to talk to you at some point as well because it does feel like you have a lot more to say than I'm able to contain in the in a single episode yeah it's my pleasure thanks a lot for inviting me I enjoyed the podcast I enjoyed our conversation very thank you very much Leo and good luck bye bye