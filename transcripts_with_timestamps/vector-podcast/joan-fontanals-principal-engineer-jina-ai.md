---
description: '<p>Topics:</p><p>00:00 Intro</p><p>00:42 Joan''s background</p><p>01:46
  What attracted Joan''s attention in Jina as a company and product?</p><p>04:39 Main
  area of focus for Joan in the product</p><p>05:46 How Open Source model works for
  Jina?</p><p>08:38 Deeper dive into Jina.AI as a product and technology stack</p><p>11:57
  Does Jina fit the use cases of smaller / mid-size players with smaller amount of
  data?</p><p>13:45 KNN/ANN algorithms available in Jina</p><p>16:05 BigANN competition
  and BuddyPQ, increasing 12% in recall over FAISS</p><p>17:07 Does Jina support customers
  in model training? Finetuner</p><p>20:46 How does Jina framework compare to Vector
  Databases?</p><p>26:46 Jina''s investment in user-friendly APIs</p><p>31:04 Applications
  of Jina beyond search engines, like question answering systems</p><p>33:20 How to
  bring bits of neural search into traditional keyword retrieval? Connection to model
  interpretability</p><p>41:14 Does Jina allow going multimodal, including images
  / audio etc?</p><p>46:03 The magical question of Why</p><p>55:20 Product announcement
  from Joan</p><p>Order your Jina swag <a href="https://docs.google.com/forms/d/e/1FAIpQLSedYVfq"
  rel="noopener noreferrer nofollow">https://docs.google.com/forms/d/e/1FAIpQLSedYVfq</a><em>iwvdzWPX-blCpVu-tQoiFiUJQz2QnI</em>HU1ggy1oyg/
  Use this promo code: vectorPodcastxJinaAI</p><p>Show notes:</p><p>- Jina.AI: <a
  href="https://jina.ai/" rel="noopener noreferrer nofollow">https://jina.ai/</a></p><p>-
  HNSW + PostgreSQL Indexer: [GitHub - jina-ai/executor-hnsw-postgres: A production-ready,
  scalable Indexer for the Jina neural search framework, based on HNSW and PSQL](<a
  href="https://github.com/jina-ai/executor-h...)" rel="noopener noreferrer nofollow">https://github.com/jina-ai/executor-h...)</a></p><p>-
  pqlite: [GitHub - jina-ai/pqlite: A fast embedded library for Approximate Nearest
  Neighbor Search integrated with the Jina ecosystem](<a href="https://github.com/jina-ai/pqlite)"
  rel="noopener noreferrer nofollow">https://github.com/jina-ai/pqlite)</a></p><p>-
  BuddyPQ: [Billion-Scale Vector Search: Team Sisu and BuddyPQ | by Dmitry Kan | Big-ANN-Benchmarks
  | Nov, 2021 | Medium](<a href="https://medium.com/big-ann-benchmarks...)" rel="noopener
  noreferrer nofollow">https://medium.com/big-ann-benchmarks...)</a></p><p>- PaddlePaddle:
  [GitHub - PaddlePaddle/Paddle: PArallel Distributed Deep LEarning: Machine Learning
  Framework from Industrial Practice （『飞桨』核心框架，深度学习&amp;机器学习高性能单机、分布式训练和跨平台部署）](<a
  href="https://github.com/PaddlePaddle/Paddle)" rel="noopener noreferrer nofollow">https://github.com/PaddlePaddle/Paddle)</a></p><p>-
  Jina Finetuner: [Finetuner 0.3.1 documentation](<a href="https://finetuner.jina.ai/)"
  rel="noopener noreferrer nofollow">https://finetuner.jina.ai/)</a></p><p>- [Not
  All Vector Databases Are Made Equal | by Dmitry Kan | Towards Data Science](<a href="https://towardsdatascience.com/milvus...)"
  rel="noopener noreferrer nofollow">https://towardsdatascience.com/milvus...)</a></p><p>-
  Fluent interface (method chaining): [Fluent interfaces in Python | Florian Einfalt
  – Developer](<a href="https://florianeinfalt.de/posts/fluen...)" rel="noopener noreferrer
  nofollow">https://florianeinfalt.de/posts/fluen...)</a></p><p>- Sujit Pal’s blog:
  [Salmon Run](<a href="http://sujitpal.blogspot.com/)" rel="noopener noreferrer nofollow">http://sujitpal.blogspot.com/)</a></p><p>-
  ByT5: Towards a token-free future with pre-trained byte-to-byte models <a href="https://arxiv.org/abs/2105.13626"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2105.13626</a></p><p>Special
  thanks to Saurabh Rai for the Podcast Thumbnail: <a href="https://twitter.com/srbhr_"
  rel="noopener noreferrer nofollow">https://twitter.com/srbhr_</a> <a href="https://www.linkedin.com/in/srbh077/"
  rel="noopener noreferrer nofollow">https://www.linkedin.com/in/srbh077/</a></p>'
image_url: https://media.rss.com/vector-podcast/20220119_090157_f67877f44bb32ae14fd380d9328691ec.jpg
pub_date: Wed, 19 Jan 2022 21:02:57 GMT
title: Joan Fontanals - Principal Engineer - Jina AI
url: https://rss.com/podcasts/vector-podcast/366298
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 22.12, "text": " Hey
  everyone, Bector Podcast is here and today we are continuing our quest to study
  more", "tokens": [50364, 1911, 1518, 11, 363, 20814, 29972, 307, 510, 293, 965,
  321, 366, 9289, 527, 866, 281, 2979, 544, 51470], "temperature": 0.0, "avg_logprob":
  -0.3706725293939764, "compression_ratio": 1.0602409638554218, "no_speech_prob":
  0.11737712472677231}, {"id": 1, "seek": 2212, "start": 22.12, "end": 31.64, "text":
  " about Bector Technologies and Beding Technologies platforms and today I have a
  guest from Gina AI,", "tokens": [50364, 466, 363, 20814, 46993, 293, 363, 9794,
  46993, 9473, 293, 965, 286, 362, 257, 8341, 490, 34711, 7318, 11, 50840], "temperature":
  0.0, "avg_logprob": -0.4015885848033277, "compression_ratio": 1.592783505154639,
  "no_speech_prob": 0.3786201775074005}, {"id": 2, "seek": 2212, "start": 31.64, "end":
  36.68, "text": " his name is John Fontanelles and he is a principal engineer at
  Gina AI. Hey John.", "tokens": [50840, 702, 1315, 307, 2619, 43901, 282, 19787,
  293, 415, 307, 257, 9716, 11403, 412, 34711, 7318, 13, 1911, 2619, 13, 51092], "temperature":
  0.0, "avg_logprob": -0.4015885848033277, "compression_ratio": 1.592783505154639,
  "no_speech_prob": 0.3786201775074005}, {"id": 3, "seek": 2212, "start": 36.68, "end":
  40.28, "text": " Hello, nice to meet you.", "tokens": [51092, 2425, 11, 1481, 281,
  1677, 291, 13, 51272], "temperature": 0.0, "avg_logprob": -0.4015885848033277, "compression_ratio":
  1.592783505154639, "no_speech_prob": 0.3786201775074005}, {"id": 4, "seek": 2212,
  "start": 40.28, "end": 45.88, "text": " Yeah, nice to meet you as well. Thanks for
  joining me today and I''m really really excited to talk about", "tokens": [51272,
  865, 11, 1481, 281, 1677, 291, 382, 731, 13, 2561, 337, 5549, 385, 965, 293, 286,
  478, 534, 534, 2919, 281, 751, 466, 51552], "temperature": 0.0, "avg_logprob": -0.4015885848033277,
  "compression_ratio": 1.592783505154639, "no_speech_prob": 0.3786201775074005}, {"id":
  5, "seek": 4588, "start": 45.88, "end": 52.760000000000005, "text": " what is Gina
  AI? I know something I used to use some kind of predecessors of Gina AI in some
  sense,", "tokens": [50364, 437, 307, 34711, 7318, 30, 286, 458, 746, 286, 1143,
  281, 764, 512, 733, 295, 24874, 45700, 295, 34711, 7318, 294, 512, 2020, 11, 50708],
  "temperature": 0.0, "avg_logprob": -0.32326271713420907, "compression_ratio": 1.6652360515021458,
  "no_speech_prob": 0.03313431888818741}, {"id": 6, "seek": 4588, "start": 52.760000000000005,
  "end": 59.72, "text": " but not like Gina AI itself. But first of all I would like
  you to introduce yourself,", "tokens": [50708, 457, 406, 411, 34711, 7318, 2564,
  13, 583, 700, 295, 439, 286, 576, 411, 291, 281, 5366, 1803, 11, 51056], "temperature":
  0.0, "avg_logprob": -0.32326271713420907, "compression_ratio": 1.6652360515021458,
  "no_speech_prob": 0.03313431888818741}, {"id": 7, "seek": 4588, "start": 59.72,
  "end": 68.36, "text": " your background to our listeners and to me. So, well I studied
  engineering degree in Barcelona,", "tokens": [51056, 428, 3678, 281, 527, 23274,
  293, 281, 385, 13, 407, 11, 731, 286, 9454, 7043, 4314, 294, 21247, 11, 51488],
  "temperature": 0.0, "avg_logprob": -0.32326271713420907, "compression_ratio": 1.6652360515021458,
  "no_speech_prob": 0.03313431888818741}, {"id": 8, "seek": 4588, "start": 68.36,
  "end": 74.36, "text": " not computer science, from general engineering with my AmiExelectical
  Engineering, Mechanical Engineering,", "tokens": [51488, 406, 3820, 3497, 11, 490,
  2674, 7043, 365, 452, 2012, 72, 11149, 68, 1809, 804, 16215, 11, 30175, 804, 16215,
  11, 51788], "temperature": 0.0, "avg_logprob": -0.32326271713420907, "compression_ratio":
  1.6652360515021458, "no_speech_prob": 0.03313431888818741}, {"id": 9, "seek": 7436,
  "start": 74.36, "end": 80.68, "text": " but I got into software engineering because
  I was related with robotics and then when I started my", "tokens": [50364, 457,
  286, 658, 666, 4722, 7043, 570, 286, 390, 4077, 365, 34145, 293, 550, 562, 286,
  1409, 452, 50680], "temperature": 0.0, "avg_logprob": -0.22764238104762802, "compression_ratio":
  1.7972972972972974, "no_speech_prob": 0.0032917847856879234}, {"id": 10, "seek":
  7436, "start": 80.68, "end": 85.72, "text": " professional career I did software
  engineering at different companies and industries and then I got", "tokens": [50680,
  4843, 3988, 286, 630, 4722, 7043, 412, 819, 3431, 293, 13284, 293, 550, 286, 658,
  50932], "temperature": 0.0, "avg_logprob": -0.22764238104762802, "compression_ratio":
  1.7972972972972974, "no_speech_prob": 0.0032917847856879234}, {"id": 11, "seek":
  7436, "start": 85.72, "end": 94.2, "text": " more into that engineering machine
  learning and these kinds of fields and then I also did some work", "tokens": [50932,
  544, 666, 300, 7043, 3479, 2539, 293, 613, 3685, 295, 7909, 293, 550, 286, 611,
  630, 512, 589, 51356], "temperature": 0.0, "avg_logprob": -0.22764238104762802,
  "compression_ratio": 1.7972972972972974, "no_speech_prob": 0.0032917847856879234},
  {"id": 12, "seek": 7436, "start": 94.2, "end": 103.24, "text": " on traditional
  search, on web search, engine so on and then just live brought me to Gina which
  was a", "tokens": [51356, 322, 5164, 3164, 11, 322, 3670, 3164, 11, 2848, 370, 322,
  293, 550, 445, 1621, 3038, 385, 281, 34711, 597, 390, 257, 51808], "temperature":
  0.0, "avg_logprob": -0.22764238104762802, "compression_ratio": 1.7972972972972974,
  "no_speech_prob": 0.0032917847856879234}, {"id": 13, "seek": 10324, "start": 103.96,
  "end": 110.03999999999999, "text": " good step in my career. Oh yeah, cool. So,
  what caught your eye in Gina AI as a company and maybe", "tokens": [50400, 665,
  1823, 294, 452, 3988, 13, 876, 1338, 11, 1627, 13, 407, 11, 437, 5415, 428, 3313,
  294, 34711, 7318, 382, 257, 2237, 293, 1310, 50704], "temperature": 0.0, "avg_logprob":
  -0.16933830579121908, "compression_ratio": 1.6382978723404256, "no_speech_prob":
  0.008950803428888321}, {"id": 14, "seek": 10324, "start": 110.03999999999999, "end":
  117.47999999999999, "text": " as a technology as a product or maybe the team? So,
  for me what caught me the eye, it was like the", "tokens": [50704, 382, 257, 2899,
  382, 257, 1674, 420, 1310, 264, 1469, 30, 407, 11, 337, 385, 437, 5415, 385, 264,
  3313, 11, 309, 390, 411, 264, 51076], "temperature": 0.0, "avg_logprob": -0.16933830579121908,
  "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.008950803428888321},
  {"id": 15, "seek": 10324, "start": 117.47999999999999, "end": 124.84, "text": "
  technology and the vision of I see that vector search embedding a semantic search
  in general can", "tokens": [51076, 2899, 293, 264, 5201, 295, 286, 536, 300, 8062,
  3164, 12240, 3584, 257, 47982, 3164, 294, 2674, 393, 51444], "temperature": 0.0,
  "avg_logprob": -0.16933830579121908, "compression_ratio": 1.6382978723404256, "no_speech_prob":
  0.008950803428888321}, {"id": 16, "seek": 10324, "start": 124.84, "end": 132.04,
  "text": " revolutionize how we understand search and can bring it to the next level
  also and adapt to", "tokens": [51444, 8894, 1125, 577, 321, 1223, 3164, 293, 393,
  1565, 309, 281, 264, 958, 1496, 611, 293, 6231, 281, 51804], "temperature": 0.0,
  "avg_logprob": -0.16933830579121908, "compression_ratio": 1.6382978723404256, "no_speech_prob":
  0.008950803428888321}, {"id": 17, "seek": 13204, "start": 132.12, "end": 139.32,
  "text": " different kind of data or so on and go beyond the typical search bar that
  we are so much used to.", "tokens": [50368, 819, 733, 295, 1412, 420, 370, 322,
  293, 352, 4399, 264, 7476, 3164, 2159, 300, 321, 366, 370, 709, 1143, 281, 13, 50728],
  "temperature": 0.0, "avg_logprob": -0.2180116203393829, "compression_ratio": 1.6462882096069869,
  "no_speech_prob": 0.007553476840257645}, {"id": 18, "seek": 13204, "start": 139.95999999999998,
  "end": 145.79999999999998, "text": " Yeah, yeah and I mean but Gina is like more
  than just embedding or kind of it''s more like a", "tokens": [50760, 865, 11, 1338,
  293, 286, 914, 457, 34711, 307, 411, 544, 813, 445, 12240, 3584, 420, 733, 295,
  309, 311, 544, 411, 257, 51052], "temperature": 0.0, "avg_logprob": -0.2180116203393829,
  "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.007553476840257645},
  {"id": 19, "seek": 13204, "start": 145.79999999999998, "end": 151.0, "text": " ecosystem
  right like it has like marketplace it has many different building blocks and", "tokens":
  [51052, 11311, 558, 411, 309, 575, 411, 19455, 309, 575, 867, 819, 2390, 8474, 293,
  51312], "temperature": 0.0, "avg_logprob": -0.2180116203393829, "compression_ratio":
  1.6462882096069869, "no_speech_prob": 0.007553476840257645}, {"id": 20, "seek":
  13204, "start": 151.0, "end": 157.48, "text": " components. This is what I think
  most of the people that might be hearing us might be wondering much", "tokens":
  [51312, 6677, 13, 639, 307, 437, 286, 519, 881, 295, 264, 561, 300, 1062, 312, 4763,
  505, 1062, 312, 6359, 709, 51636], "temperature": 0.0, "avg_logprob": -0.2180116203393829,
  "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.007553476840257645},
  {"id": 21, "seek": 15748, "start": 157.56, "end": 163.56, "text": " because it''s
  a question that we receive a lot so we are not such another vector database as the
  ones", "tokens": [50368, 570, 309, 311, 257, 1168, 300, 321, 4774, 257, 688, 370,
  321, 366, 406, 1270, 1071, 8062, 8149, 382, 264, 2306, 50668], "temperature": 0.0,
  "avg_logprob": -0.14861750072903104, "compression_ratio": 1.8726415094339623, "no_speech_prob":
  0.001868902938440442}, {"id": 22, "seek": 15748, "start": 163.56, "end": 167.56,
  "text": " that have been created in the podcast so we are treating the problem of
  semantic search and we are", "tokens": [50668, 300, 362, 668, 2942, 294, 264, 7367,
  370, 321, 366, 15083, 264, 1154, 295, 47982, 3164, 293, 321, 366, 50868], "temperature":
  0.0, "avg_logprob": -0.14861750072903104, "compression_ratio": 1.8726415094339623,
  "no_speech_prob": 0.001868902938440442}, {"id": 23, "seek": 15748, "start": 167.56,
  "end": 175.23999999999998, "text": " seeing this as a then-to-end problem and we
  are trying to build an ecosystem to help the business", "tokens": [50868, 2577,
  341, 382, 257, 550, 12, 1353, 12, 521, 1154, 293, 321, 366, 1382, 281, 1322, 364,
  11311, 281, 854, 264, 1606, 51252], "temperature": 0.0, "avg_logprob": -0.14861750072903104,
  "compression_ratio": 1.8726415094339623, "no_speech_prob": 0.001868902938440442},
  {"id": 24, "seek": 15748, "start": 175.23999999999998, "end": 182.67999999999998,
  "text": " and developers to develop their own neural search based engines and for
  that we are trying to build", "tokens": [51252, 293, 8849, 281, 1499, 641, 1065,
  18161, 3164, 2361, 12982, 293, 337, 300, 321, 366, 1382, 281, 1322, 51624], "temperature":
  0.0, "avg_logprob": -0.14861750072903104, "compression_ratio": 1.8726415094339623,
  "no_speech_prob": 0.001868902938440442}, {"id": 25, "seek": 18268, "start": 182.68,
  "end": 189.8, "text": " a ecosystem from the core to our document types where we
  are also recently", "tokens": [50364, 257, 11311, 490, 264, 4965, 281, 527, 4166,
  3467, 689, 321, 366, 611, 3938, 50720], "temperature": 0.0, "avg_logprob": -0.18552758143498346,
  "compression_ratio": 1.7102803738317758, "no_speech_prob": 0.00203107763081789},
  {"id": 26, "seek": 18268, "start": 191.64000000000001, "end": 197.72, "text": "
  and launched this fine tuner project to help you with fine tuning your models for
  your search", "tokens": [50812, 293, 8730, 341, 2489, 4267, 260, 1716, 281, 854,
  291, 365, 2489, 15164, 428, 5245, 337, 428, 3164, 51116], "temperature": 0.0, "avg_logprob":
  -0.18552758143498346, "compression_ratio": 1.7102803738317758, "no_speech_prob":
  0.00203107763081789}, {"id": 27, "seek": 18268, "start": 197.72, "end": 204.04000000000002,
  "text": " applications so we are building a whole family of products and projects
  in this around this area", "tokens": [51116, 5821, 370, 321, 366, 2390, 257, 1379,
  1605, 295, 3383, 293, 4455, 294, 341, 926, 341, 1859, 51432], "temperature": 0.0,
  "avg_logprob": -0.18552758143498346, "compression_ratio": 1.7102803738317758, "no_speech_prob":
  0.00203107763081789}, {"id": 28, "seek": 18268, "start": 204.04000000000002, "end":
  209.24, "text": " of neural search. Yeah it sounds quite ambitious and it sounds
  like all of these building blocks are", "tokens": [51432, 295, 18161, 3164, 13,
  865, 309, 3263, 1596, 20239, 293, 309, 3263, 411, 439, 295, 613, 2390, 8474, 366,
  51692], "temperature": 0.0, "avg_logprob": -0.18552758143498346, "compression_ratio":
  1.7102803738317758, "no_speech_prob": 0.00203107763081789}, {"id": 29, "seek": 20924,
  "start": 209.24, "end": 215.32000000000002, "text": " really needed for anybody
  who wants to venture into embedding world of semantic search or you", "tokens":
  [50364, 534, 2978, 337, 4472, 567, 2738, 281, 18474, 666, 12240, 3584, 1002, 295,
  47982, 3164, 420, 291, 50668], "temperature": 0.0, "avg_logprob": -0.1806302865346273,
  "compression_ratio": 1.7536231884057971, "no_speech_prob": 0.0025847635697573423},
  {"id": 30, "seek": 20924, "start": 215.32000000000002, "end": 223.32000000000002,
  "text": " know kind of bringing the power of this deep learning models. So it goes
  beyond only", "tokens": [50668, 458, 733, 295, 5062, 264, 1347, 295, 341, 2452,
  2539, 5245, 13, 407, 309, 1709, 4399, 787, 51068], "temperature": 0.0, "avg_logprob":
  -0.1806302865346273, "compression_ratio": 1.7536231884057971, "no_speech_prob":
  0.0025847635697573423}, {"id": 31, "seek": 20924, "start": 224.76000000000002, "end":
  230.68, "text": " only embedding your data and searching through it you may want
  to cut it into different pieces,", "tokens": [51140, 787, 12240, 3584, 428, 1412,
  293, 10808, 807, 309, 291, 815, 528, 281, 1723, 309, 666, 819, 3755, 11, 51436],
  "temperature": 0.0, "avg_logprob": -0.1806302865346273, "compression_ratio": 1.7536231884057971,
  "no_speech_prob": 0.0025847635697573423}, {"id": 32, "seek": 20924, "start": 231.4,
  "end": 236.84, "text": " you may want to re-run it at the end, you may want to join
  different modalities together", "tokens": [51472, 291, 815, 528, 281, 319, 12, 12997,
  309, 412, 264, 917, 11, 291, 815, 528, 281, 3917, 819, 1072, 16110, 1214, 51744],
  "temperature": 0.0, "avg_logprob": -0.1806302865346273, "compression_ratio": 1.7536231884057971,
  "no_speech_prob": 0.0025847635697573423}, {"id": 33, "seek": 23684, "start": 237.48,
  "end": 243.56, "text": " so we are trying to give and make it easy for the user
  to develop these applications so that they", "tokens": [50396, 370, 321, 366, 1382,
  281, 976, 293, 652, 309, 1858, 337, 264, 4195, 281, 1499, 613, 5821, 370, 300, 436,
  50700], "temperature": 0.0, "avg_logprob": -0.24583013931123338, "compression_ratio":
  1.6569037656903767, "no_speech_prob": 0.011452664621174335}, {"id": 34, "seek":
  23684, "start": 243.56, "end": 249.4, "text": " speak the same language and we hope
  that they will all speak gene language. Oh yeah, oh yeah, for sure.", "tokens":
  [50700, 1710, 264, 912, 2856, 293, 321, 1454, 300, 436, 486, 439, 1710, 12186, 2856,
  13, 876, 1338, 11, 1954, 1338, 11, 337, 988, 13, 50992], "temperature": 0.0, "avg_logprob":
  -0.24583013931123338, "compression_ratio": 1.6569037656903767, "no_speech_prob":
  0.011452664621174335}, {"id": 35, "seek": 23684, "start": 249.4, "end": 256.36,
  "text": " And GNI is open source, right? Yes, so can you speak a bit more like towards
  the business model or", "tokens": [50992, 400, 460, 42496, 307, 1269, 4009, 11,
  558, 30, 1079, 11, 370, 393, 291, 1710, 257, 857, 544, 411, 3030, 264, 1606, 2316,
  420, 51340], "temperature": 0.0, "avg_logprob": -0.24583013931123338, "compression_ratio":
  1.6569037656903767, "no_speech_prob": 0.011452664621174335}, {"id": 36, "seek":
  23684, "start": 256.36, "end": 261.56, "text": " kind of how GNI kind of makes money
  in a way like so basically it''s open source, anybody can go", "tokens": [51340,
  733, 295, 577, 460, 42496, 733, 295, 1669, 1460, 294, 257, 636, 411, 370, 1936,
  309, 311, 1269, 4009, 11, 4472, 393, 352, 51600], "temperature": 0.0, "avg_logprob":
  -0.24583013931123338, "compression_ratio": 1.6569037656903767, "no_speech_prob":
  0.011452664621174335}, {"id": 37, "seek": 26156, "start": 261.56, "end": 266.36,
  "text": " and download it and basically leverage in their work or is there something
  that like you have some", "tokens": [50364, 293, 5484, 309, 293, 1936, 13982, 294,
  641, 589, 420, 307, 456, 746, 300, 411, 291, 362, 512, 50604], "temperature": 0.0,
  "avg_logprob": -0.21195084398443048, "compression_ratio": 1.6939655172413792, "no_speech_prob":
  0.005834519863128662}, {"id": 38, "seek": 26156, "start": 266.36, "end": 272.52,
  "text": " products for which customers can pay and kind of right now we are right
  now we are completely open", "tokens": [50604, 3383, 337, 597, 4581, 393, 1689,
  293, 733, 295, 558, 586, 321, 366, 558, 586, 321, 366, 2584, 1269, 50912], "temperature":
  0.0, "avg_logprob": -0.21195084398443048, "compression_ratio": 1.6939655172413792,
  "no_speech_prob": 0.005834519863128662}, {"id": 39, "seek": 26156, "start": 272.52,
  "end": 281.24, "text": " source everything that you can see in our report and stuff
  each open for everyone. Yeah so so okay", "tokens": [50912, 4009, 1203, 300, 291,
  393, 536, 294, 527, 2275, 293, 1507, 1184, 1269, 337, 1518, 13, 865, 370, 370, 1392,
  51348], "temperature": 0.0, "avg_logprob": -0.21195084398443048, "compression_ratio":
  1.6939655172413792, "no_speech_prob": 0.005834519863128662}, {"id": 40, "seek":
  26156, "start": 281.24, "end": 285.64, "text": " and you are like mostly working
  on back-and-side of things so you''re not interacting with direct", "tokens": [51348,
  293, 291, 366, 411, 5240, 1364, 322, 646, 12, 474, 12, 1812, 295, 721, 370, 291,
  434, 406, 18017, 365, 2047, 51568], "temperature": 0.0, "avg_logprob": -0.21195084398443048,
  "compression_ratio": 1.6939655172413792, "no_speech_prob": 0.005834519863128662},
  {"id": 41, "seek": 28564, "start": 285.71999999999997, "end": 293.32, "text": "
  customers right? Is that okay? I''m working mostly on the main products and what
  do you hear", "tokens": [50368, 4581, 558, 30, 1119, 300, 1392, 30, 286, 478, 1364,
  5240, 322, 264, 2135, 3383, 293, 437, 360, 291, 1568, 50748], "temperature": 0.0,
  "avg_logprob": -0.2294152123587472, "compression_ratio": 1.5372340425531914, "no_speech_prob":
  0.003993872553110123}, {"id": 42, "seek": 28564, "start": 293.32, "end": 301.8,
  "text": " about use cases like how do they translate to your level of kind of day-to-day
  job? So most of our", "tokens": [50748, 466, 764, 3331, 411, 577, 360, 436, 13799,
  281, 428, 1496, 295, 733, 295, 786, 12, 1353, 12, 810, 1691, 30, 407, 881, 295,
  527, 51172], "temperature": 0.0, "avg_logprob": -0.2294152123587472, "compression_ratio":
  1.5372340425531914, "no_speech_prob": 0.003993872553110123}, {"id": 43, "seek":
  28564, "start": 301.8, "end": 307.56, "text": " solution engineers that say that
  are closer to clients and users they bring guys their pain points", "tokens": [51172,
  3827, 11955, 300, 584, 300, 366, 4966, 281, 6982, 293, 5022, 436, 1565, 1074, 641,
  1822, 2793, 51460], "temperature": 0.0, "avg_logprob": -0.2294152123587472, "compression_ratio":
  1.5372340425531914, "no_speech_prob": 0.003993872553110123}, {"id": 44, "seek":
  30756, "start": 308.12, "end": 315.64, "text": " on how they are trying to solve
  users needs and some of the main use cases that we are trying to", "tokens": [50392,
  322, 577, 436, 366, 1382, 281, 5039, 5022, 2203, 293, 512, 295, 264, 2135, 764,
  3331, 300, 321, 366, 1382, 281, 50768], "temperature": 0.0, "avg_logprob": -0.19238075528826032,
  "compression_ratio": 1.858974358974359, "no_speech_prob": 0.006196014583110809},
  {"id": 45, "seek": 30756, "start": 315.64, "end": 322.44, "text": " solve come from
  textile search, e-match search, multimodal search that is something that we are",
  "tokens": [50768, 5039, 808, 490, 42069, 3164, 11, 308, 12, 76, 852, 3164, 11, 32972,
  378, 304, 3164, 300, 307, 746, 300, 321, 366, 51108], "temperature": 0.0, "avg_logprob":
  -0.19238075528826032, "compression_ratio": 1.858974358974359, "no_speech_prob":
  0.006196014583110809}, {"id": 46, "seek": 30756, "start": 322.44, "end": 330.68,
  "text": " trying to excel at that is going beyond only just using search and text
  or images to search maybe", "tokens": [51108, 1382, 281, 24015, 412, 300, 307, 516,
  4399, 787, 445, 1228, 3164, 293, 2487, 420, 5267, 281, 3164, 1310, 51520], "temperature":
  0.0, "avg_logprob": -0.19238075528826032, "compression_ratio": 1.858974358974359,
  "no_speech_prob": 0.006196014583110809}, {"id": 47, "seek": 33068, "start": 331.24,
  "end": 338.6, "text": " trying to have a combination of walls to power search to
  the next level. So they might like bring", "tokens": [50392, 1382, 281, 362, 257,
  6562, 295, 7920, 281, 1347, 3164, 281, 264, 958, 1496, 13, 407, 436, 1062, 411,
  1565, 50760], "temperature": 0.0, "avg_logprob": -0.11148281097412109, "compression_ratio":
  1.6538461538461537, "no_speech_prob": 0.0063421037048101425}, {"id": 48, "seek":
  33068, "start": 338.6, "end": 345.24, "text": " some kind of use case that you need
  to figure out on tech level right? Yes kind of translates to", "tokens": [50760,
  512, 733, 295, 764, 1389, 300, 291, 643, 281, 2573, 484, 322, 7553, 1496, 558, 30,
  1079, 733, 295, 28468, 281, 51092], "temperature": 0.0, "avg_logprob": -0.11148281097412109,
  "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0063421037048101425},
  {"id": 49, "seek": 33068, "start": 345.24, "end": 350.04, "text": " you but on the
  other hand like you said it''s open source so it means like there is like a bunch
  of", "tokens": [51092, 291, 457, 322, 264, 661, 1011, 411, 291, 848, 309, 311, 1269,
  4009, 370, 309, 1355, 411, 456, 307, 411, 257, 3840, 295, 51332], "temperature":
  0.0, "avg_logprob": -0.11148281097412109, "compression_ratio": 1.6538461538461537,
  "no_speech_prob": 0.0063421037048101425}, {"id": 50, "seek": 33068, "start": 350.04,
  "end": 354.92, "text": " like GitHub issues coming in right and if you have like
  Slack or I don''t know if you''re using", "tokens": [51332, 411, 23331, 2663, 1348,
  294, 558, 293, 498, 291, 362, 411, 37211, 420, 286, 500, 380, 458, 498, 291, 434,
  1228, 51576], "temperature": 0.0, "avg_logprob": -0.11148281097412109, "compression_ratio":
  1.6538461538461537, "no_speech_prob": 0.0063421037048101425}, {"id": 51, "seek":
  35492, "start": 355.0, "end": 363.40000000000003, "text": " Slack anyway. Yeah,
  so like probably every day like somebody you wake up and there are questions", "tokens":
  [50368, 37211, 4033, 13, 865, 11, 370, 411, 1391, 633, 786, 411, 2618, 291, 6634,
  493, 293, 456, 366, 1651, 50788], "temperature": 0.0, "avg_logprob": -0.2297153053702889,
  "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.006950638722628355},
  {"id": 52, "seek": 35492, "start": 363.40000000000003, "end": 371.32, "text": "
  there right? So it''s also clients in a way right? Yes for me my users are our clients
  and we", "tokens": [50788, 456, 558, 30, 407, 309, 311, 611, 6982, 294, 257, 636,
  558, 30, 1079, 337, 385, 452, 5022, 366, 527, 6982, 293, 321, 51184], "temperature":
  0.0, "avg_logprob": -0.2297153053702889, "compression_ratio": 1.6196581196581197,
  "no_speech_prob": 0.006950638722628355}, {"id": 53, "seek": 35492, "start": 371.32,
  "end": 377.8, "text": " have to listen to them so that''s the big point of open
  source in my opinion is this direct feedback", "tokens": [51184, 362, 281, 2140,
  281, 552, 370, 300, 311, 264, 955, 935, 295, 1269, 4009, 294, 452, 4800, 307, 341,
  2047, 5824, 51508], "temperature": 0.0, "avg_logprob": -0.2297153053702889, "compression_ratio":
  1.6196581196581197, "no_speech_prob": 0.006950638722628355}, {"id": 54, "seek":
  35492, "start": 377.8, "end": 384.04, "text": " from the users we can you can correct
  your direction and you can measure if your APIs are", "tokens": [51508, 490, 264,
  5022, 321, 393, 291, 393, 3006, 428, 3513, 293, 291, 393, 3481, 498, 428, 21445,
  366, 51820], "temperature": 0.0, "avg_logprob": -0.2297153053702889, "compression_ratio":
  1.6196581196581197, "no_speech_prob": 0.006950638722628355}, {"id": 55, "seek":
  38404, "start": 384.04, "end": 390.12, "text": " or your design are too complex
  for the user to rush or whatever so this direct feedback is", "tokens": [50364,
  420, 428, 1715, 366, 886, 3997, 337, 264, 4195, 281, 9300, 420, 2035, 370, 341,
  2047, 5824, 307, 50668], "temperature": 0.0, "avg_logprob": -0.26971201463179156,
  "compression_ratio": 1.6157205240174672, "no_speech_prob": 0.0017228100914508104},
  {"id": 56, "seek": 38404, "start": 390.12, "end": 400.44, "text": " really useful.
  And to this point it''s manageable. Yeah yeah but it''s also like I guess I also",
  "tokens": [50668, 534, 4420, 13, 400, 281, 341, 935, 309, 311, 38798, 13, 865, 1338,
  457, 309, 311, 611, 411, 286, 2041, 286, 611, 51184], "temperature": 0.0, "avg_logprob":
  -0.26971201463179156, "compression_ratio": 1.6157205240174672, "no_speech_prob":
  0.0017228100914508104}, {"id": 57, "seek": 38404, "start": 400.44, "end": 407.96000000000004,
  "text": " alluded to this and one of the podcasts was with Bob One Lloyd from from
  semi like it''s also", "tokens": [51184, 33919, 281, 341, 293, 472, 295, 264, 24045,
  390, 365, 6085, 1485, 31401, 490, 490, 12909, 411, 309, 311, 611, 51560], "temperature":
  0.0, "avg_logprob": -0.26971201463179156, "compression_ratio": 1.6157205240174672,
  "no_speech_prob": 0.0017228100914508104}, {"id": 58, "seek": 38404, "start": 407.96000000000004,
  "end": 413.40000000000003, "text": " sometimes maybe to give up with all the questions
  right? Like if you get all these questions", "tokens": [51560, 2171, 1310, 281,
  976, 493, 365, 439, 264, 1651, 558, 30, 1743, 498, 291, 483, 439, 613, 1651, 51832],
  "temperature": 0.0, "avg_logprob": -0.26971201463179156, "compression_ratio": 1.6157205240174672,
  "no_speech_prob": 0.0017228100914508104}, {"id": 59, "seek": 41340, "start": 413.4,
  "end": 418.84, "text": " when do you find the answer to kind of really deeply answer?
  Yeah fine time for answer them yeah.", "tokens": [50364, 562, 360, 291, 915, 264,
  1867, 281, 733, 295, 534, 8760, 1867, 30, 865, 2489, 565, 337, 1867, 552, 1338,
  13, 50636], "temperature": 0.0, "avg_logprob": -0.17133570777045357, "compression_ratio":
  1.7309417040358743, "no_speech_prob": 0.002529696561396122}, {"id": 60, "seek":
  41340, "start": 418.84, "end": 424.76, "text": " So we are trying to grow our team
  into knowing that the community is something that makes us", "tokens": [50636, 407,
  321, 366, 1382, 281, 1852, 527, 1469, 666, 5276, 300, 264, 1768, 307, 746, 300,
  1669, 505, 50932], "temperature": 0.0, "avg_logprob": -0.17133570777045357, "compression_ratio":
  1.7309417040358743, "no_speech_prob": 0.002529696561396122}, {"id": 61, "seek":
  41340, "start": 424.76, "end": 430.28, "text": " special and it''s important for
  us to take care of our community so we are all trying to keep an eye", "tokens":
  [50932, 2121, 293, 309, 311, 1021, 337, 505, 281, 747, 1127, 295, 527, 1768, 370,
  321, 366, 439, 1382, 281, 1066, 364, 3313, 51208], "temperature": 0.0, "avg_logprob":
  -0.17133570777045357, "compression_ratio": 1.7309417040358743, "no_speech_prob":
  0.002529696561396122}, {"id": 62, "seek": 41340, "start": 430.28, "end": 437.71999999999997,
  "text": " on the community. Yeah yeah yeah I remember like when I was developing
  like search code we were", "tokens": [51208, 322, 264, 1768, 13, 865, 1338, 1338,
  286, 1604, 411, 562, 286, 390, 6416, 411, 3164, 3089, 321, 645, 51580], "temperature":
  0.0, "avg_logprob": -0.17133570777045357, "compression_ratio": 1.7309417040358743,
  "no_speech_prob": 0.002529696561396122}, {"id": 63, "seek": 43772, "start": 437.8,
  "end": 444.44000000000005, "text": " using like Apache Solar and I had to like customize
  some parts of solar and listen and I remember like", "tokens": [50368, 1228, 411,
  46597, 22385, 293, 286, 632, 281, 411, 19734, 512, 3166, 295, 7936, 293, 2140, 293,
  286, 1604, 411, 50700], "temperature": 0.0, "avg_logprob": -0.17635962303648603,
  "compression_ratio": 1.76, "no_speech_prob": 0.022289138287305832}, {"id": 64, "seek":
  43772, "start": 444.44000000000005, "end": 450.68, "text": " in order for me to
  kind of get up to speed I had to go to this mailing list right? And so there are",
  "tokens": [50700, 294, 1668, 337, 385, 281, 733, 295, 483, 493, 281, 3073, 286,
  632, 281, 352, 281, 341, 41612, 1329, 558, 30, 400, 370, 456, 366, 51012], "temperature":
  0.0, "avg_logprob": -0.17635962303648603, "compression_ratio": 1.76, "no_speech_prob":
  0.022289138287305832}, {"id": 65, "seek": 43772, "start": 450.68, "end": 455.32000000000005,
  "text": " like thousands and thousands emails actually Apache Solar was super active
  you know like in", "tokens": [51012, 411, 5383, 293, 5383, 12524, 767, 46597, 22385,
  390, 1687, 4967, 291, 458, 411, 294, 51244], "temperature": 0.0, "avg_logprob":
  -0.17635962303648603, "compression_ratio": 1.76, "no_speech_prob": 0.022289138287305832},
  {"id": 66, "seek": 43772, "start": 455.32000000000005, "end": 461.72, "text": "
  sillies in many ways and and I was like how can I keep up with all these questions
  but like I do need", "tokens": [51244, 37160, 530, 294, 867, 2098, 293, 293, 286,
  390, 411, 577, 393, 286, 1066, 493, 365, 439, 613, 1651, 457, 411, 286, 360, 643,
  51564], "temperature": 0.0, "avg_logprob": -0.17635962303648603, "compression_ratio":
  1.76, "no_speech_prob": 0.022289138287305832}, {"id": 67, "seek": 46172, "start":
  461.72, "end": 468.04, "text": " to somehow keep up and summarize maybe what what
  is being asked there in order to understand", "tokens": [50364, 281, 6063, 1066,
  493, 293, 20858, 1310, 437, 437, 307, 885, 2351, 456, 294, 1668, 281, 1223, 50680],
  "temperature": 0.0, "avg_logprob": -0.13355929955192233, "compression_ratio": 1.711111111111111,
  "no_speech_prob": 0.002701396122574806}, {"id": 68, "seek": 46172, "start": 468.04,
  "end": 474.04, "text": " it''s useful for me or not because when you ask a question
  on the mailing list or like today on", "tokens": [50680, 309, 311, 4420, 337, 385,
  420, 406, 570, 562, 291, 1029, 257, 1168, 322, 264, 41612, 1329, 420, 411, 965,
  322, 50980], "temperature": 0.0, "avg_logprob": -0.13355929955192233, "compression_ratio":
  1.711111111111111, "no_speech_prob": 0.002701396122574806}, {"id": 69, "seek": 46172,
  "start": 474.04, "end": 480.12, "text": " Slack sometimes you need to be ready to
  pay back right? If somebody help you in the community like", "tokens": [50980, 37211,
  2171, 291, 643, 281, 312, 1919, 281, 1689, 646, 558, 30, 759, 2618, 854, 291, 294,
  264, 1768, 411, 51284], "temperature": 0.0, "avg_logprob": -0.13355929955192233,
  "compression_ratio": 1.711111111111111, "no_speech_prob": 0.002701396122574806},
  {"id": 70, "seek": 46172, "start": 480.12, "end": 487.24, "text": " you sometimes
  need to also pay back so it''s like it''s a game. When this is seen in the community
  I", "tokens": [51284, 291, 2171, 643, 281, 611, 1689, 646, 370, 309, 311, 411, 309,
  311, 257, 1216, 13, 1133, 341, 307, 1612, 294, 264, 1768, 286, 51640], "temperature":
  0.0, "avg_logprob": -0.13355929955192233, "compression_ratio": 1.711111111111111,
  "no_speech_prob": 0.002701396122574806}, {"id": 71, "seek": 48724, "start": 487.24,
  "end": 491.8, "text": " think it''s really pleasant for all the team when community
  interacts with each other and none of", "tokens": [50364, 519, 309, 311, 534, 16232,
  337, 439, 264, 1469, 562, 1768, 43582, 365, 1184, 661, 293, 6022, 295, 50592], "temperature":
  0.0, "avg_logprob": -0.16638494420934608, "compression_ratio": 1.8358778625954197,
  "no_speech_prob": 0.0034056217409670353}, {"id": 72, "seek": 48724, "start": 491.8,
  "end": 496.68, "text": " the no one in the team has to jump in because they so they
  help each other that''s when I think", "tokens": [50592, 264, 572, 472, 294, 264,
  1469, 575, 281, 3012, 294, 570, 436, 370, 436, 854, 1184, 661, 300, 311, 562, 286,
  519, 50836], "temperature": 0.0, "avg_logprob": -0.16638494420934608, "compression_ratio":
  1.8358778625954197, "no_speech_prob": 0.0034056217409670353}, {"id": 73, "seek":
  48724, "start": 496.68, "end": 503.88, "text": " the community really scales and
  really open source goes to the next level. Yeah it''s kind of", "tokens": [50836,
  264, 1768, 534, 17408, 293, 534, 1269, 4009, 1709, 281, 264, 958, 1496, 13, 865,
  309, 311, 733, 295, 51196], "temperature": 0.0, "avg_logprob": -0.16638494420934608,
  "compression_ratio": 1.8358778625954197, "no_speech_prob": 0.0034056217409670353},
  {"id": 74, "seek": 48724, "start": 503.88, "end": 509.64, "text": " regenerating
  itself and kind of the cultural element of it so and the community drives you forward
  I mean", "tokens": [51196, 26358, 990, 2564, 293, 733, 295, 264, 6988, 4478, 295,
  309, 370, 293, 264, 1768, 11754, 291, 2128, 286, 914, 51484], "temperature": 0.0,
  "avg_logprob": -0.16638494420934608, "compression_ratio": 1.8358778625954197, "no_speech_prob":
  0.0034056217409670353}, {"id": 75, "seek": 48724, "start": 511.16, "end": 516.52,
  "text": " just driving force of the project from the interaction point and the feature
  wise as well.", "tokens": [51560, 445, 4840, 3464, 295, 264, 1716, 490, 264, 9285,
  935, 293, 264, 4111, 10829, 382, 731, 13, 51828], "temperature": 0.0, "avg_logprob":
  -0.16638494420934608, "compression_ratio": 1.8358778625954197, "no_speech_prob":
  0.0034056217409670353}, {"id": 76, "seek": 51652, "start": 516.6, "end": 523.24,
  "text": " Yeah sounds good. So John tell me more about GNI itself like as a product
  let''s say as a", "tokens": [50368, 865, 3263, 665, 13, 407, 2619, 980, 385, 544,
  466, 460, 42496, 2564, 411, 382, 257, 1674, 718, 311, 584, 382, 257, 50700], "temperature":
  0.0, "avg_logprob": -0.1818456252415975, "compression_ratio": 1.6120689655172413,
  "no_speech_prob": 0.003542924067005515}, {"id": 77, "seek": 51652, "start": 523.24,
  "end": 530.84, "text": " technology stack like what can I do as a user you know
  using GNI and yeah like is it self-series", "tokens": [50700, 2899, 8630, 411, 437,
  393, 286, 360, 382, 257, 4195, 291, 458, 1228, 460, 42496, 293, 1338, 411, 307,
  309, 2698, 12, 12484, 530, 51080], "temperature": 0.0, "avg_logprob": -0.1818456252415975,
  "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.003542924067005515},
  {"id": 78, "seek": 51652, "start": 530.84, "end": 538.68, "text": " and so on. So
  the main point of GNI is that we want to be with the user from the minute they",
  "tokens": [51080, 293, 370, 322, 13, 407, 264, 2135, 935, 295, 460, 42496, 307,
  300, 321, 528, 281, 312, 365, 264, 4195, 490, 264, 3456, 436, 51472], "temperature":
  0.0, "avg_logprob": -0.1818456252415975, "compression_ratio": 1.6120689655172413,
  "no_speech_prob": 0.003542924067005515}, {"id": 79, "seek": 51652, "start": 538.68,
  "end": 544.68, "text": " are experimenting with their search application so for
  instance we are written in Python and we", "tokens": [51472, 366, 29070, 365, 641,
  3164, 3861, 370, 337, 5197, 321, 366, 3720, 294, 15329, 293, 321, 51772], "temperature":
  0.0, "avg_logprob": -0.1818456252415975, "compression_ratio": 1.6120689655172413,
  "no_speech_prob": 0.003542924067005515}, {"id": 80, "seek": 54468, "start": 544.68,
  "end": 549.0, "text": " have a really nice API in Python to build with your documents
  that can treat with any type of", "tokens": [50364, 362, 257, 534, 1481, 9362, 294,
  15329, 281, 1322, 365, 428, 8512, 300, 393, 2387, 365, 604, 2010, 295, 50580], "temperature":
  0.0, "avg_logprob": -0.18774527595156715, "compression_ratio": 1.688073394495413,
  "no_speech_prob": 0.0003409196506254375}, {"id": 81, "seek": 54468, "start": 549.0,
  "end": 556.28, "text": " data, text, images, audio, video and we are trying to build
  a really easy to use API for this", "tokens": [50580, 1412, 11, 2487, 11, 5267,
  11, 6278, 11, 960, 293, 321, 366, 1382, 281, 1322, 257, 534, 1858, 281, 764, 9362,
  337, 341, 50944], "temperature": 0.0, "avg_logprob": -0.18774527595156715, "compression_ratio":
  1.688073394495413, "no_speech_prob": 0.0003409196506254375}, {"id": 82, "seek":
  54468, "start": 556.28, "end": 563.7199999999999, "text": " for you to run locally
  your solutions. The first experimental facing to wrap your code", "tokens": [50944,
  337, 291, 281, 1190, 16143, 428, 6547, 13, 440, 700, 17069, 7170, 281, 7019, 428,
  3089, 51316], "temperature": 0.0, "avg_logprob": -0.18774527595156715, "compression_ratio":
  1.688073394495413, "no_speech_prob": 0.0003409196506254375}, {"id": 83, "seek":
  54468, "start": 563.7199999999999, "end": 568.92, "text": " for processing loading
  the files and for processing the images or whatever and embedding them", "tokens":
  [51316, 337, 9007, 15114, 264, 7098, 293, 337, 9007, 264, 5267, 420, 2035, 293,
  12240, 3584, 552, 51576], "temperature": 0.0, "avg_logprob": -0.18774527595156715,
  "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0003409196506254375},
  {"id": 84, "seek": 56892, "start": 569.64, "end": 576.76, "text": " searching to
  do a process many as neighbors or exact nearest neighbor search then once you have
  this", "tokens": [50400, 10808, 281, 360, 257, 1399, 867, 382, 12512, 420, 1900,
  23831, 5987, 3164, 550, 1564, 291, 362, 341, 50756], "temperature": 0.0, "avg_logprob":
  -0.2781321493427405, "compression_ratio": 1.8761904761904762, "no_speech_prob":
  0.0011840553488582373}, {"id": 85, "seek": 56892, "start": 578.12, "end": 584.92,
  "text": " we make it easy for you to wrap it in some microservices what we call
  executors so first phase you", "tokens": [50824, 321, 652, 309, 1858, 337, 291,
  281, 7019, 309, 294, 512, 15547, 47480, 437, 321, 818, 7568, 830, 370, 700, 5574,
  291, 51164], "temperature": 0.0, "avg_logprob": -0.2781321493427405, "compression_ratio":
  1.8761904761904762, "no_speech_prob": 0.0011840553488582373}, {"id": 86, "seek":
  56892, "start": 584.92, "end": 590.92, "text": " deal with these document array
  types that we have come with then you come with them to the next", "tokens": [51164,
  2028, 365, 613, 4166, 10225, 3467, 300, 321, 362, 808, 365, 550, 291, 808, 365,
  552, 281, 264, 958, 51464], "temperature": 0.0, "avg_logprob": -0.2781321493427405,
  "compression_ratio": 1.8761904761904762, "no_speech_prob": 0.0011840553488582373},
  {"id": 87, "seek": 56892, "start": 590.92, "end": 596.04, "text": " layer is you
  have it with the executors so you wrap your logic in different microservices and
  then", "tokens": [51464, 4583, 307, 291, 362, 309, 365, 264, 7568, 830, 370, 291,
  7019, 428, 9952, 294, 819, 15547, 47480, 293, 550, 51720], "temperature": 0.0, "avg_logprob":
  -0.2781321493427405, "compression_ratio": 1.8761904761904762, "no_speech_prob":
  0.0011840553488582373}, {"id": 88, "seek": 59604, "start": 596.04, "end": 603.0799999999999,
  "text": " we put it in what we call a flow that is kind of a pipeline that is really
  to scale locally or", "tokens": [50364, 321, 829, 309, 294, 437, 321, 818, 257,
  3095, 300, 307, 733, 295, 257, 15517, 300, 307, 534, 281, 4373, 16143, 420, 50716],
  "temperature": 0.0, "avg_logprob": -0.1448435007139694, "compression_ratio": 1.632034632034632,
  "no_speech_prob": 0.0012566702207550406}, {"id": 89, "seek": 59604, "start": 603.0799999999999,
  "end": 608.8399999999999, "text": " remotely or even with Kubernetes so that you
  have replication and scalability taken care for.", "tokens": [50716, 20824, 420,
  754, 365, 23145, 370, 300, 291, 362, 39911, 293, 15664, 2310, 2726, 1127, 337, 13,
  51004], "temperature": 0.0, "avg_logprob": -0.1448435007139694, "compression_ratio":
  1.632034632034632, "no_speech_prob": 0.0012566702207550406}, {"id": 90, "seek":
  59604, "start": 609.64, "end": 617.56, "text": " So we are trying to bring you easily
  from your day zero of development to the production system.", "tokens": [51044,
  407, 321, 366, 1382, 281, 1565, 291, 3612, 490, 428, 786, 4018, 295, 3250, 281,
  264, 4265, 1185, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1448435007139694,
  "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0012566702207550406},
  {"id": 91, "seek": 59604, "start": 617.56, "end": 623.88, "text": " Yeah yeah sounds
  good sounds comprehensive and like what if I would like to just use like a", "tokens":
  [51440, 865, 1338, 3263, 665, 3263, 13914, 293, 411, 437, 498, 286, 576, 411, 281,
  445, 764, 411, 257, 51756], "temperature": 0.0, "avg_logprob": -0.1448435007139694,
  "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0012566702207550406},
  {"id": 92, "seek": 62388, "start": 623.96, "end": 631.0, "text": " hosted version
  can I use a hosted version from Gina AI or do I need to do an operation?", "tokens":
  [50368, 19204, 3037, 393, 286, 764, 257, 19204, 3037, 490, 34711, 7318, 420, 360,
  286, 643, 281, 360, 364, 6916, 30, 50720], "temperature": 0.0, "avg_logprob": -0.5413087463378906,
  "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.004382084123790264},
  {"id": 93, "seek": 62388, "start": 631.0, "end": 635.0, "text": " There is no hosted
  version at this point yeah so it''s basically I need it''s like a", "tokens": [50720,
  821, 307, 572, 19204, 3037, 412, 341, 935, 1338, 370, 309, 311, 1936, 286, 643,
  309, 311, 411, 257, 50920], "temperature": 0.0, "avg_logprob": -0.5413087463378906,
  "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.004382084123790264},
  {"id": 94, "seek": 62388, "start": 636.12, "end": 639.96, "text": " Lego type of
  thing right? Yes exactly. I will have a nice deployment.", "tokens": [50976, 28761,
  2010, 295, 551, 558, 30, 1079, 2293, 13, 286, 486, 362, 257, 1481, 19317, 13, 51168],
  "temperature": 0.0, "avg_logprob": -0.5413087463378906, "compression_ratio": 1.6602316602316602,
  "no_speech_prob": 0.004382084123790264}, {"id": 95, "seek": 62388, "start": 640.92,
  "end": 646.04, "text": " And we have even this marketplace as you said this with
  this helicopter cutter so you can share", "tokens": [51216, 400, 321, 362, 754,
  341, 19455, 382, 291, 848, 341, 365, 341, 19803, 25531, 370, 291, 393, 2073, 51472],
  "temperature": 0.0, "avg_logprob": -0.5413087463378906, "compression_ratio": 1.6602316602316602,
  "no_speech_prob": 0.004382084123790264}, {"id": 96, "seek": 62388, "start": 646.04,
  "end": 651.08, "text": " publicly or privately with your colleagues or with the
  community your meeting blocks that you", "tokens": [51472, 14843, 420, 31919, 365,
  428, 7734, 420, 365, 264, 1768, 428, 3440, 8474, 300, 291, 51724], "temperature":
  0.0, "avg_logprob": -0.5413087463378906, "compression_ratio": 1.6602316602316602,
  "no_speech_prob": 0.004382084123790264}, {"id": 97, "seek": 65108, "start": 651.08,
  "end": 655.72, "text": " may think they are useful for you. Yeah modern deep learning
  models that you have packed", "tokens": [50364, 815, 519, 436, 366, 4420, 337, 291,
  13, 865, 4363, 2452, 2539, 5245, 300, 291, 362, 13265, 50596], "temperature": 0.0,
  "avg_logprob": -0.4869543053637976, "compression_ratio": 1.5806451612903225, "no_speech_prob":
  0.0020556175149977207}, {"id": 98, "seek": 65108, "start": 656.44, "end": 660.76,
  "text": " processing, copy-done, re-runking, even back to research research.", "tokens":
  [50632, 9007, 11, 5055, 12, 67, 546, 11, 319, 12, 81, 3197, 278, 11, 754, 646, 281,
  2132, 2132, 13, 50848], "temperature": 0.0, "avg_logprob": -0.4869543053637976,
  "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0020556175149977207},
  {"id": 99, "seek": 65108, "start": 660.76, "end": 669.96, "text": " Yeah so and
  how does it align also with like companies or hubs like Hagen phase you know Hagen",
  "tokens": [50848, 865, 370, 293, 577, 775, 309, 7975, 611, 365, 411, 3431, 420,
  46870, 411, 389, 4698, 5574, 291, 458, 389, 4698, 51308], "temperature": 0.0, "avg_logprob":
  -0.4869543053637976, "compression_ratio": 1.5806451612903225, "no_speech_prob":
  0.0020556175149977207}, {"id": 100, "seek": 65108, "start": 669.96, "end": 674.6800000000001,
  "text": " phase is also very famous on model side right? So like let''s show somebody
  picks a model and", "tokens": [51308, 5574, 307, 611, 588, 4618, 322, 2316, 1252,
  558, 30, 407, 411, 718, 311, 855, 2618, 16137, 257, 2316, 293, 51544], "temperature":
  0.0, "avg_logprob": -0.4869543053637976, "compression_ratio": 1.5806451612903225,
  "no_speech_prob": 0.0020556175149977207}, {"id": 101, "seek": 67468, "start": 674.68,
  "end": 680.76, "text": " wants to bring it to Gina what''s the process there? So
  it''s quite I would say having", "tokens": [50364, 2738, 281, 1565, 309, 281, 34711,
  437, 311, 264, 1399, 456, 30, 407, 309, 311, 1596, 286, 576, 584, 1419, 50668],
  "temperature": 0.0, "avg_logprob": -0.246503784542992, "compression_ratio": 1.708133971291866,
  "no_speech_prob": 0.0036826268769800663}, {"id": 102, "seek": 67468, "start": 680.76,
  "end": 685.0, "text": " phase it''s quite inspirational for us in this sense in
  this marketplace community", "tokens": [50668, 5574, 309, 311, 1596, 33554, 337,
  505, 294, 341, 2020, 294, 341, 19455, 1768, 50880], "temperature": 0.0, "avg_logprob":
  -0.246503784542992, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.0036826268769800663},
  {"id": 103, "seek": 67468, "start": 685.9599999999999, "end": 694.68, "text": "
  and place it is quite similar but um Gina is this marketplace is related to our
  executor so it", "tokens": [50928, 293, 1081, 309, 307, 1596, 2531, 457, 1105, 34711,
  307, 341, 19455, 307, 4077, 281, 527, 7568, 284, 370, 309, 51364], "temperature":
  0.0, "avg_logprob": -0.246503784542992, "compression_ratio": 1.708133971291866,
  "no_speech_prob": 0.0036826268769800663}, {"id": 104, "seek": 67468, "start": 694.68,
  "end": 702.3599999999999, "text": " goes beyond only models so it''s any subsystem
  enabled and block that you can that you can build", "tokens": [51364, 1709, 4399,
  787, 5245, 370, 309, 311, 604, 2090, 9321, 15172, 293, 3461, 300, 291, 393, 300,
  291, 393, 1322, 51748], "temperature": 0.0, "avg_logprob": -0.246503784542992, "compression_ratio":
  1.708133971291866, "no_speech_prob": 0.0036826268769800663}, {"id": 105, "seek":
  70236, "start": 702.36, "end": 709.5600000000001, "text": " that is able to be part
  of this of this Gina pipeline for us and we are trying to make it", "tokens": [50364,
  300, 307, 1075, 281, 312, 644, 295, 341, 295, 341, 34711, 15517, 337, 505, 293,
  321, 366, 1382, 281, 652, 309, 50724], "temperature": 0.0, "avg_logprob": -0.16105395952860516,
  "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.0008499606628902256},
  {"id": 106, "seek": 70236, "start": 709.5600000000001, "end": 715.32, "text": "
  user-phone for you to localize it and use it in any way in a simple API and we''re
  still working", "tokens": [50724, 4195, 12, 4977, 337, 291, 281, 2654, 1125, 309,
  293, 764, 309, 294, 604, 636, 294, 257, 2199, 9362, 293, 321, 434, 920, 1364, 51012],
  "temperature": 0.0, "avg_logprob": -0.16105395952860516, "compression_ratio": 1.7043795620437956,
  "no_speech_prob": 0.0008499606628902256}, {"id": 107, "seek": 70236, "start": 715.32,
  "end": 720.6, "text": " to make it easier every time. Yeah of course because actually
  you know it tends to get a lot of", "tokens": [51012, 281, 652, 309, 3571, 633,
  565, 13, 865, 295, 1164, 570, 767, 291, 458, 309, 12258, 281, 483, 257, 688, 295,
  51276], "temperature": 0.0, "avg_logprob": -0.16105395952860516, "compression_ratio":
  1.7043795620437956, "no_speech_prob": 0.0008499606628902256}, {"id": 108, "seek":
  70236, "start": 720.6, "end": 725.24, "text": " time you know the infrastructure
  part like how do I bring my model let''s say I have a custom model", "tokens": [51276,
  565, 291, 458, 264, 6896, 644, 411, 577, 360, 286, 1565, 452, 2316, 718, 311, 584,
  286, 362, 257, 2375, 2316, 51508], "temperature": 0.0, "avg_logprob": -0.16105395952860516,
  "compression_ratio": 1.7043795620437956, "no_speech_prob": 0.0008499606628902256},
  {"id": 109, "seek": 70236, "start": 725.24, "end": 732.28, "text": " and I want
  to bring it inside Gina right so it serves as a embedding layer so how do I", "tokens":
  [51508, 293, 286, 528, 281, 1565, 309, 1854, 34711, 558, 370, 309, 13451, 382, 257,
  12240, 3584, 4583, 370, 577, 360, 286, 51860], "temperature": 0.0, "avg_logprob":
  -0.16105395952860516, "compression_ratio": 1.7043795620437956, "no_speech_prob":
  0.0008499606628902256}, {"id": 110, "seek": 73228, "start": 732.28, "end": 739.0799999999999,
  "text": " figure out all this scalability or latency parameters and so on so I think
  so the first thing", "tokens": [50364, 2573, 484, 439, 341, 15664, 2310, 420, 27043,
  9834, 293, 370, 322, 370, 286, 519, 370, 264, 700, 551, 50704], "temperature": 0.0,
  "avg_logprob": -0.25484494412882946, "compression_ratio": 1.7363636363636363, "no_speech_prob":
  0.0010467789834365249}, {"id": 111, "seek": 73228, "start": 739.0799999999999, "end":
  745.4, "text": " is to get it working we are having to we expose these with these
  executors that have some API", "tokens": [50704, 307, 281, 483, 309, 1364, 321,
  366, 1419, 281, 321, 19219, 613, 365, 613, 7568, 830, 300, 362, 512, 9362, 51020],
  "temperature": 0.0, "avg_logprob": -0.25484494412882946, "compression_ratio": 1.7363636363636363,
  "no_speech_prob": 0.0010467789834365249}, {"id": 112, "seek": 73228, "start": 745.4,
  "end": 753.88, "text": " and to that read requests with some maybe I inspired with
  this fast API approach and then you have", "tokens": [51020, 293, 281, 300, 1401,
  12475, 365, 512, 1310, 286, 7547, 365, 341, 2370, 9362, 3109, 293, 550, 291, 362,
  51444], "temperature": 0.0, "avg_logprob": -0.25484494412882946, "compression_ratio":
  1.7363636363636363, "no_speech_prob": 0.0010467789834365249}, {"id": 113, "seek":
  73228, "start": 754.52, "end": 759.8, "text": " with this row you have the parameters
  to replicate to scale and so on you you may run it in GPU", "tokens": [51476, 365,
  341, 5386, 291, 362, 264, 9834, 281, 25356, 281, 4373, 293, 370, 322, 291, 291,
  815, 1190, 309, 294, 18407, 51740], "temperature": 0.0, "avg_logprob": -0.25484494412882946,
  "compression_ratio": 1.7363636363636363, "no_speech_prob": 0.0010467789834365249},
  {"id": 114, "seek": 75980, "start": 759.88, "end": 765.9599999999999, "text": "
  whatever yeah yeah so like you can choose your cost kind of like factors right or",
  "tokens": [50368, 2035, 1338, 1338, 370, 411, 291, 393, 2826, 428, 2063, 733, 295,
  411, 6771, 558, 420, 50672], "temperature": 0.0, "avg_logprob": -0.23909169365377986,
  "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.001926393830217421},
  {"id": 115, "seek": 75980, "start": 765.9599999999999, "end": 771.0, "text": " based
  on your cost factors you can choose it''s CPU actually and then latency and for
  some models", "tokens": [50672, 2361, 322, 428, 2063, 6771, 291, 393, 2826, 309,
  311, 13199, 767, 293, 550, 27043, 293, 337, 512, 5245, 50924], "temperature": 0.0,
  "avg_logprob": -0.23909169365377986, "compression_ratio": 1.7230046948356808, "no_speech_prob":
  0.001926393830217421}, {"id": 116, "seek": 75980, "start": 771.0, "end": 779.7199999999999,
  "text": " actually CPU is fine so yeah I mean why not yeah it depends also on the
  user needs so for instance", "tokens": [50924, 767, 13199, 307, 2489, 370, 1338,
  286, 914, 983, 406, 1338, 309, 5946, 611, 322, 264, 4195, 2203, 370, 337, 5197,
  51360], "temperature": 0.0, "avg_logprob": -0.23909169365377986, "compression_ratio":
  1.7230046948356808, "no_speech_prob": 0.001926393830217421}, {"id": 117, "seek":
  75980, "start": 780.5999999999999, "end": 788.4399999999999, "text": " we are also
  seeing that neural search main not all is not needed to be only for these big",
  "tokens": [51404, 321, 366, 611, 2577, 300, 18161, 3164, 2135, 406, 439, 307, 406,
  2978, 281, 312, 787, 337, 613, 955, 51796], "temperature": 0.0, "avg_logprob": -0.23909169365377986,
  "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.001926393830217421},
  {"id": 118, "seek": 78844, "start": 788.44, "end": 792.84, "text": " giants with
  this big amount of data and big amount of resources so any company it''s more",
  "tokens": [50364, 31894, 365, 341, 955, 2372, 295, 1412, 293, 955, 2372, 295, 3593,
  370, 604, 2237, 309, 311, 544, 50584], "temperature": 0.0, "avg_logprob": -0.1439342588748572,
  "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.0014836308546364307},
  {"id": 119, "seek": 78844, "start": 792.84, "end": 799.4000000000001, "text": "
  company can benefit from the power of the neural networks to power their search
  so they may not need", "tokens": [50584, 2237, 393, 5121, 490, 264, 1347, 295, 264,
  18161, 9590, 281, 1347, 641, 3164, 370, 436, 815, 406, 643, 50912], "temperature":
  0.0, "avg_logprob": -0.1439342588748572, "compression_ratio": 1.9076305220883534,
  "no_speech_prob": 0.0014836308546364307}, {"id": 120, "seek": 78844, "start": 800.7600000000001,
  "end": 807.4000000000001, "text": " so much require so much resources or they may
  not require so much speed so it''s about and so we", "tokens": [50980, 370, 709,
  3651, 370, 709, 3593, 420, 436, 815, 406, 3651, 370, 709, 3073, 370, 309, 311, 466,
  293, 370, 321, 51312], "temperature": 0.0, "avg_logprob": -0.1439342588748572, "compression_ratio":
  1.9076305220883534, "no_speech_prob": 0.0014836308546364307}, {"id": 121, "seek":
  78844, "start": 807.4000000000001, "end": 812.44, "text": " are giving the power
  more or less to use yeah and kind of flexibility of the platform so because", "tokens":
  [51312, 366, 2902, 264, 1347, 544, 420, 1570, 281, 764, 1338, 293, 733, 295, 12635,
  295, 264, 3663, 370, 570, 51564], "temperature": 0.0, "avg_logprob": -0.1439342588748572,
  "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.0014836308546364307},
  {"id": 122, "seek": 78844, "start": 812.44, "end": 816.84, "text": " essentially
  if they wanted to do it from scratch then they would probably need to figure out",
  "tokens": [51564, 4476, 498, 436, 1415, 281, 360, 309, 490, 8459, 550, 436, 576,
  1391, 643, 281, 2573, 484, 51784], "temperature": 0.0, "avg_logprob": -0.1439342588748572,
  "compression_ratio": 1.9076305220883534, "no_speech_prob": 0.0014836308546364307},
  {"id": 123, "seek": 81684, "start": 816.84, "end": 824.52, "text": " similar things
  like component isolation and scaling and yeah like an algorithm like a quality",
  "tokens": [50364, 2531, 721, 411, 6542, 16001, 293, 21589, 293, 1338, 411, 364,
  9284, 411, 257, 3125, 50748], "temperature": 0.0, "avg_logprob": -0.17490789890289307,
  "compression_ratio": 1.7971014492753623, "no_speech_prob": 0.00211620656773448},
  {"id": 124, "seek": 81684, "start": 824.52, "end": 830.9200000000001, "text": "
  checks and so on and on the algorithm side you said like you have exact search as
  well as", "tokens": [50748, 13834, 293, 370, 322, 293, 322, 264, 9284, 1252, 291,
  848, 411, 291, 362, 1900, 3164, 382, 731, 382, 51068], "temperature": 0.0, "avg_logprob":
  -0.17490789890289307, "compression_ratio": 1.7971014492753623, "no_speech_prob":
  0.00211620656773448}, {"id": 125, "seek": 81684, "start": 830.9200000000001, "end":
  837.24, "text": " in exact search can you talk with more and kind of mention maybe
  some algorithms that you support", "tokens": [51068, 294, 1900, 3164, 393, 291,
  751, 365, 544, 293, 733, 295, 2152, 1310, 512, 14642, 300, 291, 1406, 51384], "temperature":
  0.0, "avg_logprob": -0.17490789890289307, "compression_ratio": 1.7971014492753623,
  "no_speech_prob": 0.00211620656773448}, {"id": 126, "seek": 81684, "start": 838.2800000000001,
  "end": 845.24, "text": " so yeah so right now natively we support as the main native
  quite optimized version of the", "tokens": [51436, 370, 1338, 370, 558, 586, 8470,
  356, 321, 1406, 382, 264, 2135, 8470, 1596, 26941, 3037, 295, 264, 51784], "temperature":
  0.0, "avg_logprob": -0.17490789890289307, "compression_ratio": 1.7971014492753623,
  "no_speech_prob": 0.00211620656773448}, {"id": 127, "seek": 84524, "start": 845.64,
  "end": 852.76, "text": " and exact nearest neighbor search but then for instance
  one of these building blocks can be any", "tokens": [50384, 293, 1900, 23831, 5987,
  3164, 457, 550, 337, 5197, 472, 295, 613, 2390, 8474, 393, 312, 604, 50740], "temperature":
  0.0, "avg_logprob": -0.20129682277810984, "compression_ratio": 1.7337278106508875,
  "no_speech_prob": 0.0007803683984093368}, {"id": 128, "seek": 84524, "start": 852.76,
  "end": 859.32, "text": " support wrapping any client for any other vector database
  but for instance we just realized our own", "tokens": [50740, 1406, 21993, 604,
  6423, 337, 604, 661, 8062, 8149, 457, 337, 5197, 321, 445, 5334, 527, 1065, 51068],
  "temperature": 0.0, "avg_logprob": -0.20129682277810984, "compression_ratio": 1.7337278106508875,
  "no_speech_prob": 0.0007803683984093368}, {"id": 129, "seek": 84524, "start": 860.28,
  "end": 866.76, "text": " and approximate nearest neighbor solution we have two of
  them for instance that we have developed", "tokens": [51116, 293, 30874, 23831,
  5987, 3827, 321, 362, 732, 295, 552, 337, 5197, 300, 321, 362, 4743, 51440], "temperature":
  0.0, "avg_logprob": -0.20129682277810984, "compression_ratio": 1.7337278106508875,
  "no_speech_prob": 0.0007803683984093368}, {"id": 130, "seek": 86676, "start": 866.76,
  "end": 876.28, "text": " so much so we have one that is based on hsw plus a postgres
  indexer a postgres database", "tokens": [50364, 370, 709, 370, 321, 362, 472, 300,
  307, 2361, 322, 276, 82, 86, 1804, 257, 2183, 45189, 8186, 260, 257, 2183, 45189,
  8149, 50840], "temperature": 0.0, "avg_logprob": -0.2965242119245632, "compression_ratio":
  1.6904761904761905, "no_speech_prob": 0.0017833516467362642}, {"id": 131, "seek":
  86676, "start": 876.28, "end": 882.68, "text": " for to require the documents and
  then we have built our well we just released and in Slack", "tokens": [50840, 337,
  281, 3651, 264, 8512, 293, 550, 321, 362, 3094, 527, 731, 321, 445, 4736, 293, 294,
  37211, 51160], "temperature": 0.0, "avg_logprob": -0.2965242119245632, "compression_ratio":
  1.6904761904761905, "no_speech_prob": 0.0017833516467362642}, {"id": 132, "seek":
  86676, "start": 882.68, "end": 888.6, "text": " the community can start enjoying
  it we have and build what we call pcolyte which", "tokens": [51160, 264, 1768, 393,
  722, 9929, 309, 321, 362, 293, 1322, 437, 321, 818, 280, 1291, 356, 975, 597, 51456],
  "temperature": 0.0, "avg_logprob": -0.2965242119245632, "compression_ratio": 1.6904761904761905,
  "no_speech_prob": 0.0017833516467362642}, {"id": 133, "seek": 86676, "start": 889.3199999999999,
  "end": 896.36, "text": " and works with product quantization but also has support
  for hsw you said pcolyte or how do you", "tokens": [51492, 293, 1985, 365, 1674,
  4426, 2144, 457, 611, 575, 1406, 337, 276, 82, 86, 291, 848, 280, 1291, 356, 975,
  420, 577, 360, 291, 51844], "temperature": 0.0, "avg_logprob": -0.2965242119245632,
  "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0017833516467362642},
  {"id": 134, "seek": 89636, "start": 896.36, "end": 904.6, "text": " spell that?
  P2Lite, P2Lite, which is like product quantization light version. Yes we", "tokens":
  [50364, 9827, 300, 30, 430, 17, 43, 642, 11, 430, 17, 43, 642, 11, 597, 307, 411,
  1674, 4426, 2144, 1442, 3037, 13, 1079, 321, 50776], "temperature": 0.6, "avg_logprob":
  -0.4572385281932597, "compression_ratio": 1.6017699115044248, "no_speech_prob":
  0.003951632417738438}, {"id": 135, "seek": 89636, "start": 905.48, "end": 910.36,
  "text": " and profiltering options as well. Oh with preview and how in what sense
  is it light,", "tokens": [50820, 293, 1740, 388, 391, 278, 3956, 382, 731, 13, 876,
  365, 14281, 293, 577, 294, 437, 2020, 307, 309, 1442, 11, 51064], "temperature":
  0.6, "avg_logprob": -0.4572385281932597, "compression_ratio": 1.6017699115044248,
  "no_speech_prob": 0.003951632417738438}, {"id": 136, "seek": 89636, "start": 910.92,
  "end": 917.32, "text": " compared to product quantization? No I have not been involved
  so much in this spreader right now", "tokens": [51092, 5347, 281, 1674, 4426, 2144,
  30, 883, 286, 362, 406, 668, 3288, 370, 709, 294, 341, 3974, 260, 558, 586, 51412],
  "temperature": 0.6, "avg_logprob": -0.4572385281932597, "compression_ratio": 1.6017699115044248,
  "no_speech_prob": 0.003951632417738438}, {"id": 137, "seek": 89636, "start": 917.32,
  "end": 925.8000000000001, "text": " so it''s a new thing but it is light in sense
  of that it is quite embedded and it''s quite native", "tokens": [51412, 370, 309,
  311, 257, 777, 551, 457, 309, 307, 1442, 294, 2020, 295, 300, 309, 307, 1596, 16741,
  293, 309, 311, 1596, 8470, 51836], "temperature": 0.6, "avg_logprob": -0.4572385281932597,
  "compression_ratio": 1.6017699115044248, "no_speech_prob": 0.003951632417738438},
  {"id": 138, "seek": 92580, "start": 925.8, "end": 931.8, "text": " to work with
  our document type. So it''s not so general as any object, but it is really", "tokens":
  [50364, 281, 589, 365, 527, 4166, 2010, 13, 407, 309, 311, 406, 370, 2674, 382,
  604, 2657, 11, 457, 309, 307, 534, 50664], "temperature": 0.0, "avg_logprob": -0.2690470654031505,
  "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.2745043933391571},
  {"id": 139, "seek": 92580, "start": 932.52, "end": 937.0799999999999, "text": "
  built to integrate very easily with Jina. Oh, I see. Like with specific kind of",
  "tokens": [50700, 3094, 281, 13365, 588, 3612, 365, 508, 1426, 13, 876, 11, 286,
  536, 13, 1743, 365, 2685, 733, 295, 50928], "temperature": 0.0, "avg_logprob": -0.2690470654031505,
  "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.2745043933391571},
  {"id": 140, "seek": 92580, "start": 937.7199999999999, "end": 944.12, "text": "
  schema or document types. And it''s also open source. And do you do you like obviously
  you can", "tokens": [50960, 34078, 420, 4166, 3467, 13, 400, 309, 311, 611, 1269,
  4009, 13, 400, 360, 291, 360, 291, 411, 2745, 291, 393, 51280], "temperature": 0.0,
  "avg_logprob": -0.2690470654031505, "compression_ratio": 1.6209386281588447, "no_speech_prob":
  0.2745043933391571}, {"id": 141, "seek": 92580, "start": 944.12, "end": 949.24,
  "text": " provide the links or we can also link in the show notes. But do you also
  like have some kind of", "tokens": [51280, 2893, 264, 6123, 420, 321, 393, 611,
  2113, 294, 264, 855, 5570, 13, 583, 360, 291, 611, 411, 362, 512, 733, 295, 51536],
  "temperature": 0.0, "avg_logprob": -0.2690470654031505, "compression_ratio": 1.6209386281588447,
  "no_speech_prob": 0.2745043933391571}, {"id": 142, "seek": 92580, "start": 949.9599999999999,
  "end": 954.8399999999999, "text": " latency analysis for this algorithm? Like has
  it been conducted? Do you know? Yeah, there is", "tokens": [51572, 27043, 5215,
  337, 341, 9284, 30, 1743, 575, 309, 668, 13809, 30, 1144, 291, 458, 30, 865, 11,
  456, 307, 51816], "temperature": 0.0, "avg_logprob": -0.2690470654031505, "compression_ratio":
  1.6209386281588447, "no_speech_prob": 0.2745043933391571}, {"id": 143, "seek": 95484,
  "start": 954.84, "end": 957.96, "text": " some benchmarks that you''re going to
  find in the read. I cannot have the", "tokens": [50364, 512, 43751, 300, 291, 434,
  516, 281, 915, 294, 264, 1401, 13, 286, 2644, 362, 264, 50520], "temperature": 0.0,
  "avg_logprob": -0.2548856642639753, "compression_ratio": 1.5714285714285714, "no_speech_prob":
  0.012339537963271141}, {"id": 144, "seek": 95484, "start": 958.52, "end": 963.4,
  "text": " numbers in my head right now. Yeah, but I think for portion of our audience
  it''s going to be", "tokens": [50548, 3547, 294, 452, 1378, 558, 586, 13, 865, 11,
  457, 286, 519, 337, 8044, 295, 527, 4034, 309, 311, 516, 281, 312, 50792], "temperature":
  0.0, "avg_logprob": -0.2548856642639753, "compression_ratio": 1.5714285714285714,
  "no_speech_prob": 0.012339537963271141}, {"id": 145, "seek": 95484, "start": 963.4,
  "end": 969.1600000000001, "text": " interesting to check out because as you know,
  like actually my team just completed", "tokens": [50792, 1880, 281, 1520, 484, 570,
  382, 291, 458, 11, 411, 767, 452, 1469, 445, 7365, 51080], "temperature": 0.0, "avg_logprob":
  -0.2548856642639753, "compression_ratio": 1.5714285714285714, "no_speech_prob":
  0.012339537963271141}, {"id": 146, "seek": 95484, "start": 970.6800000000001, "end":
  975.08, "text": " participation in big A&N. I don''t know if you heard about this
  competition. So it''s like", "tokens": [51156, 13487, 294, 955, 316, 5, 45, 13,
  286, 500, 380, 458, 498, 291, 2198, 466, 341, 6211, 13, 407, 309, 311, 411, 51376],
  "temperature": 0.0, "avg_logprob": -0.2548856642639753, "compression_ratio": 1.5714285714285714,
  "no_speech_prob": 0.012339537963271141}, {"id": 147, "seek": 95484, "start": 975.08,
  "end": 981.32, "text": " Villion scale approximate near nearest neighbor search.
  So we invented like a new algorithm", "tokens": [51376, 691, 11836, 4373, 30874,
  2651, 23831, 5987, 3164, 13, 407, 321, 14479, 411, 257, 777, 9284, 51688], "temperature":
  0.0, "avg_logprob": -0.2548856642639753, "compression_ratio": 1.5714285714285714,
  "no_speech_prob": 0.012339537963271141}, {"id": 148, "seek": 98132, "start": 981.32,
  "end": 987.1600000000001, "text": " called BUDGPQ. I will also link in the show
  notes like the blog post about it. So we increased", "tokens": [50364, 1219, 363,
  9438, 38, 47, 48, 13, 286, 486, 611, 2113, 294, 264, 855, 5570, 411, 264, 6968,
  2183, 466, 309, 13, 407, 321, 6505, 50656], "temperature": 0.0, "avg_logprob": -0.24560226650413022,
  "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0063160983845591545},
  {"id": 149, "seek": 98132, "start": 987.1600000000001, "end": 997.72, "text": "
  recall by 12% over FIES model. So yeah, FIES algorithm. So yeah, I think it''s great
  that you guys", "tokens": [50656, 9901, 538, 2272, 4, 670, 479, 40, 2358, 2316,
  13, 407, 1338, 11, 479, 40, 2358, 9284, 13, 407, 1338, 11, 286, 519, 309, 311, 869,
  300, 291, 1074, 51184], "temperature": 0.0, "avg_logprob": -0.24560226650413022,
  "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0063160983845591545},
  {"id": 150, "seek": 98132, "start": 997.72, "end": 1003.32, "text": " also inventing.
  I don''t know if we are testing to this billion scale. I think we are more in the",
  "tokens": [51184, 611, 7962, 278, 13, 286, 500, 380, 458, 498, 321, 366, 4997, 281,
  341, 5218, 4373, 13, 286, 519, 321, 366, 544, 294, 264, 51464], "temperature": 0.0,
  "avg_logprob": -0.24560226650413022, "compression_ratio": 1.6058091286307055, "no_speech_prob":
  0.0063160983845591545}, {"id": 151, "seek": 98132, "start": 1003.32, "end": 1009.96,
  "text": " million scale. Yeah, actually, we also ventured into billion scale, but
  in the process we figured", "tokens": [51464, 2459, 4373, 13, 865, 11, 767, 11,
  321, 611, 6931, 3831, 666, 5218, 4373, 11, 457, 294, 264, 1399, 321, 8932, 51796],
  "temperature": 0.0, "avg_logprob": -0.24560226650413022, "compression_ratio": 1.6058091286307055,
  "no_speech_prob": 0.0063160983845591545}, {"id": 152, "seek": 100996, "start": 1010.12,
  "end": 1014.84, "text": " out a solution for million scale. So it''s not for billion
  years. We don''t know yet if we can", "tokens": [50372, 484, 257, 3827, 337, 2459,
  4373, 13, 407, 309, 311, 406, 337, 5218, 924, 13, 492, 500, 380, 458, 1939, 498,
  321, 393, 50608], "temperature": 0.0, "avg_logprob": -0.15596600236563846, "compression_ratio":
  1.6513409961685823, "no_speech_prob": 0.004739090800285339}, {"id": 153, "seek":
  100996, "start": 1014.84, "end": 1018.44, "text": " generalize to that level, but
  I think we can with some additional research.", "tokens": [50608, 2674, 1125, 281,
  300, 1496, 11, 457, 286, 519, 321, 393, 365, 512, 4497, 2132, 13, 50788], "temperature":
  0.0, "avg_logprob": -0.15596600236563846, "compression_ratio": 1.6513409961685823,
  "no_speech_prob": 0.004739090800285339}, {"id": 154, "seek": 100996, "start": 1019.88,
  "end": 1023.48, "text": " Well, this is the first version. So for sure, we will
  try to improve it.", "tokens": [50860, 1042, 11, 341, 307, 264, 700, 3037, 13, 407,
  337, 988, 11, 321, 486, 853, 281, 3470, 309, 13, 51040], "temperature": 0.0, "avg_logprob":
  -0.15596600236563846, "compression_ratio": 1.6513409961685823, "no_speech_prob":
  0.004739090800285339}, {"id": 155, "seek": 100996, "start": 1024.1200000000001,
  "end": 1030.3600000000001, "text": " Yeah, awesome. Awesome. This is great. And
  have you also helped customers to like train models?", "tokens": [51072, 865, 11,
  3476, 13, 10391, 13, 639, 307, 869, 13, 400, 362, 291, 611, 4254, 4581, 281, 411,
  3847, 5245, 30, 51384], "temperature": 0.0, "avg_logprob": -0.15596600236563846,
  "compression_ratio": 1.6513409961685823, "no_speech_prob": 0.004739090800285339},
  {"id": 156, "seek": 100996, "start": 1032.04, "end": 1037.0, "text": " No, but we
  don''t, we didn''t help customers. Well, we did from our solution point of view,
  but", "tokens": [51468, 883, 11, 457, 321, 500, 380, 11, 321, 994, 380, 854, 4581,
  13, 1042, 11, 321, 630, 490, 527, 3827, 935, 295, 1910, 11, 457, 51716], "temperature":
  0.0, "avg_logprob": -0.15596600236563846, "compression_ratio": 1.6513409961685823,
  "no_speech_prob": 0.004739090800285339}, {"id": 157, "seek": 103700, "start": 1037.0,
  "end": 1041.64, "text": " this is an interesting topic because this is something
  that of the, this is one of the pains that", "tokens": [50364, 341, 307, 364, 1880,
  4829, 570, 341, 307, 746, 300, 295, 264, 11, 341, 307, 472, 295, 264, 29774, 300,
  50596], "temperature": 0.0, "avg_logprob": -0.23163244226476648, "compression_ratio":
  1.613733905579399, "no_speech_prob": 0.0018255988834425807}, {"id": 158, "seek":
  103700, "start": 1041.64, "end": 1049.72, "text": " we found quite often with our
  users. Like it was easy for them to go to that level, 70% let''s say", "tokens":
  [50596, 321, 1352, 1596, 2049, 365, 527, 5022, 13, 1743, 309, 390, 1858, 337, 552,
  281, 352, 281, 300, 1496, 11, 5285, 4, 718, 311, 584, 51000], "temperature": 0.0,
  "avg_logprob": -0.23163244226476648, "compression_ratio": 1.613733905579399, "no_speech_prob":
  0.0018255988834425807}, {"id": 159, "seek": 103700, "start": 1049.72, "end": 1056.52,
  "text": " of accuracy with any deep learning model that all these tech giants have
  developed, right?", "tokens": [51000, 295, 14170, 365, 604, 2452, 2539, 2316, 300,
  439, 613, 7553, 31894, 362, 4743, 11, 558, 30, 51340], "temperature": 0.0, "avg_logprob":
  -0.23163244226476648, "compression_ratio": 1.613733905579399, "no_speech_prob":
  0.0018255988834425807}, {"id": 160, "seek": 103700, "start": 1056.52, "end": 1063.0,
  "text": " But we believe that this last mile, this transfer learning part is important.
  And we are,", "tokens": [51340, 583, 321, 1697, 300, 341, 1036, 12620, 11, 341,
  5003, 2539, 644, 307, 1021, 13, 400, 321, 366, 11, 51664], "temperature": 0.0, "avg_logprob":
  -0.23163244226476648, "compression_ratio": 1.613733905579399, "no_speech_prob":
  0.0018255988834425807}, {"id": 161, "seek": 106300, "start": 1063.72, "end": 1070.92,
  "text": " and when we realize we started this project that is we called, well, we
  know it''s already released,", "tokens": [50400, 293, 562, 321, 4325, 321, 1409,
  341, 1716, 300, 307, 321, 1219, 11, 731, 11, 321, 458, 309, 311, 1217, 4736, 11,
  50760], "temperature": 0.0, "avg_logprob": -0.3508202573086353, "compression_ratio":
  1.6212765957446809, "no_speech_prob": 0.0016702304128557444}, {"id": 162, "seek":
  106300, "start": 1070.92, "end": 1078.52, "text": " the fine tuner. Maybe we can
  share that as well, where we try to make it easy for users to", "tokens": [50760,
  264, 2489, 4267, 260, 13, 2704, 321, 393, 2073, 300, 382, 731, 11, 689, 321, 853,
  281, 652, 309, 1858, 337, 5022, 281, 51140], "temperature": 0.0, "avg_logprob":
  -0.3508202573086353, "compression_ratio": 1.6212765957446809, "no_speech_prob":
  0.0016702304128557444}, {"id": 163, "seek": 106300, "start": 1079.48, "end": 1084.84,
  "text": " fine tune their models for a metric learning search applications. And
  they are, and it is also", "tokens": [51188, 2489, 10864, 641, 5245, 337, 257, 20678,
  2539, 3164, 5821, 13, 400, 436, 366, 11, 293, 309, 307, 611, 51456], "temperature":
  0.0, "avg_logprob": -0.3508202573086353, "compression_ratio": 1.6212765957446809,
  "no_speech_prob": 0.0016702304128557444}, {"id": 164, "seek": 106300, "start": 1084.84,
  "end": 1092.04, "text": " framework agnostic for, we support fighters, TensorFlow
  and paddle, paddle. So we realized this", "tokens": [51456, 8388, 623, 77, 19634,
  337, 11, 321, 1406, 19714, 11, 37624, 293, 31834, 11, 31834, 13, 407, 321, 5334,
  341, 51816], "temperature": 0.0, "avg_logprob": -0.3508202573086353, "compression_ratio":
  1.6212765957446809, "no_speech_prob": 0.0016702304128557444}, {"id": 165, "seek":
  109204, "start": 1092.04, "end": 1100.12, "text": " pain point for the users that
  once we have everything running at home, the quality was not as expected.", "tokens":
  [50364, 1822, 935, 337, 264, 5022, 300, 1564, 321, 362, 1203, 2614, 412, 1280, 11,
  264, 3125, 390, 406, 382, 5176, 13, 50768], "temperature": 0.0, "avg_logprob": -0.2245252245948428,
  "compression_ratio": 1.6262626262626263, "no_speech_prob": 0.0024672262370586395},
  {"id": 166, "seek": 109204, "start": 1100.12, "end": 1105.1599999999999, "text":
  " And this, and we are trying to get to help the user in our ecosystem to get to
  this,", "tokens": [50768, 400, 341, 11, 293, 321, 366, 1382, 281, 483, 281, 854,
  264, 4195, 294, 527, 11311, 281, 483, 281, 341, 11, 51020], "temperature": 0.0,
  "avg_logprob": -0.2245252245948428, "compression_ratio": 1.6262626262626263, "no_speech_prob":
  0.0024672262370586395}, {"id": 167, "seek": 109204, "start": 1106.2, "end": 1109.24,
  "text": " yeah, to this level by using this fine tuner.", "tokens": [51072, 1338,
  11, 281, 341, 1496, 538, 1228, 341, 2489, 4267, 260, 13, 51224], "temperature":
  0.0, "avg_logprob": -0.2245252245948428, "compression_ratio": 1.6262626262626263,
  "no_speech_prob": 0.0024672262370586395}, {"id": 168, "seek": 109204, "start": 1110.2,
  "end": 1114.68, "text": " So basically, can you can you explain a bit more about
  fine tuner? Like basically what,", "tokens": [51272, 407, 1936, 11, 393, 291, 393,
  291, 2903, 257, 857, 544, 466, 2489, 4267, 260, 30, 1743, 1936, 437, 11, 51496],
  "temperature": 0.0, "avg_logprob": -0.2245252245948428, "compression_ratio": 1.6262626262626263,
  "no_speech_prob": 0.0024672262370586395}, {"id": 169, "seek": 111468, "start": 1115.16,
  "end": 1119.0800000000002, "text": " what input do I need to provide as a user into
  this?", "tokens": [50388, 437, 4846, 360, 286, 643, 281, 2893, 382, 257, 4195, 666,
  341, 30, 50584], "temperature": 0.0, "avg_logprob": -0.19164693078329398, "compression_ratio":
  1.5700934579439252, "no_speech_prob": 0.0013237111270427704}, {"id": 170, "seek":
  111468, "start": 1119.8, "end": 1126.76, "text": " So fine tuner, it could feel
  similar to any fighter dataset, for instance, but we are trying to put", "tokens":
  [50620, 407, 2489, 4267, 260, 11, 309, 727, 841, 2531, 281, 604, 15932, 28872, 11,
  337, 5197, 11, 457, 321, 366, 1382, 281, 829, 50968], "temperature": 0.0, "avg_logprob":
  -0.19164693078329398, "compression_ratio": 1.5700934579439252, "no_speech_prob":
  0.0013237111270427704}, {"id": 171, "seek": 111468, "start": 1126.76, "end": 1132.68,
  "text": " our documents as our as the main citizen of our ecosystem. So you have
  to wrap your", "tokens": [50968, 527, 8512, 382, 527, 382, 264, 2135, 13326, 295,
  527, 11311, 13, 407, 291, 362, 281, 7019, 428, 51264], "temperature": 0.0, "avg_logprob":
  -0.19164693078329398, "compression_ratio": 1.5700934579439252, "no_speech_prob":
  0.0013237111270427704}, {"id": 172, "seek": 111468, "start": 1133.4, "end": 1139.8,
  "text": " any of your data into our document types, which is really easy. So it''s
  something easy to learn and", "tokens": [51300, 604, 295, 428, 1412, 666, 527, 4166,
  3467, 11, 597, 307, 534, 1858, 13, 407, 309, 311, 746, 1858, 281, 1466, 293, 51620],
  "temperature": 0.0, "avg_logprob": -0.19164693078329398, "compression_ratio": 1.5700934579439252,
  "no_speech_prob": 0.0013237111270427704}, {"id": 173, "seek": 113980, "start": 1139.8,
  "end": 1147.72, "text": " easy to use. And then you can fit your models and we have
  made it easy for you to use the most", "tokens": [50364, 1858, 281, 764, 13, 400,
  550, 291, 393, 3318, 428, 5245, 293, 321, 362, 1027, 309, 1858, 337, 291, 281, 764,
  264, 881, 50760], "temperature": 0.0, "avg_logprob": -0.20860824584960938, "compression_ratio":
  1.6555555555555554, "no_speech_prob": 0.004577599931508303}, {"id": 174, "seek":
  113980, "start": 1147.72, "end": 1154.6, "text": " typical, those functions we are
  trying to introduce, hard negative mining. We are trying to make it easy", "tokens":
  [50760, 7476, 11, 729, 6828, 321, 366, 1382, 281, 5366, 11, 1152, 3671, 15512, 13,
  492, 366, 1382, 281, 652, 309, 1858, 51104], "temperature": 0.0, "avg_logprob":
  -0.20860824584960938, "compression_ratio": 1.6555555555555554, "no_speech_prob":
  0.004577599931508303}, {"id": 175, "seek": 113980, "start": 1154.6, "end": 1163.8799999999999,
  "text": " for everyone to solve the common problems when having, when training for,
  for search applications.", "tokens": [51104, 337, 1518, 281, 5039, 264, 2689, 2740,
  562, 1419, 11, 562, 3097, 337, 11, 337, 3164, 5821, 13, 51568], "temperature": 0.0,
  "avg_logprob": -0.20860824584960938, "compression_ratio": 1.6555555555555554, "no_speech_prob":
  0.004577599931508303}, {"id": 176, "seek": 116388, "start": 1163.88, "end": 1170.0400000000002,
  "text": " And we are also trying to make an interactive labeler that helps you interactively
  through an easy", "tokens": [50364, 400, 321, 366, 611, 1382, 281, 652, 364, 15141,
  7645, 260, 300, 3665, 291, 4648, 3413, 807, 364, 1858, 50672], "temperature": 0.0,
  "avg_logprob": -0.19627386150938092, "compression_ratio": 1.6150627615062763, "no_speech_prob":
  0.006436491850763559}, {"id": 177, "seek": 116388, "start": 1170.0400000000002,
  "end": 1177.88, "text": " to use UI and tag similar objects so that you can go together
  with them. Yeah, yeah, so like,", "tokens": [50672, 281, 764, 15682, 293, 6162,
  2531, 6565, 370, 300, 291, 393, 352, 1214, 365, 552, 13, 865, 11, 1338, 11, 370,
  411, 11, 51064], "temperature": 0.0, "avg_logprob": -0.19627386150938092, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.006436491850763559}, {"id": 178, "seek":
  116388, "start": 1179.0800000000002, "end": 1185.3200000000002, "text": " kind of,
  I mean, fine tuning can be a pipeline by itself, right? In itself. So like, how
  do you get", "tokens": [51124, 733, 295, 11, 286, 914, 11, 2489, 15164, 393, 312,
  257, 15517, 538, 2564, 11, 558, 30, 682, 2564, 13, 407, 411, 11, 577, 360, 291,
  483, 51436], "temperature": 0.0, "avg_logprob": -0.19627386150938092, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.006436491850763559}, {"id": 179, "seek":
  116388, "start": 1185.88, "end": 1191.0800000000002, "text": " these data samples
  that you want to fine tune on? And you might have them with full launch or", "tokens":
  [51464, 613, 1412, 10938, 300, 291, 528, 281, 2489, 10864, 322, 30, 400, 291, 1062,
  362, 552, 365, 1577, 4025, 420, 51724], "temperature": 0.0, "avg_logprob": -0.19627386150938092,
  "compression_ratio": 1.6150627615062763, "no_speech_prob": 0.006436491850763559},
  {"id": 180, "seek": 119108, "start": 1191.8, "end": 1197.96, "text": " during test,
  after launch, and it''s like, you know, the cycle and flywheel of success, so to
  say,", "tokens": [50400, 1830, 1500, 11, 934, 4025, 11, 293, 309, 311, 411, 11,
  291, 458, 11, 264, 6586, 293, 3603, 22830, 295, 2245, 11, 370, 281, 584, 11, 50708],
  "temperature": 0.0, "avg_logprob": -0.23574898974730238, "compression_ratio": 1.6581196581196582,
  "no_speech_prob": 0.006464001722633839}, {"id": 181, "seek": 119108, "start": 1198.6799999999998,
  "end": 1205.32, "text": " right? So do you cover like the full workflow until production,
  including production, or is it", "tokens": [50744, 558, 30, 407, 360, 291, 2060,
  411, 264, 1577, 20993, 1826, 4265, 11, 3009, 4265, 11, 420, 307, 309, 51076], "temperature":
  0.0, "avg_logprob": -0.23574898974730238, "compression_ratio": 1.6581196581196582,
  "no_speech_prob": 0.006464001722633839}, {"id": 182, "seek": 119108, "start": 1205.32,
  "end": 1213.3999999999999, "text": " like pre-production? So for now, we are using
  the just embedding model. And just to get embeddings that", "tokens": [51076, 411,
  659, 12, 40827, 30, 407, 337, 586, 11, 321, 366, 1228, 264, 445, 12240, 3584, 2316,
  13, 400, 445, 281, 483, 12240, 29432, 300, 51480], "temperature": 0.0, "avg_logprob":
  -0.23574898974730238, "compression_ratio": 1.6581196581196582, "no_speech_prob":
  0.006464001722633839}, {"id": 183, "seek": 119108, "start": 1213.3999999999999,
  "end": 1219.72, "text": " get better semantics out of your data set of your specific
  use case. But we are in a really", "tokens": [51480, 483, 1101, 4361, 45298, 484,
  295, 428, 1412, 992, 295, 428, 2685, 764, 1389, 13, 583, 321, 366, 294, 257, 534,
  51796], "temperature": 0.0, "avg_logprob": -0.23574898974730238, "compression_ratio":
  1.6581196581196582, "no_speech_prob": 0.006464001722633839}, {"id": 184, "seek":
  121972, "start": 1220.52, "end": 1225.24, "text": " thing, it''s easier to point
  to release or something around in there, so there''s a long way to go.", "tokens":
  [50404, 551, 11, 309, 311, 3571, 281, 935, 281, 4374, 420, 746, 926, 294, 456, 11,
  370, 456, 311, 257, 938, 636, 281, 352, 13, 50640], "temperature": 0.0, "avg_logprob":
  -0.2805020986509717, "compression_ratio": 1.6981818181818182, "no_speech_prob":
  0.004531599581241608}, {"id": 185, "seek": 121972, "start": 1225.72, "end": 1229.32,
  "text": " Yeah, for sure. But I mean, the direction is fantastic because that''s
  exactly what,", "tokens": [50664, 865, 11, 337, 988, 13, 583, 286, 914, 11, 264,
  3513, 307, 5456, 570, 300, 311, 2293, 437, 11, 50844], "temperature": 0.0, "avg_logprob":
  -0.2805020986509717, "compression_ratio": 1.6981818181818182, "no_speech_prob":
  0.004531599581241608}, {"id": 186, "seek": 121972, "start": 1229.32, "end": 1235.64,
  "text": " what addresses the real need, any user, like fine tuning. Like it''s all
  fancy to take like", "tokens": [50844, 437, 16862, 264, 957, 643, 11, 604, 4195,
  11, 411, 2489, 15164, 13, 1743, 309, 311, 439, 10247, 281, 747, 411, 51160], "temperature":
  0.0, "avg_logprob": -0.2805020986509717, "compression_ratio": 1.6981818181818182,
  "no_speech_prob": 0.004531599581241608}, {"id": 187, "seek": 121972, "start": 1235.64,
  "end": 1241.32, "text": " a hugging case model or whatever, but like fine tuning
  it to the level when you''re users beloved,", "tokens": [51160, 257, 41706, 1389,
  2316, 420, 2035, 11, 457, 411, 2489, 15164, 309, 281, 264, 1496, 562, 291, 434,
  5022, 14553, 11, 51444], "temperature": 0.0, "avg_logprob": -0.2805020986509717,
  "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.004531599581241608},
  {"id": 188, "seek": 121972, "start": 1241.32, "end": 1248.2, "text": " that''s a
  different story. Yeah, that sounds great. But I also wanted to come back to your,
  like,", "tokens": [51444, 300, 311, 257, 819, 1657, 13, 865, 11, 300, 3263, 869,
  13, 583, 286, 611, 1415, 281, 808, 646, 281, 428, 11, 411, 11, 51788], "temperature":
  0.0, "avg_logprob": -0.2805020986509717, "compression_ratio": 1.6981818181818182,
  "no_speech_prob": 0.004531599581241608}, {"id": 189, "seek": 124820, "start": 1248.2,
  "end": 1253.72, "text": " you mentioned that Gina AI doesn''t kind of compare to
  vector databases, but I do get sometimes", "tokens": [50364, 291, 2835, 300, 34711,
  7318, 1177, 380, 733, 295, 6794, 281, 8062, 22380, 11, 457, 286, 360, 483, 2171,
  50640], "temperature": 0.0, "avg_logprob": -0.1569671630859375, "compression_ratio":
  1.6363636363636365, "no_speech_prob": 0.004010406788438559}, {"id": 190, "seek":
  124820, "start": 1253.72, "end": 1260.3600000000001, "text": " questions like how
  do these systems compare to each other? And you may or may not know, I''ve", "tokens":
  [50640, 1651, 411, 577, 360, 613, 3652, 6794, 281, 1184, 661, 30, 400, 291, 815,
  420, 815, 406, 458, 11, 286, 600, 50972], "temperature": 0.0, "avg_logprob": -0.1569671630859375,
  "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.004010406788438559},
  {"id": 191, "seek": 124820, "start": 1260.3600000000001, "end": 1265.56, "text":
  " blocked about all vector databases I knew to that point and turns out they''ve
  been six and then", "tokens": [50972, 15470, 466, 439, 8062, 22380, 286, 2586, 281,
  300, 935, 293, 4523, 484, 436, 600, 668, 2309, 293, 550, 51232], "temperature":
  0.0, "avg_logprob": -0.1569671630859375, "compression_ratio": 1.6363636363636365,
  "no_speech_prob": 0.004010406788438559}, {"id": 192, "seek": 124820, "start": 1265.56,
  "end": 1271.56, "text": " the seventh one knocked on the door, so it''s also now
  on the blog. But I didn''t cover Gina AI,", "tokens": [51232, 264, 17875, 472, 16914,
  322, 264, 2853, 11, 370, 309, 311, 611, 586, 322, 264, 6968, 13, 583, 286, 994,
  380, 2060, 34711, 7318, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1569671630859375,
  "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.004010406788438559},
  {"id": 193, "seek": 127156, "start": 1271.56, "end": 1280.6799999999998, "text":
  " I didn''t cover deep sets haystack because I thought that Gina and haystack, they''re
  like layers", "tokens": [50364, 286, 994, 380, 2060, 2452, 6352, 4842, 372, 501,
  570, 286, 1194, 300, 34711, 293, 4842, 372, 501, 11, 436, 434, 411, 7914, 50820],
  "temperature": 0.0, "avg_logprob": -0.18398407178047377, "compression_ratio": 1.4871794871794872,
  "no_speech_prob": 0.003168991534039378}, {"id": 194, "seek": 127156, "start": 1280.6799999999998,
  "end": 1288.6, "text": " above a vector database. Is that the right thinking? Yes,
  I think it makes sense. We are, we might", "tokens": [50820, 3673, 257, 8062, 8149,
  13, 1119, 300, 264, 558, 1953, 30, 1079, 11, 286, 519, 309, 1669, 2020, 13, 492,
  366, 11, 321, 1062, 51216], "temperature": 0.0, "avg_logprob": -0.18398407178047377,
  "compression_ratio": 1.4871794871794872, "no_speech_prob": 0.003168991534039378},
  {"id": 195, "seek": 127156, "start": 1288.6, "end": 1296.6, "text": " try to develop
  our solutions for the use cases that we may feel more worth. So that is, I mean,",
  "tokens": [51216, 853, 281, 1499, 527, 6547, 337, 264, 764, 3331, 300, 321, 815,
  841, 544, 3163, 13, 407, 300, 307, 11, 286, 914, 11, 51616], "temperature": 0.0,
  "avg_logprob": -0.18398407178047377, "compression_ratio": 1.4871794871794872, "no_speech_prob":
  0.003168991534039378}, {"id": 196, "seek": 129660, "start": 1296.6, "end": 1303.56,
  "text": " the one is out there to do, but yeah, I think it''s right. We are trying
  to, I think, vector databases", "tokens": [50364, 264, 472, 307, 484, 456, 281,
  360, 11, 457, 1338, 11, 286, 519, 309, 311, 558, 13, 492, 366, 1382, 281, 11, 286,
  519, 11, 8062, 22380, 50712], "temperature": 0.0, "avg_logprob": -0.18911768465625997,
  "compression_ratio": 1.7630331753554502, "no_speech_prob": 0.0485207661986351},
  {"id": 197, "seek": 129660, "start": 1303.56, "end": 1309.0, "text": " cover one
  of the parts or one of the challenges, maybe one of the main challenges of vector
  search", "tokens": [50712, 2060, 472, 295, 264, 3166, 420, 472, 295, 264, 4759,
  11, 1310, 472, 295, 264, 2135, 4759, 295, 8062, 3164, 50984], "temperature": 0.0,
  "avg_logprob": -0.18911768465625997, "compression_ratio": 1.7630331753554502, "no_speech_prob":
  0.0485207661986351}, {"id": 198, "seek": 129660, "start": 1309.0, "end": 1313.9599999999998,
  "text": " or neural search, but we try to see the whole scope and the whole pipeline.
  So,", "tokens": [50984, 420, 18161, 3164, 11, 457, 321, 853, 281, 536, 264, 1379,
  11923, 293, 264, 1379, 15517, 13, 407, 11, 51232], "temperature": 0.0, "avg_logprob":
  -0.18911768465625997, "compression_ratio": 1.7630331753554502, "no_speech_prob":
  0.0485207661986351}, {"id": 199, "seek": 129660, "start": 1315.08, "end": 1321.7199999999998,
  "text": " in Gina, we can use, you can wrap some client that will use any of the
  big vector searches,", "tokens": [51288, 294, 34711, 11, 321, 393, 764, 11, 291,
  393, 7019, 512, 6423, 300, 486, 764, 604, 295, 264, 955, 8062, 26701, 11, 51620],
  "temperature": 0.0, "avg_logprob": -0.18911768465625997, "compression_ratio": 1.7630331753554502,
  "no_speech_prob": 0.0485207661986351}, {"id": 200, "seek": 132172, "start": 1321.72,
  "end": 1326.76, "text": " big data research of how there have you done any integration
  with some vector database?", "tokens": [50364, 955, 1412, 2132, 295, 577, 456, 362,
  291, 1096, 604, 10980, 365, 512, 8062, 8149, 30, 50616], "temperature": 0.0, "avg_logprob":
  -0.2602545304731889, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.004188275430351496},
  {"id": 201, "seek": 132172, "start": 1328.1200000000001, "end": 1333.16, "text":
  " Not ourselves right now, but it would be, we might do it in the future.", "tokens":
  [50684, 1726, 4175, 558, 586, 11, 457, 309, 576, 312, 11, 321, 1062, 360, 309, 294,
  264, 2027, 13, 50936], "temperature": 0.0, "avg_logprob": -0.2602545304731889, "compression_ratio":
  1.678030303030303, "no_speech_prob": 0.004188275430351496}, {"id": 202, "seek":
  132172, "start": 1333.16, "end": 1339.0, "text": " Okay, yeah, because for now,
  you did mention that you offer GNN and algorithms, which to me", "tokens": [50936,
  1033, 11, 1338, 11, 570, 337, 586, 11, 291, 630, 2152, 300, 291, 2626, 460, 45,
  45, 293, 14642, 11, 597, 281, 385, 51228], "temperature": 0.0, "avg_logprob": -0.2602545304731889,
  "compression_ratio": 1.678030303030303, "no_speech_prob": 0.004188275430351496},
  {"id": 203, "seek": 132172, "start": 1339.0, "end": 1344.28, "text": " sounds like
  a core building block of vector database, but then of course in vector database,",
  "tokens": [51228, 3263, 411, 257, 4965, 2390, 3461, 295, 8062, 8149, 11, 457, 550,
  295, 1164, 294, 8062, 8149, 11, 51492], "temperature": 0.0, "avg_logprob": -0.2602545304731889,
  "compression_ratio": 1.678030303030303, "no_speech_prob": 0.004188275430351496},
  {"id": 204, "seek": 132172, "start": 1344.28, "end": 1349.4, "text": " you have
  many more things, right? Like, where do you store objects? How you store them? What
  about", "tokens": [51492, 291, 362, 867, 544, 721, 11, 558, 30, 1743, 11, 689, 360,
  291, 3531, 6565, 30, 1012, 291, 3531, 552, 30, 708, 466, 51748], "temperature":
  0.0, "avg_logprob": -0.2602545304731889, "compression_ratio": 1.678030303030303,
  "no_speech_prob": 0.004188275430351496}, {"id": 205, "seek": 134940, "start": 1349.4,
  "end": 1358.2800000000002, "text": " filters and so on? But we are trying to cover
  from the, for instance, we are not some,", "tokens": [50364, 15995, 293, 370, 322,
  30, 583, 321, 366, 1382, 281, 2060, 490, 264, 11, 337, 5197, 11, 321, 366, 406,
  512, 11, 50808], "temperature": 0.0, "avg_logprob": -0.2384679424228953, "compression_ratio":
  1.4943181818181819, "no_speech_prob": 0.0033319646026939154}, {"id": 206, "seek":
  134940, "start": 1358.2800000000002, "end": 1364.2, "text": " some people for some
  use cases, and just exactly as neighbor search might work just fine,", "tokens":
  [50808, 512, 561, 337, 512, 764, 3331, 11, 293, 445, 2293, 382, 5987, 3164, 1062,
  589, 445, 2489, 11, 51104], "temperature": 0.0, "avg_logprob": -0.2384679424228953,
  "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.0033319646026939154},
  {"id": 207, "seek": 134940, "start": 1364.2, "end": 1371.24, "text": " and they
  don''t need to worry about configuring fancy A&N models for their recall speed",
  "tokens": [51104, 293, 436, 500, 380, 643, 281, 3292, 466, 6662, 1345, 10247, 316,
  5, 45, 5245, 337, 641, 9901, 3073, 51456], "temperature": 0.0, "avg_logprob": -0.2384679424228953,
  "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.0033319646026939154},
  {"id": 208, "seek": 137124, "start": 1372.2, "end": 1379.56, "text": " requirements.
  So, I think there is room for everyone. So, I think it just, you have to offer",
  "tokens": [50412, 7728, 13, 407, 11, 286, 519, 456, 307, 1808, 337, 1518, 13, 407,
  11, 286, 519, 309, 445, 11, 291, 362, 281, 2626, 50780], "temperature": 0.0, "avg_logprob":
  -0.2002204123963701, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.011460872367024422}, {"id": 209, "seek": 137124, "start": 1380.2, "end": 1385.32,
  "text": " what is right for the right use case and the right need. Yeah, of course.
  And by the way,", "tokens": [50812, 437, 307, 558, 337, 264, 558, 764, 1389, 293,
  264, 558, 643, 13, 865, 11, 295, 1164, 13, 400, 538, 264, 636, 11, 51068], "temperature":
  0.0, "avg_logprob": -0.2002204123963701, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.011460872367024422}, {"id": 210, "seek": 137124, "start": 1385.32,
  "end": 1392.36, "text": " what''s the core programming language used in Gina? So,
  our core programming language is Python,", "tokens": [51068, 437, 311, 264, 4965,
  9410, 2856, 1143, 294, 34711, 30, 407, 11, 527, 4965, 9410, 2856, 307, 15329, 11,
  51420], "temperature": 0.0, "avg_logprob": -0.2002204123963701, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.011460872367024422}, {"id": 211, "seek":
  137124, "start": 1392.36, "end": 1399.88, "text": " because we are more like, since
  we are this pipeline and we are like a glue ecosystem,", "tokens": [51420, 570,
  321, 366, 544, 411, 11, 1670, 321, 366, 341, 15517, 293, 321, 366, 411, 257, 8998,
  11311, 11, 51796], "temperature": 0.0, "avg_logprob": -0.2002204123963701, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.011460872367024422}, {"id": 212, "seek":
  139988, "start": 1399.96, "end": 1407.5600000000002, "text": " most of our operations
  are wrapping models that run in optimized languages or something,", "tokens": [50368,
  881, 295, 527, 7705, 366, 21993, 5245, 300, 1190, 294, 26941, 8650, 420, 746, 11,
  50748], "temperature": 0.0, "avg_logprob": -0.15994265805120053, "compression_ratio":
  1.543103448275862, "no_speech_prob": 0.002710112603381276}, {"id": 213, "seek":
  139988, "start": 1407.5600000000002, "end": 1413.8000000000002, "text": " and that
  also Python helps us to iterate really fast, which other languages might slow us
  down.", "tokens": [50748, 293, 300, 611, 15329, 3665, 505, 281, 44497, 534, 2370,
  11, 597, 661, 8650, 1062, 2964, 505, 760, 13, 51060], "temperature": 0.0, "avg_logprob":
  -0.15994265805120053, "compression_ratio": 1.543103448275862, "no_speech_prob":
  0.002710112603381276}, {"id": 214, "seek": 139988, "start": 1414.3600000000001,
  "end": 1419.96, "text": " Yeah, that''s true. And does it also apply to the A&N
  algorithms that you mentioned,", "tokens": [51088, 865, 11, 300, 311, 2074, 13,
  400, 775, 309, 611, 3079, 281, 264, 316, 5, 45, 14642, 300, 291, 2835, 11, 51368],
  "temperature": 0.0, "avg_logprob": -0.15994265805120053, "compression_ratio": 1.543103448275862,
  "no_speech_prob": 0.002710112603381276}, {"id": 215, "seek": 139988, "start": 1419.96,
  "end": 1427.8000000000002, "text": " like BQLite? Is it also Python? I don''t know
  if we are, for instance, I think we are also", "tokens": [51368, 411, 363, 48, 43,
  642, 30, 1119, 309, 611, 15329, 30, 286, 500, 380, 458, 498, 321, 366, 11, 337,
  5197, 11, 286, 519, 321, 366, 611, 51760], "temperature": 0.0, "avg_logprob": -0.15994265805120053,
  "compression_ratio": 1.543103448275862, "no_speech_prob": 0.002710112603381276},
  {"id": 216, "seek": 142780, "start": 1427.8799999999999, "end": 1437.56, "text":
  " using some bindings for H&N. So, you are using probably C++ version of H&N SW
  binding to Python,", "tokens": [50368, 1228, 512, 14786, 1109, 337, 389, 5, 45,
  13, 407, 11, 291, 366, 1228, 1391, 383, 25472, 3037, 295, 389, 5, 45, 20346, 17359,
  281, 15329, 11, 50852], "temperature": 0.0, "avg_logprob": -0.28086015913221574,
  "compression_ratio": 1.4793814432989691, "no_speech_prob": 0.003570443019270897},
  {"id": 217, "seek": 142780, "start": 1437.56, "end": 1444.2, "text": " right? Yes,
  that''s for sure. But I don''t know if some of, for the H&N SWD, yes, for some other",
  "tokens": [50852, 558, 30, 1079, 11, 300, 311, 337, 988, 13, 583, 286, 500, 380,
  458, 498, 512, 295, 11, 337, 264, 389, 5, 45, 20346, 35, 11, 2086, 11, 337, 512,
  661, 51184], "temperature": 0.0, "avg_logprob": -0.28086015913221574, "compression_ratio":
  1.4793814432989691, "no_speech_prob": 0.003570443019270897}, {"id": 218, "seek":
  142780, "start": 1444.2, "end": 1453.0, "text": " parts, I don''t know, we are trying
  to optimize whatever we find. Yeah, but it sounds cool that,", "tokens": [51184,
  3166, 11, 286, 500, 380, 458, 11, 321, 366, 1382, 281, 19719, 2035, 321, 915, 13,
  865, 11, 457, 309, 3263, 1627, 300, 11, 51624], "temperature": 0.0, "avg_logprob":
  -0.28086015913221574, "compression_ratio": 1.4793814432989691, "no_speech_prob":
  0.003570443019270897}, {"id": 219, "seek": 145300, "start": 1453.0, "end": 1459.88,
  "text": " you know, if we still continue kind of this comparison a little bit between
  Gina and vector databases,", "tokens": [50364, 291, 458, 11, 498, 321, 920, 2354,
  733, 295, 341, 9660, 257, 707, 857, 1296, 34711, 293, 8062, 22380, 11, 50708], "temperature":
  0.0, "avg_logprob": -0.3140570660854908, "compression_ratio": 1.5360360360360361,
  "no_speech_prob": 0.010131534188985825}, {"id": 220, "seek": 145300, "start": 1460.84,
  "end": 1464.68, "text": " like vector databases, if you pick them, let''s say BIAV8
  is implemented in Go,", "tokens": [50756, 411, 8062, 22380, 11, 498, 291, 1888,
  552, 11, 718, 311, 584, 363, 6914, 53, 23, 307, 12270, 294, 1037, 11, 50948], "temperature":
  0.0, "avg_logprob": -0.3140570660854908, "compression_ratio": 1.5360360360360361,
  "no_speech_prob": 0.010131534188985825}, {"id": 221, "seek": 145300, "start": 1466.36,
  "end": 1471.56, "text": " what grant is implemented in Rust? So, these are compiling
  languages, right? So,", "tokens": [51032, 437, 6386, 307, 12270, 294, 34952, 30,
  407, 11, 613, 366, 715, 4883, 8650, 11, 558, 30, 407, 11, 51292], "temperature":
  0.0, "avg_logprob": -0.3140570660854908, "compression_ratio": 1.5360360360360361,
  "no_speech_prob": 0.010131534188985825}, {"id": 222, "seek": 145300, "start": 1472.6,
  "end": 1481.72, "text": " VESPA is like Java plus some C, I think, C++ and mostly
  Java. So, like, nobody", "tokens": [51344, 691, 2358, 10297, 307, 411, 10745, 1804,
  512, 383, 11, 286, 519, 11, 383, 25472, 293, 5240, 10745, 13, 407, 11, 411, 11,
  5079, 51800], "temperature": 0.0, "avg_logprob": -0.3140570660854908, "compression_ratio":
  1.5360360360360361, "no_speech_prob": 0.010131534188985825}, {"id": 223, "seek":
  148172, "start": 1481.72, "end": 1488.04, "text": " implements the vector search
  in pure Python, because it''s very, it''s going to be very", "tokens": [50364, 704,
  17988, 264, 8062, 3164, 294, 6075, 15329, 11, 570, 309, 311, 588, 11, 309, 311,
  516, 281, 312, 588, 50680], "temperature": 0.0, "avg_logprob": -0.265682578086853,
  "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.0032989559695124626},
  {"id": 224, "seek": 148172, "start": 1488.04, "end": 1493.72, "text": " taxing on
  the latency, you know? Sure. No, but the expensive operation, we are not running.",
  "tokens": [50680, 3366, 278, 322, 264, 27043, 11, 291, 458, 30, 4894, 13, 883, 11,
  457, 264, 5124, 6916, 11, 321, 366, 406, 2614, 13, 50964], "temperature": 0.0, "avg_logprob":
  -0.265682578086853, "compression_ratio": 1.6088888888888888, "no_speech_prob": 0.0032989559695124626},
  {"id": 225, "seek": 148172, "start": 1493.72, "end": 1499.56, "text": " So, for
  instance, the nearest neighbor search we are doing, we are based on NAMPA operations,",
  "tokens": [50964, 407, 11, 337, 5197, 11, 264, 23831, 5987, 3164, 321, 366, 884,
  11, 321, 366, 2361, 322, 426, 2865, 10297, 7705, 11, 51256], "temperature": 0.0,
  "avg_logprob": -0.265682578086853, "compression_ratio": 1.6088888888888888, "no_speech_prob":
  0.0032989559695124626}, {"id": 226, "seek": 148172, "start": 1499.56, "end": 1504.2,
  "text": " which are optimized at NAMPA level, and the approximations neighbors,
  I think, most of the", "tokens": [51256, 597, 366, 26941, 412, 426, 2865, 10297,
  1496, 11, 293, 264, 8542, 763, 12512, 11, 286, 519, 11, 881, 295, 264, 51488], "temperature":
  0.0, "avg_logprob": -0.265682578086853, "compression_ratio": 1.6088888888888888,
  "no_speech_prob": 0.0032989559695124626}, {"id": 227, "seek": 150420, "start": 1504.68,
  "end": 1511.0800000000002, "text": " heavy lifting is done on the C++ level from,
  I''m just covering our bindings.", "tokens": [50388, 4676, 15798, 307, 1096, 322,
  264, 383, 25472, 1496, 490, 11, 286, 478, 445, 10322, 527, 14786, 1109, 13, 50708],
  "temperature": 0.0, "avg_logprob": -0.24465321559532016, "compression_ratio": 1.5338983050847457,
  "no_speech_prob": 0.008307828567922115}, {"id": 228, "seek": 150420, "start": 1511.88,
  "end": 1517.72, "text": " Yeah, and I''m still curious about BQ Lite, like, is it
  the C or is it Python, but I think we need", "tokens": [50748, 865, 11, 293, 286,
  478, 920, 6369, 466, 363, 48, 32986, 11, 411, 11, 307, 309, 264, 383, 420, 307,
  309, 15329, 11, 457, 286, 519, 321, 643, 51040], "temperature": 0.0, "avg_logprob":
  -0.24465321559532016, "compression_ratio": 1.5338983050847457, "no_speech_prob":
  0.008307828567922115}, {"id": 229, "seek": 150420, "start": 1517.72, "end": 1524.1200000000001,
  "text": " to check the documentation. Yes. Yeah, I''m curious because like, I''ve
  actually invented a new", "tokens": [51040, 281, 1520, 264, 14333, 13, 1079, 13,
  865, 11, 286, 478, 6369, 570, 411, 11, 286, 600, 767, 14479, 257, 777, 51360], "temperature":
  0.0, "avg_logprob": -0.24465321559532016, "compression_ratio": 1.5338983050847457,
  "no_speech_prob": 0.008307828567922115}, {"id": 230, "seek": 150420, "start": 1524.1200000000001,
  "end": 1529.96, "text": " algorithm in NAMPA search, but I haven''t published it
  widely, it''s open source, but I haven''t", "tokens": [51360, 9284, 294, 426, 2865,
  10297, 3164, 11, 457, 286, 2378, 380, 6572, 309, 13371, 11, 309, 311, 1269, 4009,
  11, 457, 286, 2378, 380, 51652], "temperature": 0.0, "avg_logprob": -0.24465321559532016,
  "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.008307828567922115},
  {"id": 231, "seek": 152996, "start": 1530.6000000000001, "end": 1535.56, "text":
  " done the thorough benchmarking. And what I''ve faced is that, you know, like,
  in Python, even though", "tokens": [50396, 1096, 264, 12934, 18927, 278, 13, 400,
  437, 286, 600, 11446, 307, 300, 11, 291, 458, 11, 411, 11, 294, 15329, 11, 754,
  1673, 50644], "temperature": 0.0, "avg_logprob": -0.14417409896850586, "compression_ratio":
  1.6213991769547325, "no_speech_prob": 0.007663533557206392}, {"id": 232, "seek":
  152996, "start": 1535.56, "end": 1542.28, "text": " I optimized all parts of the
  algorithm, I''m using preallocation and NAMPA, it still runs out of", "tokens":
  [50644, 286, 26941, 439, 3166, 295, 264, 9284, 11, 286, 478, 1228, 659, 336, 27943,
  293, 426, 2865, 10297, 11, 309, 920, 6676, 484, 295, 50980], "temperature": 0.0,
  "avg_logprob": -0.14417409896850586, "compression_ratio": 1.6213991769547325, "no_speech_prob":
  0.007663533557206392}, {"id": 233, "seek": 152996, "start": 1542.28, "end": 1548.52,
  "text": " memory, runs out of memory as in it leaks memory, and it doesn''t explain,
  like, Python virtual machine", "tokens": [50980, 4675, 11, 6676, 484, 295, 4675,
  382, 294, 309, 28885, 4675, 11, 293, 309, 1177, 380, 2903, 11, 411, 11, 15329, 6374,
  3479, 51292], "temperature": 0.0, "avg_logprob": -0.14417409896850586, "compression_ratio":
  1.6213991769547325, "no_speech_prob": 0.007663533557206392}, {"id": 234, "seek":
  152996, "start": 1548.52, "end": 1552.28, "text": " doesn''t tell you where, like,
  you don''t have tools. Okay, there are some tools, but they''re not", "tokens":
  [51292, 1177, 380, 980, 291, 689, 11, 411, 11, 291, 500, 380, 362, 3873, 13, 1033,
  11, 456, 366, 512, 3873, 11, 457, 436, 434, 406, 51480], "temperature": 0.0, "avg_logprob":
  -0.14417409896850586, "compression_ratio": 1.6213991769547325, "no_speech_prob":
  0.007663533557206392}, {"id": 235, "seek": 155228, "start": 1552.28, "end": 1558.68,
  "text": " useful. Like, no, you''re showing a little stuff. No. So, and I''ve been
  like a little bit like", "tokens": [50364, 4420, 13, 1743, 11, 572, 11, 291, 434,
  4099, 257, 707, 1507, 13, 883, 13, 407, 11, 293, 286, 600, 668, 411, 257, 707, 857,
  411, 50684], "temperature": 0.0, "avg_logprob": -0.2797810993497334, "compression_ratio":
  1.6178571428571429, "no_speech_prob": 0.02168191783130169}, {"id": 236, "seek":
  155228, "start": 1558.68, "end": 1564.2, "text": " desperate, and I''ve been thinking,
  okay, should I now move into RAST GO territory, which might be", "tokens": [50684,
  17601, 11, 293, 286, 600, 668, 1953, 11, 1392, 11, 820, 286, 586, 1286, 666, 497,
  20398, 10365, 11360, 11, 597, 1062, 312, 50960], "temperature": 0.0, "avg_logprob":
  -0.2797810993497334, "compression_ratio": 1.6178571428571429, "no_speech_prob":
  0.02168191783130169}, {"id": 237, "seek": 155228, "start": 1564.2, "end": 1568.92,
  "text": " a little bit more dangerous, like, even though I do have some experience
  in C++, but you know, like,", "tokens": [50960, 257, 707, 857, 544, 5795, 11, 411,
  11, 754, 1673, 286, 360, 362, 512, 1752, 294, 383, 25472, 11, 457, 291, 458, 11,
  411, 11, 51196], "temperature": 0.0, "avg_logprob": -0.2797810993497334, "compression_ratio":
  1.6178571428571429, "no_speech_prob": 0.02168191783130169}, {"id": 238, "seek":
  155228, "start": 1568.92, "end": 1572.12, "text": " do I want to go there now? Like,
  Python is much more comfortable.", "tokens": [51196, 360, 286, 528, 281, 352, 456,
  586, 30, 1743, 11, 15329, 307, 709, 544, 4619, 13, 51356], "temperature": 0.0, "avg_logprob":
  -0.2797810993497334, "compression_ratio": 1.6178571428571429, "no_speech_prob":
  0.02168191783130169}, {"id": 239, "seek": 155228, "start": 1573.16, "end": 1580.92,
  "text": " The, I think, depends on the later you are working with, and it''s, so
  I think that by offering", "tokens": [51408, 440, 11, 286, 519, 11, 5946, 322, 264,
  1780, 291, 366, 1364, 365, 11, 293, 309, 311, 11, 370, 286, 519, 300, 538, 8745,
  51796], "temperature": 0.0, "avg_logprob": -0.2797810993497334, "compression_ratio":
  1.6178571428571429, "no_speech_prob": 0.02168191783130169}, {"id": 240, "seek":
  158092, "start": 1580.92, "end": 1588.2, "text": " Python APIs in the field, if
  machine learning will attract, then we''ll make everybody much easier to use.",
  "tokens": [50364, 15329, 21445, 294, 264, 2519, 11, 498, 3479, 2539, 486, 5049,
  11, 550, 321, 603, 652, 2201, 709, 3571, 281, 764, 13, 50728], "temperature": 0.0,
  "avg_logprob": -0.1616580287615458, "compression_ratio": 1.6056910569105691, "no_speech_prob":
  0.0022548306733369827}, {"id": 241, "seek": 158092, "start": 1588.92, "end": 1597.0800000000002,
  "text": " Then if you get API rights, the API is right, you might then bind it to
  whatever of your favorite", "tokens": [50764, 1396, 498, 291, 483, 9362, 4601, 11,
  264, 9362, 307, 558, 11, 291, 1062, 550, 14786, 309, 281, 2035, 295, 428, 2954,
  51172], "temperature": 0.0, "avg_logprob": -0.1616580287615458, "compression_ratio":
  1.6056910569105691, "no_speech_prob": 0.0022548306733369827}, {"id": 242, "seek":
  158092, "start": 1597.0800000000002, "end": 1602.68, "text": " languages, but I
  think getting the comfortable API for that developer to use and to love using",
  "tokens": [51172, 8650, 11, 457, 286, 519, 1242, 264, 4619, 9362, 337, 300, 10754,
  281, 764, 293, 281, 959, 1228, 51452], "temperature": 0.0, "avg_logprob": -0.1616580287615458,
  "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0022548306733369827},
  {"id": 243, "seek": 158092, "start": 1602.68, "end": 1609.0800000000002, "text":
  " is one of the key first steps. So, do you invest a lot into building these APIs?
  Can you give an", "tokens": [51452, 307, 472, 295, 264, 2141, 700, 4439, 13, 407,
  11, 360, 291, 1963, 257, 688, 666, 2390, 613, 21445, 30, 1664, 291, 976, 364, 51772],
  "temperature": 0.0, "avg_logprob": -0.1616580287615458, "compression_ratio": 1.6056910569105691,
  "no_speech_prob": 0.0022548306733369827}, {"id": 244, "seek": 160908, "start": 1609.08,
  "end": 1615.3999999999999, "text": " example of like some API within Gina that kind
  of makes the workflow easier for?", "tokens": [50364, 1365, 295, 411, 512, 9362,
  1951, 34711, 300, 733, 295, 1669, 264, 20993, 3571, 337, 30, 50680], "temperature":
  0.0, "avg_logprob": -0.1389803575432819, "compression_ratio": 1.6478260869565218,
  "no_speech_prob": 0.000956216361373663}, {"id": 245, "seek": 160908, "start": 1615.3999999999999,
  "end": 1620.6, "text": " So, for instance, we are trying to improve a lot in this
  document. So, documents are our central", "tokens": [50680, 407, 11, 337, 5197,
  11, 321, 366, 1382, 281, 3470, 257, 688, 294, 341, 4166, 13, 407, 11, 8512, 366,
  527, 5777, 50940], "temperature": 0.0, "avg_logprob": -0.1389803575432819, "compression_ratio":
  1.6478260869565218, "no_speech_prob": 0.000956216361373663}, {"id": 246, "seek":
  160908, "start": 1620.6, "end": 1627.8, "text": " logic, and documents are raised
  that these are the two core members of our family in the ecosystem.", "tokens":
  [50940, 9952, 11, 293, 8512, 366, 6005, 300, 613, 366, 264, 732, 4965, 2679, 295,
  527, 1605, 294, 264, 11311, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1389803575432819,
  "compression_ratio": 1.6478260869565218, "no_speech_prob": 0.000956216361373663},
  {"id": 247, "seek": 160908, "start": 1627.8, "end": 1636.28, "text": " So, we are
  spending a lot of time on making them easy to use. For instance, with this fluent
  pattern,", "tokens": [51300, 407, 11, 321, 366, 6434, 257, 688, 295, 565, 322, 1455,
  552, 1858, 281, 764, 13, 1171, 5197, 11, 365, 341, 40799, 5102, 11, 51724], "temperature":
  0.0, "avg_logprob": -0.1389803575432819, "compression_ratio": 1.6478260869565218,
  "no_speech_prob": 0.000956216361373663}, {"id": 248, "seek": 163628, "start": 1636.28,
  "end": 1641.48, "text": " we are trying to invest a lot of time on finding the best
  way, the more Python way to work on it.", "tokens": [50364, 321, 366, 1382, 281,
  1963, 257, 688, 295, 565, 322, 5006, 264, 1151, 636, 11, 264, 544, 15329, 636, 281,
  589, 322, 309, 13, 50624], "temperature": 0.0, "avg_logprob": -0.24750641414097377,
  "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.005503923632204533},
  {"id": 249, "seek": 163628, "start": 1642.84, "end": 1647.6399999999999, "text":
  " Yeah, so it''s a constant evolution, try and error. Yeah, of course, but it''s,",
  "tokens": [50692, 865, 11, 370, 309, 311, 257, 5754, 9303, 11, 853, 293, 6713, 13,
  865, 11, 295, 1164, 11, 457, 309, 311, 11, 50932], "temperature": 0.0, "avg_logprob":
  -0.24750641414097377, "compression_ratio": 1.6008583690987124, "no_speech_prob":
  0.005503923632204533}, {"id": 250, "seek": 163628, "start": 1648.44, "end": 1654.44,
  "text": " it''s like APIs is like exactly that layer, which is essentially like
  facing the customer, right?", "tokens": [50972, 309, 311, 411, 21445, 307, 411,
  2293, 300, 4583, 11, 597, 307, 4476, 411, 7170, 264, 5474, 11, 558, 30, 51272],
  "temperature": 0.0, "avg_logprob": -0.24750641414097377, "compression_ratio": 1.6008583690987124,
  "no_speech_prob": 0.005503923632204533}, {"id": 251, "seek": 163628, "start": 1654.44,
  "end": 1661.56, "text": " And you don''t know the scenarios they will use it in,
  and sometimes they might kind of surprise you,", "tokens": [51272, 400, 291, 500,
  380, 458, 264, 15077, 436, 486, 764, 309, 294, 11, 293, 2171, 436, 1062, 733, 295,
  6365, 291, 11, 51628], "temperature": 0.0, "avg_logprob": -0.24750641414097377,
  "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.005503923632204533},
  {"id": 252, "seek": 166156, "start": 1661.56, "end": 1666.9199999999998, "text":
  " or they might say, okay, I found some work around for your like missing parts,
  but then you think,", "tokens": [50364, 420, 436, 1062, 584, 11, 1392, 11, 286,
  1352, 512, 589, 926, 337, 428, 411, 5361, 3166, 11, 457, 550, 291, 519, 11, 50632],
  "temperature": 0.0, "avg_logprob": -0.13931622551482858, "compression_ratio": 1.6047430830039526,
  "no_speech_prob": 0.0029693488031625748}, {"id": 253, "seek": 166156, "start": 1666.9199999999998,
  "end": 1673.3999999999999, "text": " okay, I didn''t think about it, right? The
  API layer is a fantastic way of talking to your client through", "tokens": [50632,
  1392, 11, 286, 994, 380, 519, 466, 309, 11, 558, 30, 440, 9362, 4583, 307, 257,
  5456, 636, 295, 1417, 281, 428, 6423, 807, 50956], "temperature": 0.0, "avg_logprob":
  -0.13931622551482858, "compression_ratio": 1.6047430830039526, "no_speech_prob":
  0.0029693488031625748}, {"id": 254, "seek": 166156, "start": 1673.3999999999999,
  "end": 1680.36, "text": " like API contract in a way, right? Yeah, and it''s a quite
  a big challenge I would say to have the", "tokens": [50956, 411, 9362, 4364, 294,
  257, 636, 11, 558, 30, 865, 11, 293, 309, 311, 257, 1596, 257, 955, 3430, 286, 576,
  584, 281, 362, 264, 51304], "temperature": 0.0, "avg_logprob": -0.13931622551482858,
  "compression_ratio": 1.6047430830039526, "no_speech_prob": 0.0029693488031625748},
  {"id": 255, "seek": 166156, "start": 1680.36, "end": 1688.6799999999998, "text":
  " right balance between ease of use and flexibility. So, what belongs there and
  what doesn''t belong there?", "tokens": [51304, 558, 4772, 1296, 12708, 295, 764,
  293, 12635, 13, 407, 11, 437, 12953, 456, 293, 437, 1177, 380, 5784, 456, 30, 51720],
  "temperature": 0.0, "avg_logprob": -0.13931622551482858, "compression_ratio": 1.6047430830039526,
  "no_speech_prob": 0.0029693488031625748}, {"id": 256, "seek": 168868, "start": 1688.68,
  "end": 1694.68, "text": " Because there''s always a risk to put too much functionality
  in one same thing and make it very", "tokens": [50364, 1436, 456, 311, 1009, 257,
  3148, 281, 829, 886, 709, 14980, 294, 472, 912, 551, 293, 652, 309, 588, 50664],
  "temperature": 0.0, "avg_logprob": -0.2866224351820055, "compression_ratio": 1.6232558139534883,
  "no_speech_prob": 0.011051030829548836}, {"id": 257, "seek": 168868, "start": 1694.68,
  "end": 1701.64, "text": " powerful, but make it a nightmare to use. Yeah, so in
  these, in these balance, I think there is the key", "tokens": [50664, 4005, 11,
  457, 652, 309, 257, 18724, 281, 764, 13, 865, 11, 370, 294, 613, 11, 294, 613, 4772,
  11, 286, 519, 456, 307, 264, 2141, 51012], "temperature": 0.0, "avg_logprob": -0.2866224351820055,
  "compression_ratio": 1.6232558139534883, "no_speech_prob": 0.011051030829548836},
  {"id": 258, "seek": 168868, "start": 1702.68, "end": 1707.48, "text": " what is
  your choice when you have to choose? Let''s say it''s a balance of flexibility,",
  "tokens": [51064, 437, 307, 428, 3922, 562, 291, 362, 281, 2826, 30, 961, 311, 584,
  309, 311, 257, 4772, 295, 12635, 11, 51304], "temperature": 0.0, "avg_logprob":
  -0.2866224351820055, "compression_ratio": 1.6232558139534883, "no_speech_prob":
  0.011051030829548836}, {"id": 259, "seek": 168868, "start": 1707.48, "end": 1711.0,
  "text": " or like flexibility, or what did you say the ease of use, right?", "tokens":
  [51304, 420, 411, 12635, 11, 420, 437, 630, 291, 584, 264, 12708, 295, 764, 11,
  558, 30, 51480], "temperature": 0.0, "avg_logprob": -0.2866224351820055, "compression_ratio":
  1.6232558139534883, "no_speech_prob": 0.011051030829548836}, {"id": 260, "seek":
  171100, "start": 1711.08, "end": 1715.16, "text": " ease of use. I think we are
  now, I''m now", "tokens": [50368, 12708, 295, 764, 13, 286, 519, 321, 366, 586,
  11, 286, 478, 586, 50572], "temperature": 0.0, "avg_logprob": -0.2741630639922753,
  "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.02000425010919571},
  {"id": 261, "seek": 171100, "start": 1716.36, "end": 1722.44, "text": " attending
  to go for the ease of use because for instance, with these open source, I read that",
  "tokens": [50632, 15862, 281, 352, 337, 264, 12708, 295, 764, 570, 337, 5197, 11,
  365, 613, 1269, 4009, 11, 286, 1401, 300, 50936], "temperature": 0.0, "avg_logprob":
  -0.2741630639922753, "compression_ratio": 1.6176470588235294, "no_speech_prob":
  0.02000425010919571}, {"id": 262, "seek": 171100, "start": 1722.44, "end": 1729.48,
  "text": " open source teaches you well. I think at some point, we did a nearly down
  to well the APIs and", "tokens": [50936, 1269, 4009, 16876, 291, 731, 13, 286, 519,
  412, 512, 935, 11, 321, 630, 257, 6217, 760, 281, 731, 264, 21445, 293, 51288],
  "temperature": 0.0, "avg_logprob": -0.2741630639922753, "compression_ratio": 1.6176470588235294,
  "no_speech_prob": 0.02000425010919571}, {"id": 263, "seek": 171100, "start": 1729.48,
  "end": 1733.88, "text": " it was a little bit complex to use. You could do a lot
  of things, but at the end maybe not everybody", "tokens": [51288, 309, 390, 257,
  707, 857, 3997, 281, 764, 13, 509, 727, 360, 257, 688, 295, 721, 11, 457, 412, 264,
  917, 1310, 406, 2201, 51508], "temperature": 0.0, "avg_logprob": -0.2741630639922753,
  "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.02000425010919571},
  {"id": 264, "seek": 173388, "start": 1734.8400000000001, "end": 1740.8400000000001,
  "text": " was doing. So, I think it''s of use for the first century barrier. It''s
  the most important thing.", "tokens": [50412, 390, 884, 13, 407, 11, 286, 519, 309,
  311, 295, 764, 337, 264, 700, 4901, 13357, 13, 467, 311, 264, 881, 1021, 551, 13,
  50712], "temperature": 0.0, "avg_logprob": -0.22011393359583667, "compression_ratio":
  1.6631578947368422, "no_speech_prob": 0.011186445131897926}, {"id": 265, "seek":
  173388, "start": 1741.64, "end": 1746.5200000000002, "text": " Yeah, and I mean,
  also like it''s interesting, you know, like if you have a real API, let''s say",
  "tokens": [50752, 865, 11, 293, 286, 914, 11, 611, 411, 309, 311, 1880, 11, 291,
  458, 11, 411, 498, 291, 362, 257, 957, 9362, 11, 718, 311, 584, 50996], "temperature":
  0.0, "avg_logprob": -0.22011393359583667, "compression_ratio": 1.6631578947368422,
  "no_speech_prob": 0.011186445131897926}, {"id": 266, "seek": 173388, "start": 1746.5200000000002,
  "end": 1751.8000000000002, "text": " deploy it somewhere and it''s a published contract
  and people are sending queries there,", "tokens": [50996, 7274, 309, 4079, 293,
  309, 311, 257, 6572, 4364, 293, 561, 366, 7750, 24109, 456, 11, 51260], "temperature":
  0.0, "avg_logprob": -0.22011393359583667, "compression_ratio": 1.6631578947368422,
  "no_speech_prob": 0.011186445131897926}, {"id": 267, "seek": 173388, "start": 1751.8000000000002,
  "end": 1757.3200000000002, "text": " then you know actually which endpoints which
  features are being used which are not, which options", "tokens": [51260, 550, 291,
  458, 767, 597, 917, 20552, 597, 4122, 366, 885, 1143, 597, 366, 406, 11, 597, 3956,
  51536], "temperature": 0.0, "avg_logprob": -0.22011393359583667, "compression_ratio":
  1.6631578947368422, "no_speech_prob": 0.011186445131897926}, {"id": 268, "seek":
  173388, "start": 1757.3200000000002, "end": 1761.72, "text": " are completely ignored
  even though you put them in the dogs, right? But how do you go about this", "tokens":
  [51536, 366, 2584, 19735, 754, 1673, 291, 829, 552, 294, 264, 7197, 11, 558, 30,
  583, 577, 360, 291, 352, 466, 341, 51756], "temperature": 0.0, "avg_logprob": -0.22011393359583667,
  "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.011186445131897926},
  {"id": 269, "seek": 176172, "start": 1762.44, "end": 1769.64, "text": " in the open
  source code? Like somebody downloads your code, they use it somewhere, you don''t
  know how.", "tokens": [50400, 294, 264, 1269, 4009, 3089, 30, 1743, 2618, 36553,
  428, 3089, 11, 436, 764, 309, 4079, 11, 291, 500, 380, 458, 577, 13, 50760], "temperature":
  0.0, "avg_logprob": -0.20005519866943358, "compression_ratio": 1.6282051282051282,
  "no_speech_prob": 0.014889408834278584}, {"id": 270, "seek": 176172, "start": 1770.3600000000001,
  "end": 1775.08, "text": " So, how do you collect these analytics from them? Do you
  just send like call out messages,", "tokens": [50796, 407, 11, 577, 360, 291, 2500,
  613, 15370, 490, 552, 30, 1144, 291, 445, 2845, 411, 818, 484, 7897, 11, 51032],
  "temperature": 0.0, "avg_logprob": -0.20005519866943358, "compression_ratio": 1.6282051282051282,
  "no_speech_prob": 0.014889408834278584}, {"id": 271, "seek": 176172, "start": 1775.08,
  "end": 1779.88, "text": " hey guys, what do you use, what do you don''t? Right now
  we are trying to keep attention on who is", "tokens": [51032, 4177, 1074, 11, 437,
  360, 291, 764, 11, 437, 360, 291, 500, 380, 30, 1779, 586, 321, 366, 1382, 281,
  1066, 3202, 322, 567, 307, 51272], "temperature": 0.0, "avg_logprob": -0.20005519866943358,
  "compression_ratio": 1.6282051282051282, "no_speech_prob": 0.014889408834278584},
  {"id": 272, "seek": 176172, "start": 1779.88, "end": 1786.44, "text": " using guys,
  what, and when people ask us, we try to get the most information out of them,",
  "tokens": [51272, 1228, 1074, 11, 437, 11, 293, 562, 561, 1029, 505, 11, 321, 853,
  281, 483, 264, 881, 1589, 484, 295, 552, 11, 51600], "temperature": 0.0, "avg_logprob":
  -0.20005519866943358, "compression_ratio": 1.6282051282051282, "no_speech_prob":
  0.014889408834278584}, {"id": 273, "seek": 178644, "start": 1787.16, "end": 1791.48,
  "text": " not information on the business of how they use it, how they feel. So
  right now the", "tokens": [50400, 406, 1589, 322, 264, 1606, 295, 577, 436, 764,
  309, 11, 577, 436, 841, 13, 407, 558, 586, 264, 50616], "temperature": 0.0, "avg_logprob":
  -0.2196634732759916, "compression_ratio": 1.7246963562753037, "no_speech_prob":
  0.007910187356173992}, {"id": 274, "seek": 178644, "start": 1791.48, "end": 1796.28,
  "text": " community is the only source of information we have. That''s the open
  source. What?", "tokens": [50616, 1768, 307, 264, 787, 4009, 295, 1589, 321, 362,
  13, 663, 311, 264, 1269, 4009, 13, 708, 30, 50856], "temperature": 0.0, "avg_logprob":
  -0.2196634732759916, "compression_ratio": 1.7246963562753037, "no_speech_prob":
  0.007910187356173992}, {"id": 275, "seek": 178644, "start": 1797.24, "end": 1802.2,
  "text": " How do you talk to them? Like do you like send like messages saying, hey
  guys, can you vote", "tokens": [50904, 1012, 360, 291, 751, 281, 552, 30, 1743,
  360, 291, 411, 2845, 411, 7897, 1566, 11, 4177, 1074, 11, 393, 291, 4740, 51152],
  "temperature": 0.0, "avg_logprob": -0.2196634732759916, "compression_ratio": 1.7246963562753037,
  "no_speech_prob": 0.007910187356173992}, {"id": 276, "seek": 178644, "start": 1802.2,
  "end": 1807.72, "text": " about keeping this feature and removing that one or not
  exactly like this, but would you see", "tokens": [51152, 466, 5145, 341, 4111, 293,
  12720, 300, 472, 420, 406, 2293, 411, 341, 11, 457, 576, 291, 536, 51428], "temperature":
  0.0, "avg_logprob": -0.2196634732759916, "compression_ratio": 1.7246963562753037,
  "no_speech_prob": 0.007910187356173992}, {"id": 277, "seek": 178644, "start": 1808.2,
  "end": 1811.8, "text": " people that are more engaged or more or less engaged, people
  that are more", "tokens": [51452, 561, 300, 366, 544, 8237, 420, 544, 420, 1570,
  8237, 11, 561, 300, 366, 544, 51632], "temperature": 0.0, "avg_logprob": -0.2196634732759916,
  "compression_ratio": 1.7246963562753037, "no_speech_prob": 0.007910187356173992},
  {"id": 278, "seek": 181180, "start": 1812.6, "end": 1817.96, "text": " finding it
  more easier or less or having more difficulties with your with your solution.",
  "tokens": [50404, 5006, 309, 544, 3571, 420, 1570, 420, 1419, 544, 14399, 365, 428,
  365, 428, 3827, 13, 50672], "temperature": 0.0, "avg_logprob": -0.2089354294996995,
  "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.010515459813177586},
  {"id": 279, "seek": 181180, "start": 1819.3999999999999, "end": 1827.0, "text":
  " So it''s and sometimes we have a development relations team that try to get also
  feedback from", "tokens": [50744, 407, 309, 311, 293, 2171, 321, 362, 257, 3250,
  2299, 1469, 300, 853, 281, 483, 611, 5824, 490, 51124], "temperature": 0.0, "avg_logprob":
  -0.2089354294996995, "compression_ratio": 1.5674157303370786, "no_speech_prob":
  0.010515459813177586}, {"id": 280, "seek": 181180, "start": 1827.0, "end": 1834.2,
  "text": " from the community in many terms. So this is a global effort. But in the
  end you have you have a", "tokens": [51124, 490, 264, 1768, 294, 867, 2115, 13,
  407, 341, 307, 257, 4338, 4630, 13, 583, 294, 264, 917, 291, 362, 291, 362, 257,
  51484], "temperature": 0.0, "avg_logprob": -0.2089354294996995, "compression_ratio":
  1.5674157303370786, "no_speech_prob": 0.010515459813177586}, {"id": 281, "seek":
  183420, "start": 1834.2, "end": 1839.64, "text": " say, right? Like no matter what
  they ask you have a say, is that right? Well, I mean,", "tokens": [50364, 584, 11,
  558, 30, 1743, 572, 1871, 437, 436, 1029, 291, 362, 257, 584, 11, 307, 300, 558,
  30, 1042, 11, 286, 914, 11, 50636], "temperature": 0.0, "avg_logprob": -0.2862526782147296,
  "compression_ratio": 1.4974358974358974, "no_speech_prob": 0.02727777510881424},
  {"id": 282, "seek": 183420, "start": 1840.92, "end": 1848.76, "text": " sometimes
  you cannot please the community to all the extent, I don''t know, we have to keep
  a road map.", "tokens": [50700, 2171, 291, 2644, 1767, 264, 1768, 281, 439, 264,
  8396, 11, 286, 500, 380, 458, 11, 321, 362, 281, 1066, 257, 3060, 4471, 13, 51092],
  "temperature": 0.0, "avg_logprob": -0.2862526782147296, "compression_ratio": 1.4974358974358974,
  "no_speech_prob": 0.02727777510881424}, {"id": 283, "seek": 183420, "start": 1849.56,
  "end": 1856.04, "text": " For instance, people may want you to build something that
  is emanated, but maybe not so significant for", "tokens": [51132, 1171, 5197, 11,
  561, 815, 528, 291, 281, 1322, 746, 300, 307, 28211, 770, 11, 457, 1310, 406, 370,
  4776, 337, 51456], "temperature": 0.0, "avg_logprob": -0.2862526782147296, "compression_ratio":
  1.4974358974358974, "no_speech_prob": 0.02727777510881424}, {"id": 284, "seek":
  185604, "start": 1856.36, "end": 1865.96, "text": " search solutions. This is quite
  a confusion, I think. So beyond search like where can I use GNA,", "tokens": [50380,
  3164, 6547, 13, 639, 307, 1596, 257, 15075, 11, 286, 519, 13, 407, 4399, 3164, 411,
  689, 393, 286, 764, 460, 5321, 11, 50860], "temperature": 0.0, "avg_logprob": -0.30593849891840025,
  "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.005071389023214579},
  {"id": 285, "seek": 185604, "start": 1865.96, "end": 1870.76, "text": " what kind
  of other use cases have you seen beyond like kind of similarity search?", "tokens":
  [50860, 437, 733, 295, 661, 764, 3331, 362, 291, 1612, 4399, 411, 733, 295, 32194,
  3164, 30, 51100], "temperature": 0.0, "avg_logprob": -0.30593849891840025, "compression_ratio":
  1.6741071428571428, "no_speech_prob": 0.005071389023214579}, {"id": 286, "seek":
  185604, "start": 1872.36, "end": 1879.32, "text": " So since we are building these
  abstractions, it is quite easy for you to use these abstractions", "tokens": [51180,
  407, 1670, 321, 366, 2390, 613, 12649, 626, 11, 309, 307, 1596, 1858, 337, 291,
  281, 764, 613, 12649, 626, 51528], "temperature": 0.0, "avg_logprob": -0.30593849891840025,
  "compression_ratio": 1.6741071428571428, "no_speech_prob": 0.005071389023214579},
  {"id": 287, "seek": 185604, "start": 1879.32, "end": 1885.6399999999999, "text":
  " for building any classification model or anything as you really did, you could
  even deploy something", "tokens": [51528, 337, 2390, 604, 21538, 2316, 420, 1340,
  382, 291, 534, 630, 11, 291, 727, 754, 7274, 746, 51844], "temperature": 0.0, "avg_logprob":
  -0.30593849891840025, "compression_ratio": 1.6741071428571428, "no_speech_prob":
  0.005071389023214579}, {"id": 288, "seek": 188564, "start": 1885.64, "end": 1892.8400000000001,
  "text": " and use GNA to easily deploy and scale and use your segmenter and object
  segmenter model.", "tokens": [50364, 293, 764, 460, 5321, 281, 3612, 7274, 293,
  4373, 293, 764, 428, 9469, 260, 293, 2657, 9469, 260, 2316, 13, 50724], "temperature":
  0.0, "avg_logprob": -0.18430510811183765, "compression_ratio": 1.7135922330097086,
  "no_speech_prob": 0.0018924744799733162}, {"id": 289, "seek": 188564, "start": 1893.4,
  "end": 1898.5200000000002, "text": " But this is this is something that you could
  do, but GNA is born and will", "tokens": [50752, 583, 341, 307, 341, 307, 746, 300,
  291, 727, 360, 11, 457, 460, 5321, 307, 4232, 293, 486, 51008], "temperature": 0.0,
  "avg_logprob": -0.18430510811183765, "compression_ratio": 1.7135922330097086, "no_speech_prob":
  0.0018924744799733162}, {"id": 290, "seek": 188564, "start": 1899.4, "end": 1907.48,
  "text": " will be working to implement new research solutions. So you could still
  use this but might not", "tokens": [51052, 486, 312, 1364, 281, 4445, 777, 2132,
  6547, 13, 407, 291, 727, 920, 764, 341, 457, 1062, 406, 51456], "temperature": 0.0,
  "avg_logprob": -0.18430510811183765, "compression_ratio": 1.7135922330097086, "no_speech_prob":
  0.0018924744799733162}, {"id": 291, "seek": 188564, "start": 1907.48, "end": 1912.68,
  "text": " be the best tool for it. So we are not born for that, but you could do
  it. But we can see that", "tokens": [51456, 312, 264, 1151, 2290, 337, 309, 13,
  407, 321, 366, 406, 4232, 337, 300, 11, 457, 291, 727, 360, 309, 13, 583, 321, 393,
  536, 300, 51716], "temperature": 0.0, "avg_logprob": -0.18430510811183765, "compression_ratio":
  1.7135922330097086, "no_speech_prob": 0.0018924744799733162}, {"id": 292, "seek":
  191268, "start": 1913.16, "end": 1918.1200000000001, "text": " we are done. You
  can do this because for instance classification or segmenting can object can be",
  "tokens": [50388, 321, 366, 1096, 13, 509, 393, 360, 341, 570, 337, 5197, 21538,
  420, 9469, 278, 393, 2657, 393, 312, 50636], "temperature": 0.0, "avg_logprob":
  -0.34182002932526345, "compression_ratio": 1.7511737089201878, "no_speech_prob":
  0.005156046710908413}, {"id": 293, "seek": 191268, "start": 1918.1200000000001,
  "end": 1923.48, "text": " part of your pipeline, but in theory we are born to support
  search applications.", "tokens": [50636, 644, 295, 428, 15517, 11, 457, 294, 5261,
  321, 366, 4232, 281, 1406, 3164, 5821, 13, 50904], "temperature": 0.0, "avg_logprob":
  -0.34182002932526345, "compression_ratio": 1.7511737089201878, "no_speech_prob":
  0.005156046710908413}, {"id": 294, "seek": 191268, "start": 1924.28, "end": 1929.96,
  "text": " Yeah, yeah. So like, or for example, something that is or search applications
  or something that you", "tokens": [50944, 865, 11, 1338, 13, 407, 411, 11, 420,
  337, 1365, 11, 746, 300, 307, 420, 3164, 5821, 420, 746, 300, 291, 51228], "temperature":
  0.0, "avg_logprob": -0.34182002932526345, "compression_ratio": 1.7511737089201878,
  "no_speech_prob": 0.005156046710908413}, {"id": 295, "seek": 191268, "start": 1929.96,
  "end": 1936.8400000000001, "text": " can frame as a search application right now,
  for instance, a question-answering system that you", "tokens": [51228, 393, 3920,
  382, 257, 3164, 3861, 558, 586, 11, 337, 5197, 11, 257, 1168, 12, 43904, 278, 1185,
  300, 291, 51572], "temperature": 0.0, "avg_logprob": -0.34182002932526345, "compression_ratio":
  1.7511737089201878, "no_speech_prob": 0.005156046710908413}, {"id": 296, "seek":
  193684, "start": 1936.9199999999998, "end": 1940.9199999999998, "text": " can frame
  as a part where you will do something like research or", "tokens": [50368, 393,
  3920, 382, 257, 644, 689, 291, 486, 360, 746, 411, 2132, 420, 50568], "temperature":
  0.0, "avg_logprob": -0.20958545684814453, "compression_ratio": 1.7892561983471074,
  "no_speech_prob": 0.0024767343420535326}, {"id": 297, "seek": 193684, "start": 1941.8,
  "end": 1948.36, "text": " spare search and then you have some real or model that
  extracts more information from something.", "tokens": [50612, 13798, 3164, 293,
  550, 291, 362, 512, 957, 420, 2316, 300, 8947, 82, 544, 1589, 490, 746, 13, 50940],
  "temperature": 0.0, "avg_logprob": -0.20958545684814453, "compression_ratio": 1.7892561983471074,
  "no_speech_prob": 0.0024767343420535326}, {"id": 298, "seek": 193684, "start": 1948.36,
  "end": 1952.76, "text": " So anything that falls into this domain, you can do it.
  Yeah, I guess you can also", "tokens": [50940, 407, 1340, 300, 8804, 666, 341, 9274,
  11, 291, 393, 360, 309, 13, 865, 11, 286, 2041, 291, 393, 611, 51160], "temperature":
  0.0, "avg_logprob": -0.20958545684814453, "compression_ratio": 1.7892561983471074,
  "no_speech_prob": 0.0024767343420535326}, {"id": 299, "seek": 193684, "start": 1953.8,
  "end": 1960.12, "text": " like based on the research and some practice happening
  in data augmentation based on retrieving,", "tokens": [51212, 411, 2361, 322, 264,
  2132, 293, 512, 3124, 2737, 294, 1412, 14501, 19631, 2361, 322, 19817, 798, 11,
  51528], "temperature": 0.0, "avg_logprob": -0.20958545684814453, "compression_ratio":
  1.7892561983471074, "no_speech_prob": 0.0024767343420535326}, {"id": 300, "seek":
  193684, "start": 1960.12, "end": 1965.6399999999999, "text": " you can also formulate
  data augmentation as a process of search in principle, right? So the", "tokens":
  [51528, 291, 393, 611, 47881, 1412, 14501, 19631, 382, 257, 1399, 295, 3164, 294,
  8665, 11, 558, 30, 407, 264, 51804], "temperature": 0.0, "avg_logprob": -0.20958545684814453,
  "compression_ratio": 1.7892561983471074, "no_speech_prob": 0.0024767343420535326},
  {"id": 301, "seek": 196564, "start": 1965.64, "end": 1971.72, "text": " output will
  be your augmented data, but you use search in the middle.", "tokens": [50364, 5598,
  486, 312, 428, 36155, 1412, 11, 457, 291, 764, 3164, 294, 264, 2808, 13, 50668],
  "temperature": 0.0, "avg_logprob": -0.268896210059691, "compression_ratio": 1.577092511013216,
  "no_speech_prob": 0.0038744050543755293}, {"id": 302, "seek": 196564, "start": 1972.5200000000002,
  "end": 1978.92, "text": " Yes, actually that might be but search can be so many
  problems can be framed into search. I", "tokens": [50708, 1079, 11, 767, 300, 1062,
  312, 457, 3164, 393, 312, 370, 867, 2740, 393, 312, 30420, 666, 3164, 13, 286, 51028],
  "temperature": 0.0, "avg_logprob": -0.268896210059691, "compression_ratio": 1.577092511013216,
  "no_speech_prob": 0.0038744050543755293}, {"id": 303, "seek": 196564, "start": 1978.92,
  "end": 1987.16, "text": " don''t know. At the end is like vectors are somehow like
  the truth, not like the semantic information,", "tokens": [51028, 500, 380, 458,
  13, 1711, 264, 917, 307, 411, 18875, 366, 6063, 411, 264, 3494, 11, 406, 411, 264,
  47982, 1589, 11, 51440], "temperature": 0.0, "avg_logprob": -0.268896210059691,
  "compression_ratio": 1.577092511013216, "no_speech_prob": 0.0038744050543755293},
  {"id": 304, "seek": 196564, "start": 1987.16, "end": 1993.72, "text": " so how we
  don''t understand exactly why but is encoded there, right? So just by clustering
  them", "tokens": [51440, 370, 577, 321, 500, 380, 1223, 2293, 983, 457, 307, 2058,
  12340, 456, 11, 558, 30, 407, 445, 538, 596, 48673, 552, 51768], "temperature":
  0.0, "avg_logprob": -0.268896210059691, "compression_ratio": 1.577092511013216,
  "no_speech_prob": 0.0038744050543755293}, {"id": 305, "seek": 199372, "start": 1993.72,
  "end": 1998.52, "text": " together, somehow we have some understanding, so so many
  things to confirm with.", "tokens": [50364, 1214, 11, 6063, 321, 362, 512, 3701,
  11, 370, 370, 867, 721, 281, 9064, 365, 13, 50604], "temperature": 0.0, "avg_logprob":
  -0.22372118048711653, "compression_ratio": 1.5229681978798586, "no_speech_prob":
  0.0020545891020447016}, {"id": 306, "seek": 199372, "start": 2000.84, "end": 2005.32,
  "text": " I also wanted to ask you a little bit like closer to the similarity search
  itself,", "tokens": [50720, 286, 611, 1415, 281, 1029, 291, 257, 707, 857, 411,
  4966, 281, 264, 32194, 3164, 2564, 11, 50944], "temperature": 0.0, "avg_logprob":
  -0.22372118048711653, "compression_ratio": 1.5229681978798586, "no_speech_prob":
  0.0020545891020447016}, {"id": 307, "seek": 199372, "start": 2005.32, "end": 2011.88,
  "text": " you know, let''s say I built a traditional kind of text search engine,
  okay? And I''m moving away", "tokens": [50944, 291, 458, 11, 718, 311, 584, 286,
  3094, 257, 5164, 733, 295, 2487, 3164, 2848, 11, 1392, 30, 400, 286, 478, 2684,
  1314, 51272], "temperature": 0.0, "avg_logprob": -0.22372118048711653, "compression_ratio":
  1.5229681978798586, "no_speech_prob": 0.0020545891020447016}, {"id": 308, "seek":
  199372, "start": 2011.88, "end": 2017.56, "text": " from BM25, which is like probably
  majority of this market today. So I''m thinking, okay,", "tokens": [51272, 490,
  15901, 6074, 11, 597, 307, 411, 1391, 6286, 295, 341, 2142, 965, 13, 407, 286, 478,
  1953, 11, 1392, 11, 51556], "temperature": 0.0, "avg_logprob": -0.22372118048711653,
  "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.0020545891020447016},
  {"id": 309, "seek": 199372, "start": 2017.56, "end": 2021.56, "text": " what are
  these cool kids doing? Maybe I should try it out, plug in some bird model.", "tokens":
  [51556, 437, 366, 613, 1627, 2301, 884, 30, 2704, 286, 820, 853, 309, 484, 11, 5452,
  294, 512, 5255, 2316, 13, 51756], "temperature": 0.0, "avg_logprob": -0.22372118048711653,
  "compression_ratio": 1.5229681978798586, "no_speech_prob": 0.0020545891020447016},
  {"id": 310, "seek": 202156, "start": 2022.12, "end": 2030.2, "text": " So and but
  then in my UI, I am also showing snippets and it''s very easy to show snippets when
  it''s a", "tokens": [50392, 407, 293, 457, 550, 294, 452, 15682, 11, 286, 669, 611,
  4099, 35623, 1385, 293, 309, 311, 588, 1858, 281, 855, 35623, 1385, 562, 309, 311,
  257, 50796], "temperature": 0.0, "avg_logprob": -0.19450630680207284, "compression_ratio":
  1.665137614678899, "no_speech_prob": 0.01120330486446619}, {"id": 311, "seek": 202156,
  "start": 2030.2, "end": 2037.6399999999999, "text": " keyboard search, right? So
  what should I do or what can I do with model like bird and genie AI", "tokens":
  [50796, 10186, 3164, 11, 558, 30, 407, 437, 820, 286, 360, 420, 437, 393, 286, 360,
  365, 2316, 411, 5255, 293, 1049, 414, 7318, 51168], "temperature": 0.0, "avg_logprob":
  -0.19450630680207284, "compression_ratio": 1.665137614678899, "no_speech_prob":
  0.01120330486446619}, {"id": 312, "seek": 202156, "start": 2037.6399999999999, "end":
  2041.32, "text": " to show snippets or something that will resemble snippets to
  the users?", "tokens": [51168, 281, 855, 35623, 1385, 420, 746, 300, 486, 36870,
  35623, 1385, 281, 264, 5022, 30, 51352], "temperature": 0.0, "avg_logprob": -0.19450630680207284,
  "compression_ratio": 1.665137614678899, "no_speech_prob": 0.01120330486446619},
  {"id": 313, "seek": 202156, "start": 2042.84, "end": 2048.84, "text": " Maybe you
  can also change your information so that you check where the attention is put in
  your", "tokens": [51428, 2704, 291, 393, 611, 1319, 428, 1589, 370, 300, 291, 1520,
  689, 264, 3202, 307, 829, 294, 428, 51728], "temperature": 0.0, "avg_logprob": -0.19450630680207284,
  "compression_ratio": 1.665137614678899, "no_speech_prob": 0.01120330486446619},
  {"id": 314, "seek": 204884, "start": 2048.84, "end": 2055.6400000000003, "text":
  " model or somehow, but yeah, I think also there is a thing that we are framed and
  we have been", "tokens": [50364, 2316, 420, 6063, 11, 457, 1338, 11, 286, 519, 611,
  456, 307, 257, 551, 300, 321, 366, 30420, 293, 321, 362, 668, 50704], "temperature":
  0.0, "avg_logprob": -0.22729716350122825, "compression_ratio": 1.6543778801843319,
  "no_speech_prob": 0.0010044240625575185}, {"id": 315, "seek": 204884, "start": 2055.6400000000003,
  "end": 2062.1200000000003, "text": " grown into this keyboard search that it''s
  so interpretable and so easy to use and even so", "tokens": [50704, 7709, 666, 341,
  10186, 3164, 300, 309, 311, 370, 7302, 712, 293, 370, 1858, 281, 764, 293, 754,
  370, 51028], "temperature": 0.0, "avg_logprob": -0.22729716350122825, "compression_ratio":
  1.6543778801843319, "no_speech_prob": 0.0010044240625575185}, {"id": 316, "seek":
  204884, "start": 2062.1200000000003, "end": 2066.84, "text": " easy to hack. So
  how, you know, you know, you as a user know how to drive your", "tokens": [51028,
  1858, 281, 10339, 13, 407, 577, 11, 291, 458, 11, 291, 458, 11, 291, 382, 257, 4195,
  458, 577, 281, 3332, 428, 51264], "temperature": 0.0, "avg_logprob": -0.22729716350122825,
  "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.0010044240625575185},
  {"id": 317, "seek": 204884, "start": 2066.84, "end": 2073.08, "text": " search if
  you don''t find it, right? Okay, this word might find you here. And I think since
  these", "tokens": [51264, 3164, 498, 291, 500, 380, 915, 309, 11, 558, 30, 1033,
  11, 341, 1349, 1062, 915, 291, 510, 13, 400, 286, 519, 1670, 613, 51576], "temperature":
  0.0, "avg_logprob": -0.22729716350122825, "compression_ratio": 1.6543778801843319,
  "no_speech_prob": 0.0010044240625575185}, {"id": 318, "seek": 207308, "start": 2073.08,
  "end": 2081.4, "text": " models are kind of black box for many of us, I think in
  this kind of sense, this interpretability is", "tokens": [50364, 5245, 366, 733,
  295, 2211, 2424, 337, 867, 295, 505, 11, 286, 519, 294, 341, 733, 295, 2020, 11,
  341, 7302, 2310, 307, 50780], "temperature": 0.0, "avg_logprob": -0.21640856903378325,
  "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0023971577174961567},
  {"id": 319, "seek": 207308, "start": 2081.4, "end": 2085.56, "text": " one of the
  main challenges and I think one of the main focus that research should go.", "tokens":
  [50780, 472, 295, 264, 2135, 4759, 293, 286, 519, 472, 295, 264, 2135, 1879, 300,
  2132, 820, 352, 13, 50988], "temperature": 0.0, "avg_logprob": -0.21640856903378325,
  "compression_ratio": 1.654708520179372, "no_speech_prob": 0.0023971577174961567},
  {"id": 320, "seek": 207308, "start": 2086.2, "end": 2091.08, "text": " Yeah, but
  I mean, you you you call it out as an interpretability, but like for the user sent",
  "tokens": [51020, 865, 11, 457, 286, 914, 11, 291, 291, 291, 818, 309, 484, 382,
  364, 7302, 2310, 11, 457, 411, 337, 264, 4195, 2279, 51264], "temperature": 0.0,
  "avg_logprob": -0.21640856903378325, "compression_ratio": 1.654708520179372, "no_speech_prob":
  0.0023971577174961567}, {"id": 321, "seek": 207308, "start": 2091.08, "end": 2096.2799999999997,
  "text": " for me, let''s say I''m a product manager, I don''t care about is it bird
  model, is it VM25?", "tokens": [51264, 337, 385, 11, 718, 311, 584, 286, 478, 257,
  1674, 6598, 11, 286, 500, 380, 1127, 466, 307, 309, 5255, 2316, 11, 307, 309, 18038,
  6074, 30, 51524], "temperature": 0.0, "avg_logprob": -0.21640856903378325, "compression_ratio":
  1.654708520179372, "no_speech_prob": 0.0023971577174961567}, {"id": 322, "seek":
  209628, "start": 2096.36, "end": 2102.92, "text": " I used to see snippets, I want
  to see them now. So like, what should the point in VM25?", "tokens": [50368, 286,
  1143, 281, 536, 35623, 1385, 11, 286, 528, 281, 536, 552, 586, 13, 407, 411, 11,
  437, 820, 264, 935, 294, 18038, 6074, 30, 50696], "temperature": 0.0, "avg_logprob":
  -0.2243355941772461, "compression_ratio": 1.5875, "no_speech_prob": 0.008157463744282722},
  {"id": 323, "seek": 209628, "start": 2103.6400000000003, "end": 2111.0, "text":
  " I can give you a snippet because I know why I have this solution. Here is, well,
  it correlates and", "tokens": [50732, 286, 393, 976, 291, 257, 35623, 302, 570,
  286, 458, 983, 286, 362, 341, 3827, 13, 1692, 307, 11, 731, 11, 309, 13983, 1024,
  293, 51100], "temperature": 0.0, "avg_logprob": -0.2243355941772461, "compression_ratio":
  1.5875, "no_speech_prob": 0.008157463744282722}, {"id": 324, "seek": 209628, "start":
  2111.0, "end": 2116.1200000000003, "text": " but where the information that I want
  is there. Maybe for instance, in dinner, one of the main", "tokens": [51100, 457,
  689, 264, 1589, 300, 286, 528, 307, 456, 13, 2704, 337, 5197, 11, 294, 6148, 11,
  472, 295, 264, 2135, 51356], "temperature": 0.0, "avg_logprob": -0.2243355941772461,
  "compression_ratio": 1.5875, "no_speech_prob": 0.008157463744282722}, {"id": 325,
  "seek": 209628, "start": 2117.5600000000004, "end": 2122.0400000000004, "text":
  " building blocks that we have is our document is our recursive structure because
  most of the things,", "tokens": [51428, 2390, 8474, 300, 321, 362, 307, 527, 4166,
  307, 527, 20560, 488, 3877, 570, 881, 295, 264, 721, 11, 51652], "temperature":
  0.0, "avg_logprob": -0.2243355941772461, "compression_ratio": 1.5875, "no_speech_prob":
  0.008157463744282722}, {"id": 326, "seek": 212204, "start": 2122.04, "end": 2126.68,
  "text": " for instance, if you find the search, if you search a document, a text
  or document, you might need", "tokens": [50364, 337, 5197, 11, 498, 291, 915, 264,
  3164, 11, 498, 291, 3164, 257, 4166, 11, 257, 2487, 420, 4166, 11, 291, 1062, 643,
  50596], "temperature": 0.0, "avg_logprob": -0.18899994753719715, "compression_ratio":
  1.8267326732673268, "no_speech_prob": 0.005950938444584608}, {"id": 327, "seek":
  212204, "start": 2126.68, "end": 2134.2799999999997, "text": " to break into paragraphs
  into digs and so on. So maybe what you can do is you do the vector search", "tokens":
  [50596, 281, 1821, 666, 48910, 666, 2528, 82, 293, 370, 322, 13, 407, 1310, 437,
  291, 393, 360, 307, 291, 360, 264, 8062, 3164, 50976], "temperature": 0.0, "avg_logprob":
  -0.18899994753719715, "compression_ratio": 1.8267326732673268, "no_speech_prob":
  0.005950938444584608}, {"id": 328, "seek": 212204, "start": 2134.2799999999997,
  "end": 2139.8, "text": " at the variety level is at sentence level, but then the
  results might be shown at paragraph or", "tokens": [50976, 412, 264, 5673, 1496,
  307, 412, 8174, 1496, 11, 457, 550, 264, 3542, 1062, 312, 4898, 412, 18865, 420,
  51252], "temperature": 0.0, "avg_logprob": -0.18899994753719715, "compression_ratio":
  1.8267326732673268, "no_speech_prob": 0.005950938444584608}, {"id": 329, "seek":
  212204, "start": 2139.8, "end": 2144.68, "text": " at sentence level. So you can
  highlight very easily the sentence that really", "tokens": [51252, 412, 8174, 1496,
  13, 407, 291, 393, 5078, 588, 3612, 264, 8174, 300, 534, 51496], "temperature":
  0.0, "avg_logprob": -0.18899994753719715, "compression_ratio": 1.8267326732673268,
  "no_speech_prob": 0.005950938444584608}, {"id": 330, "seek": 214468, "start": 2145.64,
  "end": 2154.7599999999998, "text": " and drove the search to this page. I remember
  actually, I don''t know if you know the block,", "tokens": [50412, 293, 13226, 264,
  3164, 281, 341, 3028, 13, 286, 1604, 767, 11, 286, 500, 380, 458, 498, 291, 458,
  264, 3461, 11, 50868], "temperature": 0.0, "avg_logprob": -0.3132782716017503, "compression_ratio":
  1.446808510638298, "no_speech_prob": 0.01839555613696575}, {"id": 331, "seek": 214468,
  "start": 2154.7599999999998, "end": 2160.9199999999996, "text": " was it salmon
  run, like Sujitpal, he''s doing a lot of blogging in the area of like,", "tokens":
  [50868, 390, 309, 18518, 1190, 11, 411, 2746, 73, 270, 31862, 11, 415, 311, 884,
  257, 688, 295, 6968, 3249, 294, 264, 1859, 295, 411, 11, 51176], "temperature":
  0.0, "avg_logprob": -0.3132782716017503, "compression_ratio": 1.446808510638298,
  "no_speech_prob": 0.01839555613696575}, {"id": 332, "seek": 214468, "start": 2162.44,
  "end": 2167.7999999999997, "text": " here is the problem, how do I solve it? And
  then he, quite usually he goes into deep learning or", "tokens": [51252, 510, 307,
  264, 1154, 11, 577, 360, 286, 5039, 309, 30, 400, 550, 415, 11, 1596, 2673, 415,
  1709, 666, 2452, 2539, 420, 51520], "temperature": 0.0, "avg_logprob": -0.3132782716017503,
  "compression_ratio": 1.446808510638298, "no_speech_prob": 0.01839555613696575},
  {"id": 333, "seek": 216780, "start": 2168.52, "end": 2173.96, "text": " trying out
  some vector search, maybe or not. And I remember like he was saying that", "tokens":
  [50400, 1382, 484, 512, 8062, 3164, 11, 1310, 420, 406, 13, 400, 286, 1604, 411,
  415, 390, 1566, 300, 50672], "temperature": 0.0, "avg_logprob": -0.2136070556640625,
  "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.020604722201824188},
  {"id": 334, "seek": 216780, "start": 2176.36, "end": 2180.52, "text": " to solve
  this snippeting problem, how he would do it because he comes from his additional",
  "tokens": [50792, 281, 5039, 341, 35623, 9880, 1154, 11, 577, 415, 576, 360, 309,
  570, 415, 1487, 490, 702, 4497, 51000], "temperature": 0.0, "avg_logprob": -0.2136070556640625,
  "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.020604722201824188},
  {"id": 335, "seek": 216780, "start": 2180.52, "end": 2186.6000000000004, "text":
  " search and I do in a way. And like he said, okay, you can kind of build, like,
  if I remember correctly,", "tokens": [51000, 3164, 293, 286, 360, 294, 257, 636,
  13, 400, 411, 415, 848, 11, 1392, 11, 291, 393, 733, 295, 1322, 11, 411, 11, 498,
  286, 1604, 8944, 11, 51304], "temperature": 0.0, "avg_logprob": -0.2136070556640625,
  "compression_ratio": 1.8804780876494025, "no_speech_prob": 0.020604722201824188},
  {"id": 336, "seek": 216780, "start": 2187.48, "end": 2191.4, "text": " if you can
  do like almost like a dictionary, right? So let''s say you take a word, you can
  embed it,", "tokens": [51348, 498, 291, 393, 360, 411, 1920, 411, 257, 25890, 11,
  558, 30, 407, 718, 311, 584, 291, 747, 257, 1349, 11, 291, 393, 12240, 309, 11,
  51544], "temperature": 0.0, "avg_logprob": -0.2136070556640625, "compression_ratio":
  1.8804780876494025, "no_speech_prob": 0.020604722201824188}, {"id": 337, "seek":
  216780, "start": 2191.4, "end": 2195.8, "text": " take a word, you can embed it,
  like you can embed a dictionary, right? Now when you found that", "tokens": [51544,
  747, 257, 1349, 11, 291, 393, 12240, 309, 11, 411, 291, 393, 12240, 257, 25890,
  11, 558, 30, 823, 562, 291, 1352, 300, 51764], "temperature": 0.0, "avg_logprob":
  -0.2136070556640625, "compression_ratio": 1.8804780876494025, "no_speech_prob":
  0.020604722201824188}, {"id": 338, "seek": 219580, "start": 2195.8, "end": 2202.28,
  "text": " document, you can kind of from embeddings, you can map back to the words.
  If they happen to be", "tokens": [50364, 4166, 11, 291, 393, 733, 295, 490, 12240,
  29432, 11, 291, 393, 4471, 646, 281, 264, 2283, 13, 759, 436, 1051, 281, 312, 50688],
  "temperature": 0.0, "avg_logprob": -0.13280782197651111, "compression_ratio": 1.6866359447004609,
  "no_speech_prob": 0.0025919703766703606}, {"id": 339, "seek": 219580, "start": 2202.28,
  "end": 2207.2400000000002, "text": " closed enough, like geometrically, you can
  find closed enough words. So you can kind of try to say,", "tokens": [50688, 5395,
  1547, 11, 411, 12956, 81, 984, 11, 291, 393, 915, 5395, 1547, 2283, 13, 407, 291,
  393, 733, 295, 853, 281, 584, 11, 50936], "temperature": 0.0, "avg_logprob": -0.13280782197651111,
  "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.0025919703766703606},
  {"id": 340, "seek": 219580, "start": 2207.2400000000002, "end": 2213.4, "text":
  " okay, maybe these keywords are representative of this text, but I''m not 100%
  sure, but at least you try.", "tokens": [50936, 1392, 11, 1310, 613, 21009, 366,
  12424, 295, 341, 2487, 11, 457, 286, 478, 406, 2319, 4, 988, 11, 457, 412, 1935,
  291, 853, 13, 51244], "temperature": 0.0, "avg_logprob": -0.13280782197651111, "compression_ratio":
  1.6866359447004609, "no_speech_prob": 0.0025919703766703606}, {"id": 341, "seek":
  219580, "start": 2214.2000000000003, "end": 2217.2400000000002, "text": " So you
  go backwards, like reverse engineering from the embeddings.", "tokens": [51284,
  407, 291, 352, 12204, 11, 411, 9943, 7043, 490, 264, 12240, 29432, 13, 51436], "temperature":
  0.0, "avg_logprob": -0.13280782197651111, "compression_ratio": 1.6866359447004609,
  "no_speech_prob": 0.0025919703766703606}, {"id": 342, "seek": 221724, "start": 2218.2,
  "end": 2225.64, "text": " It''s interesting, sir. You may need to go through all
  the pain of dramatizing kind of these kind of", "tokens": [50412, 467, 311, 1880,
  11, 4735, 13, 509, 815, 643, 281, 352, 807, 439, 264, 1822, 295, 42749, 3319, 733,
  295, 613, 733, 295, 50784], "temperature": 0.0, "avg_logprob": -0.3267186608644995,
  "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.010406569577753544},
  {"id": 343, "seek": 221724, "start": 2225.64, "end": 2231.7999999999997, "text":
  " stuff that you may have saved by going through semantic search and now you are
  back to it. So,", "tokens": [50784, 1507, 300, 291, 815, 362, 6624, 538, 516, 807,
  47982, 3164, 293, 586, 291, 366, 646, 281, 309, 13, 407, 11, 51092], "temperature":
  0.0, "avg_logprob": -0.3267186608644995, "compression_ratio": 1.6048387096774193,
  "no_speech_prob": 0.010406569577753544}, {"id": 344, "seek": 221724, "start": 2232.8399999999997,
  "end": 2239.24, "text": " like straight-offs, but yeah, it might be a good... Yeah,
  dramatization is another thing, but like I think", "tokens": [51144, 411, 2997,
  12, 19231, 11, 457, 1338, 11, 309, 1062, 312, 257, 665, 485, 865, 11, 42749, 2144,
  307, 1071, 551, 11, 457, 411, 286, 519, 51464], "temperature": 0.0, "avg_logprob":
  -0.3267186608644995, "compression_ratio": 1.6048387096774193, "no_speech_prob":
  0.010406569577753544}, {"id": 345, "seek": 221724, "start": 2239.7999999999997,
  "end": 2246.3599999999997, "text": " there was this paper from, I believe Google
  about byte level training, right? So they don''t care", "tokens": [51492, 456, 390,
  341, 3035, 490, 11, 286, 1697, 3329, 466, 40846, 1496, 3097, 11, 558, 30, 407, 436,
  500, 380, 1127, 51820], "temperature": 0.0, "avg_logprob": -0.3267186608644995,
  "compression_ratio": 1.6048387096774193, "no_speech_prob": 0.010406569577753544},
  {"id": 346, "seek": 224636, "start": 2246.36, "end": 2252.6, "text": " if it''s
  like lemma or if it''s like suffix or prefix, they just go byte level. They don''t
  go sub-word", "tokens": [50364, 498, 309, 311, 411, 7495, 1696, 420, 498, 309, 311,
  411, 3889, 970, 420, 46969, 11, 436, 445, 352, 40846, 1496, 13, 814, 500, 380, 352,
  1422, 12, 7462, 50676], "temperature": 0.0, "avg_logprob": -0.13911275251196065,
  "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.001156385987997055},
  {"id": 347, "seek": 224636, "start": 2252.6, "end": 2258.36, "text": " level. They
  go byte level. And then with byte level, you can essentially kind of like, okay,
  now I can compute", "tokens": [50676, 1496, 13, 814, 352, 40846, 1496, 13, 400,
  550, 365, 40846, 1496, 11, 291, 393, 4476, 733, 295, 411, 11, 1392, 11, 586, 286,
  393, 14722, 50964], "temperature": 0.0, "avg_logprob": -0.13911275251196065, "compression_ratio":
  1.6958333333333333, "no_speech_prob": 0.001156385987997055}, {"id": 348, "seek":
  224636, "start": 2258.36, "end": 2264.04, "text": " the distance again, right? Okay,
  how close is this to this dictionary word or not? But then again,", "tokens": [50964,
  264, 4560, 797, 11, 558, 30, 1033, 11, 577, 1998, 307, 341, 281, 341, 25890, 1349,
  420, 406, 30, 583, 550, 797, 11, 51248], "temperature": 0.0, "avg_logprob": -0.13911275251196065,
  "compression_ratio": 1.6958333333333333, "no_speech_prob": 0.001156385987997055},
  {"id": 349, "seek": 224636, "start": 2264.6, "end": 2269.7200000000003, "text":
  " from there, in order to produce a snippet that will look like natural language,
  you will have to", "tokens": [51276, 490, 456, 11, 294, 1668, 281, 5258, 257, 35623,
  302, 300, 486, 574, 411, 3303, 2856, 11, 291, 486, 362, 281, 51532], "temperature":
  0.0, "avg_logprob": -0.13911275251196065, "compression_ratio": 1.6958333333333333,
  "no_speech_prob": 0.001156385987997055}, {"id": 350, "seek": 226972, "start": 2269.7999999999997,
  "end": 2277.16, "text": " use some kind of model like GPT or like in general, generate
  the sentence. And at that point,", "tokens": [50368, 764, 512, 733, 295, 2316, 411,
  26039, 51, 420, 411, 294, 2674, 11, 8460, 264, 8174, 13, 400, 412, 300, 935, 11,
  50736], "temperature": 0.0, "avg_logprob": -0.22329714638846263, "compression_ratio":
  1.455, "no_speech_prob": 0.0027010105550289154}, {"id": 351, "seek": 226972, "start":
  2277.16, "end": 2283.16, "text": " it might actually go completely different direction
  from your text, right? Start like hallucinating or", "tokens": [50736, 309, 1062,
  767, 352, 2584, 819, 3513, 490, 428, 2487, 11, 558, 30, 6481, 411, 35212, 8205,
  420, 51036], "temperature": 0.0, "avg_logprob": -0.22329714638846263, "compression_ratio":
  1.455, "no_speech_prob": 0.0027010105550289154}, {"id": 352, "seek": 226972, "start":
  2284.2, "end": 2292.68, "text": " write a news item that doesn''t exist. So yeah.
  Well, maybe you can use these extractive models", "tokens": [51088, 2464, 257, 2583,
  3174, 300, 1177, 380, 2514, 13, 407, 1338, 13, 1042, 11, 1310, 291, 393, 764, 613,
  8947, 488, 5245, 51512], "temperature": 0.0, "avg_logprob": -0.22329714638846263,
  "compression_ratio": 1.455, "no_speech_prob": 0.0027010105550289154}, {"id": 353,
  "seek": 229268, "start": 2292.7599999999998, "end": 2302.3599999999997, "text":
  " from a sentence, giving a context, but nature, all these top-notch research is
  basically.", "tokens": [50368, 490, 257, 8174, 11, 2902, 257, 4319, 11, 457, 3687,
  11, 439, 613, 1192, 12, 2247, 339, 2132, 307, 1936, 13, 50848], "temperature": 0.0,
  "avg_logprob": -0.32912081400553383, "compression_ratio": 1.5265957446808511, "no_speech_prob":
  0.011391001753509045}, {"id": 354, "seek": 229268, "start": 2303.08, "end": 2308.7599999999998,
  "text": " Yeah, yeah. But I mean, like attention, what you mentioned, attention
  probably can be used here,", "tokens": [50884, 865, 11, 1338, 13, 583, 286, 914,
  11, 411, 3202, 11, 437, 291, 2835, 11, 3202, 1391, 393, 312, 1143, 510, 11, 51168],
  "temperature": 0.0, "avg_logprob": -0.32912081400553383, "compression_ratio": 1.5265957446808511,
  "no_speech_prob": 0.011391001753509045}, {"id": 355, "seek": 229268, "start": 2308.7599999999998,
  "end": 2314.9199999999996, "text": " right? So like you can ask the model, okay,
  what did you pay attention to when you did the matching,", "tokens": [51168, 558,
  30, 407, 411, 291, 393, 1029, 264, 2316, 11, 1392, 11, 437, 630, 291, 1689, 3202,
  281, 562, 291, 630, 264, 14324, 11, 51476], "temperature": 0.0, "avg_logprob": -0.32912081400553383,
  "compression_ratio": 1.5265957446808511, "no_speech_prob": 0.011391001753509045},
  {"id": 356, "seek": 231492, "start": 2315.7200000000003, "end": 2322.6800000000003,
  "text": " but still it''s not some people, as you say, like you can say it interpretability,
  but on that hand,", "tokens": [50404, 457, 920, 309, 311, 406, 512, 561, 11, 382,
  291, 584, 11, 411, 291, 393, 584, 309, 7302, 2310, 11, 457, 322, 300, 1011, 11,
  50752], "temperature": 0.0, "avg_logprob": -0.14122381015699736, "compression_ratio":
  1.6842105263157894, "no_speech_prob": 0.01424292754381895}, {"id": 357, "seek":
  231492, "start": 2322.6800000000003, "end": 2328.44, "text": " it''s kind of like
  when you go specifically to that product case, you need that snippet or you need",
  "tokens": [50752, 309, 311, 733, 295, 411, 562, 291, 352, 4682, 281, 300, 1674,
  1389, 11, 291, 643, 300, 35623, 302, 420, 291, 643, 51040], "temperature": 0.0,
  "avg_logprob": -0.14122381015699736, "compression_ratio": 1.6842105263157894, "no_speech_prob":
  0.01424292754381895}, {"id": 358, "seek": 231492, "start": 2328.44, "end": 2334.6,
  "text": " that kind of context of the match. Or like if you said mathematics and
  it picked algebra,", "tokens": [51040, 300, 733, 295, 4319, 295, 264, 2995, 13,
  1610, 411, 498, 291, 848, 18666, 293, 309, 6183, 21989, 11, 51348], "temperature":
  0.0, "avg_logprob": -0.14122381015699736, "compression_ratio": 1.6842105263157894,
  "no_speech_prob": 0.01424292754381895}, {"id": 359, "seek": 231492, "start": 2334.6,
  "end": 2339.08, "text": " like why did it pick algebra? At least can you explain?
  Because here it''s more or less obvious,", "tokens": [51348, 411, 983, 630, 309,
  1888, 21989, 30, 1711, 1935, 393, 291, 2903, 30, 1436, 510, 309, 311, 544, 420,
  1570, 6322, 11, 51572], "temperature": 0.0, "avg_logprob": -0.14122381015699736,
  "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.01424292754381895},
  {"id": 360, "seek": 233908, "start": 2339.08, "end": 2344.52, "text": " but in a
  specific domain, it might not be, right? Yes, like what do we do?", "tokens": [50364,
  457, 294, 257, 2685, 9274, 11, 309, 1062, 406, 312, 11, 558, 30, 1079, 11, 411,
  437, 360, 321, 360, 30, 50636], "temperature": 0.0, "avg_logprob": -0.2686802724773964,
  "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.019221186637878418},
  {"id": 361, "seek": 233908, "start": 2346.68, "end": 2351.24, "text": " Maybe you
  are not using the right tool, I don''t know. Maybe we are obsessed on using the",
  "tokens": [50744, 2704, 291, 366, 406, 1228, 264, 558, 2290, 11, 286, 500, 380,
  458, 13, 2704, 321, 366, 16923, 322, 1228, 264, 50972], "temperature": 0.0, "avg_logprob":
  -0.2686802724773964, "compression_ratio": 1.5818181818181818, "no_speech_prob":
  0.019221186637878418}, {"id": 362, "seek": 233908, "start": 2352.2799999999997,
  "end": 2360.04, "text": " declared for everything. But I think these two walls of
  keyword, what we call traditional", "tokens": [51024, 15489, 337, 1203, 13, 583,
  286, 519, 613, 732, 7920, 295, 20428, 11, 437, 321, 818, 5164, 51412], "temperature":
  0.0, "avg_logprob": -0.2686802724773964, "compression_ratio": 1.5818181818181818,
  "no_speech_prob": 0.019221186637878418}, {"id": 363, "seek": 233908, "start": 2360.04,
  "end": 2365.4, "text": " search and this neural search, I think they can be combined
  to power things to the next level.", "tokens": [51412, 3164, 293, 341, 18161, 3164,
  11, 286, 519, 436, 393, 312, 9354, 281, 1347, 721, 281, 264, 958, 1496, 13, 51680],
  "temperature": 0.0, "avg_logprob": -0.2686802724773964, "compression_ratio": 1.5818181818181818,
  "no_speech_prob": 0.019221186637878418}, {"id": 364, "seek": 236540, "start": 2365.48,
  "end": 2371.7200000000003, "text": " I think they need to be enemies and there is
  good and bad team both sides. Do you have any thoughts", "tokens": [50368, 286,
  519, 436, 643, 281, 312, 7805, 293, 456, 307, 665, 293, 1578, 1469, 1293, 4881,
  13, 1144, 291, 362, 604, 4598, 50680], "temperature": 0.0, "avg_logprob": -0.24581652323404948,
  "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.009404975920915604},
  {"id": 365, "seek": 236540, "start": 2371.7200000000003, "end": 2381.96, "text":
  " how you would combine? For instance, in any solution, you can have solution, I
  don''t know, maybe you can", "tokens": [50680, 577, 291, 576, 10432, 30, 1171, 5197,
  11, 294, 604, 3827, 11, 291, 393, 362, 3827, 11, 286, 500, 380, 458, 11, 1310, 291,
  393, 51192], "temperature": 0.0, "avg_logprob": -0.24581652323404948, "compression_ratio":
  1.5904255319148937, "no_speech_prob": 0.009404975920915604}, {"id": 366, "seek":
  236540, "start": 2381.96, "end": 2391.0, "text": " get results based on both sides
  and then at our ranking steps consider what is best, you know,", "tokens": [51192,
  483, 3542, 2361, 322, 1293, 4881, 293, 550, 412, 527, 17833, 4439, 1949, 437, 307,
  1151, 11, 291, 458, 11, 51644], "temperature": 0.0, "avg_logprob": -0.24581652323404948,
  "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.009404975920915604},
  {"id": 367, "seek": 239100, "start": 2391.64, "end": 2402.28, "text": " is this
  a complex query? Maybe I''m looking more for some semantically reached solution.",
  "tokens": [50396, 307, 341, 257, 3997, 14581, 30, 2704, 286, 478, 1237, 544, 337,
  512, 4361, 49505, 6488, 3827, 13, 50928], "temperature": 0.0, "avg_logprob": -0.3067806995276249,
  "compression_ratio": 1.451086956521739, "no_speech_prob": 0.008017083629965782},
  {"id": 368, "seek": 239100, "start": 2403.24, "end": 2410.68, "text": " Did this
  guy just send a couple of keywords? Is it semantically reached enough? No,", "tokens":
  [50976, 2589, 341, 2146, 445, 2845, 257, 1916, 295, 21009, 30, 1119, 309, 4361,
  49505, 6488, 1547, 30, 883, 11, 51348], "temperature": 0.0, "avg_logprob": -0.3067806995276249,
  "compression_ratio": 1.451086956521739, "no_speech_prob": 0.008017083629965782},
  {"id": 369, "seek": 239100, "start": 2411.32, "end": 2419.0, "text": " this user
  might be expecting keyword based feedback. Yeah, that''s true. Well, you could even
  go", "tokens": [51380, 341, 4195, 1062, 312, 9650, 20428, 2361, 5824, 13, 865, 11,
  300, 311, 2074, 13, 1042, 11, 291, 727, 754, 352, 51764], "temperature": 0.0, "avg_logprob":
  -0.3067806995276249, "compression_ratio": 1.451086956521739, "no_speech_prob": 0.008017083629965782},
  {"id": 370, "seek": 241900, "start": 2419.0, "end": 2426.36, "text": " as simple
  as giving that control to users. So, if they know that it''s keyword, they first
  want to", "tokens": [50364, 382, 2199, 382, 2902, 300, 1969, 281, 5022, 13, 407,
  11, 498, 436, 458, 300, 309, 311, 20428, 11, 436, 700, 528, 281, 50732], "temperature":
  0.0, "avg_logprob": -0.3150469636263913, "compression_ratio": 1.5901639344262295,
  "no_speech_prob": 0.007296734489500523}, {"id": 371, "seek": 241900, "start": 2426.36,
  "end": 2431.32, "text": " go with what they know, what works or may not work and
  then if they are not satisfied enough,", "tokens": [50732, 352, 365, 437, 436, 458,
  11, 437, 1985, 420, 815, 406, 589, 293, 550, 498, 436, 366, 406, 11239, 1547, 11,
  50980], "temperature": 0.0, "avg_logprob": -0.3150469636263913, "compression_ratio":
  1.5901639344262295, "no_speech_prob": 0.007296734489500523}, {"id": 372, "seek":
  241900, "start": 2431.32, "end": 2438.6, "text": " then they optimize for equal,
  they might go into explorative mode, that''s on the similarity search.", "tokens":
  [50980, 550, 436, 19719, 337, 2681, 11, 436, 1062, 352, 666, 24765, 1166, 4391,
  11, 300, 311, 322, 264, 32194, 3164, 13, 51344], "temperature": 0.0, "avg_logprob":
  -0.3150469636263913, "compression_ratio": 1.5901639344262295, "no_speech_prob":
  0.007296734489500523}, {"id": 373, "seek": 243860, "start": 2439.0, "end": 2448.2799999999997,
  "text": " That might be quite viable. So, it''s interesting. The problem is that
  keyword search,", "tokens": [50384, 663, 1062, 312, 1596, 22024, 13, 407, 11, 309,
  311, 1880, 13, 440, 1154, 307, 300, 20428, 3164, 11, 50848], "temperature": 0.0,
  "avg_logprob": -0.3180640846170405, "compression_ratio": 1.5570175438596492, "no_speech_prob":
  0.1174476370215416}, {"id": 374, "seek": 243860, "start": 2448.2799999999997, "end":
  2455.72, "text": " well, as far as search, it might have not a good future for image
  based search or any other", "tokens": [50848, 731, 11, 382, 1400, 382, 3164, 11,
  309, 1062, 362, 406, 257, 665, 2027, 337, 3256, 2361, 3164, 420, 604, 661, 51220],
  "temperature": 0.0, "avg_logprob": -0.3180640846170405, "compression_ratio": 1.5570175438596492,
  "no_speech_prob": 0.1174476370215416}, {"id": 375, "seek": 243860, "start": 2455.72,
  "end": 2461.16, "text": " mobility related search. Yeah, exactly. The moment you
  go beyond text, what do you do?", "tokens": [51220, 16199, 4077, 3164, 13, 865,
  11, 2293, 13, 440, 1623, 291, 352, 4399, 2487, 11, 437, 360, 291, 360, 30, 51492],
  "temperature": 0.0, "avg_logprob": -0.3180640846170405, "compression_ratio": 1.5570175438596492,
  "no_speech_prob": 0.1174476370215416}, {"id": 376, "seek": 243860, "start": 2461.96,
  "end": 2467.64, "text": " That''s a big power, I think, and the big future that
  Neurochurch has ahead. There is where", "tokens": [51532, 663, 311, 257, 955, 1347,
  11, 286, 519, 11, 293, 264, 955, 2027, 300, 1734, 7052, 339, 2476, 575, 2286, 13,
  821, 307, 689, 51816], "temperature": 0.0, "avg_logprob": -0.3180640846170405, "compression_ratio":
  1.5570175438596492, "no_speech_prob": 0.1174476370215416}, {"id": 377, "seek": 246860,
  "start": 2468.92, "end": 2473.0, "text": " not any traditional search solution,
  I think, will keep up.", "tokens": [50380, 406, 604, 5164, 3164, 3827, 11, 286,
  519, 11, 486, 1066, 493, 13, 50584], "temperature": 0.0, "avg_logprob": -0.24995876761043773,
  "compression_ratio": 1.6164383561643836, "no_speech_prob": 0.005052740685641766},
  {"id": 378, "seek": 246860, "start": 2473.96, "end": 2476.8399999999997, "text":
  " So, if I want to build a multimodal search, can I pick some", "tokens": [50632,
  407, 11, 498, 286, 528, 281, 1322, 257, 32972, 378, 304, 3164, 11, 393, 286, 1888,
  512, 50776], "temperature": 0.0, "avg_logprob": -0.24995876761043773, "compression_ratio":
  1.6164383561643836, "no_speech_prob": 0.005052740685641766}, {"id": 379, "seek":
  246860, "start": 2477.72, "end": 2482.04, "text": " executor from the marketplace
  and plug it into Gina today and do it?", "tokens": [50820, 7568, 284, 490, 264,
  19455, 293, 5452, 309, 666, 34711, 965, 293, 360, 309, 30, 51036], "temperature":
  0.0, "avg_logprob": -0.24995876761043773, "compression_ratio": 1.6164383561643836,
  "no_speech_prob": 0.005052740685641766}, {"id": 380, "seek": 246860, "start": 2483.56,
  "end": 2487.48, "text": " Yeah, I don''t know. I think we have some, for instance,
  but you can use clip", "tokens": [51112, 865, 11, 286, 500, 380, 458, 13, 286, 519,
  321, 362, 512, 11, 337, 5197, 11, 457, 291, 393, 764, 7353, 51308], "temperature":
  0.0, "avg_logprob": -0.24995876761043773, "compression_ratio": 1.6164383561643836,
  "no_speech_prob": 0.005052740685641766}, {"id": 381, "seek": 246860, "start": 2488.2,
  "end": 2493.0, "text": " that clip. You can use clip to encode. I think there is
  audio, you can text, or there is", "tokens": [51344, 300, 7353, 13, 509, 393, 764,
  7353, 281, 2058, 1429, 13, 286, 519, 456, 307, 6278, 11, 291, 393, 2487, 11, 420,
  456, 307, 51584], "temperature": 0.0, "avg_logprob": -0.24995876761043773, "compression_ratio":
  1.6164383561643836, "no_speech_prob": 0.005052740685641766}, {"id": 382, "seek":
  249300, "start": 2493.72, "end": 2499.24, "text": " image and text, and it performs
  very well. We have wrapped it in one of these executors and", "tokens": [50400,
  3256, 293, 2487, 11, 293, 309, 26213, 588, 731, 13, 492, 362, 14226, 309, 294, 472,
  295, 613, 7568, 830, 293, 50676], "temperature": 0.0, "avg_logprob": -0.28962413124416186,
  "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.006543709896504879},
  {"id": 383, "seek": 249300, "start": 2499.24, "end": 2504.36, "text": " hot modules,
  and you can use these clip models to do your close model search.", "tokens": [50676,
  2368, 16679, 11, 293, 291, 393, 764, 613, 7353, 5245, 281, 360, 428, 1998, 2316,
  3164, 13, 50932], "temperature": 0.0, "avg_logprob": -0.28962413124416186, "compression_ratio":
  1.6069868995633187, "no_speech_prob": 0.006543709896504879}, {"id": 384, "seek":
  249300, "start": 2505.96, "end": 2511.56, "text": " It''s quite efficient without
  the match-faint tuning to search for images given text and the other", "tokens":
  [51012, 467, 311, 1596, 7148, 1553, 264, 2995, 12, 69, 5114, 15164, 281, 3164, 337,
  5267, 2212, 2487, 293, 264, 661, 51292], "temperature": 0.0, "avg_logprob": -0.28962413124416186,
  "compression_ratio": 1.6069868995633187, "no_speech_prob": 0.006543709896504879},
  {"id": 385, "seek": 249300, "start": 2511.56, "end": 2517.8, "text": " way around.
  It''s quite impressive. Yeah, that sounds cool. When I was thinking, if I want to
  combine", "tokens": [51292, 636, 926, 13, 467, 311, 1596, 8992, 13, 865, 11, 300,
  3263, 1627, 13, 1133, 286, 390, 1953, 11, 498, 286, 528, 281, 10432, 51604], "temperature":
  0.0, "avg_logprob": -0.28962413124416186, "compression_ratio": 1.6069868995633187,
  "no_speech_prob": 0.006543709896504879}, {"id": 386, "seek": 251780, "start": 2517.88,
  "end": 2523.6400000000003, "text": " like speech, text, and image, then I need to
  probably come up with some meta model of that.", "tokens": [50368, 411, 6218, 11,
  2487, 11, 293, 3256, 11, 550, 286, 643, 281, 1391, 808, 493, 365, 512, 19616, 2316,
  295, 300, 13, 50656], "temperature": 0.0, "avg_logprob": -0.19282292210778526, "compression_ratio":
  1.6944444444444444, "no_speech_prob": 0.006866848096251488}, {"id": 387, "seek":
  251780, "start": 2523.6400000000003, "end": 2529.4, "text": " Right? There is some
  research in this area where it is not that like modalities are treated", "tokens":
  [50656, 1779, 30, 821, 307, 512, 2132, 294, 341, 1859, 689, 309, 307, 406, 300,
  411, 1072, 16110, 366, 8668, 50944], "temperature": 0.0, "avg_logprob": -0.19282292210778526,
  "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.006866848096251488},
  {"id": 388, "seek": 251780, "start": 2529.4, "end": 2535.1600000000003, "text":
  " differently and encoded separately, but where they are considered together, even
  there is some", "tokens": [50944, 7614, 293, 2058, 12340, 14759, 11, 457, 689, 436,
  366, 4888, 1214, 11, 754, 456, 307, 512, 51232], "temperature": 0.0, "avg_logprob":
  -0.19282292210778526, "compression_ratio": 1.6944444444444444, "no_speech_prob":
  0.006866848096251488}, {"id": 389, "seek": 251780, "start": 2535.1600000000003,
  "end": 2541.2400000000002, "text": " research where there is multimodality and some
  contact switch, so they move the vector.", "tokens": [51232, 2132, 689, 456, 307,
  32972, 378, 1860, 293, 512, 3385, 3679, 11, 370, 436, 1286, 264, 8062, 13, 51536],
  "temperature": 0.0, "avg_logprob": -0.19282292210778526, "compression_ratio": 1.6944444444444444,
  "no_speech_prob": 0.006866848096251488}, {"id": 390, "seek": 254124, "start": 2541.72,
  "end": 2548.04, "text": " So that''s also possible to get the latest research, wrap
  it into one of these models and", "tokens": [50388, 407, 300, 311, 611, 1944, 281,
  483, 264, 6792, 2132, 11, 7019, 309, 666, 472, 295, 613, 5245, 293, 50704], "temperature":
  0.0, "avg_logprob": -0.2535543441772461, "compression_ratio": 1.6123348017621146,
  "no_speech_prob": 0.003322682110592723}, {"id": 391, "seek": 254124, "start": 2548.04,
  "end": 2554.4399999999996, "text": " deploy it in production. But this is not so
  easy. For us, we didn''t focus on building these", "tokens": [50704, 7274, 309,
  294, 4265, 13, 583, 341, 307, 406, 370, 1858, 13, 1171, 505, 11, 321, 994, 380,
  1879, 322, 2390, 613, 51024], "temperature": 0.0, "avg_logprob": -0.2535543441772461,
  "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.003322682110592723},
  {"id": 392, "seek": 254124, "start": 2554.4399999999996, "end": 2561.16, "text":
  " front scratch, but we''re also looking to having these top-notch researchers into
  building", "tokens": [51024, 1868, 8459, 11, 457, 321, 434, 611, 1237, 281, 1419,
  613, 1192, 12, 2247, 339, 10309, 666, 2390, 51360], "temperature": 0.0, "avg_logprob":
  -0.2535543441772461, "compression_ratio": 1.6123348017621146, "no_speech_prob":
  0.003322682110592723}, {"id": 393, "seek": 254124, "start": 2562.12, "end": 2570.52,
  "text": " these modules in here. So like, in that case, would you prefer communities
  to help out to bring", "tokens": [51408, 613, 16679, 294, 510, 13, 407, 411, 11,
  294, 300, 1389, 11, 576, 291, 4382, 4456, 281, 854, 484, 281, 1565, 51828], "temperature":
  0.0, "avg_logprob": -0.2535543441772461, "compression_ratio": 1.6123348017621146,
  "no_speech_prob": 0.003322682110592723}, {"id": 394, "seek": 257052, "start": 2570.52,
  "end": 2578.92, "text": " in the model, or are you helping to do that? Right now,
  we are driving this direction to offer", "tokens": [50364, 294, 264, 2316, 11, 420,
  366, 291, 4315, 281, 360, 300, 30, 1779, 586, 11, 321, 366, 4840, 341, 3513, 281,
  2626, 50784], "temperature": 0.0, "avg_logprob": -0.3101684794706457, "compression_ratio":
  1.619718309859155, "no_speech_prob": 0.0037170234136283398}, {"id": 395, "seek":
  257052, "start": 2578.92, "end": 2585.08, "text": " these for the community. I think
  that our dream as an open source is to have the community", "tokens": [50784, 613,
  337, 264, 1768, 13, 286, 519, 300, 527, 3055, 382, 364, 1269, 4009, 307, 281, 362,
  264, 1768, 51092], "temperature": 0.0, "avg_logprob": -0.3101684794706457, "compression_ratio":
  1.619718309859155, "no_speech_prob": 0.0037170234136283398}, {"id": 396, "seek":
  257052, "start": 2585.08, "end": 2592.2, "text": " flourish and be alive upon itself.
  So the future should be community driven.", "tokens": [51092, 38311, 293, 312, 5465,
  3564, 2564, 13, 407, 264, 2027, 820, 312, 1768, 9555, 13, 51448], "temperature":
  0.0, "avg_logprob": -0.3101684794706457, "compression_ratio": 1.619718309859155,
  "no_speech_prob": 0.0037170234136283398}, {"id": 397, "seek": 257052, "start": 2593.16,
  "end": 2599.0, "text": " Yeah, because in the end, community might also know kind
  of when these growths be,", "tokens": [51496, 865, 11, 570, 294, 264, 917, 11, 1768,
  1062, 611, 458, 733, 295, 562, 613, 4599, 82, 312, 11, 51788], "temperature": 0.0,
  "avg_logprob": -0.3101684794706457, "compression_ratio": 1.619718309859155, "no_speech_prob":
  0.0037170234136283398}, {"id": 398, "seek": 259900, "start": 2599.0, "end": 2604.92,
  "text": " you know, community will be kind of helping each other. Like some of these
  things will become", "tokens": [50364, 291, 458, 11, 1768, 486, 312, 733, 295, 4315,
  1184, 661, 13, 1743, 512, 295, 613, 721, 486, 1813, 50660], "temperature": 0.0,
  "avg_logprob": -0.13075381462727118, "compression_ratio": 1.8638132295719845, "no_speech_prob":
  0.004427563864737749}, {"id": 399, "seek": 259900, "start": 2605.48, "end": 2610.76,
  "text": " what you may call commodity to some extent, right? Or at least the way
  you integrate might become", "tokens": [50688, 437, 291, 815, 818, 29125, 281, 512,
  8396, 11, 558, 30, 1610, 412, 1935, 264, 636, 291, 13365, 1062, 1813, 50952], "temperature":
  0.0, "avg_logprob": -0.13075381462727118, "compression_ratio": 1.8638132295719845,
  "no_speech_prob": 0.004427563864737749}, {"id": 400, "seek": 259900, "start": 2610.76,
  "end": 2615.48, "text": " commodity and the use cases might become commodity and
  there will be new use cases which are", "tokens": [50952, 29125, 293, 264, 764,
  3331, 1062, 1813, 29125, 293, 456, 486, 312, 777, 764, 3331, 597, 366, 51188], "temperature":
  0.0, "avg_logprob": -0.13075381462727118, "compression_ratio": 1.8638132295719845,
  "no_speech_prob": 0.004427563864737749}, {"id": 401, "seek": 259900, "start": 2615.48,
  "end": 2620.44, "text": " untapped, but I think community can definitely help out
  each other. What we might need to focus on", "tokens": [51188, 517, 1328, 3320,
  11, 457, 286, 519, 1768, 393, 2138, 854, 484, 1184, 661, 13, 708, 321, 1062, 643,
  281, 1879, 322, 51436], "temperature": 0.0, "avg_logprob": -0.13075381462727118,
  "compression_ratio": 1.8638132295719845, "no_speech_prob": 0.004427563864737749},
  {"id": 402, "seek": 259900, "start": 2620.44, "end": 2627.08, "text": " to make
  these models easier to use or easier to find if we have a marketplace where everything,",
  "tokens": [51436, 281, 652, 613, 5245, 3571, 281, 764, 420, 3571, 281, 915, 498,
  321, 362, 257, 19455, 689, 1203, 11, 51768], "temperature": 0.0, "avg_logprob":
  -0.13075381462727118, "compression_ratio": 1.8638132295719845, "no_speech_prob":
  0.004427563864737749}, {"id": 403, "seek": 262708, "start": 2627.08, "end": 2632.68,
  "text": " maybe we need to help the community on finding what they need in every
  time.", "tokens": [50364, 1310, 321, 643, 281, 854, 264, 1768, 322, 5006, 437, 436,
  643, 294, 633, 565, 13, 50644], "temperature": 0.0, "avg_logprob": -0.2001635996500651,
  "compression_ratio": 1.5260663507109005, "no_speech_prob": 0.005009770393371582},
  {"id": 404, "seek": 262708, "start": 2633.48, "end": 2640.36, "text": " Yeah, yeah.
  Content wise, hopefully there is a time where community is the main contributor
  there.", "tokens": [50684, 865, 11, 1338, 13, 30078, 10829, 11, 4696, 456, 307,
  257, 565, 689, 1768, 307, 264, 2135, 42859, 456, 13, 51028], "temperature": 0.0,
  "avg_logprob": -0.2001635996500651, "compression_ratio": 1.5260663507109005, "no_speech_prob":
  0.005009770393371582}, {"id": 405, "seek": 262708, "start": 2641.16, "end": 2644.44,
  "text": " Was there something else in Gina that we should know about as users?",
  "tokens": [51068, 3027, 456, 746, 1646, 294, 34711, 300, 321, 820, 458, 466, 382,
  5022, 30, 51232], "temperature": 0.0, "avg_logprob": -0.2001635996500651, "compression_ratio":
  1.5260663507109005, "no_speech_prob": 0.005009770393371582}, {"id": 406, "seek":
  262708, "start": 2645.16, "end": 2650.44, "text": " Some cool feature or some system
  that you think doesn''t exist in competitors?", "tokens": [51268, 2188, 1627, 4111,
  420, 512, 1185, 300, 291, 519, 1177, 380, 2514, 294, 18333, 30, 51532], "temperature":
  0.0, "avg_logprob": -0.2001635996500651, "compression_ratio": 1.5260663507109005,
  "no_speech_prob": 0.005009770393371582}, {"id": 407, "seek": 265044, "start": 2650.44,
  "end": 2653.48, "text": " Is there something at all to school?", "tokens": [50364,
  1119, 456, 746, 412, 439, 281, 1395, 30, 50516], "temperature": 0.0, "avg_logprob":
  -0.526416318962373, "compression_ratio": 1.592964824120603, "no_speech_prob": 0.03870006650686264},
  {"id": 408, "seek": 265044, "start": 2654.92, "end": 2661.16, "text": " I don''t
  know right now about competitors. So I think what I like the most is the easy,",
  "tokens": [50588, 286, 500, 380, 458, 558, 586, 466, 18333, 13, 407, 286, 519, 437,
  286, 411, 264, 881, 307, 264, 1858, 11, 50900], "temperature": 0.0, "avg_logprob":
  -0.526416318962373, "compression_ratio": 1.592964824120603, "no_speech_prob": 0.03870006650686264},
  {"id": 409, "seek": 265044, "start": 2662.04, "end": 2670.76, "text": " the easy
  views and the time saving. You go out to our readme and you try to build from zero
  to", "tokens": [50944, 264, 1858, 6809, 293, 264, 565, 6816, 13, 509, 352, 484,
  281, 527, 1401, 1398, 293, 291, 853, 281, 1322, 490, 4018, 281, 51380], "temperature":
  0.0, "avg_logprob": -0.526416318962373, "compression_ratio": 1.592964824120603,
  "no_speech_prob": 0.03870006650686264}, {"id": 410, "seek": 265044, "start": 2672.2000000000003,
  "end": 2676.36, "text": " to the plighting core net is an neural search solution
  and image search solution. I think you will", "tokens": [51452, 281, 264, 499, 397,
  278, 4965, 2533, 307, 364, 18161, 3164, 3827, 293, 3256, 3164, 3827, 13, 286, 519,
  291, 486, 51660], "temperature": 0.0, "avg_logprob": -0.526416318962373, "compression_ratio":
  1.592964824120603, "no_speech_prob": 0.03870006650686264}, {"id": 411, "seek": 267636,
  "start": 2676.76, "end": 2683.56, "text": " you will all enjoy days and nights.
  Yeah. Yeah. So it''s like kind of well-oiled machine.", "tokens": [50384, 291, 486,
  439, 2103, 1708, 293, 13249, 13, 865, 13, 865, 13, 407, 309, 311, 411, 733, 295,
  731, 12, 78, 7292, 3479, 13, 50724], "temperature": 0.0, "avg_logprob": -0.29376270294189455,
  "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.025605594739317894},
  {"id": 412, "seek": 267636, "start": 2684.36, "end": 2691.32, "text": " But can
  I also bring it up on my laptop? Yes. You can try on your laptop everything.", "tokens":
  [50764, 583, 393, 286, 611, 1565, 309, 493, 322, 452, 10732, 30, 1079, 13, 509,
  393, 853, 322, 428, 10732, 1203, 13, 51112], "temperature": 0.0, "avg_logprob":
  -0.29376270294189455, "compression_ratio": 1.5676855895196506, "no_speech_prob":
  0.025605594739317894}, {"id": 413, "seek": 267636, "start": 2691.32, "end": 2699.1600000000003,
  "text": " So the point is you may not be able to index so many images but you can
  get the first feeling", "tokens": [51112, 407, 264, 935, 307, 291, 815, 406, 312,
  1075, 281, 8186, 370, 867, 5267, 457, 291, 393, 483, 264, 700, 2633, 51504], "temperature":
  0.0, "avg_logprob": -0.29376270294189455, "compression_ratio": 1.5676855895196506,
  "no_speech_prob": 0.025605594739317894}, {"id": 414, "seek": 267636, "start": 2699.1600000000003,
  "end": 2703.6400000000003, "text": " with your laptop. Yeah, I mean if I want to
  be like a demo to impress my manager, you know,", "tokens": [51504, 365, 428, 10732,
  13, 865, 11, 286, 914, 498, 286, 528, 281, 312, 411, 257, 10723, 281, 6729, 452,
  6598, 11, 291, 458, 11, 51728], "temperature": 0.0, "avg_logprob": -0.29376270294189455,
  "compression_ratio": 1.5676855895196506, "no_speech_prob": 0.025605594739317894},
  {"id": 415, "seek": 270364, "start": 2704.12, "end": 2709.4, "text": " so I usually
  use my laptop right? Like that''s maybe one way. Gina is really for that.", "tokens":
  [50388, 370, 286, 2673, 764, 452, 10732, 558, 30, 1743, 300, 311, 1310, 472, 636,
  13, 34711, 307, 534, 337, 300, 13, 50652], "temperature": 0.0, "avg_logprob": -0.2590641055190772,
  "compression_ratio": 1.7153846153846153, "no_speech_prob": 0.02220165729522705},
  {"id": 416, "seek": 270364, "start": 2709.7999999999997, "end": 2714.68, "text":
  " Yeah, that''s that''s pretty cool. And I think also like it''s nice that you said
  it''s", "tokens": [50672, 865, 11, 300, 311, 300, 311, 1238, 1627, 13, 400, 286,
  519, 611, 411, 309, 311, 1481, 300, 291, 848, 309, 311, 50916], "temperature": 0.0,
  "avg_logprob": -0.2590641055190772, "compression_ratio": 1.7153846153846153, "no_speech_prob":
  0.02220165729522705}, {"id": 417, "seek": 270364, "start": 2715.56, "end": 2722.6,
  "text": " Python friendly so it opens doors to so many things like especially like
  on hiking face it''s", "tokens": [50960, 15329, 9208, 370, 309, 9870, 8077, 281,
  370, 867, 721, 411, 2318, 411, 322, 23784, 1851, 309, 311, 51312], "temperature":
  0.0, "avg_logprob": -0.2590641055190772, "compression_ratio": 1.7153846153846153,
  "no_speech_prob": 0.02220165729522705}, {"id": 418, "seek": 270364, "start": 2722.6,
  "end": 2727.24, "text": " pretty much all Python right so I need to pick some models
  like it in and do I need to", "tokens": [51312, 1238, 709, 439, 15329, 558, 370,
  286, 643, 281, 1888, 512, 5245, 411, 309, 294, 293, 360, 286, 643, 281, 51544],
  "temperature": 0.0, "avg_logprob": -0.2590641055190772, "compression_ratio": 1.7153846153846153,
  "no_speech_prob": 0.02220165729522705}, {"id": 419, "seek": 270364, "start": 2727.24,
  "end": 2732.2, "text": " containerize it maybe or figure out isolation and so on
  like just plug it in and start using it.", "tokens": [51544, 10129, 1125, 309, 1310,
  420, 2573, 484, 16001, 293, 370, 322, 411, 445, 5452, 309, 294, 293, 722, 1228,
  309, 13, 51792], "temperature": 0.0, "avg_logprob": -0.2590641055190772, "compression_ratio":
  1.7153846153846153, "no_speech_prob": 0.02220165729522705}, {"id": 420, "seek":
  273220, "start": 2732.2, "end": 2740.04, "text": " I think that''s also a great
  boost to productivity and actually kind of implementing the use case", "tokens":
  [50364, 286, 519, 300, 311, 611, 257, 869, 9194, 281, 15604, 293, 767, 733, 295,
  18114, 264, 764, 1389, 50756], "temperature": 0.0, "avg_logprob": -0.1659651096050556,
  "compression_ratio": 1.541237113402062, "no_speech_prob": 0.0006990849506109953},
  {"id": 421, "seek": 273220, "start": 2740.04, "end": 2747.24, "text": " rather than
  focusing on some mundane components and parts and processes right? Yeah and even
  these", "tokens": [50756, 2831, 813, 8416, 322, 512, 43497, 6677, 293, 3166, 293,
  7555, 558, 30, 865, 293, 754, 613, 51116], "temperature": 0.0, "avg_logprob": -0.1659651096050556,
  "compression_ratio": 1.541237113402062, "no_speech_prob": 0.0006990849506109953},
  {"id": 422, "seek": 273220, "start": 2747.24, "end": 2755.3999999999996, "text":
  " modules that we have they are already containerized for you. So we have on our
  end we build a container", "tokens": [51116, 16679, 300, 321, 362, 436, 366, 1217,
  10129, 1602, 337, 291, 13, 407, 321, 362, 322, 527, 917, 321, 1322, 257, 10129,
  51524], "temperature": 0.0, "avg_logprob": -0.1659651096050556, "compression_ratio":
  1.541237113402062, "no_speech_prob": 0.0006990849506109953}, {"id": 423, "seek":
  275540, "start": 2756.04, "end": 2763.0, "text": " for you so that you can be in
  an isolated way with your all your dependencies and stuff.", "tokens": [50396, 337,
  291, 370, 300, 291, 393, 312, 294, 364, 14621, 636, 365, 428, 439, 428, 36606, 293,
  1507, 13, 50744], "temperature": 0.0, "avg_logprob": -0.23395265534866688, "compression_ratio":
  1.5194805194805194, "no_speech_prob": 0.003008848987519741}, {"id": 424, "seek":
  275540, "start": 2763.0, "end": 2768.44, "text": " Yeah, yeah. Sounds great. I mean
  I think we now have pretty good understanding of Gina.", "tokens": [50744, 865,
  11, 1338, 13, 14576, 869, 13, 286, 914, 286, 519, 321, 586, 362, 1238, 665, 3701,
  295, 34711, 13, 51016], "temperature": 0.0, "avg_logprob": -0.23395265534866688,
  "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.003008848987519741},
  {"id": 425, "seek": 275540, "start": 2768.44, "end": 2773.8, "text": " Of course
  we didn''t read the docs yet but it''s sounds promising so I hope some of", "tokens":
  [51016, 2720, 1164, 321, 994, 380, 1401, 264, 45623, 1939, 457, 309, 311, 3263,
  20257, 370, 286, 1454, 512, 295, 51284], "temperature": 0.0, "avg_logprob": -0.23395265534866688,
  "compression_ratio": 1.5194805194805194, "no_speech_prob": 0.003008848987519741},
  {"id": 426, "seek": 275540, "start": 2773.8, "end": 2780.6800000000003, "text":
  " listeners and audience will take it out. I wanted to go more into this kind of
  philosophical", "tokens": [51284, 23274, 293, 4034, 486, 747, 309, 484, 13, 286,
  1415, 281, 352, 544, 666, 341, 733, 295, 25066, 51628], "temperature": 0.0, "avg_logprob":
  -0.23395265534866688, "compression_ratio": 1.5194805194805194, "no_speech_prob":
  0.003008848987519741}, {"id": 427, "seek": 278068, "start": 2780.68, "end": 2785.16,
  "text": " level like what what drives you in this space? Like you said that you''ve
  been working in web", "tokens": [50364, 1496, 411, 437, 437, 11754, 291, 294, 341,
  1901, 30, 1743, 291, 848, 300, 291, 600, 668, 1364, 294, 3670, 50588], "temperature":
  0.0, "avg_logprob": -0.20741359486299402, "compression_ratio": 1.7464114832535884,
  "no_speech_prob": 0.009304668754339218}, {"id": 428, "seek": 278068, "start": 2785.16,
  "end": 2789.96, "text": " scale search as well before right? And like some other
  search and engineering in general.", "tokens": [50588, 4373, 3164, 382, 731, 949,
  558, 30, 400, 411, 512, 661, 3164, 293, 7043, 294, 2674, 13, 50828], "temperature":
  0.0, "avg_logprob": -0.20741359486299402, "compression_ratio": 1.7464114832535884,
  "no_speech_prob": 0.009304668754339218}, {"id": 429, "seek": 278068, "start": 2790.6,
  "end": 2796.68, "text": " So what drives you here now in this area when you join
  Gina and why why you join Gina?", "tokens": [50860, 407, 437, 11754, 291, 510, 586,
  294, 341, 1859, 562, 291, 3917, 34711, 293, 983, 983, 291, 3917, 34711, 30, 51164],
  "temperature": 0.0, "avg_logprob": -0.20741359486299402, "compression_ratio": 1.7464114832535884,
  "no_speech_prob": 0.009304668754339218}, {"id": 430, "seek": 278068, "start": 2797.64,
  "end": 2805.48, "text": " So I joined Gina especially as I was in this traditional
  search space I was working on training", "tokens": [51212, 407, 286, 6869, 34711,
  2318, 382, 286, 390, 294, 341, 5164, 3164, 1901, 286, 390, 1364, 322, 3097, 51604],
  "temperature": 0.0, "avg_logprob": -0.20741359486299402, "compression_ratio": 1.7464114832535884,
  "no_speech_prob": 0.009304668754339218}, {"id": 431, "seek": 280548, "start": 2805.48,
  "end": 2815.96, "text": " ranking models. So what drove me more is to enable this
  search system, this search experience", "tokens": [50364, 17833, 5245, 13, 407,
  437, 13226, 385, 544, 307, 281, 9528, 341, 3164, 1185, 11, 341, 3164, 1752, 50888],
  "temperature": 0.0, "avg_logprob": -0.26310432563393804, "compression_ratio": 1.4947368421052631,
  "no_speech_prob": 0.0009810883784666657}, {"id": 432, "seek": 280548, "start": 2815.96,
  "end": 2823.64, "text": " instance beyond text. I was super curious about how we
  can extend it''s impressive to me how the", "tokens": [50888, 5197, 4399, 2487,
  13, 286, 390, 1687, 6369, 466, 577, 321, 393, 10101, 309, 311, 8992, 281, 385, 577,
  264, 51272], "temperature": 0.0, "avg_logprob": -0.26310432563393804, "compression_ratio":
  1.4947368421052631, "no_speech_prob": 0.0009810883784666657}, {"id": 433, "seek":
  280548, "start": 2823.64, "end": 2829.32, "text": " same framework of getting something
  that extracts meaningful vectors with semantic information", "tokens": [51272, 912,
  8388, 295, 1242, 746, 300, 8947, 82, 10995, 18875, 365, 47982, 1589, 51556], "temperature":
  0.0, "avg_logprob": -0.26310432563393804, "compression_ratio": 1.4947368421052631,
  "no_speech_prob": 0.0009810883784666657}, {"id": 434, "seek": 282932, "start": 2830.04,
  "end": 2837.32, "text": " can be used for images, for video, for audio, for anything.
  These frameworks I think it has a lot", "tokens": [50400, 393, 312, 1143, 337, 5267,
  11, 337, 960, 11, 337, 6278, 11, 337, 1340, 13, 1981, 29834, 286, 519, 309, 575,
  257, 688, 50764], "temperature": 0.0, "avg_logprob": -0.24318938657461878, "compression_ratio":
  1.660633484162896, "no_speech_prob": 0.005732949823141098}, {"id": 435, "seek":
  282932, "start": 2837.32, "end": 2845.1600000000003, "text": " of features because
  it''s quite and also how the how the different research areas from different", "tokens":
  [50764, 295, 4122, 570, 309, 311, 1596, 293, 611, 577, 264, 577, 264, 819, 2132,
  3179, 490, 819, 51156], "temperature": 0.0, "avg_logprob": -0.24318938657461878,
  "compression_ratio": 1.660633484162896, "no_speech_prob": 0.005732949823141098},
  {"id": 436, "seek": 282932, "start": 2845.1600000000003, "end": 2849.48, "text":
  " modalities interact with each other. I don''t know I don''t know. Trans the conversion",
  "tokens": [51156, 1072, 16110, 4648, 365, 1184, 661, 13, 286, 500, 380, 458, 286,
  500, 380, 458, 13, 6531, 264, 14298, 51372], "temperature": 0.0, "avg_logprob":
  -0.24318938657461878, "compression_ratio": 1.660633484162896, "no_speech_prob":
  0.005732949823141098}, {"id": 437, "seek": 282932, "start": 2849.48, "end": 2856.04,
  "text": " neural network appeared even some text classification used to do these
  then appeared the", "tokens": [51372, 18161, 3209, 8516, 754, 512, 2487, 21538,
  1143, 281, 360, 613, 550, 8516, 264, 51700], "temperature": 0.0, "avg_logprob":
  -0.24318938657461878, "compression_ratio": 1.660633484162896, "no_speech_prob":
  0.005732949823141098}, {"id": 438, "seek": 285604, "start": 2856.04, "end": 2862.12,
  "text": " transformer right now the computer vision community is getting in love
  with transformers.", "tokens": [50364, 31782, 558, 586, 264, 3820, 5201, 1768, 307,
  1242, 294, 959, 365, 4088, 433, 13, 50668], "temperature": 0.0, "avg_logprob": -0.26536799025261537,
  "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.00353757431730628},
  {"id": 439, "seek": 285604, "start": 2862.92, "end": 2869.32, "text": " These back
  and forth I think that it''s impressive but also if you think of the magic of getting",
  "tokens": [50708, 1981, 646, 293, 5220, 286, 519, 300, 309, 311, 8992, 457, 611,
  498, 291, 519, 295, 264, 5585, 295, 1242, 51028], "temperature": 0.0, "avg_logprob":
  -0.26536799025261537, "compression_ratio": 1.6460176991150441, "no_speech_prob":
  0.00353757431730628}, {"id": 440, "seek": 285604, "start": 2869.32, "end": 2875.24,
  "text": " this vector and having so much meaning there it''s quite amazing. Yeah
  it''s true I mean it''s", "tokens": [51028, 341, 8062, 293, 1419, 370, 709, 3620,
  456, 309, 311, 1596, 2243, 13, 865, 309, 311, 2074, 286, 914, 309, 311, 51324],
  "temperature": 0.0, "avg_logprob": -0.26536799025261537, "compression_ratio": 1.6460176991150441,
  "no_speech_prob": 0.00353757431730628}, {"id": 441, "seek": 285604, "start": 2875.72,
  "end": 2883.32, "text": " very powerful you know like that the the the sheer fact
  that you don''t need to build a synonym", "tokens": [51348, 588, 4005, 291, 458,
  411, 300, 264, 264, 264, 23061, 1186, 300, 291, 500, 380, 643, 281, 1322, 257, 5451,
  12732, 51728], "temperature": 0.0, "avg_logprob": -0.26536799025261537, "compression_ratio":
  1.6460176991150441, "no_speech_prob": 0.00353757431730628}, {"id": 442, "seek":
  288332, "start": 2883.32, "end": 2889.6400000000003, "text": " like dictionary if
  you go full text right like it just tells you that yeah mathematics is close to",
  "tokens": [50364, 411, 25890, 498, 291, 352, 1577, 2487, 558, 411, 309, 445, 5112,
  291, 300, 1338, 18666, 307, 1998, 281, 50680], "temperature": 0.0, "avg_logprob":
  -0.10486579209231259, "compression_ratio": 1.7627906976744185, "no_speech_prob":
  0.010772858746349812}, {"id": 443, "seek": 288332, "start": 2889.6400000000003,
  "end": 2895.96, "text": " algebra or you know like you throw data at it and it''s
  an unsupervised right it just tells you", "tokens": [50680, 21989, 420, 291, 458,
  411, 291, 3507, 1412, 412, 309, 293, 309, 311, 364, 2693, 12879, 24420, 558, 309,
  445, 5112, 291, 50996], "temperature": 0.0, "avg_logprob": -0.10486579209231259,
  "compression_ratio": 1.7627906976744185, "no_speech_prob": 0.010772858746349812},
  {"id": 444, "seek": 288332, "start": 2895.96, "end": 2901.6400000000003, "text":
  " hey I''ve trained it up like now okay I can tell you what''s close to each other
  geometrically", "tokens": [50996, 4177, 286, 600, 8895, 309, 493, 411, 586, 1392,
  286, 393, 980, 291, 437, 311, 1998, 281, 1184, 661, 12956, 81, 984, 51280], "temperature":
  0.0, "avg_logprob": -0.10486579209231259, "compression_ratio": 1.7627906976744185,
  "no_speech_prob": 0.010772858746349812}, {"id": 445, "seek": 288332, "start": 2901.6400000000003,
  "end": 2907.2400000000002, "text": " it also has the mathematical beauty there right
  geometric closeness rather than kind of some", "tokens": [51280, 309, 611, 575,
  264, 18894, 6643, 456, 558, 33246, 2611, 15264, 2831, 813, 733, 295, 512, 51560],
  "temperature": 0.0, "avg_logprob": -0.10486579209231259, "compression_ratio": 1.7627906976744185,
  "no_speech_prob": 0.010772858746349812}, {"id": 446, "seek": 290724, "start": 2907.24,
  "end": 2914.68, "text": " obscure strange abstract sparse closeness it''s quite
  elegant yeah yeah you have", "tokens": [50364, 34443, 5861, 12649, 637, 11668, 2611,
  15264, 309, 311, 1596, 21117, 1338, 1338, 291, 362, 50736], "temperature": 0.0,
  "avg_logprob": -0.2207587787083217, "compression_ratio": 1.7109004739336493, "no_speech_prob":
  0.003598213894292712}, {"id": 447, "seek": 290724, "start": 2914.68, "end": 2919.0,
  "text": " tracked all these knowledge all these and you have that this simple thing
  that you can", "tokens": [50736, 31703, 439, 613, 3601, 439, 613, 293, 291, 362,
  300, 341, 2199, 551, 300, 291, 393, 50952], "temperature": 0.0, "avg_logprob": -0.2207587787083217,
  "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.003598213894292712},
  {"id": 448, "seek": 290724, "start": 2919.64, "end": 2926.52, "text": " imagine
  in your head as a 3D space and that is simple as algebra from I don''t know which
  grade", "tokens": [50984, 3811, 294, 428, 1378, 382, 257, 805, 35, 1901, 293, 300,
  307, 2199, 382, 21989, 490, 286, 500, 380, 458, 597, 7204, 51328], "temperature":
  0.0, "avg_logprob": -0.2207587787083217, "compression_ratio": 1.7109004739336493,
  "no_speech_prob": 0.003598213894292712}, {"id": 449, "seek": 290724, "start": 2926.52,
  "end": 2932.52, "text": " but quite simple yeah I think in simplicity there is a
  lot of beauty yeah it''s very easy to explain", "tokens": [51328, 457, 1596, 2199,
  1338, 286, 519, 294, 25632, 456, 307, 257, 688, 295, 6643, 1338, 309, 311, 588,
  1858, 281, 2903, 51628], "temperature": 0.0, "avg_logprob": -0.2207587787083217,
  "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.003598213894292712},
  {"id": 450, "seek": 293252, "start": 2932.6, "end": 2938.68, "text": " to your granny
  like I''m doing this you know like it''s 3D space kind of there are points and I''m
  just", "tokens": [50368, 281, 428, 44797, 411, 286, 478, 884, 341, 291, 458, 411,
  309, 311, 805, 35, 1901, 733, 295, 456, 366, 2793, 293, 286, 478, 445, 50672], "temperature":
  0.0, "avg_logprob": -0.14355464463823298, "compression_ratio": 1.7623318385650224,
  "no_speech_prob": 0.003687912132591009}, {"id": 451, "seek": 293252, "start": 2938.68,
  "end": 2945.48, "text": " looking the closest one I expect to have something that
  puts things close to each other that makes", "tokens": [50672, 1237, 264, 13699,
  472, 286, 2066, 281, 362, 746, 300, 8137, 721, 1998, 281, 1184, 661, 300, 1669,
  51012], "temperature": 0.0, "avg_logprob": -0.14355464463823298, "compression_ratio":
  1.7623318385650224, "no_speech_prob": 0.003687912132591009}, {"id": 452, "seek":
  293252, "start": 2945.48, "end": 2951.4, "text": " sense together that is what we
  expect from these black box exactly exactly and then and then the", "tokens": [51012,
  2020, 1214, 300, 307, 437, 321, 2066, 490, 613, 2211, 2424, 2293, 2293, 293, 550,
  293, 550, 264, 51308], "temperature": 0.0, "avg_logprob": -0.14355464463823298,
  "compression_ratio": 1.7623318385650224, "no_speech_prob": 0.003687912132591009},
  {"id": 453, "seek": 293252, "start": 2951.4, "end": 2955.96, "text": " question
  of scale like if you go to 10 million hundred million billion billion then okay
  can you", "tokens": [51308, 1168, 295, 4373, 411, 498, 291, 352, 281, 1266, 2459,
  3262, 2459, 5218, 5218, 550, 1392, 393, 291, 51536], "temperature": 0.0, "avg_logprob":
  -0.14355464463823298, "compression_ratio": 1.7623318385650224, "no_speech_prob":
  0.003687912132591009}, {"id": 454, "seek": 295596, "start": 2956.04, "end": 2964.52,
  "text": " trade some of that closeness precision you know and kind of get past the
  speed so yeah it''s very", "tokens": [50368, 4923, 512, 295, 300, 2611, 15264, 18356,
  291, 458, 293, 733, 295, 483, 1791, 264, 3073, 370, 1338, 309, 311, 588, 50792],
  "temperature": 0.0, "avg_logprob": -0.16583452958327075, "compression_ratio": 1.7349397590361446,
  "no_speech_prob": 0.0026092110201716423}, {"id": 455, "seek": 295596, "start": 2964.52,
  "end": 2970.6, "text": " interesting I mean it''s um does it does it interest you
  more like on deep learning side or on", "tokens": [50792, 1880, 286, 914, 309, 311,
  1105, 775, 309, 775, 309, 1179, 291, 544, 411, 322, 2452, 2539, 1252, 420, 322,
  51096], "temperature": 0.0, "avg_logprob": -0.16583452958327075, "compression_ratio":
  1.7349397590361446, "no_speech_prob": 0.0026092110201716423}, {"id": 456, "seek":
  295596, "start": 2970.6, "end": 2978.52, "text": " mathematics side or engineering
  side like or maybe some other side in every side from mathematics", "tokens": [51096,
  18666, 1252, 420, 7043, 1252, 411, 420, 1310, 512, 661, 1252, 294, 633, 1252, 490,
  18666, 51492], "temperature": 0.0, "avg_logprob": -0.16583452958327075, "compression_ratio":
  1.7349397590361446, "no_speech_prob": 0.0026092110201716423}, {"id": 457, "seek":
  297852, "start": 2979.48, "end": 2986.92, "text": " I enjoy a lot the beauty of
  it sometimes it''s too obscure for me but I really like and understand", "tokens":
  [50412, 286, 2103, 257, 688, 264, 6643, 295, 309, 2171, 309, 311, 886, 34443, 337,
  385, 457, 286, 534, 411, 293, 1223, 50784], "temperature": 0.0, "avg_logprob": -0.13012472299429087,
  "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.020431529730558395},
  {"id": 458, "seek": 297852, "start": 2986.92, "end": 2996.7599999999998, "text":
  " deep learning I like it although I feel that some of some of the research doesn''t
  seem to be so", "tokens": [50784, 2452, 2539, 286, 411, 309, 4878, 286, 841, 300,
  512, 295, 512, 295, 264, 2132, 1177, 380, 1643, 281, 312, 370, 51276], "temperature":
  0.0, "avg_logprob": -0.13012472299429087, "compression_ratio": 1.6101694915254237,
  "no_speech_prob": 0.020431529730558395}, {"id": 459, "seek": 297852, "start": 2996.7599999999998,
  "end": 3005.32, "text": " innovative and maybe we should spend more time checking
  other paths and deep learning which", "tokens": [51276, 12999, 293, 1310, 321, 820,
  3496, 544, 565, 8568, 661, 14518, 293, 2452, 2539, 597, 51704], "temperature": 0.0,
  "avg_logprob": -0.13012472299429087, "compression_ratio": 1.6101694915254237, "no_speech_prob":
  0.020431529730558395}, {"id": 460, "seek": 300532, "start": 3005.32, "end": 3010.2000000000003,
  "text": " are the paths like I don''t know you should be honest I don''t know I''m
  just feel I just feel that", "tokens": [50364, 366, 264, 14518, 411, 286, 500, 380,
  458, 291, 820, 312, 3245, 286, 500, 380, 458, 286, 478, 445, 841, 286, 445, 841,
  300, 50608], "temperature": 0.0, "avg_logprob": -0.22727193114578084, "compression_ratio":
  1.7085201793721974, "no_speech_prob": 0.0034583949018269777}, {"id": 461, "seek":
  300532, "start": 3010.76, "end": 3015.96, "text": " there''s so much literature
  that I cannot keep up with and then from the engineering side I think", "tokens":
  [50636, 456, 311, 370, 709, 10394, 300, 286, 2644, 1066, 493, 365, 293, 550, 490,
  264, 7043, 1252, 286, 519, 50896], "temperature": 0.0, "avg_logprob": -0.22727193114578084,
  "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.0034583949018269777},
  {"id": 462, "seek": 300532, "start": 3015.96, "end": 3024.28, "text": " it''s cool
  it''s just space I think I also can provide more value and sometimes concepts are",
  "tokens": [50896, 309, 311, 1627, 309, 311, 445, 1901, 286, 519, 286, 611, 393,
  2893, 544, 2158, 293, 2171, 10392, 366, 51312], "temperature": 0.0, "avg_logprob":
  -0.22727193114578084, "compression_ratio": 1.7085201793721974, "no_speech_prob":
  0.0034583949018269777}, {"id": 463, "seek": 300532, "start": 3024.28, "end": 3031.0,
  "text": " to abstract from yeah like for me like I want to call out but you like
  point on is deep learning", "tokens": [51312, 281, 12649, 490, 1338, 411, 337, 385,
  411, 286, 528, 281, 818, 484, 457, 291, 411, 935, 322, 307, 2452, 2539, 51648],
  "temperature": 0.0, "avg_logprob": -0.22727193114578084, "compression_ratio": 1.7085201793721974,
  "no_speech_prob": 0.0034583949018269777}, {"id": 464, "seek": 303100, "start": 3031.0,
  "end": 3037.96, "text": " the only way you know like for example one scary thing
  is that these models are becoming more", "tokens": [50364, 264, 787, 636, 291, 458,
  411, 337, 1365, 472, 6958, 551, 307, 300, 613, 5245, 366, 5617, 544, 50712], "temperature":
  0.0, "avg_logprob": -0.11038015080594468, "compression_ratio": 1.7894736842105263,
  "no_speech_prob": 0.002288981806486845}, {"id": 465, "seek": 303100, "start": 3037.96,
  "end": 3043.32, "text": " and more kind of parameterized so you have like hundreds
  of billions parameters maybe billion", "tokens": [50712, 293, 544, 733, 295, 13075,
  1602, 370, 291, 362, 411, 6779, 295, 17375, 9834, 1310, 5218, 50980], "temperature":
  0.0, "avg_logprob": -0.11038015080594468, "compression_ratio": 1.7894736842105263,
  "no_speech_prob": 0.002288981806486845}, {"id": 466, "seek": 303100, "start": 3043.32,
  "end": 3050.92, "text": " trillion like how many more can you have zillion parameters
  in there but first of all it''s", "tokens": [50980, 18723, 411, 577, 867, 544, 393,
  291, 362, 710, 11836, 9834, 294, 456, 457, 700, 295, 439, 309, 311, 51360], "temperature":
  0.0, "avg_logprob": -0.11038015080594468, "compression_ratio": 1.7894736842105263,
  "no_speech_prob": 0.002288981806486845}, {"id": 467, "seek": 303100, "start": 3050.92,
  "end": 3055.72, "text": " impractical so if you take that model you try to plug
  it in it doesn''t plug in because it''s too", "tokens": [51360, 704, 1897, 804,
  370, 498, 291, 747, 300, 2316, 291, 853, 281, 5452, 309, 294, 309, 1177, 380, 5452,
  294, 570, 309, 311, 886, 51600], "temperature": 0.0, "avg_logprob": -0.11038015080594468,
  "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.002288981806486845},
  {"id": 468, "seek": 305572, "start": 3055.72, "end": 3062.8399999999997, "text":
  " expensive and also you might not have that much data in the first place right
  so why should you", "tokens": [50364, 5124, 293, 611, 291, 1062, 406, 362, 300,
  709, 1412, 294, 264, 700, 1081, 558, 370, 983, 820, 291, 50720], "temperature":
  0.0, "avg_logprob": -0.12929350679570978, "compression_ratio": 1.6858407079646018,
  "no_speech_prob": 0.0019374772673472762}, {"id": 469, "seek": 305572, "start": 3062.8399999999997,
  "end": 3070.3599999999997, "text": " care like web scale search engines probably
  will but like you as a researcher in let''s say a startup", "tokens": [50720, 1127,
  411, 3670, 4373, 3164, 12982, 1391, 486, 457, 411, 291, 382, 257, 21751, 294, 718,
  311, 584, 257, 18578, 51096], "temperature": 0.0, "avg_logprob": -0.12929350679570978,
  "compression_ratio": 1.6858407079646018, "no_speech_prob": 0.0019374772673472762},
  {"id": 470, "seek": 305572, "start": 3070.3599999999997, "end": 3076.7599999999998,
  "text": " you don''t know if you need that much you need to sell solve that specific
  thing right so it will", "tokens": [51096, 291, 500, 380, 458, 498, 291, 643, 300,
  709, 291, 643, 281, 3607, 5039, 300, 2685, 551, 558, 370, 309, 486, 51416], "temperature":
  0.0, "avg_logprob": -0.12929350679570978, "compression_ratio": 1.6858407079646018,
  "no_speech_prob": 0.0019374772673472762}, {"id": 471, "seek": 305572, "start": 3076.7599999999998,
  "end": 3083.3999999999996, "text": " it will look really strange to bring this huge
  microscope and like GBT model in and say", "tokens": [51416, 309, 486, 574, 534,
  5861, 281, 1565, 341, 2603, 29753, 293, 411, 460, 33853, 2316, 294, 293, 584, 51748],
  "temperature": 0.0, "avg_logprob": -0.12929350679570978, "compression_ratio": 1.6858407079646018,
  "no_speech_prob": 0.0019374772673472762}, {"id": 472, "seek": 308340, "start": 3083.4,
  "end": 3089.0, "text": " this is what we need to use and then like the whole budget
  goes into paying that model or whatever", "tokens": [50364, 341, 307, 437, 321,
  643, 281, 764, 293, 550, 411, 264, 1379, 4706, 1709, 666, 6229, 300, 2316, 420,
  2035, 50644], "temperature": 0.0, "avg_logprob": -0.13988312896417113, "compression_ratio":
  1.786046511627907, "no_speech_prob": 0.006199564319103956}, {"id": 473, "seek":
  308340, "start": 3089.0, "end": 3096.36, "text": " you know like it''s in it''s
  in practical so that that direction by itself like I think it''s a little", "tokens":
  [50644, 291, 458, 411, 309, 311, 294, 309, 311, 294, 8496, 370, 300, 300, 3513,
  538, 2564, 411, 286, 519, 309, 311, 257, 707, 51012], "temperature": 0.0, "avg_logprob":
  -0.13988312896417113, "compression_ratio": 1.786046511627907, "no_speech_prob":
  0.006199564319103956}, {"id": 474, "seek": 308340, "start": 3096.36, "end": 3102.36,
  "text": " bit like doomed or like I don''t know like how you feel about it yeah
  it''s a it''s a race where I", "tokens": [51012, 857, 411, 33847, 420, 411, 286,
  500, 380, 458, 411, 577, 291, 841, 466, 309, 1338, 309, 311, 257, 309, 311, 257,
  4569, 689, 286, 51312], "temperature": 0.0, "avg_logprob": -0.13988312896417113,
  "compression_ratio": 1.786046511627907, "no_speech_prob": 0.006199564319103956},
  {"id": 475, "seek": 308340, "start": 3102.36, "end": 3108.12, "text": " add another
  layer and get more parameters and I win and I think I''m not but it feels that",
  "tokens": [51312, 909, 1071, 4583, 293, 483, 544, 9834, 293, 286, 1942, 293, 286,
  519, 286, 478, 406, 457, 309, 3417, 300, 51600], "temperature": 0.0, "avg_logprob":
  -0.13988312896417113, "compression_ratio": 1.786046511627907, "no_speech_prob":
  0.006199564319103956}, {"id": 476, "seek": 310812, "start": 3108.8399999999997,
  "end": 3115.16, "text": " the first step to move away from this is to really understand
  how things are learned and why", "tokens": [50400, 264, 700, 1823, 281, 1286, 1314,
  490, 341, 307, 281, 534, 1223, 577, 721, 366, 3264, 293, 983, 50716], "temperature":
  0.0, "avg_logprob": -0.1281579907020826, "compression_ratio": 1.8516746411483254,
  "no_speech_prob": 0.004840914625674486}, {"id": 477, "seek": 310812, "start": 3115.16,
  "end": 3120.52, "text": " are learned the way they are I don''t know any match you
  have these models to bring back to the", "tokens": [50716, 366, 3264, 264, 636,
  436, 366, 286, 500, 380, 458, 604, 2995, 291, 362, 613, 5245, 281, 1565, 646, 281,
  264, 50984], "temperature": 0.0, "avg_logprob": -0.1281579907020826, "compression_ratio":
  1.8516746411483254, "no_speech_prob": 0.004840914625674486}, {"id": 478, "seek":
  310812, "start": 3120.52, "end": 3127.4, "text": " image where the where the filters
  are learned more or less you have some idea or where the model is", "tokens": [50984,
  3256, 689, 264, 689, 264, 15995, 366, 3264, 544, 420, 1570, 291, 362, 512, 1558,
  420, 689, 264, 2316, 307, 51328], "temperature": 0.0, "avg_logprob": -0.1281579907020826,
  "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.004840914625674486},
  {"id": 479, "seek": 310812, "start": 3127.4, "end": 3133.88, "text": " looking but
  maybe to put more research on slow down let''s slow down these race and let''s understand",
  "tokens": [51328, 1237, 457, 1310, 281, 829, 544, 2132, 322, 2964, 760, 718, 311,
  2964, 760, 613, 4569, 293, 718, 311, 1223, 51652], "temperature": 0.0, "avg_logprob":
  -0.1281579907020826, "compression_ratio": 1.8516746411483254, "no_speech_prob":
  0.004840914625674486}, {"id": 480, "seek": 313388, "start": 3133.96, "end": 3140.44,
  "text": " and maybe we find a way to make it more sustainable for everyone yeah
  because I remember like", "tokens": [50368, 293, 1310, 321, 915, 257, 636, 281,
  652, 309, 544, 11235, 337, 1518, 1338, 570, 286, 1604, 411, 50692], "temperature":
  0.0, "avg_logprob": -0.1108939124316704, "compression_ratio": 1.7168141592920354,
  "no_speech_prob": 0.005945454817265272}, {"id": 481, "seek": 313388, "start": 3140.44,
  "end": 3146.04, "text": " when I was doing my PhD in machine translation it was
  using like statistical models like Moses and", "tokens": [50692, 562, 286, 390,
  884, 452, 14476, 294, 3479, 12853, 309, 390, 1228, 411, 22820, 5245, 411, 17580,
  293, 50972], "temperature": 0.0, "avg_logprob": -0.1108939124316704, "compression_ratio":
  1.7168141592920354, "no_speech_prob": 0.005945454817265272}, {"id": 482, "seek":
  313388, "start": 3146.04, "end": 3151.56, "text": " you know statistical machine
  translation and so it would suffer from things like out of vocabulary", "tokens":
  [50972, 291, 458, 22820, 3479, 12853, 293, 370, 309, 576, 9753, 490, 721, 411, 484,
  295, 19864, 51248], "temperature": 0.0, "avg_logprob": -0.1108939124316704, "compression_ratio":
  1.7168141592920354, "no_speech_prob": 0.005945454817265272}, {"id": 483, "seek":
  313388, "start": 3151.56, "end": 3158.2000000000003, "text": " and you know how
  do I bring syntax in and whatnot but then like when deep learning came like all",
  "tokens": [51248, 293, 291, 458, 577, 360, 286, 1565, 28431, 294, 293, 25882, 457,
  550, 411, 562, 2452, 2539, 1361, 411, 439, 51580], "temperature": 0.0, "avg_logprob":
  -0.1108939124316704, "compression_ratio": 1.7168141592920354, "no_speech_prob":
  0.005945454817265272}, {"id": 484, "seek": 315820, "start": 3158.2, "end": 3164.04,
  "text": " of a sudden you see that it translates much much better and you think
  wow probably probably", "tokens": [50364, 295, 257, 3990, 291, 536, 300, 309, 28468,
  709, 709, 1101, 293, 291, 519, 6076, 1391, 1391, 50656], "temperature": 0.0, "avg_logprob":
  -0.09600380117242986, "compression_ratio": 1.7851851851851852, "no_speech_prob":
  0.0020337915048003197}, {"id": 485, "seek": 315820, "start": 3164.04, "end": 3169.48,
  "text": " they solved it now right the claim from 50s that we will solve machine
  translation probably now is", "tokens": [50656, 436, 13041, 309, 586, 558, 264,
  3932, 490, 2625, 82, 300, 321, 486, 5039, 3479, 12853, 1391, 586, 307, 50928], "temperature":
  0.0, "avg_logprob": -0.09600380117242986, "compression_ratio": 1.7851851851851852,
  "no_speech_prob": 0.0020337915048003197}, {"id": 486, "seek": 315820, "start": 3169.48,
  "end": 3175.8799999999997, "text": " delivered that the promise but but then you
  notice it''s fluent but it''s kind of like I don''t want", "tokens": [50928, 10144,
  300, 264, 6228, 457, 457, 550, 291, 3449, 309, 311, 40799, 457, 309, 311, 733, 295,
  411, 286, 500, 380, 528, 51248], "temperature": 0.0, "avg_logprob": -0.09600380117242986,
  "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0020337915048003197},
  {"id": 487, "seek": 315820, "start": 3175.8799999999997, "end": 3180.52, "text":
  " to use the word stupid but it just doesn''t get it right like it it makes wolf
  subject an object", "tokens": [51248, 281, 764, 264, 1349, 6631, 457, 309, 445,
  1177, 380, 483, 309, 558, 411, 309, 309, 1669, 19216, 3983, 364, 2657, 51480], "temperature":
  0.0, "avg_logprob": -0.09600380117242986, "compression_ratio": 1.7851851851851852,
  "no_speech_prob": 0.0020337915048003197}, {"id": 488, "seek": 315820, "start": 3180.52,
  "end": 3186.6, "text": " easily it may go and hallucinate about something that doesn''t
  exist there or it actually goes and", "tokens": [51480, 3612, 309, 815, 352, 293,
  35212, 13923, 466, 746, 300, 1177, 380, 2514, 456, 420, 309, 767, 1709, 293, 51784],
  "temperature": 0.0, "avg_logprob": -0.09600380117242986, "compression_ratio": 1.7851851851851852,
  "no_speech_prob": 0.0020337915048003197}, {"id": 489, "seek": 318660, "start": 3186.6,
  "end": 3192.92, "text": " translates into like single letters all of all of a sudden
  you know or repeating engrams or like", "tokens": [50364, 28468, 666, 411, 2167,
  7825, 439, 295, 439, 295, 257, 3990, 291, 458, 420, 18617, 465, 1342, 82, 420, 411,
  50680], "temperature": 0.0, "avg_logprob": -0.23640477657318115, "compression_ratio":
  1.7455357142857142, "no_speech_prob": 0.0007487453403882682}, {"id": 490, "seek":
  318660, "start": 3192.92, "end": 3200.2, "text": " you see that it didn''t exactly
  solve it right you wouldn''t trans your life to such a system yet", "tokens": [50680,
  291, 536, 300, 309, 994, 380, 2293, 5039, 309, 558, 291, 2759, 380, 1145, 428, 993,
  281, 1270, 257, 1185, 1939, 51044], "temperature": 0.0, "avg_logprob": -0.23640477657318115,
  "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0007487453403882682},
  {"id": 491, "seek": 318660, "start": 3200.2, "end": 3205.56, "text": " no yeah and
  then you kind of come back and like okay and I used to do it like in a rule-based
  approach", "tokens": [51044, 572, 1338, 293, 550, 291, 733, 295, 808, 646, 293,
  411, 1392, 293, 286, 1143, 281, 360, 309, 411, 294, 257, 4978, 12, 6032, 3109, 51312],
  "temperature": 0.0, "avg_logprob": -0.23640477657318115, "compression_ratio": 1.7455357142857142,
  "no_speech_prob": 0.0007487453403882682}, {"id": 492, "seek": 318660, "start": 3205.56,
  "end": 3212.7599999999998, "text": " so I could understand the syntax of the of
  the sentence and then semantics like nod in the tree", "tokens": [51312, 370, 286,
  727, 1223, 264, 28431, 295, 264, 295, 264, 8174, 293, 550, 4361, 45298, 411, 15224,
  294, 264, 4230, 51672], "temperature": 0.0, "avg_logprob": -0.23640477657318115,
  "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0007487453403882682},
  {"id": 493, "seek": 321276, "start": 3213.7200000000003, "end": 3219.88, "text":
  " and then when I translate I use some semantic like function and it''s all well-defined
  in the", "tokens": [50412, 293, 550, 562, 286, 13799, 286, 764, 512, 47982, 411,
  2445, 293, 309, 311, 439, 731, 12, 37716, 294, 264, 50720], "temperature": 0.0,
  "avg_logprob": -0.1517265131185343, "compression_ratio": 1.6933333333333334, "no_speech_prob":
  0.005797029007226229}, {"id": 494, "seek": 321276, "start": 3219.88, "end": 3224.6000000000004,
  "text": " the astronomy of semantic functions and so on like okay now I go back
  to deploying do you have", "tokens": [50720, 264, 37844, 295, 47982, 6828, 293,
  370, 322, 411, 1392, 586, 286, 352, 646, 281, 34198, 360, 291, 362, 50956], "temperature":
  0.0, "avg_logprob": -0.1517265131185343, "compression_ratio": 1.6933333333333334,
  "no_speech_prob": 0.005797029007226229}, {"id": 495, "seek": 321276, "start": 3224.6000000000004,
  "end": 3231.1600000000003, "text": " anything like that no it''s like just space
  maybe there is a there should be a way to combine I don''t", "tokens": [50956, 1340,
  411, 300, 572, 309, 311, 411, 445, 1901, 1310, 456, 307, 257, 456, 820, 312, 257,
  636, 281, 10432, 286, 500, 380, 51284], "temperature": 0.0, "avg_logprob": -0.1517265131185343,
  "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.005797029007226229},
  {"id": 496, "seek": 321276, "start": 3231.1600000000003, "end": 3236.28, "text":
  " know we have built I think as humans we have built this complex way of talking
  to each other", "tokens": [51284, 458, 321, 362, 3094, 286, 519, 382, 6255, 321,
  362, 3094, 341, 3997, 636, 295, 1417, 281, 1184, 661, 51540], "temperature": 0.0,
  "avg_logprob": -0.1517265131185343, "compression_ratio": 1.6933333333333334, "no_speech_prob":
  0.005797029007226229}, {"id": 497, "seek": 323628, "start": 3236.36, "end": 3245.0,
  "text": " which I mean multiple languages and stuff and there is no way that all
  these language can go back", "tokens": [50368, 597, 286, 914, 3866, 8650, 293, 1507,
  293, 456, 307, 572, 636, 300, 439, 613, 2856, 393, 352, 646, 50800], "temperature":
  0.0, "avg_logprob": -0.35277674275059856, "compression_ratio": 1.576271186440678,
  "no_speech_prob": 0.0032870080322027206}, {"id": 498, "seek": 323628, "start": 3245.0,
  "end": 3255.88, "text": " to this deep learning world world it seems country to
  if div at least yeah yeah so you are certain that", "tokens": [50800, 281, 341,
  2452, 2539, 1002, 1002, 309, 2544, 1941, 281, 498, 3414, 412, 1935, 1338, 1338,
  370, 291, 366, 1629, 300, 51344], "temperature": 0.0, "avg_logprob": -0.35277674275059856,
  "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0032870080322027206},
  {"id": 499, "seek": 323628, "start": 3255.88, "end": 3262.76, "text": " like maybe
  the voice of those who build alternative models to deploying off a", "tokens": [51344,
  411, 1310, 264, 3177, 295, 729, 567, 1322, 8535, 5245, 281, 34198, 766, 257, 51688],
  "temperature": 0.0, "avg_logprob": -0.35277674275059856, "compression_ratio": 1.576271186440678,
  "no_speech_prob": 0.0032870080322027206}, {"id": 500, "seek": 326276, "start": 3262.76,
  "end": 3268.5200000000004, "text": " alternative approach should be maybe louder
  yeah I think that we may suffer from the bias of", "tokens": [50364, 8535, 3109,
  820, 312, 1310, 22717, 1338, 286, 519, 300, 321, 815, 9753, 490, 264, 12577, 295,
  50652], "temperature": 0.0, "avg_logprob": -0.17831427630256205, "compression_ratio":
  1.7924528301886793, "no_speech_prob": 0.0033132873941212893}, {"id": 501, "seek":
  326276, "start": 3268.5200000000004, "end": 3274.0400000000004, "text": " the winner
  no I mean maybe the first one who opens a door might not do in the race because",
  "tokens": [50652, 264, 8507, 572, 286, 914, 1310, 264, 700, 472, 567, 9870, 257,
  2853, 1062, 406, 360, 294, 264, 4569, 570, 50928], "temperature": 0.0, "avg_logprob":
  -0.17831427630256205, "compression_ratio": 1.7924528301886793, "no_speech_prob":
  0.0033132873941212893}, {"id": 502, "seek": 326276, "start": 3274.76, "end": 3281.6400000000003,
  "text": " but even if they show another way that the race might go I think they
  might deserve more attention", "tokens": [50964, 457, 754, 498, 436, 855, 1071,
  636, 300, 264, 4569, 1062, 352, 286, 519, 436, 1062, 9948, 544, 3202, 51308], "temperature":
  0.0, "avg_logprob": -0.17831427630256205, "compression_ratio": 1.7924528301886793,
  "no_speech_prob": 0.0033132873941212893}, {"id": 503, "seek": 326276, "start": 3282.36,
  "end": 3290.36, "text": " yeah yeah this this is quite deep thanks for this white
  white section like you like you you think", "tokens": [51344, 1338, 1338, 341, 341,
  307, 1596, 2452, 3231, 337, 341, 2418, 2418, 3541, 411, 291, 411, 291, 291, 519,
  51744], "temperature": 0.0, "avg_logprob": -0.17831427630256205, "compression_ratio":
  1.7924528301886793, "no_speech_prob": 0.0033132873941212893}, {"id": 504, "seek":
  329036, "start": 3290.36, "end": 3295.6400000000003, "text": " about it a lot like
  kind of okay not to be biased okay yes there are challenges but", "tokens": [50364,
  466, 309, 257, 688, 411, 733, 295, 1392, 406, 281, 312, 28035, 1392, 2086, 456,
  366, 4759, 457, 50628], "temperature": 0.0, "avg_logprob": -0.1141632263918957,
  "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0027028413023799658},
  {"id": 505, "seek": 329036, "start": 3296.36, "end": 3302.76, "text": " it might
  not be the only right approach and giving you experience as well like you can judge
  a", "tokens": [50664, 309, 1062, 406, 312, 264, 787, 558, 3109, 293, 2902, 291,
  1752, 382, 731, 411, 291, 393, 6995, 257, 50984], "temperature": 0.0, "avg_logprob":
  -0.1141632263918957, "compression_ratio": 1.6807511737089202, "no_speech_prob":
  0.0027028413023799658}, {"id": 506, "seek": 329036, "start": 3302.76, "end": 3310.36,
  "text": " little bit like with with your open eyes I think we should explore more
  and maybe not one", "tokens": [50984, 707, 857, 411, 365, 365, 428, 1269, 2575,
  286, 519, 321, 820, 6839, 544, 293, 1310, 406, 472, 51364], "temperature": 0.0,
  "avg_logprob": -0.1141632263918957, "compression_ratio": 1.6807511737089202, "no_speech_prob":
  0.0027028413023799658}, {"id": 507, "seek": 329036, "start": 3311.1600000000003,
  "end": 3317.0, "text": " focus of that yeah and and probably explore with Gina right
  that''s the talk for sure yeah", "tokens": [51404, 1879, 295, 300, 1338, 293, 293,
  1391, 6839, 365, 34711, 558, 300, 311, 264, 751, 337, 988, 1338, 51696], "temperature":
  0.0, "avg_logprob": -0.1141632263918957, "compression_ratio": 1.6807511737089202,
  "no_speech_prob": 0.0027028413023799658}, {"id": 508, "seek": 331700, "start": 3317.64,
  "end": 3322.44, "text": " so this is this is super great is there something you
  want to share you already shared that the", "tokens": [50396, 370, 341, 307, 341,
  307, 1687, 869, 307, 456, 746, 291, 528, 281, 2073, 291, 1217, 5507, 300, 264, 50636],
  "temperature": 0.0, "avg_logprob": -0.1539045447733865, "compression_ratio": 1.6436781609195403,
  "no_speech_prob": 0.005217256955802441}, {"id": 509, "seek": 331700, "start": 3322.44,
  "end": 3328.68, "text": " fine tuner is available so our listeners can go and check
  it out right is there something else that", "tokens": [50636, 2489, 4267, 260, 307,
  2435, 370, 527, 23274, 393, 352, 293, 1520, 309, 484, 558, 307, 456, 746, 1646,
  300, 50948], "temperature": 0.0, "avg_logprob": -0.1539045447733865, "compression_ratio":
  1.6436781609195403, "no_speech_prob": 0.005217256955802441}, {"id": 510, "seek":
  331700, "start": 3329.32, "end": 3339.48, "text": " like we need to be expecting
  and early next year we should be releasing our 3.0 version so", "tokens": [50980,
  411, 321, 643, 281, 312, 9650, 293, 2440, 958, 1064, 321, 820, 312, 16327, 527,
  805, 13, 15, 3037, 370, 51488], "temperature": 0.0, "avg_logprob": -0.1539045447733865,
  "compression_ratio": 1.6436781609195403, "no_speech_prob": 0.005217256955802441},
  {"id": 511, "seek": 333948, "start": 3340.44, "end": 3350.76, "text": " the stay
  tuned for that yeah yeah comment we will be moving fast and the next times yeah
  this is", "tokens": [50412, 264, 1754, 10870, 337, 300, 1338, 1338, 2871, 321, 486,
  312, 2684, 2370, 293, 264, 958, 1413, 1338, 341, 307, 50928], "temperature": 0.0,
  "avg_logprob": -0.24917275977857184, "compression_ratio": 1.653179190751445, "no_speech_prob":
  0.005728862248361111}, {"id": 512, "seek": 333948, "start": 3350.76, "end": 3356.12,
  "text": " fantastic I mean thanks so much for all this information and detail on
  Gina and also like your", "tokens": [50928, 5456, 286, 914, 3231, 370, 709, 337,
  439, 341, 1589, 293, 2607, 322, 34711, 293, 611, 411, 428, 51196], "temperature":
  0.0, "avg_logprob": -0.24917275977857184, "compression_ratio": 1.653179190751445,
  "no_speech_prob": 0.005728862248361111}, {"id": 513, "seek": 333948, "start": 3357.2400000000002,
  "end": 3363.56, "text": " your ambition and then kind of like you''re thinking here
  I mean it''s really nice that you keep", "tokens": [51252, 428, 22814, 293, 550,
  733, 295, 411, 291, 434, 1953, 510, 286, 914, 309, 311, 534, 1481, 300, 291, 1066,
  51568], "temperature": 0.0, "avg_logprob": -0.24917275977857184, "compression_ratio":
  1.653179190751445, "no_speech_prob": 0.005728862248361111}, {"id": 514, "seek":
  336356, "start": 3363.56, "end": 3370.68, "text": " your open mind available to
  all of our listeners yeah thanks so much it was a pleasure to talk to", "tokens":
  [50364, 428, 1269, 1575, 2435, 281, 439, 295, 527, 23274, 1338, 3231, 370, 709,
  309, 390, 257, 6834, 281, 751, 281, 50720], "temperature": 0.0, "avg_logprob": -0.3123651146888733,
  "compression_ratio": 1.608, "no_speech_prob": 0.0056417565792799}, {"id": 515, "seek":
  336356, "start": 3370.68, "end": 3379.16, "text": " you John today yeah thank you
  so much yeah thank you much looking forward to 3.0 yeah thank you bye bye", "tokens":
  [50720, 291, 2619, 965, 1338, 1309, 291, 370, 709, 1338, 1309, 291, 709, 1237, 2128,
  281, 805, 13, 15, 1338, 1309, 291, 6543, 6543, 51144], "temperature": 0.0, "avg_logprob":
  -0.3123651146888733, "compression_ratio": 1.608, "no_speech_prob": 0.0056417565792799},
  {"id": 516, "seek": 339356, "start": 3393.56, "end": 3396.16, "text": " you", "tokens":
  [50406, 291, 50494], "temperature": 1.0, "avg_logprob": -2.8450746536254883, "compression_ratio":
  0.2727272727272727, "no_speech_prob": 0.537224292755127}]'
---

Hey everyone, Bector Podcast is here and today we are continuing our quest to study more about Bector Technologies and Beding Technologies platforms and today I have a guest from Gina AI, his name is John Fontanelles and he is a principal engineer at Gina AI. Hey John. Hello, nice to meet you.
Yeah, nice to meet you as well. Thanks for joining me today and I'm really really excited to talk about what is Gina AI? I know something I used to use some kind of predecessors of Gina AI in some sense, but not like Gina AI itself.
But first of all I would like you to introduce yourself, your background to our listeners and to me.
 So, well I studied engineering degree in Barcelona, not computer science, from general engineering with my AmiExelectical Engineering, Mechanical Engineering, but I got into software engineering because I was related with robotics and then when I started my professional career I did software engineering at different companies and industries and then I got more into that engineering machine learning and these kinds of fields and then I also did some work on traditional search, on web search, engine so on and then just live brought me to Gina which was a good step in my career.
Oh yeah, cool.
So, what caught your eye in Gina AI as a company and maybe as a technology as a product or maybe the team?
 So, for me what caught me the eye, it was like the technology and the vision of I see that vector search embedding a semantic search in general can revolutionize how we understand search and can bring it to the next level also and adapt to different kind of data or so on and go beyond the typical search bar that we are so much used to.
Yeah, yeah and I mean but Gina is like more than just embedding or kind of it's more like a ecosystem right like it has like marketplace it has many different building blocks and components.
 This is what I think most of the people that might be hearing us might be wondering much because it's a question that we receive a lot so we are not such another vector database as the ones that have been created in the podcast so we are treating the problem of semantic search and we are seeing this as a then-to-end problem and we are trying to build an ecosystem to help the business and developers to develop their own neural search based engines and for that we are trying to build a ecosystem from the core to our document types where we are also recently and launched this fine tuner project to help you with fine tuning your models for your search applications so we are building a whole family of products and projects in this around this area of neural search.
Yeah it sounds quite ambitious and it sounds like all of these building blocks are really needed for anybody who wants to venture into embedding world of semantic search or you know kind of bringing the power of this deep learning models.
 So it goes beyond only only embedding your data and searching through it you may want to cut it into different pieces, you may want to re-run it at the end, you may want to join different modalities together so we are trying to give and make it easy for the user to develop these applications so that they speak the same language and we hope that they will all speak gene language.
Oh yeah, oh yeah, for sure.
And GNI is open source, right?
 Yes, so can you speak a bit more like towards the business model or kind of how GNI kind of makes money in a way like so basically it's open source, anybody can go and download it and basically leverage in their work or is there something that like you have some products for which customers can pay and kind of right now we are right now we are completely open source everything that you can see in our report and stuff each open for everyone.
Yeah so so okay and you are like mostly working on back-and-side of things so you're not interacting with direct customers right? Is that okay? I'm working mostly on the main products and what do you hear about use cases like how do they translate to your level of kind of day-to-day job?
 So most of our solution engineers that say that are closer to clients and users they bring guys their pain points on how they are trying to solve users needs and some of the main use cases that we are trying to solve come from textile search, e-match search, multimodal search that is something that we are trying to excel at that is going beyond only just using search and text or images to search maybe trying to have a combination of walls to power search to the next level.
So they might like bring some kind of use case that you need to figure out on tech level right?
Yes kind of translates to you but on the other hand like you said it's open source so it means like there is like a bunch of like GitHub issues coming in right and if you have like Slack or I don't know if you're using Slack anyway.
Yeah, so like probably every day like somebody you wake up and there are questions there right? So it's also clients in a way right?
 Yes for me my users are our clients and we have to listen to them so that's the big point of open source in my opinion is this direct feedback from the users we can you can correct your direction and you can measure if your APIs are or your design are too complex for the user to rush or whatever so this direct feedback is really useful.
And to this point it's manageable.
Yeah yeah but it's also like I guess I also alluded to this and one of the podcasts was with Bob One Lloyd from from semi like it's also sometimes maybe to give up with all the questions right? Like if you get all these questions when do you find the answer to kind of really deeply answer?
Yeah fine time for answer them yeah.
So we are trying to grow our team into knowing that the community is something that makes us special and it's important for us to take care of our community so we are all trying to keep an eye on the community.
Yeah yeah yeah I remember like when I was developing like search code we were using like Apache Solar and I had to like customize some parts of solar and listen and I remember like in order for me to kind of get up to speed I had to go to this mailing list right?
 And so there are like thousands and thousands emails actually Apache Solar was super active you know like in sillies in many ways and and I was like how can I keep up with all these questions but like I do need to somehow keep up and summarize maybe what what is being asked there in order to understand it's useful for me or not because when you ask a question on the mailing list or like today on Slack sometimes you need to be ready to pay back right?
If somebody help you in the community like you sometimes need to also pay back so it's like it's a game.
 When this is seen in the community I think it's really pleasant for all the team when community interacts with each other and none of the no one in the team has to jump in because they so they help each other that's when I think the community really scales and really open source goes to the next level.
Yeah it's kind of regenerating itself and kind of the cultural element of it so and the community drives you forward I mean just driving force of the project from the interaction point and the feature wise as well. Yeah sounds good.
So John tell me more about GNI itself like as a product let's say as a technology stack like what can I do as a user you know using GNI and yeah like is it self-series and so on.
 So the main point of GNI is that we want to be with the user from the minute they are experimenting with their search application so for instance we are written in Python and we have a really nice API in Python to build with your documents that can treat with any type of data, text, images, audio, video and we are trying to build a really easy to use API for this for you to run locally your solutions.
 The first experimental facing to wrap your code for processing loading the files and for processing the images or whatever and embedding them searching to do a process many as neighbors or exact nearest neighbor search then once you have this we make it easy for you to wrap it in some microservices what we call executors so first phase you deal with these document array types that we have come with then you come with them to the next layer is you have it with the executors so you wrap your logic in different microservices and then we put it in what we call a flow that is kind of a pipeline that is really to scale locally or remotely or even with Kubernetes so that you have replication and scalability taken care for.
So we are trying to bring you easily from your day zero of development to the production system.
Yeah yeah sounds good sounds comprehensive and like what if I would like to just use like a hosted version can I use a hosted version from Gina AI or do I need to do an operation? There is no hosted version at this point yeah so it's basically I need it's like a Lego type of thing right?
Yes exactly.
I will have a nice deployment. And we have even this marketplace as you said this with this helicopter cutter so you can share publicly or privately with your colleagues or with the community your meeting blocks that you may think they are useful for you.
Yeah modern deep learning models that you have packed processing, copy-done, re-runking, even back to research research.
Yeah so and how does it align also with like companies or hubs like Hagen phase you know Hagen phase is also very famous on model side right? So like let's show somebody picks a model and wants to bring it to Gina what's the process there?
 So it's quite I would say having phase it's quite inspirational for us in this sense in this marketplace community and place it is quite similar but um Gina is this marketplace is related to our executor so it goes beyond only models so it's any subsystem enabled and block that you can that you can build that is able to be part of this of this Gina pipeline for us and we are trying to make it user-phone for you to localize it and use it in any way in a simple API and we're still working to make it easier every time.
 Yeah of course because actually you know it tends to get a lot of time you know the infrastructure part like how do I bring my model let's say I have a custom model and I want to bring it inside Gina right so it serves as a embedding layer so how do I figure out all this scalability or latency parameters and so on so I think so the first thing is to get it working we are having to we expose these with these executors that have some API and to that read requests with some maybe I inspired with this fast API approach and then you have with this row you have the parameters to replicate to scale and so on you you may run it in GPU whatever yeah yeah so like you can choose your cost kind of like factors right or based on your cost factors you can choose it's CPU actually and then latency and for some models actually CPU is fine so yeah I mean why not yeah it depends also on the user needs so for instance we are also seeing that neural search main not all is not needed to be only for these big giants with this big amount of data and big amount of resources so any company it's more company can benefit from the power of the neural networks to power their search so they may not need so much require so much resources or they may not require so much speed so it's about and so we are giving the power more or less to use yeah and kind of flexibility of the platform so because essentially if they wanted to do it from scratch then they would probably need to figure out similar things like component isolation and scaling and yeah like an algorithm like a quality checks and so on and on the algorithm side you said like you have exact search as well as in exact search can you talk with more and kind of mention maybe some algorithms that you support so yeah so right now natively we support as the main native quite optimized version of the and exact nearest neighbor search but then for instance one of these building blocks can be any support wrapping any client for any other vector database but for instance we just realized our own and approximate nearest neighbor solution we have two of them for instance that we have developed so much so we have one that is based on hsw plus a postgres indexer a postgres database for to require the documents and then we have built our well we just released and in Slack the community can start enjoying it we have and build what we call pcolyte which and works with product quantization but also has support for hsw you said pcolyte or how do you spell that?
P2Lite, P2Lite, which is like product quantization light version.
Yes we and profiltering options as well.
Oh with preview and how in what sense is it light, compared to product quantization? No I have not been involved so much in this spreader right now so it's a new thing but it is light in sense of that it is quite embedded and it's quite native to work with our document type.
So it's not so general as any object, but it is really built to integrate very easily with Jina. Oh, I see. Like with specific kind of schema or document types. And it's also open source. And do you do you like obviously you can provide the links or we can also link in the show notes.
But do you also like have some kind of latency analysis for this algorithm? Like has it been conducted? Do you know? Yeah, there is some benchmarks that you're going to find in the read. I cannot have the numbers in my head right now.
Yeah, but I think for portion of our audience it's going to be interesting to check out because as you know, like actually my team just completed participation in big A&N. I don't know if you heard about this competition. So it's like Villion scale approximate near nearest neighbor search.
So we invented like a new algorithm called BUDGPQ. I will also link in the show notes like the blog post about it. So we increased recall by 12% over FIES model. So yeah, FIES algorithm. So yeah, I think it's great that you guys also inventing. I don't know if we are testing to this billion scale.
I think we are more in the million scale. Yeah, actually, we also ventured into billion scale, but in the process we figured out a solution for million scale. So it's not for billion years. We don't know yet if we can generalize to that level, but I think we can with some additional research.
Well, this is the first version. So for sure, we will try to improve it. Yeah, awesome. Awesome. This is great. And have you also helped customers to like train models? No, but we don't, we didn't help customers.
Well, we did from our solution point of view, but this is an interesting topic because this is something that of the, this is one of the pains that we found quite often with our users.
Like it was easy for them to go to that level, 70% let's say of accuracy with any deep learning model that all these tech giants have developed, right? But we believe that this last mile, this transfer learning part is important.
And we are, and when we realize we started this project that is we called, well, we know it's already released, the fine tuner. Maybe we can share that as well, where we try to make it easy for users to fine tune their models for a metric learning search applications.
And they are, and it is also framework agnostic for, we support fighters, TensorFlow and paddle, paddle. So we realized this pain point for the users that once we have everything running at home, the quality was not as expected.
And this, and we are trying to get to help the user in our ecosystem to get to this, yeah, to this level by using this fine tuner.
So basically, can you can you explain a bit more about fine tuner? Like basically what, what input do I need to provide as a user into this? So fine tuner, it could feel similar to any fighter dataset, for instance, but we are trying to put our documents as our as the main citizen of our ecosystem.
So you have to wrap your any of your data into our document types, which is really easy. So it's something easy to learn and easy to use. And then you can fit your models and we have made it easy for you to use the most typical, those functions we are trying to introduce, hard negative mining.
We are trying to make it easy for everyone to solve the common problems when having, when training for, for search applications.
And we are also trying to make an interactive labeler that helps you interactively through an easy to use UI and tag similar objects so that you can go together with them. Yeah, yeah, so like, kind of, I mean, fine tuning can be a pipeline by itself, right? In itself.
So like, how do you get these data samples that you want to fine tune on? And you might have them with full launch or during test, after launch, and it's like, you know, the cycle and flywheel of success, so to say, right?
So do you cover like the full workflow until production, including production, or is it like pre-production? So for now, we are using the just embedding model.
And just to get embeddings that get better semantics out of your data set of your specific use case. But we are in a really thing, it's easier to point to release or something around in there, so there's a long way to go. Yeah, for sure.
But I mean, the direction is fantastic because that's exactly what, what addresses the real need, any user, like fine tuning. Like it's all fancy to take like a hugging case model or whatever, but like fine tuning it to the level when you're users beloved, that's a different story.
Yeah, that sounds great.
But I also wanted to come back to your, like, you mentioned that Gina AI doesn't kind of compare to vector databases, but I do get sometimes questions like how do these systems compare to each other?
And you may or may not know, I've blocked about all vector databases I knew to that point and turns out they've been six and then the seventh one knocked on the door, so it's also now on the blog.
But I didn't cover Gina AI, I didn't cover deep sets haystack because I thought that Gina and haystack, they're like layers above a vector database. Is that the right thinking? Yes, I think it makes sense. We are, we might try to develop our solutions for the use cases that we may feel more worth.
So that is, I mean, the one is out there to do, but yeah, I think it's right. We are trying to, I think, vector databases cover one of the parts or one of the challenges, maybe one of the main challenges of vector search or neural search, but we try to see the whole scope and the whole pipeline.
So, in Gina, we can use, you can wrap some client that will use any of the big vector searches, big data research of how there have you done any integration with some vector database? Not ourselves right now, but it would be, we might do it in the future.
Okay, yeah, because for now, you did mention that you offer GNN and algorithms, which to me sounds like a core building block of vector database, but then of course in vector database, you have many more things, right? Like, where do you store objects? How you store them?
What about filters and so on? But we are trying to cover from the, for instance, we are not some, some people for some use cases, and just exactly as neighbor search might work just fine, and they don't need to worry about configuring fancy A&N models for their recall speed requirements.
So, I think there is room for everyone. So, I think it just, you have to offer what is right for the right use case and the right need. Yeah, of course.
And by the way, what's the core programming language used in Gina?
 So, our core programming language is Python, because we are more like, since we are this pipeline and we are like a glue ecosystem, most of our operations are wrapping models that run in optimized languages or something, and that also Python helps us to iterate really fast, which other languages might slow us down.
Yeah, that's true. And does it also apply to the A&N algorithms that you mentioned, like BQLite? Is it also Python? I don't know if we are, for instance, I think we are also using some bindings for H&N. So, you are using probably C++ version of H&N SW binding to Python, right? Yes, that's for sure.
But I don't know if some of, for the H&N SWD, yes, for some other parts, I don't know, we are trying to optimize whatever we find.
Yeah, but it sounds cool that, you know, if we still continue kind of this comparison a little bit between Gina and vector databases, like vector databases, if you pick them, let's say BIAV8 is implemented in Go, what grant is implemented in Rust? So, these are compiling languages, right?
So, VESPA is like Java plus some C, I think, C++ and mostly Java.
So, like, nobody implements the vector search in pure Python, because it's very, it's going to be very taxing on the latency, you know? Sure. No, but the expensive operation, we are not running.
So, for instance, the nearest neighbor search we are doing, we are based on NAMPA operations, which are optimized at NAMPA level, and the approximations neighbors, I think, most of the heavy lifting is done on the C++ level from, I'm just covering our bindings.
Yeah, and I'm still curious about BQ Lite, like, is it the C or is it Python, but I think we need to check the documentation. Yes.
Yeah, I'm curious because like, I've actually invented a new algorithm in NAMPA search, but I haven't published it widely, it's open source, but I haven't done the thorough benchmarking.
 And what I've faced is that, you know, like, in Python, even though I optimized all parts of the algorithm, I'm using preallocation and NAMPA, it still runs out of memory, runs out of memory as in it leaks memory, and it doesn't explain, like, Python virtual machine doesn't tell you where, like, you don't have tools.
Okay, there are some tools, but they're not useful. Like, no, you're showing a little stuff. No.
So, and I've been like a little bit like desperate, and I've been thinking, okay, should I now move into RAST GO territory, which might be a little bit more dangerous, like, even though I do have some experience in C++, but you know, like, do I want to go there now?
Like, Python is much more comfortable.
The, I think, depends on the later you are working with, and it's, so I think that by offering Python APIs in the field, if machine learning will attract, then we'll make everybody much easier to use.
Then if you get API rights, the API is right, you might then bind it to whatever of your favorite languages, but I think getting the comfortable API for that developer to use and to love using is one of the key first steps.
So, do you invest a lot into building these APIs? Can you give an example of like some API within Gina that kind of makes the workflow easier for? So, for instance, we are trying to improve a lot in this document.
So, documents are our central logic, and documents are raised that these are the two core members of our family in the ecosystem. So, we are spending a lot of time on making them easy to use.
For instance, with this fluent pattern, we are trying to invest a lot of time on finding the best way, the more Python way to work on it. Yeah, so it's a constant evolution, try and error.
Yeah, of course, but it's, it's like APIs is like exactly that layer, which is essentially like facing the customer, right?
And you don't know the scenarios they will use it in, and sometimes they might kind of surprise you, or they might say, okay, I found some work around for your like missing parts, but then you think, okay, I didn't think about it, right?
The API layer is a fantastic way of talking to your client through like API contract in a way, right? Yeah, and it's a quite a big challenge I would say to have the right balance between ease of use and flexibility.
So, what belongs there and what doesn't belong there? Because there's always a risk to put too much functionality in one same thing and make it very powerful, but make it a nightmare to use.
Yeah, so in these, in these balance, I think there is the key what is your choice when you have to choose? Let's say it's a balance of flexibility, or like flexibility, or what did you say the ease of use, right? ease of use.
I think we are now, I'm now attending to go for the ease of use because for instance, with these open source, I read that open source teaches you well. I think at some point, we did a nearly down to well the APIs and it was a little bit complex to use.
You could do a lot of things, but at the end maybe not everybody was doing. So, I think it's of use for the first century barrier. It's the most important thing.
 Yeah, and I mean, also like it's interesting, you know, like if you have a real API, let's say deploy it somewhere and it's a published contract and people are sending queries there, then you know actually which endpoints which features are being used which are not, which options are completely ignored even though you put them in the dogs, right?
But how do you go about this in the open source code? Like somebody downloads your code, they use it somewhere, you don't know how.
So, how do you collect these analytics from them? Do you just send like call out messages, hey guys, what do you use, what do you don't?
Right now we are trying to keep attention on who is using guys, what, and when people ask us, we try to get the most information out of them, not information on the business of how they use it, how they feel.
So right now the community is the only source of information we have. That's the open source.
What? How do you talk to them?
 Like do you like send like messages saying, hey guys, can you vote about keeping this feature and removing that one or not exactly like this, but would you see people that are more engaged or more or less engaged, people that are more finding it more easier or less or having more difficulties with your with your solution.
So it's and sometimes we have a development relations team that try to get also feedback from from the community in many terms. So this is a global effort.
But in the end you have you have a say, right? Like no matter what they ask you have a say, is that right? Well, I mean, sometimes you cannot please the community to all the extent, I don't know, we have to keep a road map.
For instance, people may want you to build something that is emanated, but maybe not so significant for search solutions. This is quite a confusion, I think.
So beyond search like where can I use GNA, what kind of other use cases have you seen beyond like kind of similarity search?
So since we are building these abstractions, it is quite easy for you to use these abstractions for building any classification model or anything as you really did, you could even deploy something and use GNA to easily deploy and scale and use your segmenter and object segmenter model.
But this is this is something that you could do, but GNA is born and will will be working to implement new research solutions. So you could still use this but might not be the best tool for it. So we are not born for that, but you could do it. But we can see that we are done.
You can do this because for instance classification or segmenting can object can be part of your pipeline, but in theory we are born to support search applications. Yeah, yeah.
 So like, or for example, something that is or search applications or something that you can frame as a search application right now, for instance, a question-answering system that you can frame as a part where you will do something like research or spare search and then you have some real or model that extracts more information from something.
So anything that falls into this domain, you can do it.
Yeah, I guess you can also like based on the research and some practice happening in data augmentation based on retrieving, you can also formulate data augmentation as a process of search in principle, right? So the output will be your augmented data, but you use search in the middle.
Yes, actually that might be but search can be so many problems can be framed into search. I don't know.
At the end is like vectors are somehow like the truth, not like the semantic information, so how we don't understand exactly why but is encoded there, right? So just by clustering them together, somehow we have some understanding, so so many things to confirm with.
I also wanted to ask you a little bit like closer to the similarity search itself, you know, let's say I built a traditional kind of text search engine, okay? And I'm moving away from BM25, which is like probably majority of this market today.
So I'm thinking, okay, what are these cool kids doing? Maybe I should try it out, plug in some bird model.
So and but then in my UI, I am also showing snippets and it's very easy to show snippets when it's a keyboard search, right? So what should I do or what can I do with model like bird and genie AI to show snippets or something that will resemble snippets to the users?
Maybe you can also change your information so that you check where the attention is put in your model or somehow, but yeah, I think also there is a thing that we are framed and we have been grown into this keyboard search that it's so interpretable and so easy to use and even so easy to hack.
So how, you know, you know, you as a user know how to drive your search if you don't find it, right? Okay, this word might find you here.
And I think since these models are kind of black box for many of us, I think in this kind of sense, this interpretability is one of the main challenges and I think one of the main focus that research should go.
Yeah, but I mean, you you you call it out as an interpretability, but like for the user sent for me, let's say I'm a product manager, I don't care about is it bird model, is it VM25? I used to see snippets, I want to see them now.
So like, what should the point in VM25? I can give you a snippet because I know why I have this solution. Here is, well, it correlates and but where the information that I want is there.
Maybe for instance, in dinner, one of the main building blocks that we have is our document is our recursive structure because most of the things, for instance, if you find the search, if you search a document, a text or document, you might need to break into paragraphs into digs and so on.
So maybe what you can do is you do the vector search at the variety level is at sentence level, but then the results might be shown at paragraph or at sentence level. So you can highlight very easily the sentence that really and drove the search to this page.
I remember actually, I don't know if you know the block, was it salmon run, like Sujitpal, he's doing a lot of blogging in the area of like, here is the problem, how do I solve it? And then he, quite usually he goes into deep learning or trying out some vector search, maybe or not.
And I remember like he was saying that to solve this snippeting problem, how he would do it because he comes from his additional search and I do in a way.
And like he said, okay, you can kind of build, like, if I remember correctly, if you can do like almost like a dictionary, right? So let's say you take a word, you can embed it, take a word, you can embed it, like you can embed a dictionary, right?
Now when you found that document, you can kind of from embeddings, you can map back to the words.
If they happen to be closed enough, like geometrically, you can find closed enough words. So you can kind of try to say, okay, maybe these keywords are representative of this text, but I'm not 100% sure, but at least you try. So you go backwards, like reverse engineering from the embeddings.
It's interesting, sir. You may need to go through all the pain of dramatizing kind of these kind of stuff that you may have saved by going through semantic search and now you are back to it. So, like straight-offs, but yeah, it might be a good...
Yeah, dramatization is another thing, but like I think there was this paper from, I believe Google about byte level training, right? So they don't care if it's like lemma or if it's like suffix or prefix, they just go byte level. They don't go sub-word level. They go byte level.
And then with byte level, you can essentially kind of like, okay, now I can compute the distance again, right? Okay, how close is this to this dictionary word or not?
But then again, from there, in order to produce a snippet that will look like natural language, you will have to use some kind of model like GPT or like in general, generate the sentence.
And at that point, it might actually go completely different direction from your text, right? Start like hallucinating or write a news item that doesn't exist. So yeah.
Well, maybe you can use these extractive models from a sentence, giving a context, but nature, all these top-notch research is basically. Yeah, yeah.
But I mean, like attention, what you mentioned, attention probably can be used here, right?
 So like you can ask the model, okay, what did you pay attention to when you did the matching, but still it's not some people, as you say, like you can say it interpretability, but on that hand, it's kind of like when you go specifically to that product case, you need that snippet or you need that kind of context of the match.
Or like if you said mathematics and it picked algebra, like why did it pick algebra? At least can you explain? Because here it's more or less obvious, but in a specific domain, it might not be, right? Yes, like what do we do? Maybe you are not using the right tool, I don't know.
Maybe we are obsessed on using the declared for everything. But I think these two walls of keyword, what we call traditional search and this neural search, I think they can be combined to power things to the next level. I think they need to be enemies and there is good and bad team both sides.
Do you have any thoughts how you would combine? For instance, in any solution, you can have solution, I don't know, maybe you can get results based on both sides and then at our ranking steps consider what is best, you know, is this a complex query?
Maybe I'm looking more for some semantically reached solution.
Did this guy just send a couple of keywords? Is it semantically reached enough? No, this user might be expecting keyword based feedback. Yeah, that's true. Well, you could even go as simple as giving that control to users.
So, if they know that it's keyword, they first want to go with what they know, what works or may not work and then if they are not satisfied enough, then they optimize for equal, they might go into explorative mode, that's on the similarity search. That might be quite viable. So, it's interesting.
The problem is that keyword search, well, as far as search, it might have not a good future for image based search or any other mobility related search. Yeah, exactly. The moment you go beyond text, what do you do? That's a big power, I think, and the big future that Neurochurch has ahead.
There is where not any traditional search solution, I think, will keep up. So, if I want to build a multimodal search, can I pick some executor from the marketplace and plug it into Gina today and do it? Yeah, I don't know. I think we have some, for instance, but you can use clip that clip.
You can use clip to encode. I think there is audio, you can text, or there is image and text, and it performs very well. We have wrapped it in one of these executors and hot modules, and you can use these clip models to do your close model search.
It's quite efficient without the match-faint tuning to search for images given text and the other way around. It's quite impressive. Yeah, that sounds cool. When I was thinking, if I want to combine like speech, text, and image, then I need to probably come up with some meta model of that.
Right? There is some research in this area where it is not that like modalities are treated differently and encoded separately, but where they are considered together, even there is some research where there is multimodality and some contact switch, so they move the vector.
So that's also possible to get the latest research, wrap it into one of these models and deploy it in production. But this is not so easy. For us, we didn't focus on building these front scratch, but we're also looking to having these top-notch researchers into building these modules in here.
So like, in that case, would you prefer communities to help out to bring in the model, or are you helping to do that? Right now, we are driving this direction to offer these for the community. I think that our dream as an open source is to have the community flourish and be alive upon itself.
So the future should be community driven. Yeah, because in the end, community might also know kind of when these growths be, you know, community will be kind of helping each other.
Like some of these things will become what you may call commodity to some extent, right? Or at least the way you integrate might become commodity and the use cases might become commodity and there will be new use cases which are untapped, but I think community can definitely help out each other.
What we might need to focus on to make these models easier to use or easier to find if we have a marketplace where everything, maybe we need to help the community on finding what they need in every time. Yeah, yeah.
Content wise, hopefully there is a time where community is the main contributor there.
Was there something else in Gina that we should know about as users? Some cool feature or some system that you think doesn't exist in competitors? Is there something at all to school? I don't know right now about competitors.
So I think what I like the most is the easy, the easy views and the time saving. You go out to our readme and you try to build from zero to to the plighting core net is an neural search solution and image search solution. I think you will you will all enjoy days and nights. Yeah. Yeah.
So it's like kind of well-oiled machine. But can I also bring it up on my laptop? Yes. You can try on your laptop everything. So the point is you may not be able to index so many images but you can get the first feeling with your laptop.
Yeah, I mean if I want to be like a demo to impress my manager, you know, so I usually use my laptop right? Like that's maybe one way. Gina is really for that. Yeah, that's that's pretty cool.
 And I think also like it's nice that you said it's Python friendly so it opens doors to so many things like especially like on hiking face it's pretty much all Python right so I need to pick some models like it in and do I need to containerize it maybe or figure out isolation and so on like just plug it in and start using it.
I think that's also a great boost to productivity and actually kind of implementing the use case rather than focusing on some mundane components and parts and processes right? Yeah and even these modules that we have they are already containerized for you.
So we have on our end we build a container for you so that you can be in an isolated way with your all your dependencies and stuff. Yeah, yeah. Sounds great. I mean I think we now have pretty good understanding of Gina.
Of course we didn't read the docs yet but it's sounds promising so I hope some of listeners and audience will take it out.
I wanted to go more into this kind of philosophical level like what what drives you in this space? Like you said that you've been working in web scale search as well before right? And like some other search and engineering in general.
So what drives you here now in this area when you join Gina and why why you join Gina? So I joined Gina especially as I was in this traditional search space I was working on training ranking models. So what drove me more is to enable this search system, this search experience instance beyond text.
I was super curious about how we can extend it's impressive to me how the same framework of getting something that extracts meaningful vectors with semantic information can be used for images, for video, for audio, for anything.
These frameworks I think it has a lot of features because it's quite and also how the how the different research areas from different modalities interact with each other. I don't know I don't know.
Trans the conversion neural network appeared even some text classification used to do these then appeared the transformer right now the computer vision community is getting in love with transformers.
These back and forth I think that it's impressive but also if you think of the magic of getting this vector and having so much meaning there it's quite amazing.
 Yeah it's true I mean it's very powerful you know like that the the the sheer fact that you don't need to build a synonym like dictionary if you go full text right like it just tells you that yeah mathematics is close to algebra or you know like you throw data at it and it's an unsupervised right it just tells you hey I've trained it up like now okay I can tell you what's close to each other geometrically it also has the mathematical beauty there right geometric closeness rather than kind of some obscure strange abstract sparse closeness it's quite elegant yeah yeah you have tracked all these knowledge all these and you have that this simple thing that you can imagine in your head as a 3D space and that is simple as algebra from I don't know which grade but quite simple yeah I think in simplicity there is a lot of beauty yeah it's very easy to explain to your granny like I'm doing this you know like it's 3D space kind of there are points and I'm just looking the closest one I expect to have something that puts things close to each other that makes sense together that is what we expect from these black box exactly exactly and then and then the question of scale like if you go to 10 million hundred million billion billion then okay can you trade some of that closeness precision you know and kind of get past the speed so yeah it's very interesting I mean it's um does it does it interest you more like on deep learning side or on mathematics side or engineering side like or maybe some other side in every side from mathematics I enjoy a lot the beauty of it sometimes it's too obscure for me but I really like and understand deep learning I like it although I feel that some of some of the research doesn't seem to be so innovative and maybe we should spend more time checking other paths and deep learning which are the paths like I don't know you should be honest I don't know I'm just feel I just feel that there's so much literature that I cannot keep up with and then from the engineering side I think it's cool it's just space I think I also can provide more value and sometimes concepts are to abstract from yeah like for me like I want to call out but you like point on is deep learning the only way you know like for example one scary thing is that these models are becoming more and more kind of parameterized so you have like hundreds of billions parameters maybe billion trillion like how many more can you have zillion parameters in there but first of all it's impractical so if you take that model you try to plug it in it doesn't plug in because it's too expensive and also you might not have that much data in the first place right so why should you care like web scale search engines probably will but like you as a researcher in let's say a startup you don't know if you need that much you need to sell solve that specific thing right so it will it will look really strange to bring this huge microscope and like GBT model in and say this is what we need to use and then like the whole budget goes into paying that model or whatever you know like it's in it's in practical so that that direction by itself like I think it's a little bit like doomed or like I don't know like how you feel about it yeah it's a it's a race where I add another layer and get more parameters and I win and I think I'm not but it feels that the first step to move away from this is to really understand how things are learned and why are learned the way they are I don't know any match you have these models to bring back to the image where the where the filters are learned more or less you have some idea or where the model is looking but maybe to put more research on slow down let's slow down these race and let's understand and maybe we find a way to make it more sustainable for everyone yeah because I remember like when I was doing my PhD in machine translation it was using like statistical models like Moses and you know statistical machine translation and so it would suffer from things like out of vocabulary and you know how do I bring syntax in and whatnot but then like when deep learning came like all of a sudden you see that it translates much much better and you think wow probably probably they solved it now right the claim from 50s that we will solve machine translation probably now is delivered that the promise but but then you notice it's fluent but it's kind of like I don't want to use the word stupid but it just doesn't get it right like it it makes wolf subject an object easily it may go and hallucinate about something that doesn't exist there or it actually goes and translates into like single letters all of all of a sudden you know or repeating engrams or like you see that it didn't exactly solve it right you wouldn't trans your life to such a system yet no yeah and then you kind of come back and like okay and I used to do it like in a rule-based approach so I could understand the syntax of the of the sentence and then semantics like nod in the tree and then when I translate I use some semantic like function and it's all well-defined in the the astronomy of semantic functions and so on like okay now I go back to deploying do you have anything like that no it's like just space maybe there is a there should be a way to combine I don't know we have built I think as humans we have built this complex way of talking to each other which I mean multiple languages and stuff and there is no way that all these language can go back to this deep learning world world it seems country to if div at least yeah yeah so you are certain that like maybe the voice of those who build alternative models to deploying off a alternative approach should be maybe louder yeah I think that we may suffer from the bias of the winner no I mean maybe the first one who opens a door might not do in the race because but even if they show another way that the race might go I think they might deserve more attention yeah yeah this this is quite deep thanks for this white white section like you like you you think about it a lot like kind of okay not to be biased okay yes there are challenges but it might not be the only right approach and giving you experience as well like you can judge a little bit like with with your open eyes I think we should explore more and maybe not one focus of that yeah and and probably explore with Gina right that's the talk for sure yeah so this is this is super great is there something you want to share you already shared that the fine tuner is available so our listeners can go and check it out right is there something else that like we need to be expecting and early next year we should be releasing our 3.
0 version so the stay tuned for that yeah yeah comment we will be moving fast and the next times yeah this is fantastic I mean thanks so much for all this information and detail on Gina and also like your your ambition and then kind of like you're thinking here I mean it's really nice that you keep your open mind available to all of our listeners yeah thanks so much it was a pleasure to talk to you John today yeah thank you so much yeah thank you much looking forward to 3.
0 yeah thank you bye bye you