---
description: '<p>Vector Podcast website: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://vectorpodcast.com">https://vectorpodcast.com</a></p><p></p><p>Get
  your copy of John''s new book "Prompt Engineering for LLMs: The Art and Science
  of Building Large Language Modelâ€“Based Applications": <a target="_blank" rel="noopener
  noreferrer nofollow" href="https://amzn.to/4fMj2Ef">https://amzn.to/4fMj2Ef</a></p><p></p><p><em>John
  Berryman is the founder and principal consultant of </em><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://arcturus-labs.com/"><em>Arcturus Labs</em></a><em>,
  where he specializes in AI application development (Agency and RAG). As an early
  engineer on GitHub Copilot, John contributed to the development of its completions
  and chat functionalities, working at the forefront of AI-assisted coding tools.
  John is coauthor of </em><a target="_blank" rel="noopener noreferrer nofollow" href="https://amzn.to/4fMj2Ef"><em>"Prompt
  Engineering for LLMs" (O''Reilly)</em></a><em>.Before his work on Copilot, John''s
  focus was search technology. His diverse experience includes helping to develop
  next-generation search system for the US Patent Office, building search and recommendations
  for Eventbrite, and contributing to GitHub''s code search infrastructure. John is
  also coauthor of </em><a target="_blank" rel="noopener noreferrer nofollow" href="https://amzn.to/3TXmDHk"><em>"Relevant
  Search" (Manning)</em></a><em>, a book that distills his expertise in the field.John''s
  unique background, spanning both cutting-edge AI applications and foundational search
  technologies, positions him at the forefront of innovation in LLM applications and
  information retrieval.</em></p><p></p><p>00:00 Intro</p><p>02:19 John''s background
  and story in search and ML</p><p>06:03 Is RAG just a prompt engineering technique?</p><p>10:15
  John''s progression from a search engineer to ML researcher</p><p>13:40 LLM predictability
  vs more traditional programming</p><p>22:31 Code assist with GitHub Copilot</p><p>29:44
  Role of keyword search for code at GitHub</p><p>35:01 GenAI: existential risk or
  pure magic? AI Natives</p><p>39:40 What are Artifacts</p><p>46:59 Demo!</p><p>55:13
  Typed artifacts, tools, accordion artifacts</p><p>56:21 From Web 2.0 to Idea exchange</p><p>57:51
  Spam will transform into Slop</p><p>58:56 John''s new book and Acturus Labs intro</p><p></p><p>Show
  notes:</p><p>- John Berryman on X: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://x.com/JnBrymn">https://x.com/JnBrymn</a></p><p>- Acturus Labs: <a
  target="_blank" rel="noopener noreferrer nofollow" href="https://arcturus-labs.com/">https://arcturus-labs.com/</a></p><p>-
  John''s blog on Artifacts (see demo in the episode): <a target="_blank" rel="noopener
  noreferrer nofollow" href="https://arcturus-labs.com/blog/2024/11/11/cut-the-chit-chat-with-artifacts/">https://arcturus-labs.com/blog/2024/11/11/cut-the-chit-chat-with-artifacts/</a></p><p></p><p>YouTube:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://youtu.be/60HAtHVBYj8">https://youtu.be/60HAtHVBYj8</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20250209_090249_4151453caa902e94e1bbf399c57f535b.png
pub_date: Mon, 10 Feb 2025 03:21:48 GMT
title: Code search, Copilot, LLM prompting with empathy and Artifacts with John Berryman
url: https://rss.com/podcasts/vector-podcast/1888857
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 21.44, "text": " Hello
  everyone, Vector podcast is back. Season 3. We are wrapping up the season with some",
  "tokens": [50364, 2425, 1518, 11, 691, 20814, 7367, 307, 646, 13, 16465, 805, 13,
  492, 366, 21993, 493, 264, 3196, 365, 512, 51436], "temperature": 0.0, "avg_logprob":
  -0.24522976253343665, "compression_ratio": 1.2919708029197081, "no_speech_prob":
  0.2688979208469391}, {"id": 1, "seek": 0, "start": 21.44, "end": 26.88, "text":
  " really, really juicy episodes. I''m sure you will love this one. I have the privilege
  of", "tokens": [51436, 534, 11, 534, 24696, 9313, 13, 286, 478, 988, 291, 486, 959,
  341, 472, 13, 286, 362, 264, 12122, 295, 51708], "temperature": 0.0, "avg_logprob":
  -0.24522976253343665, "compression_ratio": 1.2919708029197081, "no_speech_prob":
  0.2688979208469391}, {"id": 2, "seek": 2688, "start": 26.88, "end": 34.8, "text":
  " talking to John Barryman today. He is an ex senior machine learning researcher
  who worked on", "tokens": [50364, 1417, 281, 2619, 21639, 1601, 965, 13, 634, 307,
  364, 454, 7965, 3479, 2539, 21751, 567, 2732, 322, 50760], "temperature": 0.0, "avg_logprob":
  -0.23774630402865476, "compression_ratio": 1.4019607843137254, "no_speech_prob":
  0.26456302404403687}, {"id": 3, "seek": 2688, "start": 34.8, "end": 42.879999999999995,
  "text": " GitHub Copilot. Currently, he runs his own consultancy actress labs. I''m
  sure he will talk more", "tokens": [50760, 23331, 11579, 31516, 13, 19964, 11, 415,
  6676, 702, 1065, 7189, 6717, 15410, 20339, 13, 286, 478, 988, 415, 486, 751, 544,
  51164], "temperature": 0.0, "avg_logprob": -0.23774630402865476, "compression_ratio":
  1.4019607843137254, "no_speech_prob": 0.26456302404403687}, {"id": 4, "seek": 2688,
  "start": 42.879999999999995, "end": 52.64, "text": " about that. Yeah, welcome,
  John. Good to be here. How''s it going? Awesome. I actually just picked", "tokens":
  [51164, 466, 300, 13, 865, 11, 2928, 11, 2619, 13, 2205, 281, 312, 510, 13, 1012,
  311, 309, 516, 30, 10391, 13, 286, 767, 445, 6183, 51652], "temperature": 0.0, "avg_logprob":
  -0.23774630402865476, "compression_ratio": 1.4019607843137254, "no_speech_prob":
  0.26456302404403687}, {"id": 5, "seek": 5264, "start": 52.64, "end": 60.160000000000004,
  "text": " the book of yours and the book that you and Dr. Moll have written together.
  I''ve interviewed", "tokens": [50364, 264, 1446, 295, 6342, 293, 264, 1446, 300,
  291, 293, 2491, 13, 376, 1833, 362, 3720, 1214, 13, 286, 600, 19770, 50740], "temperature":
  0.0, "avg_logprob": -0.1765301648308249, "compression_ratio": 1.6101694915254237,
  "no_speech_prob": 0.18279415369033813}, {"id": 6, "seek": 5264, "start": 60.160000000000004,
  "end": 66.8, "text": " Doug a couple of times already on the podcast. He has a lot
  to say. And I realized you''ve written", "tokens": [50740, 12742, 257, 1916, 295,
  1413, 1217, 322, 264, 7367, 13, 634, 575, 257, 688, 281, 584, 13, 400, 286, 5334,
  291, 600, 3720, 51072], "temperature": 0.0, "avg_logprob": -0.1765301648308249,
  "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.18279415369033813},
  {"id": 7, "seek": 5264, "start": 66.8, "end": 73.6, "text": " this book together.
  It''s my go-to source of wisdom on search. Do you still remember which chapters",
  "tokens": [51072, 341, 1446, 1214, 13, 467, 311, 452, 352, 12, 1353, 4009, 295,
  10712, 322, 3164, 13, 1144, 291, 920, 1604, 597, 20013, 51412], "temperature": 0.0,
  "avg_logprob": -0.1765301648308249, "compression_ratio": 1.6101694915254237, "no_speech_prob":
  0.18279415369033813}, {"id": 8, "seek": 5264, "start": 73.6, "end": 82.24000000000001,
  "text": " you covered? Oh my gosh. It''s been a long time, I''m sure. Yeah, if you
  told me the chapter", "tokens": [51412, 291, 5343, 30, 876, 452, 6502, 13, 467,
  311, 668, 257, 938, 565, 11, 286, 478, 988, 13, 865, 11, 498, 291, 1907, 385, 264,
  7187, 51844], "temperature": 0.0, "avg_logprob": -0.1765301648308249, "compression_ratio":
  1.6101694915254237, "no_speech_prob": 0.18279415369033813}, {"id": 9, "seek": 8224,
  "start": 82.24, "end": 87.6, "text": " title, I could probably say whether it is
  mere Doug. I did all the fun ones that did all hard ones.", "tokens": [50364, 4876,
  11, 286, 727, 1391, 584, 1968, 309, 307, 8401, 12742, 13, 286, 630, 439, 264, 1019,
  2306, 300, 630, 439, 1152, 2306, 13, 50632], "temperature": 0.0, "avg_logprob":
  -0.2895876432719984, "compression_ratio": 1.6724890829694323, "no_speech_prob":
  0.02608640119433403}, {"id": 10, "seek": 8224, "start": 89.11999999999999, "end":
  93.44, "text": " And we both did chapter one in our own times. I mean, we were all
  in chapter twice.", "tokens": [50708, 400, 321, 1293, 630, 7187, 472, 294, 527,
  1065, 1413, 13, 286, 914, 11, 321, 645, 439, 294, 7187, 6091, 13, 50924], "temperature":
  0.0, "avg_logprob": -0.2895876432719984, "compression_ratio": 1.6724890829694323,
  "no_speech_prob": 0.02608640119433403}, {"id": 11, "seek": 8224, "start": 93.44,
  "end": 98.39999999999999, "text": " I want to read maybe everything, but in the
  search relevance problems, search under the hood,", "tokens": [50924, 286, 528,
  281, 1401, 1310, 1203, 11, 457, 294, 264, 3164, 32684, 2740, 11, 3164, 833, 264,
  13376, 11, 51172], "temperature": 0.0, "avg_logprob": -0.2895876432719984, "compression_ratio":
  1.6724890829694323, "no_speech_prob": 0.02608640119433403}, {"id": 12, "seek": 8224,
  "start": 98.39999999999999, "end": 106.72, "text": " debugging, relevance problem,
  tame in tokens, basic multi-field search, how you build relevance function,", "tokens":
  [51172, 45592, 11, 32684, 1154, 11, 45774, 294, 22667, 11, 3875, 4825, 12, 7610,
  3164, 11, 577, 291, 1322, 32684, 2445, 11, 51588], "temperature": 0.0, "avg_logprob":
  -0.2895876432719984, "compression_ratio": 1.6724890829694323, "no_speech_prob":
  0.02608640119433403}, {"id": 13, "seek": 10672, "start": 106.72, "end": 116.8, "text":
  " feed relevance feedback. Yeah, relevance centered enterprise. That''s interesting.
  And then", "tokens": [50364, 3154, 32684, 5824, 13, 865, 11, 32684, 18988, 14132,
  13, 663, 311, 1880, 13, 400, 550, 50868], "temperature": 0.0, "avg_logprob": -0.33205181278594553,
  "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.013508201576769352},
  {"id": 14, "seek": 10672, "start": 116.8, "end": 125.2, "text": " semantic and personalized
  search. Wow. Back in when was this published? I think we published that", "tokens":
  [50868, 47982, 293, 28415, 3164, 13, 3153, 13, 5833, 294, 562, 390, 341, 6572, 30,
  286, 519, 321, 6572, 300, 51288], "temperature": 0.0, "avg_logprob": -0.33205181278594553,
  "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.013508201576769352},
  {"id": 15, "seek": 10672, "start": 125.2, "end": 133.12, "text": " in 2016, I see.
  Yeah, 2016. Yeah, well, it''s been almost 10 years. Yeah, that''s the version I
  have.", "tokens": [51288, 294, 6549, 11, 286, 536, 13, 865, 11, 6549, 13, 865, 11,
  731, 11, 309, 311, 668, 1920, 1266, 924, 13, 865, 11, 300, 311, 264, 3037, 286,
  362, 13, 51684], "temperature": 0.0, "avg_logprob": -0.33205181278594553, "compression_ratio":
  1.4922279792746114, "no_speech_prob": 0.013508201576769352}, {"id": 16, "seek":
  13312, "start": 133.6, "end": 140.16, "text": " So you do have semantic search in
  the end there. Yeah, awesome. Yeah. But yeah, Joan,", "tokens": [50388, 407, 291,
  360, 362, 47982, 3164, 294, 264, 917, 456, 13, 865, 11, 3476, 13, 865, 13, 583,
  1338, 11, 25748, 11, 50716], "temperature": 0.0, "avg_logprob": -0.204515147518802,
  "compression_ratio": 1.4248704663212435, "no_speech_prob": 0.015206818468868732},
  {"id": 17, "seek": 13312, "start": 140.16, "end": 149.76, "text": " it''s interesting
  to introduce yourself to our audience. What''s your background? How you got here?",
  "tokens": [50716, 309, 311, 1880, 281, 5366, 1803, 281, 527, 4034, 13, 708, 311,
  428, 3678, 30, 1012, 291, 658, 510, 30, 51196], "temperature": 0.0, "avg_logprob":
  -0.204515147518802, "compression_ratio": 1.4248704663212435, "no_speech_prob": 0.015206818468868732},
  {"id": 18, "seek": 13312, "start": 149.76, "end": 156.72, "text": " What are you
  up to? Oh, well, I guess that''s a long story. I''ve had a very circuitous path.",
  "tokens": [51196, 708, 366, 291, 493, 281, 30, 876, 11, 731, 11, 286, 2041, 300,
  311, 257, 938, 1657, 13, 286, 600, 632, 257, 588, 9048, 563, 3100, 13, 51544], "temperature":
  0.0, "avg_logprob": -0.204515147518802, "compression_ratio": 1.4248704663212435,
  "no_speech_prob": 0.015206818468868732}, {"id": 19, "seek": 15672, "start": 157.68,
  "end": 164.8, "text": " I started out in aerospace engineering because I like the
  math. And as I got into the field,", "tokens": [50412, 286, 1409, 484, 294, 46817,
  7043, 570, 286, 411, 264, 5221, 13, 400, 382, 286, 658, 666, 264, 2519, 11, 50768],
  "temperature": 0.0, "avg_logprob": -0.17873286399520746, "compression_ratio": 1.7745454545454546,
  "no_speech_prob": 0.05893902853131294}, {"id": 20, "seek": 15672, "start": 164.8,
  "end": 169.44, "text": " I found that that''s a thing that I really liked once the
  math and was the software. You could do", "tokens": [50768, 286, 1352, 300, 300,
  311, 257, 551, 300, 286, 534, 4501, 1564, 264, 5221, 293, 390, 264, 4722, 13, 509,
  727, 360, 51000], "temperature": 0.0, "avg_logprob": -0.17873286399520746, "compression_ratio":
  1.7745454545454546, "no_speech_prob": 0.05893902853131294}, {"id": 21, "seek": 15672,
  "start": 169.44, "end": 173.68, "text": " anything with those. And so while everyone
  was geeking out about satellites and stuff, I thought", "tokens": [51000, 1340,
  365, 729, 13, 400, 370, 1339, 1518, 390, 36162, 278, 484, 466, 24960, 293, 1507,
  11, 286, 1194, 51212], "temperature": 0.0, "avg_logprob": -0.17873286399520746,
  "compression_ratio": 1.7745454545454546, "no_speech_prob": 0.05893902853131294},
  {"id": 22, "seek": 15672, "start": 173.68, "end": 178.48, "text": " that was really
  cool. But I realized that there''s a big, big world out there that you could address",
  "tokens": [51212, 300, 390, 534, 1627, 13, 583, 286, 5334, 300, 456, 311, 257, 955,
  11, 955, 1002, 484, 456, 300, 291, 727, 2985, 51452], "temperature": 0.0, "avg_logprob":
  -0.17873286399520746, "compression_ratio": 1.7745454545454546, "no_speech_prob":
  0.05893902853131294}, {"id": 23, "seek": 15672, "start": 178.48, "end": 185.28,
  "text": " the whole thing with software and math. So I breached out and got that
  book in your hand. My next big", "tokens": [51452, 264, 1379, 551, 365, 4722, 293,
  5221, 13, 407, 286, 1403, 15095, 484, 293, 658, 300, 1446, 294, 428, 1011, 13, 1222,
  958, 955, 51792], "temperature": 0.0, "avg_logprob": -0.17873286399520746, "compression_ratio":
  1.7745454545454546, "no_speech_prob": 0.05893902853131294}, {"id": 24, "seek": 18528,
  "start": 186.16, "end": 192.32, "text": " adventure was into search. I joined a
  concerted consultancy in Charlottesville, Virginia,", "tokens": [50408, 9868, 390,
  666, 3164, 13, 286, 6869, 257, 8543, 292, 7189, 6717, 294, 14130, 1521, 279, 8386,
  11, 10956, 11, 50716], "temperature": 0.0, "avg_logprob": -0.19640815258026123,
  "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.003105930984020233},
  {"id": 25, "seek": 18528, "start": 192.96, "end": 198.72, "text": " worked with
  Doug Turnbull. I did had amazing adventures, hop on planes. And I talked to Zappos,",
  "tokens": [50748, 2732, 365, 12742, 7956, 37290, 13, 286, 630, 632, 2243, 20905,
  11, 3818, 322, 14952, 13, 400, 286, 2825, 281, 1176, 1746, 329, 11, 51036], "temperature":
  0.0, "avg_logprob": -0.19640815258026123, "compression_ratio": 1.6538461538461537,
  "no_speech_prob": 0.003105930984020233}, {"id": 26, "seek": 18528, "start": 199.36,
  "end": 205.2, "text": " shoe sales and worked with a patent office. And then I got
  the opportunity to write that book with Doug.", "tokens": [51068, 12796, 5763, 293,
  2732, 365, 257, 20495, 3398, 13, 400, 550, 286, 658, 264, 2650, 281, 2464, 300,
  1446, 365, 12742, 13, 51360], "temperature": 0.0, "avg_logprob": -0.19640815258026123,
  "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.003105930984020233},
  {"id": 27, "seek": 18528, "start": 205.2, "end": 211.04, "text": " So that pushed
  me all really, really far. I got the opportunity to start working for some really",
  "tokens": [51360, 407, 300, 9152, 385, 439, 534, 11, 534, 1400, 13, 286, 658, 264,
  2650, 281, 722, 1364, 337, 512, 534, 51652], "temperature": 0.0, "avg_logprob":
  -0.19640815258026123, "compression_ratio": 1.6538461538461537, "no_speech_prob":
  0.003105930984020233}, {"id": 28, "seek": 21104, "start": 211.04, "end": 217.28,
  "text": " interesting companies or for Eventbrite and built out their search and
  recommendation twice.", "tokens": [50364, 1880, 3431, 420, 337, 13222, 1443, 642,
  293, 3094, 484, 641, 3164, 293, 11879, 6091, 13, 50676], "temperature": 0.0, "avg_logprob":
  -0.19951191167721802, "compression_ratio": 1.7293577981651376, "no_speech_prob":
  0.0012346090516075492}, {"id": 29, "seek": 21104, "start": 218.23999999999998, "end":
  225.44, "text": " And then I got a chance to parly that into GitHub. So I went to
  GitHub and built out their", "tokens": [50724, 400, 550, 286, 658, 257, 2931, 281,
  971, 356, 300, 666, 23331, 13, 407, 286, 1437, 281, 23331, 293, 3094, 484, 641,
  51084], "temperature": 0.0, "avg_logprob": -0.19951191167721802, "compression_ratio":
  1.7293577981651376, "no_speech_prob": 0.0012346090516075492}, {"id": 30, "seek":
  21104, "start": 225.44, "end": 231.68, "text": " last search-based code search infrastructure.
  The old search infrastructure had smoke coming out", "tokens": [51084, 1036, 3164,
  12, 6032, 3089, 3164, 6896, 13, 440, 1331, 3164, 6896, 632, 8439, 1348, 484, 51396],
  "temperature": 0.0, "avg_logprob": -0.19951191167721802, "compression_ratio": 1.7293577981651376,
  "no_speech_prob": 0.0012346090516075492}, {"id": 31, "seek": 21104, "start": 231.68,
  "end": 240.23999999999998, "text": " of it. So we came in, rebuilt infrastructure
  from ground up. And after a while, I was search was", "tokens": [51396, 295, 309,
  13, 407, 321, 1361, 294, 11, 38532, 6896, 490, 2727, 493, 13, 400, 934, 257, 1339,
  11, 286, 390, 3164, 390, 51824], "temperature": 0.0, "avg_logprob": -0.19951191167721802,
  "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.0012346090516075492},
  {"id": 32, "seek": 24024, "start": 240.24, "end": 245.84, "text": " fun. But I was
  always trying to get a little bit back towards math, towards data science tips.",
  "tokens": [50364, 1019, 13, 583, 286, 390, 1009, 1382, 281, 483, 257, 707, 857,
  646, 3030, 5221, 11, 3030, 1412, 3497, 6082, 13, 50644], "temperature": 0.0, "avg_logprob":
  -0.283728274670276, "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0016589078586548567},
  {"id": 33, "seek": 24024, "start": 246.64000000000001, "end": 252.48000000000002,
  "text": " And in about 2021, I got my chance to make the leak to data science.",
  "tokens": [50684, 400, 294, 466, 7201, 11, 286, 658, 452, 2931, 281, 652, 264, 17143,
  281, 1412, 3497, 13, 50976], "temperature": 0.0, "avg_logprob": -0.283728274670276,
  "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0016589078586548567},
  {"id": 34, "seek": 24024, "start": 253.12, "end": 259.36, "text": " Join data science
  at GitHub. And from there, it ended up getting the opportunity, just right", "tokens":
  [51008, 19642, 1412, 3497, 412, 23331, 13, 400, 490, 456, 11, 309, 4590, 493, 1242,
  264, 2650, 11, 445, 558, 51320], "temperature": 0.0, "avg_logprob": -0.283728274670276,
  "compression_ratio": 1.5474137931034482, "no_speech_prob": 0.0016589078586548567},
  {"id": 35, "seek": 24024, "start": 259.36, "end": 265.36, "text": " place, right
  time to join Copilot. Because that was kind of, you know, ML machine learning type
  stuff.", "tokens": [51320, 1081, 11, 558, 565, 281, 3917, 11579, 31516, 13, 1436,
  300, 390, 733, 295, 11, 291, 458, 11, 21601, 3479, 2539, 2010, 1507, 13, 51620],
  "temperature": 0.0, "avg_logprob": -0.283728274670276, "compression_ratio": 1.5474137931034482,
  "no_speech_prob": 0.0016589078586548567}, {"id": 36, "seek": 26536, "start": 265.92,
  "end": 273.84000000000003, "text": " And I was in the data science group to that
  point. And I was, I came on to Copilot", "tokens": [50392, 400, 286, 390, 294, 264,
  1412, 3497, 1594, 281, 300, 935, 13, 400, 286, 390, 11, 286, 1361, 322, 281, 11579,
  31516, 50788], "temperature": 0.0, "avg_logprob": -0.1416790783405304, "compression_ratio":
  1.6363636363636365, "no_speech_prob": 0.0052206916734576225}, {"id": 37, "seek":
  26536, "start": 274.64, "end": 281.44, "text": " after the research team had wrapped
  up. There was a research team, brilliant people from GitHub next.", "tokens": [50828,
  934, 264, 2132, 1469, 632, 14226, 493, 13, 821, 390, 257, 2132, 1469, 11, 10248,
  561, 490, 23331, 958, 13, 51168], "temperature": 0.0, "avg_logprob": -0.1416790783405304,
  "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0052206916734576225},
  {"id": 38, "seek": 26536, "start": 281.44, "end": 285.76, "text": " They said, while
  look at these large language models, they''re going to do amazing things. I think",
  "tokens": [51168, 814, 848, 11, 1339, 574, 412, 613, 2416, 2856, 5245, 11, 436,
  434, 516, 281, 360, 2243, 721, 13, 286, 519, 51384], "temperature": 0.0, "avg_logprob":
  -0.1416790783405304, "compression_ratio": 1.6363636363636365, "no_speech_prob":
  0.0052206916734576225}, {"id": 39, "seek": 26536, "start": 285.76, "end": 293.2,
  "text": " it''s time. And they built this prototype. And then I came in on the team
  that was there when it", "tokens": [51384, 309, 311, 565, 13, 400, 436, 3094, 341,
  19475, 13, 400, 550, 286, 1361, 294, 322, 264, 1469, 300, 390, 456, 562, 309, 51756],
  "temperature": 0.0, "avg_logprob": -0.1416790783405304, "compression_ratio": 1.6363636363636365,
  "no_speech_prob": 0.0052206916734576225}, {"id": 40, "seek": 29320, "start": 293.2,
  "end": 298.64, "text": " was going into production. So how to get this shipped to
  everyone, how to start improving it, how to", "tokens": [50364, 390, 516, 666, 4265,
  13, 407, 577, 281, 483, 341, 25312, 281, 1518, 11, 577, 281, 722, 11470, 309, 11,
  577, 281, 50636], "temperature": 0.0, "avg_logprob": -0.14121625029925003, "compression_ratio":
  1.6837606837606838, "no_speech_prob": 0.002984345890581608}, {"id": 41, "seek":
  29320, "start": 298.64, "end": 304.88, "text": " measure, you know, what was working
  and what wasn''t not working. And then from there, I went into", "tokens": [50636,
  3481, 11, 291, 458, 11, 437, 390, 1364, 293, 437, 2067, 380, 406, 1364, 13, 400,
  550, 490, 456, 11, 286, 1437, 666, 50948], "temperature": 0.0, "avg_logprob": -0.14121625029925003,
  "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.002984345890581608},
  {"id": 42, "seek": 29320, "start": 305.84, "end": 313.2, "text": " chat, Copilot
  chat. I was working with some of those features inside the web app. And finally,",
  "tokens": [50996, 5081, 11, 11579, 31516, 5081, 13, 286, 390, 1364, 365, 512, 295,
  729, 4122, 1854, 264, 3670, 724, 13, 400, 2721, 11, 51364], "temperature": 0.0,
  "avg_logprob": -0.14121625029925003, "compression_ratio": 1.6837606837606838, "no_speech_prob":
  0.002984345890581608}, {"id": 43, "seek": 29320, "start": 313.2, "end": 317.68,
  "text": " I was like, well, you know, I''ve got a little bit of knowledge in my
  head now, time to write another", "tokens": [51364, 286, 390, 411, 11, 731, 11,
  291, 458, 11, 286, 600, 658, 257, 707, 857, 295, 3601, 294, 452, 1378, 586, 11,
  565, 281, 2464, 1071, 51588], "temperature": 0.0, "avg_logprob": -0.14121625029925003,
  "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.002984345890581608},
  {"id": 44, "seek": 31768, "start": 318.24, "end": 325.36, "text": " book. And I
  connected with one of the research scientists that was on the original team. Albert",
  "tokens": [50392, 1446, 13, 400, 286, 4582, 365, 472, 295, 264, 2132, 7708, 300,
  390, 322, 264, 3380, 1469, 13, 20812, 50748], "temperature": 0.0, "avg_logprob":
  -0.25054440778844494, "compression_ratio": 1.4615384615384615, "no_speech_prob":
  0.0027968345675617456}, {"id": 45, "seek": 31768, "start": 325.36, "end": 330.64,
  "text": " Ziegler, we wrote the book, Prompt Engineering for Elements. It''s about
  building the Elements", "tokens": [50748, 1176, 20408, 1918, 11, 321, 4114, 264,
  1446, 11, 15833, 662, 16215, 337, 8024, 1117, 13, 467, 311, 466, 2390, 264, 8024,
  1117, 51012], "temperature": 0.0, "avg_logprob": -0.25054440778844494, "compression_ratio":
  1.4615384615384615, "no_speech_prob": 0.0027968345675617456}, {"id": 46, "seek":
  31768, "start": 330.64, "end": 339.92, "text": " applications. And with that, just
  published two weeks ago, officially published, I have started", "tokens": [51012,
  5821, 13, 400, 365, 300, 11, 445, 6572, 732, 3259, 2057, 11, 12053, 6572, 11, 286,
  362, 1409, 51476], "temperature": 0.0, "avg_logprob": -0.25054440778844494, "compression_ratio":
  1.4615384615384615, "no_speech_prob": 0.0027968345675617456}, {"id": 47, "seek":
  33992, "start": 340.0, "end": 347.68, "text": " out on a new adventure. Yet again,
  I am running Arturus Labs. I''m an indie consultant. And I''m", "tokens": [50368,
  484, 322, 257, 777, 9868, 13, 10890, 797, 11, 286, 669, 2614, 5735, 374, 301, 40047,
  13, 286, 478, 364, 33184, 24676, 13, 400, 286, 478, 50752], "temperature": 0.0,
  "avg_logprob": -0.22500356655676387, "compression_ratio": 1.5378486055776892, "no_speech_prob":
  0.03571538254618645}, {"id": 48, "seek": 33992, "start": 347.68, "end": 354.48,
  "text": " focusing on everything, large language models, Prompt Engineering, how
  to build applications, you", "tokens": [50752, 8416, 322, 1203, 11, 2416, 2856,
  5245, 11, 15833, 662, 16215, 11, 577, 281, 1322, 5821, 11, 291, 51092], "temperature":
  0.0, "avg_logprob": -0.22500356655676387, "compression_ratio": 1.5378486055776892,
  "no_speech_prob": 0.03571538254618645}, {"id": 49, "seek": 33992, "start": 354.48,
  "end": 360.32, "text": " know, it''s feasibility, evaluations, stuff like that.
  Kind of anything you want at this point. And", "tokens": [51092, 458, 11, 309, 311,
  21781, 2841, 11, 43085, 11, 1507, 411, 300, 13, 9242, 295, 1340, 291, 528, 412,
  341, 935, 13, 400, 51384], "temperature": 0.0, "avg_logprob": -0.22500356655676387,
  "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.03571538254618645},
  {"id": 50, "seek": 33992, "start": 360.32, "end": 367.84000000000003, "text": "
  it''s a blast. Oh, well, fantastic journey. Yeah, thanks for sharing that. It''s
  very, you know,", "tokens": [51384, 309, 311, 257, 12035, 13, 876, 11, 731, 11,
  5456, 4671, 13, 865, 11, 3231, 337, 5414, 300, 13, 467, 311, 588, 11, 291, 458,
  11, 51760], "temperature": 0.0, "avg_logprob": -0.22500356655676387, "compression_ratio":
  1.5378486055776892, "no_speech_prob": 0.03571538254618645}, {"id": 51, "seek": 36784,
  "start": 368.79999999999995, "end": 375.11999999999995, "text": " it says a lot
  there. You will believe it or not. But I actually advertised your recent books,",
  "tokens": [50412, 309, 1619, 257, 688, 456, 13, 509, 486, 1697, 309, 420, 406, 13,
  583, 286, 767, 42310, 428, 5162, 3642, 11, 50728], "temperature": 0.0, "avg_logprob":
  -0.20105866711549084, "compression_ratio": 1.5826446280991735, "no_speech_prob":
  0.03132854402065277}, {"id": 52, "seek": 36784, "start": 375.11999999999995, "end":
  382.08, "text": " the Prompt Engineering, to my students on the recent course that
  we caught up with my former", "tokens": [50728, 264, 15833, 662, 16215, 11, 281,
  452, 1731, 322, 264, 5162, 1164, 300, 321, 5415, 493, 365, 452, 5819, 51076], "temperature":
  0.0, "avg_logprob": -0.20105866711549084, "compression_ratio": 1.5826446280991735,
  "no_speech_prob": 0.03132854402065277}, {"id": 53, "seek": 36784, "start": 382.79999999999995,
  "end": 389.52, "text": " colleagues on LLMs and Generative AI. So I took the chapter
  on the rag. And I thought that rag", "tokens": [51112, 7734, 322, 441, 43, 26386,
  293, 15409, 1166, 7318, 13, 407, 286, 1890, 264, 7187, 322, 264, 17539, 13, 400,
  286, 1194, 300, 17539, 51448], "temperature": 0.0, "avg_logprob": -0.20105866711549084,
  "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.03132854402065277},
  {"id": 54, "seek": 36784, "start": 389.52, "end": 396.4, "text": " is nothing else
  than Prompt Engineering, really. Well, yeah, it''s interesting. I mean, that''s
  a topic", "tokens": [51448, 307, 1825, 1646, 813, 15833, 662, 16215, 11, 534, 13,
  1042, 11, 1338, 11, 309, 311, 1880, 13, 286, 914, 11, 300, 311, 257, 4829, 51792],
  "temperature": 0.0, "avg_logprob": -0.20105866711549084, "compression_ratio": 1.5826446280991735,
  "no_speech_prob": 0.03132854402065277}, {"id": 55, "seek": 39640, "start": 396.47999999999996,
  "end": 404.96, "text": " in and of itself. Are we going to open that kind of worms?
  Of course. Sure. Yeah, rag is an interesting", "tokens": [50368, 294, 293, 295,
  2564, 13, 2014, 321, 516, 281, 1269, 300, 733, 295, 28271, 30, 2720, 1164, 13, 4894,
  13, 865, 11, 17539, 307, 364, 1880, 50792], "temperature": 0.0, "avg_logprob": -0.19188976287841797,
  "compression_ratio": 1.672340425531915, "no_speech_prob": 0.010984758846461773},
  {"id": 56, "seek": 39640, "start": 404.96, "end": 412.71999999999997, "text": "
  thing because everyone talks about rag as if it''s own entity, that it''s a special
  thing. But if", "tokens": [50792, 551, 570, 1518, 6686, 466, 17539, 382, 498, 309,
  311, 1065, 13977, 11, 300, 309, 311, 257, 2121, 551, 13, 583, 498, 51180], "temperature":
  0.0, "avg_logprob": -0.19188976287841797, "compression_ratio": 1.672340425531915,
  "no_speech_prob": 0.010984758846461773}, {"id": 57, "seek": 39640, "start": 412.71999999999997,
  "end": 417.2, "text": " you like look at it, especially from my background, which
  has been searched and then large language", "tokens": [51180, 291, 411, 574, 412,
  309, 11, 2318, 490, 452, 3678, 11, 597, 575, 668, 22961, 293, 550, 2416, 2856, 51404],
  "temperature": 0.0, "avg_logprob": -0.19188976287841797, "compression_ratio": 1.672340425531915,
  "no_speech_prob": 0.010984758846461773}, {"id": 58, "seek": 39640, "start": 417.2,
  "end": 423.84, "text": " models, you click look at rag and it is search and then
  the large language models. And if you", "tokens": [51404, 5245, 11, 291, 2052, 574,
  412, 17539, 293, 309, 307, 3164, 293, 550, 264, 2416, 2856, 5245, 13, 400, 498,
  291, 51736], "temperature": 0.0, "avg_logprob": -0.19188976287841797, "compression_ratio":
  1.672340425531915, "no_speech_prob": 0.010984758846461773}, {"id": 59, "seek": 42384,
  "start": 423.84, "end": 429.44, "text": " combine them both, then it''s really hard
  to get a good understanding of what''s working and what''s", "tokens": [50364, 10432,
  552, 1293, 11, 550, 309, 311, 534, 1152, 281, 483, 257, 665, 3701, 295, 437, 311,
  1364, 293, 437, 311, 50644], "temperature": 0.0, "avg_logprob": -0.13690835686140163,
  "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0008702923078089952},
  {"id": 60, "seek": 42384, "start": 429.44, "end": 436.23999999999995, "text": "
  not working. You just, you know, you throw up the basic chain application, connect
  the data source.", "tokens": [50644, 406, 1364, 13, 509, 445, 11, 291, 458, 11,
  291, 3507, 493, 264, 3875, 5021, 3861, 11, 1745, 264, 1412, 4009, 13, 50984], "temperature":
  0.0, "avg_logprob": -0.13690835686140163, "compression_ratio": 1.6176470588235294,
  "no_speech_prob": 0.0008702923078089952}, {"id": 61, "seek": 42384, "start": 436.79999999999995,
  "end": 443.2, "text": " And I guess you just pray that it works. But really, what
  it breaks, if you break it down to its", "tokens": [51012, 400, 286, 2041, 291,
  445, 3690, 300, 309, 1985, 13, 583, 534, 11, 437, 309, 9857, 11, 498, 291, 1821,
  309, 760, 281, 1080, 51332], "temperature": 0.0, "avg_logprob": -0.13690835686140163,
  "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0008702923078089952},
  {"id": 62, "seek": 42384, "start": 443.84, "end": 449.91999999999996, "text": "
  components, then you''ve got a search application and the Prompt Engineering large
  language", "tokens": [51364, 6677, 11, 550, 291, 600, 658, 257, 3164, 3861, 293,
  264, 15833, 662, 16215, 2416, 2856, 51668], "temperature": 0.0, "avg_logprob": -0.13690835686140163,
  "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.0008702923078089952},
  {"id": 63, "seek": 44992, "start": 449.92, "end": 456.8, "text": " application that
  it overlaps. But a lot of it''s kind of downstream. And if you can look at those",
  "tokens": [50364, 3861, 300, 309, 15986, 2382, 13, 583, 257, 688, 295, 309, 311,
  733, 295, 30621, 13, 400, 498, 291, 393, 574, 412, 729, 50708], "temperature": 0.0,
  "avg_logprob": -0.17091063795418576, "compression_ratio": 1.791044776119403, "no_speech_prob":
  0.00626370171085}, {"id": 64, "seek": 44992, "start": 456.8, "end": 463.20000000000005,
  "text": " two chunks separately, it becomes a lot easier to debug problems. Rather
  than saying, you know,", "tokens": [50708, 732, 24004, 14759, 11, 309, 3643, 257,
  688, 3571, 281, 24083, 2740, 13, 16571, 813, 1566, 11, 291, 458, 11, 51028], "temperature":
  0.0, "avg_logprob": -0.17091063795418576, "compression_ratio": 1.791044776119403,
  "no_speech_prob": 0.00626370171085}, {"id": 65, "seek": 44992, "start": 463.20000000000005,
  "end": 467.52000000000004, "text": " user asks this question, I got a garbage answer.
  You can say the user asks this question, the", "tokens": [51028, 4195, 8962, 341,
  1168, 11, 286, 658, 257, 14150, 1867, 13, 509, 393, 584, 264, 4195, 8962, 341, 1168,
  11, 264, 51244], "temperature": 0.0, "avg_logprob": -0.17091063795418576, "compression_ratio":
  1.791044776119403, "no_speech_prob": 0.00626370171085}, {"id": 66, "seek": 44992,
  "start": 467.52000000000004, "end": 473.84000000000003, "text": " large language
  model interpreted it as this search, this search, return these results. And maybe",
  "tokens": [51244, 2416, 2856, 2316, 26749, 309, 382, 341, 3164, 11, 341, 3164, 11,
  2736, 613, 3542, 13, 400, 1310, 51560], "temperature": 0.0, "avg_logprob": -0.17091063795418576,
  "compression_ratio": 1.791044776119403, "no_speech_prob": 0.00626370171085}, {"id":
  67, "seek": 44992, "start": 473.84000000000003, "end": 477.84000000000003, "text":
  " that''s the, maybe that''s where the problem was. And you can start debugging
  that. And the search", "tokens": [51560, 300, 311, 264, 11, 1310, 300, 311, 689,
  264, 1154, 390, 13, 400, 291, 393, 722, 45592, 300, 13, 400, 264, 3164, 51760],
  "temperature": 0.0, "avg_logprob": -0.17091063795418576, "compression_ratio": 1.791044776119403,
  "no_speech_prob": 0.00626370171085}, {"id": 68, "seek": 47784, "start": 477.84,
  "end": 482.32, "text": " results got interpreted this way. And maybe you''re not
  presenting it right to the model.", "tokens": [50364, 3542, 658, 26749, 341, 636,
  13, 400, 1310, 291, 434, 406, 15578, 309, 558, 281, 264, 2316, 13, 50588], "temperature":
  0.0, "avg_logprob": -0.13551448324452275, "compression_ratio": 1.711191335740072,
  "no_speech_prob": 0.01667843759059906}, {"id": 69, "seek": 47784, "start": 483.03999999999996,
  "end": 487.76, "text": " So always the name of the game with it''s probably everything
  we''re going to talk about today is,", "tokens": [50624, 407, 1009, 264, 1315, 295,
  264, 1216, 365, 309, 311, 1391, 1203, 321, 434, 516, 281, 751, 466, 965, 307, 11,
  50860], "temperature": 0.0, "avg_logprob": -0.13551448324452275, "compression_ratio":
  1.711191335740072, "no_speech_prob": 0.01667843759059906}, {"id": 70, "seek": 47784,
  "start": 487.76, "end": 492.71999999999997, "text": " you know, figured out how
  to take this giant black box and break it down into components and figure", "tokens":
  [50860, 291, 458, 11, 8932, 484, 577, 281, 747, 341, 7410, 2211, 2424, 293, 1821,
  309, 760, 666, 6677, 293, 2573, 51108], "temperature": 0.0, "avg_logprob": -0.13551448324452275,
  "compression_ratio": 1.711191335740072, "no_speech_prob": 0.01667843759059906},
  {"id": 71, "seek": 47784, "start": 492.71999999999997, "end": 498.71999999999997,
  "text": " out what is, you know, what''s it made of and what possibly is going wrong
  and put sensors there", "tokens": [51108, 484, 437, 307, 11, 291, 458, 11, 437,
  311, 309, 1027, 295, 293, 437, 6264, 307, 516, 2085, 293, 829, 14840, 456, 51408],
  "temperature": 0.0, "avg_logprob": -0.13551448324452275, "compression_ratio": 1.711191335740072,
  "no_speech_prob": 0.01667843759059906}, {"id": 72, "seek": 47784, "start": 498.71999999999997,
  "end": 504.47999999999996, "text": " and actually debugger. Yeah, you''re absolutely
  right. And in the, in the lecture, I actually", "tokens": [51408, 293, 767, 24083,
  1321, 13, 865, 11, 291, 434, 3122, 558, 13, 400, 294, 264, 11, 294, 264, 7991, 11,
  286, 767, 51696], "temperature": 0.0, "avg_logprob": -0.13551448324452275, "compression_ratio":
  1.711191335740072, "no_speech_prob": 0.01667843759059906}, {"id": 73, "seek": 50448,
  "start": 504.56, "end": 511.12, "text": " longed code from someone, I forgot their
  name, but I''ll make sure to link it. We''ve built a rag", "tokens": [50368, 938,
  292, 3089, 490, 1580, 11, 286, 5298, 641, 1315, 11, 457, 286, 603, 652, 988, 281,
  2113, 309, 13, 492, 600, 3094, 257, 17539, 50696], "temperature": 0.0, "avg_logprob":
  -0.25309247877991314, "compression_ratio": 1.517509727626459, "no_speech_prob":
  0.004667668137699366}, {"id": 74, "seek": 50448, "start": 511.12, "end": 516.08,
  "text": " ground up without using any framework whatsoever. You didn''t mention
  Langchain, that''s one way of", "tokens": [50696, 2727, 493, 1553, 1228, 604, 8388,
  17076, 13, 509, 994, 380, 2152, 13313, 339, 491, 11, 300, 311, 472, 636, 295, 50944],
  "temperature": 0.0, "avg_logprob": -0.25309247877991314, "compression_ratio": 1.517509727626459,
  "no_speech_prob": 0.004667668137699366}, {"id": 75, "seek": 50448, "start": 516.08,
  "end": 524.5600000000001, "text": " doing it for sure. But we just really built,
  you know, naive, can and search and just use the model", "tokens": [50944, 884,
  309, 337, 988, 13, 583, 321, 445, 534, 3094, 11, 291, 458, 11, 29052, 11, 393, 293,
  3164, 293, 445, 764, 264, 2316, 51368], "temperature": 0.0, "avg_logprob": -0.25309247877991314,
  "compression_ratio": 1.517509727626459, "no_speech_prob": 0.004667668137699366},
  {"id": 76, "seek": 50448, "start": 524.5600000000001, "end": 531.2, "text": " out
  of the box, sentence, bird. And then I''ve noticed that because we did use dot product
  there,", "tokens": [51368, 484, 295, 264, 2424, 11, 8174, 11, 5255, 13, 400, 550,
  286, 600, 5694, 300, 570, 321, 630, 764, 5893, 1674, 456, 11, 51700], "temperature":
  0.0, "avg_logprob": -0.25309247877991314, "compression_ratio": 1.517509727626459,
  "no_speech_prob": 0.004667668137699366}, {"id": 77, "seek": 53120, "start": 531.2,
  "end": 537.76, "text": " I''ve noticed that it would favor longer passages over
  shorter ones, right? For example, it would", "tokens": [50364, 286, 600, 5694, 300,
  309, 576, 2294, 2854, 31589, 670, 11639, 2306, 11, 558, 30, 1171, 1365, 11, 309,
  576, 50692], "temperature": 0.0, "avg_logprob": -0.12641617624383222, "compression_ratio":
  1.6092436974789917, "no_speech_prob": 0.003477048361673951}, {"id": 78, "seek":
  53120, "start": 537.76, "end": 547.76, "text": " pull up an appendix of a AI powered
  book, AI powered search book. And I was like, like, you could", "tokens": [50692,
  2235, 493, 364, 34116, 970, 295, 257, 7318, 17786, 1446, 11, 7318, 17786, 3164,
  1446, 13, 400, 286, 390, 411, 11, 411, 11, 291, 727, 51192], "temperature": 0.0,
  "avg_logprob": -0.12641617624383222, "compression_ratio": 1.6092436974789917, "no_speech_prob":
  0.003477048361673951}, {"id": 79, "seek": 53120, "start": 547.76, "end": 553.6,
  "text": " clearly see that it''s missing the point. It''s not able to pull up one
  short sentence where the", "tokens": [51192, 4448, 536, 300, 309, 311, 5361, 264,
  935, 13, 467, 311, 406, 1075, 281, 2235, 493, 472, 2099, 8174, 689, 264, 51484],
  "temperature": 0.0, "avg_logprob": -0.12641617624383222, "compression_ratio": 1.6092436974789917,
  "no_speech_prob": 0.003477048361673951}, {"id": 80, "seek": 53120, "start": 553.6,
  "end": 559.6800000000001, "text": " answer lies. It just pulls something else remotely
  related. And that''s exactly what you said,", "tokens": [51484, 1867, 9134, 13,
  467, 445, 16982, 746, 1646, 20824, 4077, 13, 400, 300, 311, 2293, 437, 291, 848,
  11, 51788], "temperature": 0.0, "avg_logprob": -0.12641617624383222, "compression_ratio":
  1.6092436974789917, "no_speech_prob": 0.003477048361673951}, {"id": 81, "seek":
  55968, "start": 559.68, "end": 564.0799999999999, "text": " right? Like you need
  to start debugging what''s going on there. And you need to start fixing on", "tokens":
  [50364, 558, 30, 1743, 291, 643, 281, 722, 45592, 437, 311, 516, 322, 456, 13, 400,
  291, 643, 281, 722, 19442, 322, 50584], "temperature": 0.0, "avg_logprob": -0.13390055156889416,
  "compression_ratio": 1.7830882352941178, "no_speech_prob": 0.025069870054721832},
  {"id": 82, "seek": 55968, "start": 564.0799999999999, "end": 570.7199999999999,
  "text": " figuring out maybe change the model, maybe change the chunking. But yeah,
  I agree. It felt a bit like", "tokens": [50584, 15213, 484, 1310, 1319, 264, 2316,
  11, 1310, 1319, 264, 16635, 278, 13, 583, 1338, 11, 286, 3986, 13, 467, 2762, 257,
  857, 411, 50916], "temperature": 0.0, "avg_logprob": -0.13390055156889416, "compression_ratio":
  1.7830882352941178, "no_speech_prob": 0.025069870054721832}, {"id": 83, "seek":
  55968, "start": 570.7199999999999, "end": 576.4, "text": " black box, but less so
  when you implement it ground up, right? So you don''t depend on any framework.",
  "tokens": [50916, 2211, 2424, 11, 457, 1570, 370, 562, 291, 4445, 309, 2727, 493,
  11, 558, 30, 407, 291, 500, 380, 5672, 322, 604, 8388, 13, 51200], "temperature":
  0.0, "avg_logprob": -0.13390055156889416, "compression_ratio": 1.7830882352941178,
  "no_speech_prob": 0.025069870054721832}, {"id": 84, "seek": 55968, "start": 578.0799999999999,
  "end": 582.88, "text": " And when you implement it ground up, you find out that
  it''s not all of that complicated. And", "tokens": [51284, 400, 562, 291, 4445,
  309, 2727, 493, 11, 291, 915, 484, 300, 309, 311, 406, 439, 295, 300, 6179, 13,
  400, 51524], "temperature": 0.0, "avg_logprob": -0.13390055156889416, "compression_ratio":
  1.7830882352941178, "no_speech_prob": 0.025069870054721832}, {"id": 85, "seek":
  55968, "start": 582.88, "end": 587.04, "text": " once you''ve built every piece
  of it, like, you know, I mean, you''ve already seen the black box", "tokens": [51524,
  1564, 291, 600, 3094, 633, 2522, 295, 309, 11, 411, 11, 291, 458, 11, 286, 914,
  11, 291, 600, 1217, 1612, 264, 2211, 2424, 51732], "temperature": 0.0, "avg_logprob":
  -0.13390055156889416, "compression_ratio": 1.7830882352941178, "no_speech_prob":
  0.025069870054721832}, {"id": 86, "seek": 58704, "start": 587.04, "end": 591.04,
  "text": " broken down to its sub pieces. It''s not a black box anymore. So yeah,
  that''s", "tokens": [50364, 5463, 760, 281, 1080, 1422, 3755, 13, 467, 311, 406,
  257, 2211, 2424, 3602, 13, 407, 1338, 11, 300, 311, 50564], "temperature": 0.0,
  "avg_logprob": -0.12920911095359108, "compression_ratio": 1.650735294117647, "no_speech_prob":
  0.007051675580441952}, {"id": 87, "seek": 58704, "start": 592.0799999999999, "end":
  596.8, "text": " typically since the whole industry now is sorting itself, trying
  to figure out what tools are", "tokens": [50616, 5850, 1670, 264, 1379, 3518, 586,
  307, 32411, 2564, 11, 1382, 281, 2573, 484, 437, 3873, 366, 50852], "temperature":
  0.0, "avg_logprob": -0.12920911095359108, "compression_ratio": 1.650735294117647,
  "no_speech_prob": 0.007051675580441952}, {"id": 88, "seek": 58704, "start": 596.8,
  "end": 603.5999999999999, "text": " useful and what tools are not going to be useful,
  I often advocate that people start as close to", "tokens": [50852, 4420, 293, 437,
  3873, 366, 406, 516, 281, 312, 4420, 11, 286, 2049, 14608, 300, 561, 722, 382, 1998,
  281, 51192], "temperature": 0.0, "avg_logprob": -0.12920911095359108, "compression_ratio":
  1.650735294117647, "no_speech_prob": 0.007051675580441952}, {"id": 89, "seek": 58704,
  "start": 603.5999999999999, "end": 607.8399999999999, "text": " the metal as possible.
  Because these models are actually pretty friendly, pretty fun to play with.", "tokens":
  [51192, 264, 5760, 382, 1944, 13, 1436, 613, 5245, 366, 767, 1238, 9208, 11, 1238,
  1019, 281, 862, 365, 13, 51404], "temperature": 0.0, "avg_logprob": -0.12920911095359108,
  "compression_ratio": 1.650735294117647, "no_speech_prob": 0.007051675580441952},
  {"id": 90, "seek": 58704, "start": 607.8399999999999, "end": 612.64, "text": " Don''t
  put layers on top of it that obfuscate, you know, what''s actually happening.",
  "tokens": [51404, 1468, 380, 829, 7914, 322, 1192, 295, 309, 300, 1111, 69, 32601,
  473, 11, 291, 458, 11, 437, 311, 767, 2737, 13, 51644], "temperature": 0.0, "avg_logprob":
  -0.12920911095359108, "compression_ratio": 1.650735294117647, "no_speech_prob":
  0.007051675580441952}, {"id": 91, "seek": 61264, "start": 613.4399999999999, "end":
  620.56, "text": " Yeah, absolutely. I''m really itching to ask you more about now,
  like your time at GitHub.", "tokens": [50404, 865, 11, 3122, 13, 286, 478, 534,
  309, 17354, 281, 1029, 291, 544, 466, 586, 11, 411, 428, 565, 412, 23331, 13, 50760],
  "temperature": 0.0, "avg_logprob": -0.20421616784457503, "compression_ratio": 1.6459143968871595,
  "no_speech_prob": 0.05906231328845024}, {"id": 92, "seek": 61264, "start": 620.56,
  "end": 624.64, "text": " But before that, I also want to like a little bit take
  your,", "tokens": [50760, 583, 949, 300, 11, 286, 611, 528, 281, 411, 257, 707,
  857, 747, 428, 11, 50964], "temperature": 0.0, "avg_logprob": -0.20421616784457503,
  "compression_ratio": 1.6459143968871595, "no_speech_prob": 0.05906231328845024},
  {"id": 93, "seek": 61264, "start": 626.16, "end": 630.56, "text": " you know, take
  a look at your approach, how you view your career, right? So you look,", "tokens":
  [51040, 291, 458, 11, 747, 257, 574, 412, 428, 3109, 11, 577, 291, 1910, 428, 3988,
  11, 558, 30, 407, 291, 574, 11, 51260], "temperature": 0.0, "avg_logprob": -0.20421616784457503,
  "compression_ratio": 1.6459143968871595, "no_speech_prob": 0.05906231328845024},
  {"id": 94, "seek": 61264, "start": 630.56, "end": 636.08, "text": " you worked on
  search, but then you ended up in the hottest place in the way, applying", "tokens":
  [51260, 291, 2732, 322, 3164, 11, 457, 550, 291, 4590, 493, 294, 264, 32780, 1081,
  294, 264, 636, 11, 9275, 51536], "temperature": 0.0, "avg_logprob": -0.20421616784457503,
  "compression_ratio": 1.6459143968871595, "no_speech_prob": 0.05906231328845024},
  {"id": 95, "seek": 61264, "start": 636.08, "end": 641.2, "text": " all the lamps,
  right? And you needed to convert in a way to an ML engineer. Do you view it that
  way?", "tokens": [51536, 439, 264, 34887, 11, 558, 30, 400, 291, 2978, 281, 7620,
  294, 257, 636, 281, 364, 21601, 11403, 13, 1144, 291, 1910, 309, 300, 636, 30, 51792],
  "temperature": 0.0, "avg_logprob": -0.20421616784457503, "compression_ratio": 1.6459143968871595,
  "no_speech_prob": 0.05906231328845024}, {"id": 96, "seek": 64120, "start": 641.2,
  "end": 647.84, "text": " And also if you do, how did you prepare yourself to become
  a machine learning researcher,", "tokens": [50364, 400, 611, 498, 291, 360, 11,
  577, 630, 291, 5940, 1803, 281, 1813, 257, 3479, 2539, 21751, 11, 50696], "temperature":
  0.0, "avg_logprob": -0.16394857830471463, "compression_ratio": 1.5546218487394958,
  "no_speech_prob": 0.009676489047706127}, {"id": 97, "seek": 64120, "start": 647.84,
  "end": 653.6, "text": " actually not even an engineer, right? You are focusing on
  research aspects of things. So you", "tokens": [50696, 767, 406, 754, 364, 11403,
  11, 558, 30, 509, 366, 8416, 322, 2132, 7270, 295, 721, 13, 407, 291, 50984], "temperature":
  0.0, "avg_logprob": -0.16394857830471463, "compression_ratio": 1.5546218487394958,
  "no_speech_prob": 0.009676489047706127}, {"id": 98, "seek": 64120, "start": 653.6,
  "end": 660.96, "text": " needed to move the needle in the research space. I don''t
  know if I have a good answer for you.", "tokens": [50984, 2978, 281, 1286, 264,
  11037, 294, 264, 2132, 1901, 13, 286, 500, 380, 458, 498, 286, 362, 257, 665, 1867,
  337, 291, 13, 51352], "temperature": 0.0, "avg_logprob": -0.16394857830471463, "compression_ratio":
  1.5546218487394958, "no_speech_prob": 0.009676489047706127}, {"id": 99, "seek":
  64120, "start": 660.96, "end": 665.76, "text": " Like if anyone thinks my career
  has been successful, which in many ways I''ve done all right,", "tokens": [51352,
  1743, 498, 2878, 7309, 452, 3988, 575, 668, 4406, 11, 597, 294, 867, 2098, 286,
  600, 1096, 439, 558, 11, 51592], "temperature": 0.0, "avg_logprob": -0.16394857830471463,
  "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.009676489047706127},
  {"id": 100, "seek": 66576, "start": 666.72, "end": 672.0, "text": " it''s been luckily
  like tripping and falling uphill. Every time I fall down, it''s like in the uphill",
  "tokens": [50412, 309, 311, 668, 22880, 411, 1376, 3759, 293, 7440, 39132, 13, 2048,
  565, 286, 2100, 760, 11, 309, 311, 411, 294, 264, 39132, 50676], "temperature":
  0.0, "avg_logprob": -0.18522885867527553, "compression_ratio": 1.5101010101010102,
  "no_speech_prob": 0.002982220146805048}, {"id": 101, "seek": 66576, "start": 672.0,
  "end": 679.92, "text": " direction. And I don''t, I''m the hand of Providence. And
  so what do I do with any of these crazy jumps", "tokens": [50676, 3513, 13, 400,
  286, 500, 380, 11, 286, 478, 264, 1011, 295, 15685, 2778, 13, 400, 370, 437, 360,
  286, 360, 365, 604, 295, 613, 3219, 16704, 51072], "temperature": 0.0, "avg_logprob":
  -0.18522885867527553, "compression_ratio": 1.5101010101010102, "no_speech_prob":
  0.002982220146805048}, {"id": 102, "seek": 66576, "start": 679.92, "end": 688.88,
  "text": " that I make to prepare? Pretty much, I just take the jump. I think I''m
  going to say how I''m going", "tokens": [51072, 300, 286, 652, 281, 5940, 30, 10693,
  709, 11, 286, 445, 747, 264, 3012, 13, 286, 519, 286, 478, 516, 281, 584, 577, 286,
  478, 516, 51520], "temperature": 0.0, "avg_logprob": -0.18522885867527553, "compression_ratio":
  1.5101010101010102, "no_speech_prob": 0.002982220146805048}, {"id": 103, "seek":
  68888, "start": 689.28, "end": 696.4, "text": " to prepare for the next jump. I
  take, I see the jump. And then I jump into it and like almost", "tokens": [50384,
  281, 5940, 337, 264, 958, 3012, 13, 286, 747, 11, 286, 536, 264, 3012, 13, 400,
  550, 286, 3012, 666, 309, 293, 411, 1920, 50740], "temperature": 0.0, "avg_logprob":
  -0.2002301279703776, "compression_ratio": 1.5555555555555556, "no_speech_prob":
  0.000904655666090548}, {"id": 104, "seek": 68888, "start": 696.4, "end": 706.8,
  "text": " drown every single time by surviving. So in this particular case, yeah,
  the move towards AI", "tokens": [50740, 20337, 633, 2167, 565, 538, 24948, 13, 407,
  294, 341, 1729, 1389, 11, 1338, 11, 264, 1286, 3030, 7318, 51260], "temperature":
  0.0, "avg_logprob": -0.2002301279703776, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.000904655666090548}, {"id": 105, "seek": 68888, "start": 707.76,
  "end": 715.12, "text": " researcher, I mean, there''s a lot in that, there''s a
  lot of weight in that phrase that maybe I", "tokens": [51308, 21751, 11, 286, 914,
  11, 456, 311, 257, 688, 294, 300, 11, 456, 311, 257, 688, 295, 3364, 294, 300, 9535,
  300, 1310, 286, 51676], "temperature": 0.0, "avg_logprob": -0.2002301279703776,
  "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.000904655666090548},
  {"id": 106, "seek": 71512, "start": 715.12, "end": 721.2, "text": " don''t necessarily
  feel in my own career. By beginning search for so long and by wanting to do", "tokens":
  [50364, 500, 380, 4725, 841, 294, 452, 1065, 3988, 13, 3146, 2863, 3164, 337, 370,
  938, 293, 538, 7935, 281, 360, 50668], "temperature": 0.0, "avg_logprob": -0.13878718289462003,
  "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0011525267036631703},
  {"id": 107, "seek": 71512, "start": 721.2, "end": 727.36, "text": " data science
  for so long, I made myself, you know, over time, pretty aware of how things were,",
  "tokens": [50668, 1412, 3497, 337, 370, 938, 11, 286, 1027, 2059, 11, 291, 458,
  11, 670, 565, 11, 1238, 3650, 295, 577, 721, 645, 11, 50976], "temperature": 0.0,
  "avg_logprob": -0.13878718289462003, "compression_ratio": 1.7205882352941178, "no_speech_prob":
  0.0011525267036631703}, {"id": 108, "seek": 71512, "start": 727.36, "end": 732.48,
  "text": " you know, just the typical approach to the model. So I was never caught
  any of this in school,", "tokens": [50976, 291, 458, 11, 445, 264, 7476, 3109, 281,
  264, 2316, 13, 407, 286, 390, 1128, 5415, 604, 295, 341, 294, 1395, 11, 51232],
  "temperature": 0.0, "avg_logprob": -0.13878718289462003, "compression_ratio": 1.7205882352941178,
  "no_speech_prob": 0.0011525267036631703}, {"id": 109, "seek": 71512, "start": 732.48,
  "end": 736.88, "text": " but you know, you read, you read the right books and you
  know, go through the right examples.", "tokens": [51232, 457, 291, 458, 11, 291,
  1401, 11, 291, 1401, 264, 558, 3642, 293, 291, 458, 11, 352, 807, 264, 558, 5110,
  13, 51452], "temperature": 0.0, "avg_logprob": -0.13878718289462003, "compression_ratio":
  1.7205882352941178, "no_speech_prob": 0.0011525267036631703}, {"id": 110, "seek":
  71512, "start": 738.0, "end": 743.6, "text": " Yeah, so I have gained, I wouldn''t
  say just an absolute comfort with any of this even now.", "tokens": [51508, 865,
  11, 370, 286, 362, 12634, 11, 286, 2759, 380, 584, 445, 364, 8236, 3400, 365, 604,
  295, 341, 754, 586, 13, 51788], "temperature": 0.0, "avg_logprob": -0.13878718289462003,
  "compression_ratio": 1.7205882352941178, "no_speech_prob": 0.0011525267036631703},
  {"id": 111, "seek": 74360, "start": 744.24, "end": 749.36, "text": " But you know,
  familiarity of being around it for periods of this point. And then when I jumped",
  "tokens": [50396, 583, 291, 458, 11, 49828, 295, 885, 926, 309, 337, 13804, 295,
  341, 935, 13, 400, 550, 562, 286, 13864, 50652], "temperature": 0.0, "avg_logprob":
  -0.1570124894045712, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0022936412133276463},
  {"id": 112, "seek": 74360, "start": 749.36, "end": 756.96, "text": " into the large
  language modeling stuff, it''s actually kind of interesting because it''s a different
  type", "tokens": [50652, 666, 264, 2416, 2856, 15983, 1507, 11, 309, 311, 767, 733,
  295, 1880, 570, 309, 311, 257, 819, 2010, 51032], "temperature": 0.0, "avg_logprob":
  -0.1570124894045712, "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0022936412133276463},
  {"id": 113, "seek": 74360, "start": 756.96, "end": 763.2, "text": " of AI expert
  than we''ve had before and maybe an easier entrance for a lot of people.", "tokens":
  [51032, 295, 7318, 5844, 813, 321, 600, 632, 949, 293, 1310, 364, 3571, 12014, 337,
  257, 688, 295, 561, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1570124894045712,
  "compression_ratio": 1.548780487804878, "no_speech_prob": 0.0022936412133276463},
  {"id": 114, "seek": 74360, "start": 763.84, "end": 769.9200000000001, "text": "
  Much of my career, I have been an engineer and really I still, I predominantly think
  of myself as", "tokens": [51376, 12313, 295, 452, 3988, 11, 286, 362, 668, 364,
  11403, 293, 534, 286, 920, 11, 286, 29893, 519, 295, 2059, 382, 51680], "temperature":
  0.0, "avg_logprob": -0.1570124894045712, "compression_ratio": 1.548780487804878,
  "no_speech_prob": 0.0022936412133276463}, {"id": 115, "seek": 76992, "start": 769.92,
  "end": 776.56, "text": " an engineering mindset. And so when you come into, you
  know, large language models, it''s actually", "tokens": [50364, 364, 7043, 12543,
  13, 400, 370, 562, 291, 808, 666, 11, 291, 458, 11, 2416, 2856, 5245, 11, 309, 311,
  767, 50696], "temperature": 0.0, "avg_logprob": -0.15691834307731467, "compression_ratio":
  1.6440677966101696, "no_speech_prob": 0.0017042603576555848}, {"id": 116, "seek":
  76992, "start": 776.56, "end": 785.12, "text": " really approachable. You don''t
  have to immediately know everything about, you know, what choice of", "tokens":
  [50696, 534, 3109, 712, 13, 509, 500, 380, 362, 281, 4258, 458, 1203, 466, 11, 291,
  458, 11, 437, 3922, 295, 51124], "temperature": 0.0, "avg_logprob": -0.15691834307731467,
  "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0017042603576555848},
  {"id": 117, "seek": 76992, "start": 785.12, "end": 790.0799999999999, "text": "
  models to use and like, you know, how to train and have the whole outside and evaluate.
  And", "tokens": [51124, 5245, 281, 764, 293, 411, 11, 291, 458, 11, 577, 281, 3847,
  293, 362, 264, 1379, 2380, 293, 13059, 13, 400, 51372], "temperature": 0.0, "avg_logprob":
  -0.15691834307731467, "compression_ratio": 1.6440677966101696, "no_speech_prob":
  0.0017042603576555848}, {"id": 118, "seek": 76992, "start": 791.92, "end": 798.7199999999999,
  "text": " you can just go to work and at first, at least, just experiment and I
  really encourage people to do", "tokens": [51464, 291, 393, 445, 352, 281, 589,
  293, 412, 700, 11, 412, 1935, 11, 445, 5120, 293, 286, 534, 5373, 561, 281, 360,
  51804], "temperature": 0.0, "avg_logprob": -0.15691834307731467, "compression_ratio":
  1.6440677966101696, "no_speech_prob": 0.0017042603576555848}, {"id": 119, "seek":
  79872, "start": 798.72, "end": 802.32, "text": " this when they''re building on,
  they''re on an application. Rather than, you know, thinking about all", "tokens":
  [50364, 341, 562, 436, 434, 2390, 322, 11, 436, 434, 322, 364, 3861, 13, 16571,
  813, 11, 291, 458, 11, 1953, 466, 439, 50544], "temperature": 0.0, "avg_logprob":
  -0.1820230975593488, "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0031977645121514797},
  {"id": 120, "seek": 79872, "start": 802.32, "end": 807.12, "text": " the evaluations
  and stuff at front, you''ll need, don''t worry, you''ll get to them. But just get
  your", "tokens": [50544, 264, 43085, 293, 1507, 412, 1868, 11, 291, 603, 643, 11,
  500, 380, 3292, 11, 291, 603, 483, 281, 552, 13, 583, 445, 483, 428, 50784], "temperature":
  0.0, "avg_logprob": -0.1820230975593488, "compression_ratio": 1.606694560669456,
  "no_speech_prob": 0.0031977645121514797}, {"id": 121, "seek": 79872, "start": 807.12,
  "end": 814.96, "text": " hands, hands dirty, start using the, the APIs and build
  up some intuition and a weird way", "tokens": [50784, 2377, 11, 2377, 9360, 11,
  722, 1228, 264, 11, 264, 21445, 293, 1322, 493, 512, 24002, 293, 257, 3657, 636,
  51176], "temperature": 0.0, "avg_logprob": -0.1820230975593488, "compression_ratio":
  1.606694560669456, "no_speech_prob": 0.0031977645121514797}, {"id": 122, "seek":
  79872, "start": 815.52, "end": 823.44, "text": " empathy for these large language
  models. Yeah, yeah, this is brilliantly said. I just recently", "tokens": [51204,
  18701, 337, 613, 2416, 2856, 5245, 13, 865, 11, 1338, 11, 341, 307, 8695, 42580,
  848, 13, 286, 445, 3938, 51600], "temperature": 0.0, "avg_logprob": -0.1820230975593488,
  "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0031977645121514797},
  {"id": 123, "seek": 82344, "start": 824.32, "end": 833.6, "text": " listened to
  the episode of Lex Friedman with the ontropic team. So the CEO and some of the",
  "tokens": [50408, 13207, 281, 264, 3500, 295, 24086, 17605, 1601, 365, 264, 6592,
  39173, 1469, 13, 407, 264, 9282, 293, 512, 295, 264, 50872], "temperature": 0.0,
  "avg_logprob": -0.19162458769032653, "compression_ratio": 1.598901098901099, "no_speech_prob":
  0.03295965492725372}, {"id": 124, "seek": 82344, "start": 833.6, "end": 841.6, "text":
  " researchers there. And one of them said, yeah, exactly. And one of them said that
  you, along the", "tokens": [50872, 10309, 456, 13, 400, 472, 295, 552, 848, 11,
  1338, 11, 2293, 13, 400, 472, 295, 552, 848, 300, 291, 11, 2051, 264, 51272], "temperature":
  0.0, "avg_logprob": -0.19162458769032653, "compression_ratio": 1.598901098901099,
  "no_speech_prob": 0.03295965492725372}, {"id": 125, "seek": 82344, "start": 841.6,
  "end": 846.4000000000001, "text": " lines of what you just said about empathy towards
  the model that when you know where model succeeds and", "tokens": [51272, 3876,
  295, 437, 291, 445, 848, 466, 18701, 3030, 264, 2316, 300, 562, 291, 458, 689, 2316,
  49263, 293, 51512], "temperature": 0.0, "avg_logprob": -0.19162458769032653, "compression_ratio":
  1.598901098901099, "no_speech_prob": 0.03295965492725372}, {"id": 126, "seek": 84640,
  "start": 846.4, "end": 852.16, "text": " where it kind of fails, you learn how to
  prompt it. Right. You know, like which risks you will", "tokens": [50364, 689, 309,
  733, 295, 18199, 11, 291, 1466, 577, 281, 12391, 309, 13, 1779, 13, 509, 458, 11,
  411, 597, 10888, 291, 486, 50652], "temperature": 0.0, "avg_logprob": -0.20678904182032534,
  "compression_ratio": 1.4742268041237114, "no_speech_prob": 0.01795823872089386},
  {"id": 127, "seek": 84640, "start": 852.8, "end": 858.72, "text": " encounter and
  you should be okay with those, but you don''t tilt towards more risky areas,", "tokens":
  [50684, 8593, 293, 291, 820, 312, 1392, 365, 729, 11, 457, 291, 500, 380, 18446,
  3030, 544, 21137, 3179, 11, 50980], "temperature": 0.0, "avg_logprob": -0.20678904182032534,
  "compression_ratio": 1.4742268041237114, "no_speech_prob": 0.01795823872089386},
  {"id": 128, "seek": 84640, "start": 859.6, "end": 866.3199999999999, "text": " in
  the west to succeed in some specific thing. So I don''t know, I like that. But what
  is your take on", "tokens": [51024, 294, 264, 7009, 281, 7754, 294, 512, 2685, 551,
  13, 407, 286, 500, 380, 458, 11, 286, 411, 300, 13, 583, 437, 307, 428, 747, 322,
  51360], "temperature": 0.0, "avg_logprob": -0.20678904182032534, "compression_ratio":
  1.4742268041237114, "no_speech_prob": 0.01795823872089386}, {"id": 129, "seek":
  86632, "start": 867.0400000000001, "end": 877.6, "text": " LLM unpredictability
  compared to more, if you will, you know, traditional programming per se. Right.",
  "tokens": [50400, 441, 43, 44, 28341, 2310, 5347, 281, 544, 11, 498, 291, 486, 11,
  291, 458, 11, 5164, 9410, 680, 369, 13, 1779, 13, 50928], "temperature": 0.0, "avg_logprob":
  -0.1997564371349742, "compression_ratio": 1.5925925925925926, "no_speech_prob":
  0.018304863944649696}, {"id": 130, "seek": 86632, "start": 877.6, "end": 883.2800000000001,
  "text": " So for example, when you, when we used to, when you used to write code,
  and I don''t know, C++", "tokens": [50928, 407, 337, 1365, 11, 562, 291, 11, 562,
  321, 1143, 281, 11, 562, 291, 1143, 281, 2464, 3089, 11, 293, 286, 500, 380, 458,
  11, 383, 25472, 51212], "temperature": 0.0, "avg_logprob": -0.1997564371349742,
  "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.018304863944649696},
  {"id": 131, "seek": 86632, "start": 883.2800000000001, "end": 888.72, "text": "
  Java, what have you? It was very deterministic in many ways. Maybe there have been
  some things", "tokens": [51212, 10745, 11, 437, 362, 291, 30, 467, 390, 588, 15957,
  3142, 294, 867, 2098, 13, 2704, 456, 362, 668, 512, 721, 51484], "temperature":
  0.0, "avg_logprob": -0.1997564371349742, "compression_ratio": 1.5925925925925926,
  "no_speech_prob": 0.018304863944649696}, {"id": 132, "seek": 86632, "start": 888.72,
  "end": 895.0400000000001, "text": " non deterministic like runtime and so on, but
  still you felt like you, you are in control of many", "tokens": [51484, 2107, 15957,
  3142, 411, 34474, 293, 370, 322, 11, 457, 920, 291, 2762, 411, 291, 11, 291, 366,
  294, 1969, 295, 867, 51800], "temperature": 0.0, "avg_logprob": -0.1997564371349742,
  "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.018304863944649696},
  {"id": 133, "seek": 89504, "start": 895.04, "end": 902.7199999999999, "text": "
  things, right. With LLM, it''s different. For example, when you ask an LLM to summarize
  a document for", "tokens": [50364, 721, 11, 558, 13, 2022, 441, 43, 44, 11, 309,
  311, 819, 13, 1171, 1365, 11, 562, 291, 1029, 364, 441, 43, 44, 281, 20858, 257,
  4166, 337, 50748], "temperature": 0.0, "avg_logprob": -0.10019888833304431, "compression_ratio":
  1.6890756302521008, "no_speech_prob": 0.006261066067963839}, {"id": 134, "seek":
  89504, "start": 902.7199999999999, "end": 909.12, "text": " you, and then you ask
  second time, the answer will be different. It will be, you know, in subtle ways,",
  "tokens": [50748, 291, 11, 293, 550, 291, 1029, 1150, 565, 11, 264, 1867, 486, 312,
  819, 13, 467, 486, 312, 11, 291, 458, 11, 294, 13743, 2098, 11, 51068], "temperature":
  0.0, "avg_logprob": -0.10019888833304431, "compression_ratio": 1.6890756302521008,
  "no_speech_prob": 0.006261066067963839}, {"id": 135, "seek": 89504, "start": 909.12,
  "end": 916.48, "text": " it will be different. And so that also creates, in my mind,
  some issues around, okay, if I have", "tokens": [51068, 309, 486, 312, 819, 13,
  400, 370, 300, 611, 7829, 11, 294, 452, 1575, 11, 512, 2663, 926, 11, 1392, 11,
  498, 286, 362, 51436], "temperature": 0.0, "avg_logprob": -0.10019888833304431,
  "compression_ratio": 1.6890756302521008, "no_speech_prob": 0.006261066067963839},
  {"id": 136, "seek": 89504, "start": 916.48, "end": 920.8, "text": " several users
  accessing the same document, should they compute the summary on the fly, or should
  they", "tokens": [51436, 2940, 5022, 26440, 264, 912, 4166, 11, 820, 436, 14722,
  264, 12691, 322, 264, 3603, 11, 420, 820, 436, 51652], "temperature": 0.0, "avg_logprob":
  -0.10019888833304431, "compression_ratio": 1.6890756302521008, "no_speech_prob":
  0.006261066067963839}, {"id": 137, "seek": 92080, "start": 920.88, "end": 926.7199999999999,
  "text": " compute it once and store it and then show the same copy to all of them,
  right. But that also means", "tokens": [50368, 14722, 309, 1564, 293, 3531, 309,
  293, 550, 855, 264, 912, 5055, 281, 439, 295, 552, 11, 558, 13, 583, 300, 611, 1355,
  50660], "temperature": 0.0, "avg_logprob": -0.15744239027782153, "compression_ratio":
  1.628691983122363, "no_speech_prob": 0.006235597655177116}, {"id": 138, "seek":
  92080, "start": 926.7199999999999, "end": 931.52, "text": " that if the original
  summary was not good enough for some reason and subsequent versions were better,",
  "tokens": [50660, 300, 498, 264, 3380, 12691, 390, 406, 665, 1547, 337, 512, 1778,
  293, 19962, 9606, 645, 1101, 11, 50900], "temperature": 0.0, "avg_logprob": -0.15744239027782153,
  "compression_ratio": 1.628691983122363, "no_speech_prob": 0.006235597655177116},
  {"id": 139, "seek": 92080, "start": 931.52, "end": 935.8399999999999, "text": "
  I will never show those better versions. Right. So like, you start asking all these
  like", "tokens": [50900, 286, 486, 1128, 855, 729, 1101, 9606, 13, 1779, 13, 407,
  411, 11, 291, 722, 3365, 439, 613, 411, 51116], "temperature": 0.0, "avg_logprob":
  -0.15744239027782153, "compression_ratio": 1.628691983122363, "no_speech_prob":
  0.006235597655177116}, {"id": 140, "seek": 92080, "start": 935.8399999999999, "end":
  943.28, "text": " multitude of questions, or am I asking the wrong questions? It''s
  such a challenge. And I don''t,", "tokens": [51116, 36358, 295, 1651, 11, 420, 669,
  286, 3365, 264, 2085, 1651, 30, 467, 311, 1270, 257, 3430, 13, 400, 286, 500, 380,
  11, 51488], "temperature": 0.0, "avg_logprob": -0.15744239027782153, "compression_ratio":
  1.628691983122363, "no_speech_prob": 0.006235597655177116}, {"id": 141, "seek":
  94328, "start": 943.8399999999999, "end": 953.52, "text": " yeah, it''s it''s a
  period. Right. Like if you''re used to doing something with Python, it''s going",
  "tokens": [50392, 1338, 11, 309, 311, 309, 311, 257, 2896, 13, 1779, 13, 1743, 498,
  291, 434, 1143, 281, 884, 746, 365, 15329, 11, 309, 311, 516, 50876], "temperature":
  0.0, "avg_logprob": -0.2122426466508345, "compression_ratio": 1.5755102040816327,
  "no_speech_prob": 0.010112723335623741}, {"id": 142, "seek": 94328, "start": 953.52,
  "end": 959.12, "text": " to be the exact same answer every single time. With these
  models, it''s just like, you know,", "tokens": [50876, 281, 312, 264, 1900, 912,
  1867, 633, 2167, 565, 13, 2022, 613, 5245, 11, 309, 311, 445, 411, 11, 291, 458,
  11, 51156], "temperature": 0.0, "avg_logprob": -0.2122426466508345, "compression_ratio":
  1.5755102040816327, "no_speech_prob": 0.010112723335623741}, {"id": 143, "seek":
  94328, "start": 960.4, "end": 965.36, "text": " a very finicky person that keeps
  changing their opinion. And you ask them the same question twice,", "tokens": [51220,
  257, 588, 962, 20539, 954, 300, 5965, 4473, 641, 4800, 13, 400, 291, 1029, 552,
  264, 912, 1168, 6091, 11, 51468], "temperature": 0.0, "avg_logprob": -0.2122426466508345,
  "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.010112723335623741},
  {"id": 144, "seek": 94328, "start": 965.36, "end": 969.92, "text": " and they''ve
  forgotten what they just said. Because it''s a new session, so they literally don''t
  have", "tokens": [51468, 293, 436, 600, 11832, 437, 436, 445, 848, 13, 1436, 309,
  311, 257, 777, 5481, 11, 370, 436, 3736, 500, 380, 362, 51696], "temperature": 0.0,
  "avg_logprob": -0.2122426466508345, "compression_ratio": 1.5755102040816327, "no_speech_prob":
  0.010112723335623741}, {"id": 145, "seek": 96992, "start": 969.92, "end": 977.1999999999999,
  "text": " them. You''re fully just that they start over. I think we''re going to
  see a shift in this is not", "tokens": [50364, 552, 13, 509, 434, 4498, 445, 300,
  436, 722, 670, 13, 286, 519, 321, 434, 516, 281, 536, 257, 5513, 294, 341, 307,
  406, 50728], "temperature": 0.0, "avg_logprob": -0.21017076454910577, "compression_ratio":
  1.7636363636363637, "no_speech_prob": 0.0027035088278353214}, {"id": 146, "seek":
  96992, "start": 977.1999999999999, "end": 985.5999999999999, "text": " going to
  change anytime soon. Just it''s almost as if you kind of plug a fake human into
  the circuit.", "tokens": [50728, 516, 281, 1319, 13038, 2321, 13, 1449, 309, 311,
  1920, 382, 498, 291, 733, 295, 5452, 257, 7592, 1952, 666, 264, 9048, 13, 51148],
  "temperature": 0.0, "avg_logprob": -0.21017076454910577, "compression_ratio": 1.7636363636363637,
  "no_speech_prob": 0.0027035088278353214}, {"id": 147, "seek": 96992, "start": 985.5999999999999,
  "end": 990.24, "text": " It''s like it''s going to be independent. That''s the nature
  of it. And that nature is not going to", "tokens": [51148, 467, 311, 411, 309, 311,
  516, 281, 312, 6695, 13, 663, 311, 264, 3687, 295, 309, 13, 400, 300, 3687, 307,
  406, 516, 281, 51380], "temperature": 0.0, "avg_logprob": -0.21017076454910577,
  "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0027035088278353214},
  {"id": 148, "seek": 96992, "start": 990.24, "end": 999.68, "text": " change anytime
  soon. So I think what you''re going to see is a modification in the way we build",
  "tokens": [51380, 1319, 13038, 2321, 13, 407, 286, 519, 437, 291, 434, 516, 281,
  536, 307, 257, 26747, 294, 264, 636, 321, 1322, 51852], "temperature": 0.0, "avg_logprob":
  -0.21017076454910577, "compression_ratio": 1.7636363636363637, "no_speech_prob":
  0.0027035088278353214}, {"id": 149, "seek": 99968, "start": 999.68, "end": 1005.68,
  "text": " code around these things. I think the pain point is when you assume that
  it''s going to be as", "tokens": [50364, 3089, 926, 613, 721, 13, 286, 519, 264,
  1822, 935, 307, 562, 291, 6552, 300, 309, 311, 516, 281, 312, 382, 50664], "temperature":
  0.0, "avg_logprob": -0.09679614067077637, "compression_ratio": 1.5805084745762712,
  "no_speech_prob": 0.0008801878429949284}, {"id": 150, "seek": 99968, "start": 1005.68,
  "end": 1010.4799999999999, "text": " predictable as a code that you''re used to.
  But once you get over it, you realize that, okay, well,", "tokens": [50664, 27737,
  382, 257, 3089, 300, 291, 434, 1143, 281, 13, 583, 1564, 291, 483, 670, 309, 11,
  291, 4325, 300, 11, 1392, 11, 731, 11, 50904], "temperature": 0.0, "avg_logprob":
  -0.09679614067077637, "compression_ratio": 1.5805084745762712, "no_speech_prob":
  0.0008801878429949284}, {"id": 151, "seek": 99968, "start": 1010.4799999999999,
  "end": 1014.7199999999999, "text": " if I just literally had a human in the loop,
  there''s like an API to connect to a human,", "tokens": [50904, 498, 286, 445, 3736,
  632, 257, 1952, 294, 264, 6367, 11, 456, 311, 411, 364, 9362, 281, 1745, 281, 257,
  1952, 11, 51116], "temperature": 0.0, "avg_logprob": -0.09679614067077637, "compression_ratio":
  1.5805084745762712, "no_speech_prob": 0.0008801878429949284}, {"id": 152, "seek":
  99968, "start": 1015.4399999999999, "end": 1023.3599999999999, "text": " then I
  have to be build a user experience that is somehow tolerant to that. And so let''s
  see.", "tokens": [51152, 550, 286, 362, 281, 312, 1322, 257, 4195, 1752, 300, 307,
  6063, 45525, 281, 300, 13, 400, 370, 718, 311, 536, 13, 51548], "temperature": 0.0,
  "avg_logprob": -0.09679614067077637, "compression_ratio": 1.5805084745762712, "no_speech_prob":
  0.0008801878429949284}, {"id": 153, "seek": 102336, "start": 1024.32, "end": 1032.24,
  "text": " A lot of times people are hoping the first phrase into interacting with
  these things. They say,", "tokens": [50412, 316, 688, 295, 1413, 561, 366, 7159,
  264, 700, 9535, 666, 18017, 365, 613, 721, 13, 814, 584, 11, 50808], "temperature":
  0.0, "avg_logprob": -0.1808109680811564, "compression_ratio": 1.4894736842105263,
  "no_speech_prob": 0.0036443881690502167}, {"id": 154, "seek": 102336, "start": 1033.6,
  "end": 1039.28, "text": " here''s a specification, build this code, and they expect
  the answer to just forward. Now,", "tokens": [50876, 510, 311, 257, 31256, 11, 1322,
  341, 3089, 11, 293, 436, 2066, 264, 1867, 281, 445, 2128, 13, 823, 11, 51160], "temperature":
  0.0, "avg_logprob": -0.1808109680811564, "compression_ratio": 1.4894736842105263,
  "no_speech_prob": 0.0036443881690502167}, {"id": 155, "seek": 102336, "start": 1039.28,
  "end": 1048.08, "text": " that can fail in one of two big ways. One way is that
  it''s just too complex. The model you can do", "tokens": [51160, 300, 393, 3061,
  294, 472, 295, 732, 955, 2098, 13, 1485, 636, 307, 300, 309, 311, 445, 886, 3997,
  13, 440, 2316, 291, 393, 360, 51600], "temperature": 0.0, "avg_logprob": -0.1808109680811564,
  "compression_ratio": 1.4894736842105263, "no_speech_prob": 0.0036443881690502167},
  {"id": 156, "seek": 104808, "start": 1048.1599999999999, "end": 1053.28, "text":
  " chain of thought reasoning, 01 has it built in and it''s magic. And it''s going
  to get better. But with", "tokens": [50368, 5021, 295, 1194, 21577, 11, 23185, 575,
  309, 3094, 294, 293, 309, 311, 5585, 13, 400, 309, 311, 516, 281, 483, 1101, 13,
  583, 365, 50624], "temperature": 0.0, "avg_logprob": -0.1606650451819102, "compression_ratio":
  1.6866952789699572, "no_speech_prob": 0.0043530636467039585}, {"id": 157, "seek":
  104808, "start": 1053.28, "end": 1061.9199999999998, "text": " any sufficiently
  large request, complex request, since you''re just appending one token at a time,",
  "tokens": [50624, 604, 31868, 2416, 5308, 11, 3997, 5308, 11, 1670, 291, 434, 445,
  724, 2029, 472, 14862, 412, 257, 565, 11, 51056], "temperature": 0.0, "avg_logprob":
  -0.1606650451819102, "compression_ratio": 1.6866952789699572, "no_speech_prob":
  0.0043530636467039585}, {"id": 158, "seek": 104808, "start": 1062.3999999999999,
  "end": 1067.76, "text": " it''s just too easy to paint yourself into a corner. So
  models will get better and they''ll be less", "tokens": [51080, 309, 311, 445, 886,
  1858, 281, 4225, 1803, 666, 257, 4538, 13, 407, 5245, 486, 483, 1101, 293, 436,
  603, 312, 1570, 51348], "temperature": 0.0, "avg_logprob": -0.1606650451819102,
  "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.0043530636467039585},
  {"id": 159, "seek": 104808, "start": 1067.76, "end": 1072.3999999999999, "text":
  " and less likely to paint themselves into a corner. But it''ll always be the case
  with sufficient", "tokens": [51348, 293, 1570, 3700, 281, 4225, 2969, 666, 257,
  4538, 13, 583, 309, 603, 1009, 312, 264, 1389, 365, 11563, 51580], "temperature":
  0.0, "avg_logprob": -0.1606650451819102, "compression_ratio": 1.6866952789699572,
  "no_speech_prob": 0.0043530636467039585}, {"id": 160, "seek": 107240, "start": 1072.4,
  "end": 1080.0, "text": " complexity. The other issue that you run into and why we''ll
  never ever get there is because", "tokens": [50364, 14024, 13, 440, 661, 2734, 300,
  291, 1190, 666, 293, 983, 321, 603, 1128, 1562, 483, 456, 307, 570, 50744], "temperature":
  0.0, "avg_logprob": -0.14833736419677734, "compression_ratio": 1.6363636363636365,
  "no_speech_prob": 0.012517302297055721}, {"id": 161, "seek": 107240, "start": 1082.0800000000002,
  "end": 1090.96, "text": " when I describe something, the domain of possible implementations,
  possible completions that", "tokens": [50848, 562, 286, 6786, 746, 11, 264, 9274,
  295, 1944, 4445, 763, 11, 1944, 1557, 626, 300, 51292], "temperature": 0.0, "avg_logprob":
  -0.14833736419677734, "compression_ratio": 1.6363636363636365, "no_speech_prob":
  0.012517302297055721}, {"id": 162, "seek": 107240, "start": 1090.96, "end": 1096.0,
  "text": " match that input is so much larger than whatever I have in my head right
  now. And so if you have", "tokens": [51292, 2995, 300, 4846, 307, 370, 709, 4833,
  813, 2035, 286, 362, 294, 452, 1378, 558, 586, 13, 400, 370, 498, 291, 362, 51544],
  "temperature": 0.0, "avg_logprob": -0.14833736419677734, "compression_ratio": 1.6363636363636365,
  "no_speech_prob": 0.012517302297055721}, {"id": 163, "seek": 107240, "start": 1096.0,
  "end": 1101.3600000000001, "text": " a company that''s like, you know, we''re going
  to have like, you say the specification or code and", "tokens": [51544, 257, 2237,
  300, 311, 411, 11, 291, 458, 11, 321, 434, 516, 281, 362, 411, 11, 291, 584, 264,
  31256, 420, 3089, 293, 51812], "temperature": 0.0, "avg_logprob": -0.14833736419677734,
  "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.012517302297055721},
  {"id": 164, "seek": 110136, "start": 1101.36, "end": 1105.6799999999998, "text":
  " it will just always make the code. It''s like, you don''t realize into the codes
  written which you", "tokens": [50364, 309, 486, 445, 1009, 652, 264, 3089, 13, 467,
  311, 411, 11, 291, 500, 380, 4325, 666, 264, 14211, 3720, 597, 291, 50580], "temperature":
  0.0, "avg_logprob": -0.1755941076186097, "compression_ratio": 1.7212389380530972,
  "no_speech_prob": 0.0015101423487067223}, {"id": 165, "seek": 110136, "start": 1105.6799999999998,
  "end": 1110.32, "text": " even wanted. You don''t, and then you go back and change
  it. You don''t realize the codes written", "tokens": [50580, 754, 1415, 13, 509,
  500, 380, 11, 293, 550, 291, 352, 646, 293, 1319, 309, 13, 509, 500, 380, 4325,
  264, 14211, 3720, 50812], "temperature": 0.0, "avg_logprob": -0.1755941076186097,
  "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0015101423487067223},
  {"id": 166, "seek": 110136, "start": 1111.4399999999998, "end": 1116.1599999999999,
  "text": " and written incorrectly, you know, that, oh, that it''s doing what I said.
  That''s not what I meant.", "tokens": [50868, 293, 3720, 42892, 11, 291, 458, 11,
  300, 11, 1954, 11, 300, 309, 311, 884, 437, 286, 848, 13, 663, 311, 406, 437, 286,
  4140, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1755941076186097, "compression_ratio":
  1.7212389380530972, "no_speech_prob": 0.0015101423487067223}, {"id": 167, "seek":
  110136, "start": 1118.32, "end": 1125.6, "text": " So what does this all mean? I
  think that future implementations have to do a lot to keep the user", "tokens":
  [51212, 407, 437, 775, 341, 439, 914, 30, 286, 519, 300, 2027, 4445, 763, 362, 281,
  360, 257, 688, 281, 1066, 264, 4195, 51576], "temperature": 0.0, "avg_logprob":
  -0.1755941076186097, "compression_ratio": 1.7212389380530972, "no_speech_prob":
  0.0015101423487067223}, {"id": 168, "seek": 112560, "start": 1126.08, "end": 1131.76,
  "text": " in the loop and make the experience so that the user doesn''t feel like
  they''re just shouting", "tokens": [50388, 294, 264, 6367, 293, 652, 264, 1752,
  370, 300, 264, 4195, 1177, 380, 841, 411, 436, 434, 445, 20382, 50672], "temperature":
  0.0, "avg_logprob": -0.09021330570829088, "compression_ratio": 1.5698324022346368,
  "no_speech_prob": 0.0050799851305782795}, {"id": 169, "seek": 112560, "start": 1131.76,
  "end": 1138.6399999999999, "text": " instructions at a thing and then hoping that
  it works. But the user has to be interacting with", "tokens": [50672, 9415, 412,
  257, 551, 293, 550, 7159, 300, 309, 1985, 13, 583, 264, 4195, 575, 281, 312, 18017,
  365, 51016], "temperature": 0.0, "avg_logprob": -0.09021330570829088, "compression_ratio":
  1.5698324022346368, "no_speech_prob": 0.0050799851305782795}, {"id": 170, "seek":
  112560, "start": 1138.6399999999999, "end": 1148.48, "text": " this thing and, you
  know, converging towards a solution. So you see this in a couple of ways.", "tokens":
  [51016, 341, 551, 293, 11, 291, 458, 11, 9652, 3249, 3030, 257, 3827, 13, 407, 291,
  536, 341, 294, 257, 1916, 295, 2098, 13, 51508], "temperature": 0.0, "avg_logprob":
  -0.09021330570829088, "compression_ratio": 1.5698324022346368, "no_speech_prob":
  0.0050799851305782795}, {"id": 171, "seek": 114848, "start": 1149.28, "end": 1156.88,
  "text": " One way is like with the assistant interface. And cursor, forgive me,
  GitHub, per se, the cursor is", "tokens": [50404, 1485, 636, 307, 411, 365, 264,
  10994, 9226, 13, 400, 28169, 11, 10718, 385, 11, 23331, 11, 680, 369, 11, 264, 28169,
  307, 50784], "temperature": 0.0, "avg_logprob": -0.2207514338132714, "compression_ratio":
  1.778181818181818, "no_speech_prob": 0.007209557108581066}, {"id": 172, "seek":
  114848, "start": 1157.44, "end": 1162.88, "text": " just a really good example here
  where you feel like you''re chatting with someone that is working", "tokens": [50812,
  445, 257, 534, 665, 1365, 510, 689, 291, 841, 411, 291, 434, 24654, 365, 1580, 300,
  307, 1364, 51084], "temperature": 0.0, "avg_logprob": -0.2207514338132714, "compression_ratio":
  1.778181818181818, "no_speech_prob": 0.007209557108581066}, {"id": 173, "seek":
  114848, "start": 1162.88, "end": 1167.92, "text": " with you to, to, on this code.
  It gets into something I hope we talk about a little bit later. Art", "tokens":
  [51084, 365, 291, 281, 11, 281, 11, 322, 341, 3089, 13, 467, 2170, 666, 746, 286,
  1454, 321, 751, 466, 257, 707, 857, 1780, 13, 5735, 51336], "temperature": 0.0,
  "avg_logprob": -0.2207514338132714, "compression_ratio": 1.778181818181818, "no_speech_prob":
  0.007209557108581066}, {"id": 174, "seek": 114848, "start": 1167.92, "end": 1173.2,
  "text": " of facts, you know, they''re, you''re having this conversation here, but
  you''re working on these", "tokens": [51336, 295, 9130, 11, 291, 458, 11, 436, 434,
  11, 291, 434, 1419, 341, 3761, 510, 11, 457, 291, 434, 1364, 322, 613, 51600], "temperature":
  0.0, "avg_logprob": -0.2207514338132714, "compression_ratio": 1.778181818181818,
  "no_speech_prob": 0.007209557108581066}, {"id": 175, "seek": 114848, "start": 1173.2,
  "end": 1178.08, "text": " artifacts. You''re working on these things. And these
  assistants under, you understand what they''re", "tokens": [51600, 24617, 13, 509,
  434, 1364, 322, 613, 721, 13, 400, 613, 34949, 833, 11, 291, 1223, 437, 436, 434,
  51844], "temperature": 0.0, "avg_logprob": -0.2207514338132714, "compression_ratio":
  1.778181818181818, "no_speech_prob": 0.007209557108581066}, {"id": 176, "seek":
  117808, "start": 1178.08, "end": 1182.6399999999999, "text": " looking at. Whenever
  they make a recommendation to change something, you understand how it''s", "tokens":
  [50364, 1237, 412, 13, 14159, 436, 652, 257, 11879, 281, 1319, 746, 11, 291, 1223,
  577, 309, 311, 50592], "temperature": 0.0, "avg_logprob": -0.16628951633099429,
  "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00010489464330021292},
  {"id": 177, "seek": 117808, "start": 1183.28, "end": 1188.32, "text": " going to
  change your code. You are still in control as a human, say yes or no for all this
  stuff.", "tokens": [50624, 516, 281, 1319, 428, 3089, 13, 509, 366, 920, 294, 1969,
  382, 257, 1952, 11, 584, 2086, 420, 572, 337, 439, 341, 1507, 13, 50876], "temperature":
  0.0, "avg_logprob": -0.16628951633099429, "compression_ratio": 1.6382978723404256,
  "no_speech_prob": 0.00010489464330021292}, {"id": 178, "seek": 117808, "start":
  1188.8799999999999, "end": 1194.08, "text": " And that''s one way that they keep
  the users in the loop. The other way that we keep users in the", "tokens": [50904,
  400, 300, 311, 472, 636, 300, 436, 1066, 264, 5022, 294, 264, 6367, 13, 440, 661,
  636, 300, 321, 1066, 5022, 294, 264, 51164], "temperature": 0.0, "avg_logprob":
  -0.16628951633099429, "compression_ratio": 1.6382978723404256, "no_speech_prob":
  0.00010489464330021292}, {"id": 179, "seek": 117808, "start": 1194.08, "end": 1203.28,
  "text": " loop, and I promise I''ll shut up soon, is there''s a assistant type behavior
  and then there''s like", "tokens": [51164, 6367, 11, 293, 286, 6228, 286, 603, 5309,
  493, 2321, 11, 307, 456, 311, 257, 10994, 2010, 5223, 293, 550, 456, 311, 411, 51624],
  "temperature": 0.0, "avg_logprob": -0.16628951633099429, "compression_ratio": 1.6382978723404256,
  "no_speech_prob": 0.00010489464330021292}, {"id": 180, "seek": 120328, "start":
  1203.28, "end": 1210.56, "text": " workflows where a human is, it''s still in the
  loop. But there is a human at the beginning that", "tokens": [50364, 43461, 689,
  257, 1952, 307, 11, 309, 311, 920, 294, 264, 6367, 13, 583, 456, 307, 257, 1952,
  412, 264, 2863, 300, 50728], "temperature": 0.0, "avg_logprob": -0.15281468994763434,
  "compression_ratio": 1.5949367088607596, "no_speech_prob": 0.009639846161007881},
  {"id": 181, "seek": 120328, "start": 1210.56, "end": 1216.32, "text": " designed
  a workflow as like a set of steps. You can''t just say look at this website and
  pull out", "tokens": [50728, 4761, 257, 20993, 382, 411, 257, 992, 295, 4439, 13,
  509, 393, 380, 445, 584, 574, 412, 341, 3144, 293, 2235, 484, 51016], "temperature":
  0.0, "avg_logprob": -0.15281468994763434, "compression_ratio": 1.5949367088607596,
  "no_speech_prob": 0.009639846161007881}, {"id": 182, "seek": 120328, "start": 1217.28,
  "end": 1222.48, "text": " all the phone numbers, all of the menu items, all of the,
  you know, the structure content.", "tokens": [51064, 439, 264, 2593, 3547, 11, 439,
  295, 264, 6510, 4754, 11, 439, 295, 264, 11, 291, 458, 11, 264, 3877, 2701, 13,
  51324], "temperature": 0.0, "avg_logprob": -0.15281468994763434, "compression_ratio":
  1.5949367088607596, "no_speech_prob": 0.009639846161007881}, {"id": 183, "seek":
  120328, "start": 1223.2, "end": 1229.2, "text": " And always expected to work. Sometimes
  it''s better to say, let''s take this big thing and have a", "tokens": [51360, 400,
  1009, 5176, 281, 589, 13, 4803, 309, 311, 1101, 281, 584, 11, 718, 311, 747, 341,
  955, 551, 293, 362, 257, 51660], "temperature": 0.0, "avg_logprob": -0.15281468994763434,
  "compression_ratio": 1.5949367088607596, "no_speech_prob": 0.009639846161007881},
  {"id": 184, "seek": 122920, "start": 1229.2, "end": 1237.52, "text": " human, a
  human in this loop is defining all the steps that it''s going to take to implement
  this workflow.", "tokens": [50364, 1952, 11, 257, 1952, 294, 341, 6367, 307, 17827,
  439, 264, 4439, 300, 309, 311, 516, 281, 747, 281, 4445, 341, 20993, 13, 50780],
  "temperature": 0.0, "avg_logprob": -0.15759757070830374, "compression_ratio": 1.7085201793721974,
  "no_speech_prob": 0.0026067395228892565}, {"id": 185, "seek": 122920, "start": 1237.52,
  "end": 1243.28, "text": " And that way it''s still, you can make something that
  is recoverable, you know, that there''s", "tokens": [50780, 400, 300, 636, 309,
  311, 920, 11, 291, 393, 652, 746, 300, 307, 8114, 712, 11, 291, 458, 11, 300, 456,
  311, 51068], "temperature": 0.0, "avg_logprob": -0.15759757070830374, "compression_ratio":
  1.7085201793721974, "no_speech_prob": 0.0026067395228892565}, {"id": 186, "seek":
  122920, "start": 1243.28, "end": 1248.0800000000002, "text": " airstates for some
  of these steps and you can get out of them, pass it back up to a real human.", "tokens":
  [51068, 1988, 372, 1024, 337, 512, 295, 613, 4439, 293, 291, 393, 483, 484, 295,
  552, 11, 1320, 309, 646, 493, 281, 257, 957, 1952, 13, 51308], "temperature": 0.0,
  "avg_logprob": -0.15759757070830374, "compression_ratio": 1.7085201793721974, "no_speech_prob":
  0.0026067395228892565}, {"id": 187, "seek": 122920, "start": 1249.68, "end": 1255.6000000000001,
  "text": " But yeah, all along the way of saying these things are going to remain
  hard to predict,", "tokens": [51388, 583, 1338, 11, 439, 2051, 264, 636, 295, 1566,
  613, 721, 366, 516, 281, 6222, 1152, 281, 6069, 11, 51684], "temperature": 0.0,
  "avg_logprob": -0.15759757070830374, "compression_ratio": 1.7085201793721974, "no_speech_prob":
  0.0026067395228892565}, {"id": 188, "seek": 125560, "start": 1256.24, "end": 1260.9599999999998,
  "text": " but the code that''s built around them, I think, is going to become very
  tolerant of that and", "tokens": [50396, 457, 264, 3089, 300, 311, 3094, 926, 552,
  11, 286, 519, 11, 307, 516, 281, 1813, 588, 45525, 295, 300, 293, 50632], "temperature":
  0.0, "avg_logprob": -0.18884249566828162, "compression_ratio": 1.6291666666666667,
  "no_speech_prob": 0.0264284685254097}, {"id": 189, "seek": 125560, "start": 1260.9599999999998,
  "end": 1266.8, "text": " by pulling the users into the conversation constantly.
  Yeah, so you basically, if I got your idea", "tokens": [50632, 538, 8407, 264, 5022,
  666, 264, 3761, 6460, 13, 865, 11, 370, 291, 1936, 11, 498, 286, 658, 428, 1558,
  50924], "temperature": 0.0, "avg_logprob": -0.18884249566828162, "compression_ratio":
  1.6291666666666667, "no_speech_prob": 0.0264284685254097}, {"id": 190, "seek": 125560,
  "start": 1266.8, "end": 1276.0, "text": " right, is that you put the user in the
  driver''s seat, right? And the model or whatever LLM app is still,", "tokens": [50924,
  558, 11, 307, 300, 291, 829, 264, 4195, 294, 264, 6787, 311, 6121, 11, 558, 30,
  400, 264, 2316, 420, 2035, 441, 43, 44, 724, 307, 920, 11, 51384], "temperature":
  0.0, "avg_logprob": -0.18884249566828162, "compression_ratio": 1.6291666666666667,
  "no_speech_prob": 0.0264284685254097}, {"id": 191, "seek": 125560, "start": 1277.12,
  "end": 1283.84, "text": " it''s kind of like an assistant, as you said, or companion,
  whatever you want to call it, right?", "tokens": [51440, 309, 311, 733, 295, 411,
  364, 10994, 11, 382, 291, 848, 11, 420, 22363, 11, 2035, 291, 528, 281, 818, 309,
  11, 558, 30, 51776], "temperature": 0.0, "avg_logprob": -0.18884249566828162, "compression_ratio":
  1.6291666666666667, "no_speech_prob": 0.0264284685254097}, {"id": 192, "seek": 128384,
  "start": 1283.84, "end": 1291.84, "text": " But you, like, you still, I guess we
  are still at that point in time when we need to know exactly", "tokens": [50364,
  583, 291, 11, 411, 11, 291, 920, 11, 286, 2041, 321, 366, 920, 412, 300, 935, 294,
  565, 562, 321, 643, 281, 458, 2293, 50764], "temperature": 0.0, "avg_logprob": -0.11282600806309627,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.006080771796405315},
  {"id": 193, "seek": 128384, "start": 1291.84, "end": 1300.1599999999999, "text":
  " what we want, right? As users. And I think we also need to know how to get it
  out of the model,", "tokens": [50764, 437, 321, 528, 11, 558, 30, 1018, 5022, 13,
  400, 286, 519, 321, 611, 643, 281, 458, 577, 281, 483, 309, 484, 295, 264, 2316,
  11, 51180], "temperature": 0.0, "avg_logprob": -0.11282600806309627, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.006080771796405315}, {"id": 194, "seek":
  128384, "start": 1300.1599999999999, "end": 1307.28, "text": " right? Because sometimes
  no matter what you know, it''s not somehow achievable, maybe because you", "tokens":
  [51180, 558, 30, 1436, 2171, 572, 1871, 437, 291, 458, 11, 309, 311, 406, 6063,
  3538, 17915, 11, 1310, 570, 291, 51536], "temperature": 0.0, "avg_logprob": -0.11282600806309627,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.006080771796405315},
  {"id": 195, "seek": 128384, "start": 1307.28, "end": 1313.6799999999998, "text":
  " don''t know how to prompt well or, you know, you just go into the loops, I frequently
  go there,", "tokens": [51536, 500, 380, 458, 577, 281, 12391, 731, 420, 11, 291,
  458, 11, 291, 445, 352, 666, 264, 16121, 11, 286, 10374, 352, 456, 11, 51856], "temperature":
  0.0, "avg_logprob": -0.11282600806309627, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.006080771796405315}, {"id": 196, "seek": 131368, "start": 1313.68,
  "end": 1320.3200000000002, "text": " when I, for example, chat to, I don''t know,
  chat GPT or it could be any other tool. When it just", "tokens": [50364, 562, 286,
  11, 337, 1365, 11, 5081, 281, 11, 286, 500, 380, 458, 11, 5081, 26039, 51, 420,
  309, 727, 312, 604, 661, 2290, 13, 1133, 309, 445, 50696], "temperature": 0.0, "avg_logprob":
  -0.13618355327182347, "compression_ratio": 1.596774193548387, "no_speech_prob":
  0.008403436280786991}, {"id": 197, "seek": 131368, "start": 1320.3200000000002,
  "end": 1326.16, "text": " keeps going and returning to the point which didn''t work
  already, because the alternative doesn''t work", "tokens": [50696, 5965, 516, 293,
  12678, 281, 264, 935, 597, 994, 380, 589, 1217, 11, 570, 264, 8535, 1177, 380, 589,
  50988], "temperature": 0.0, "avg_logprob": -0.13618355327182347, "compression_ratio":
  1.596774193548387, "no_speech_prob": 0.008403436280786991}, {"id": 198, "seek":
  131368, "start": 1326.16, "end": 1331.1200000000001, "text": " now. And I''m like,
  okay, neither work. Like what you propose just doesn''t work. What should I do?",
  "tokens": [50988, 586, 13, 400, 286, 478, 411, 11, 1392, 11, 9662, 589, 13, 1743,
  437, 291, 17421, 445, 1177, 380, 589, 13, 708, 820, 286, 360, 30, 51236], "temperature":
  0.0, "avg_logprob": -0.13618355327182347, "compression_ratio": 1.596774193548387,
  "no_speech_prob": 0.008403436280786991}, {"id": 199, "seek": 131368, "start": 1332.16,
  "end": 1338.16, "text": " But still, I feel like I became much more productive as
  a, I don''t write code every day, you know,", "tokens": [51288, 583, 920, 11, 286,
  841, 411, 286, 3062, 709, 544, 13304, 382, 257, 11, 286, 500, 380, 2464, 3089, 633,
  786, 11, 291, 458, 11, 51588], "temperature": 0.0, "avg_logprob": -0.13618355327182347,
  "compression_ratio": 1.596774193548387, "no_speech_prob": 0.008403436280786991},
  {"id": 200, "seek": 133816, "start": 1338.24, "end": 1345.0400000000002, "text":
  " for my work anymore, but for leaving. But when I do, I feel like I saved, I don''t
  know,", "tokens": [50368, 337, 452, 589, 3602, 11, 457, 337, 5012, 13, 583, 562,
  286, 360, 11, 286, 841, 411, 286, 6624, 11, 286, 500, 380, 458, 11, 50708], "temperature":
  0.0, "avg_logprob": -0.14045983842275675, "compression_ratio": 1.5263157894736843,
  "no_speech_prob": 0.014122546650469303}, {"id": 201, "seek": 133816, "start": 1345.0400000000002,
  "end": 1352.3200000000002, "text": " three, five days of my time by using these
  tools. But there is still this kind of unpredictable", "tokens": [50708, 1045, 11,
  1732, 1708, 295, 452, 565, 538, 1228, 613, 3873, 13, 583, 456, 307, 920, 341, 733,
  295, 31160, 51072], "temperature": 0.0, "avg_logprob": -0.14045983842275675, "compression_ratio":
  1.5263157894736843, "no_speech_prob": 0.014122546650469303}, {"id": 202, "seek":
  133816, "start": 1352.3200000000002, "end": 1357.92, "text": " component to it,
  you know, I''ll give you one example, very specific one. So I was building like",
  "tokens": [51072, 6542, 281, 309, 11, 291, 458, 11, 286, 603, 976, 291, 472, 1365,
  11, 588, 2685, 472, 13, 407, 286, 390, 2390, 411, 51352], "temperature": 0.0, "avg_logprob":
  -0.14045983842275675, "compression_ratio": 1.5263157894736843, "no_speech_prob":
  0.014122546650469303}, {"id": 203, "seek": 133816, "start": 1359.0400000000002,
  "end": 1366.24, "text": " like simple Python code, which would draw a diagram. And
  on the x-axis, it would need to put, you", "tokens": [51408, 411, 2199, 15329, 3089,
  11, 597, 576, 2642, 257, 10686, 13, 400, 322, 264, 2031, 12, 24633, 11, 309, 576,
  643, 281, 829, 11, 291, 51768], "temperature": 0.0, "avg_logprob": -0.14045983842275675,
  "compression_ratio": 1.5263157894736843, "no_speech_prob": 0.014122546650469303},
  {"id": 204, "seek": 136624, "start": 1366.24, "end": 1376.16, "text": " know, these
  values like 1, 1.5, 2, 2.5 and so on. And so the model made a mistake by rounding
  all", "tokens": [50364, 458, 11, 613, 4190, 411, 502, 11, 502, 13, 20, 11, 568,
  11, 568, 13, 20, 293, 370, 322, 13, 400, 370, 264, 2316, 1027, 257, 6146, 538, 48237,
  439, 50860], "temperature": 0.0, "avg_logprob": -0.13836764452750222, "compression_ratio":
  1.640495867768595, "no_speech_prob": 0.00875595211982727}, {"id": 205, "seek": 136624,
  "start": 1376.16, "end": 1382.08, "text": " these values to an integer. And so when
  x-axis, all of a sudden, I saw the same values, right? And", "tokens": [50860, 613,
  4190, 281, 364, 24922, 13, 400, 370, 562, 2031, 12, 24633, 11, 439, 295, 257, 3990,
  11, 286, 1866, 264, 912, 4190, 11, 558, 30, 400, 51156], "temperature": 0.0, "avg_logprob":
  -0.13836764452750222, "compression_ratio": 1.640495867768595, "no_speech_prob":
  0.00875595211982727}, {"id": 206, "seek": 136624, "start": 1382.08, "end": 1387.04,
  "text": " the model doesn''t have the reasoning component to realize that it made
  a mistake. Or at least call", "tokens": [51156, 264, 2316, 1177, 380, 362, 264,
  21577, 6542, 281, 4325, 300, 309, 1027, 257, 6146, 13, 1610, 412, 1935, 818, 51404],
  "temperature": 0.0, "avg_logprob": -0.13836764452750222, "compression_ratio": 1.640495867768595,
  "no_speech_prob": 0.00875595211982727}, {"id": 207, "seek": 136624, "start": 1387.04,
  "end": 1393.28, "text": " it out and say, do you want it this way? Or should I do
  it another way? I had to correct it because I", "tokens": [51404, 309, 484, 293,
  584, 11, 360, 291, 528, 309, 341, 636, 30, 1610, 820, 286, 360, 309, 1071, 636,
  30, 286, 632, 281, 3006, 309, 570, 286, 51716], "temperature": 0.0, "avg_logprob":
  -0.13836764452750222, "compression_ratio": 1.640495867768595, "no_speech_prob":
  0.00875595211982727}, {"id": 208, "seek": 139328, "start": 1393.28, "end": 1399.92,
  "text": " knew that I needed to cast it to float. But if I didn''t know programming,
  I wouldn''t be able to do", "tokens": [50364, 2586, 300, 286, 2978, 281, 4193, 309,
  281, 15706, 13, 583, 498, 286, 994, 380, 458, 9410, 11, 286, 2759, 380, 312, 1075,
  281, 360, 50696], "temperature": 0.0, "avg_logprob": -0.1118520164489746, "compression_ratio":
  1.646808510638298, "no_speech_prob": 0.019360436126589775}, {"id": 209, "seek":
  139328, "start": 1399.92, "end": 1408.24, "text": " that, right? I would be stuck
  right there. And so that''s the level lake of sophistication we are", "tokens":
  [50696, 300, 11, 558, 30, 286, 576, 312, 5541, 558, 456, 13, 400, 370, 300, 311,
  264, 1496, 11001, 295, 15572, 399, 321, 366, 51112], "temperature": 0.0, "avg_logprob":
  -0.1118520164489746, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.019360436126589775},
  {"id": 210, "seek": 139328, "start": 1408.24, "end": 1413.12, "text": " in still,
  right? If we''re talking about code completion, but I wonder what you feel about
  this?", "tokens": [51112, 294, 920, 11, 558, 30, 759, 321, 434, 1417, 466, 3089,
  19372, 11, 457, 286, 2441, 437, 291, 841, 466, 341, 30, 51356], "temperature": 0.0,
  "avg_logprob": -0.1118520164489746, "compression_ratio": 1.646808510638298, "no_speech_prob":
  0.019360436126589775}, {"id": 211, "seek": 139328, "start": 1413.12, "end": 1419.84,
  "text": " What do you think about code complete? You did call out cursor as the
  tool you use the probably", "tokens": [51356, 708, 360, 291, 519, 466, 3089, 3566,
  30, 509, 630, 818, 484, 28169, 382, 264, 2290, 291, 764, 264, 1391, 51692], "temperature":
  0.0, "avg_logprob": -0.1118520164489746, "compression_ratio": 1.646808510638298,
  "no_speech_prob": 0.019360436126589775}, {"id": 212, "seek": 141984, "start": 1419.84,
  "end": 1426.9599999999998, "text": " more often now, but you did work on that in
  GitHub, Copilot team. And what was your sense of", "tokens": [50364, 544, 2049,
  586, 11, 457, 291, 630, 589, 322, 300, 294, 23331, 11, 11579, 31516, 1469, 13, 400,
  437, 390, 428, 2020, 295, 50720], "temperature": 0.0, "avg_logprob": -0.16853753725687662,
  "compression_ratio": 1.5375, "no_speech_prob": 0.03003728576004505}, {"id": 213,
  "seek": 141984, "start": 1428.24, "end": 1434.9599999999998, "text": " its quality
  and like challenges around it? And in general, how did you approach the task that",
  "tokens": [50784, 1080, 3125, 293, 411, 4759, 926, 309, 30, 400, 294, 2674, 11,
  577, 630, 291, 3109, 264, 5633, 300, 51120], "temperature": 0.0, "avg_logprob":
  -0.16853753725687662, "compression_ratio": 1.5375, "no_speech_prob": 0.03003728576004505},
  {"id": 214, "seek": 141984, "start": 1434.9599999999998, "end": 1443.28, "text":
  " research challenge? I can speak a little bit of that. There''s two ways in which
  I will be", "tokens": [51120, 2132, 3430, 30, 286, 393, 1710, 257, 707, 857, 295,
  300, 13, 821, 311, 732, 2098, 294, 597, 286, 486, 312, 51536], "temperature": 0.0,
  "avg_logprob": -0.16853753725687662, "compression_ratio": 1.5375, "no_speech_prob":
  0.03003728576004505}, {"id": 215, "seek": 141984, "start": 1443.28, "end": 1447.6799999999998,
  "text": " unsatisfactory here. One, I can''t get into all the details probably.
  And another way is I''ve", "tokens": [51536, 2693, 25239, 21840, 510, 13, 1485,
  11, 286, 393, 380, 483, 666, 439, 264, 4365, 1391, 13, 400, 1071, 636, 307, 286,
  600, 51756], "temperature": 0.0, "avg_logprob": -0.16853753725687662, "compression_ratio":
  1.5375, "no_speech_prob": 0.03003728576004505}, {"id": 216, "seek": 144768, "start":
  1447.68, "end": 1455.3600000000001, "text": " been gone since May. So I''m sure
  that that makes an amazing change since then. But this", "tokens": [50364, 668,
  2780, 1670, 1891, 13, 407, 286, 478, 988, 300, 300, 1669, 364, 2243, 1319, 1670,
  550, 13, 583, 341, 50748], "temperature": 0.0, "avg_logprob": -0.3235819267504143,
  "compression_ratio": 1.603448275862069, "no_speech_prob": 0.00283340853638947},
  {"id": 217, "seek": 144768, "start": 1455.3600000000001, "end": 1462.3200000000002,
  "text": " Copilot completions was one of the first successful applications of large
  language models. And", "tokens": [50748, 11579, 31516, 1557, 626, 390, 472, 295,
  264, 700, 4406, 5821, 295, 2416, 2856, 5245, 13, 400, 51096], "temperature": 0.0,
  "avg_logprob": -0.3235819267504143, "compression_ratio": 1.603448275862069, "no_speech_prob":
  0.00283340853638947}, {"id": 218, "seek": 144768, "start": 1463.1200000000001, "end":
  1469.3600000000001, "text": " outside of the pure model, chat to BT, a large language
  model as a large language model service.", "tokens": [51136, 2380, 295, 264, 6075,
  2316, 11, 5081, 281, 31144, 11, 257, 2416, 2856, 2316, 382, 257, 2416, 2856, 2316,
  2643, 13, 51448], "temperature": 0.0, "avg_logprob": -0.3235819267504143, "compression_ratio":
  1.603448275862069, "no_speech_prob": 0.00283340853638947}, {"id": 219, "seek": 146936,
  "start": 1469.36, "end": 1478.8, "text": " Like this is, this was just the, I guess
  it was the first. So the implementation was actually", "tokens": [50364, 1743, 341,
  307, 11, 341, 390, 445, 264, 11, 286, 2041, 309, 390, 264, 700, 13, 407, 264, 11420,
  390, 767, 50836], "temperature": 0.0, "avg_logprob": -0.18281660218169724, "compression_ratio":
  1.5698324022346368, "no_speech_prob": 0.012653964571654797}, {"id": 220, "seek":
  146936, "start": 1478.8, "end": 1486.4799999999998, "text": " fairly simplistic.
  Basically, they, we weren''t using chat models at the time. Those didn''t", "tokens":
  [50836, 6457, 44199, 13, 8537, 11, 436, 11, 321, 4999, 380, 1228, 5081, 5245, 412,
  264, 565, 13, 3950, 994, 380, 51220], "temperature": 0.0, "avg_logprob": -0.18281660218169724,
  "compression_ratio": 1.5698324022346368, "no_speech_prob": 0.012653964571654797},
  {"id": 221, "seek": 146936, "start": 1486.4799999999998, "end": 1495.1999999999998,
  "text": " exist. We were only using completion models. Completion models, basically,
  I mean, your audience", "tokens": [51220, 2514, 13, 492, 645, 787, 1228, 19372,
  5245, 13, 31804, 313, 5245, 11, 1936, 11, 286, 914, 11, 428, 4034, 51656], "temperature":
  0.0, "avg_logprob": -0.18281660218169724, "compression_ratio": 1.5698324022346368,
  "no_speech_prob": 0.012653964571654797}, {"id": 222, "seek": 149520, "start": 1495.2,
  "end": 1501.76, "text": " probably knows this, but given the top part of a document,
  then all the model does. And it''s", "tokens": [50364, 1391, 3255, 341, 11, 457,
  2212, 264, 1192, 644, 295, 257, 4166, 11, 550, 439, 264, 2316, 775, 13, 400, 309,
  311, 50692], "temperature": 0.0, "avg_logprob": -0.11065304881394511, "compression_ratio":
  1.748878923766816, "no_speech_prob": 0.006104898639023304}, {"id": 223, "seek":
  149520, "start": 1501.76, "end": 1507.6000000000001, "text": " useful to think of
  the model this way. It simplifies things. All the model does is it picks the next",
  "tokens": [50692, 4420, 281, 519, 295, 264, 2316, 341, 636, 13, 467, 6883, 11221,
  721, 13, 1057, 264, 2316, 775, 307, 309, 16137, 264, 958, 50984], "temperature":
  0.0, "avg_logprob": -0.11065304881394511, "compression_ratio": 1.748878923766816,
  "no_speech_prob": 0.006104898639023304}, {"id": 224, "seek": 149520, "start": 1507.6000000000001,
  "end": 1512.64, "text": " token. What is the most likely token based on all these
  words before it? What''s the next token?", "tokens": [50984, 14862, 13, 708, 307,
  264, 881, 3700, 14862, 2361, 322, 439, 613, 2283, 949, 309, 30, 708, 311, 264, 958,
  14862, 30, 51236], "temperature": 0.0, "avg_logprob": -0.11065304881394511, "compression_ratio":
  1.748878923766816, "no_speech_prob": 0.006104898639023304}, {"id": 225, "seek":
  149520, "start": 1512.64, "end": 1518.64, "text": " And then you append that one
  and you did it again and again. And so the big aha moment that happened", "tokens":
  [51236, 400, 550, 291, 34116, 300, 472, 293, 291, 630, 309, 797, 293, 797, 13, 400,
  370, 264, 955, 47340, 1623, 300, 2011, 51536], "temperature": 0.0, "avg_logprob":
  -0.11065304881394511, "compression_ratio": 1.748878923766816, "no_speech_prob":
  0.006104898639023304}, {"id": 226, "seek": 151864, "start": 1519.44, "end": 1527.5200000000002,
  "text": " probably in 2019, as well before my time on Copilot was, look, I can take
  this top half the code", "tokens": [50404, 1391, 294, 6071, 11, 382, 731, 949, 452,
  565, 322, 11579, 31516, 390, 11, 574, 11, 286, 393, 747, 341, 1192, 1922, 264, 3089,
  50808], "temperature": 0.0, "avg_logprob": -0.2215027364095052, "compression_ratio":
  1.4623115577889447, "no_speech_prob": 0.0026177691761404276}, {"id": 227, "seek":
  151864, "start": 1527.5200000000002, "end": 1534.8000000000002, "text": " down to
  the function. And the answer, you know, the completion that it makes is surprisingly
  good.", "tokens": [50808, 760, 281, 264, 2445, 13, 400, 264, 1867, 11, 291, 458,
  11, 264, 19372, 300, 309, 1669, 307, 17600, 665, 13, 51172], "temperature": 0.0,
  "avg_logprob": -0.2215027364095052, "compression_ratio": 1.4623115577889447, "no_speech_prob":
  0.0026177691761404276}, {"id": 228, "seek": 151864, "start": 1536.0, "end": 1543.68,
  "text": " So like maybe it''s time to just wrap or wrap up for application around
  it. And then after that,", "tokens": [51232, 407, 411, 1310, 309, 311, 565, 281,
  445, 7019, 420, 7019, 493, 337, 3861, 926, 309, 13, 400, 550, 934, 300, 11, 51616],
  "temperature": 0.0, "avg_logprob": -0.2215027364095052, "compression_ratio": 1.4623115577889447,
  "no_speech_prob": 0.0026177691761404276}, {"id": 229, "seek": 154368, "start": 1544.64,
  "end": 1552.24, "text": " everybody''s learning these lessons at this point. But
  it''s all about the context that you put", "tokens": [50412, 2201, 311, 2539, 613,
  8820, 412, 341, 935, 13, 583, 309, 311, 439, 466, 264, 4319, 300, 291, 829, 50792],
  "temperature": 0.0, "avg_logprob": -0.14130511965070452, "compression_ratio": 1.5573770491803278,
  "no_speech_prob": 0.0016411576652899384}, {"id": 230, "seek": 154368, "start": 1552.24,
  "end": 1558.16, "text": " around it and how you present it so that the model can
  make sense of it. At the time that I started", "tokens": [50792, 926, 309, 293,
  577, 291, 1974, 309, 370, 300, 264, 2316, 393, 652, 2020, 295, 309, 13, 1711, 264,
  565, 300, 286, 1409, 51088], "temperature": 0.0, "avg_logprob": -0.14130511965070452,
  "compression_ratio": 1.5573770491803278, "no_speech_prob": 0.0016411576652899384},
  {"id": 231, "seek": 154368, "start": 1558.16, "end": 1564.8, "text": " with Copilot,
  we were still using the completion models. And it was, the context itself was",
  "tokens": [51088, 365, 11579, 31516, 11, 321, 645, 920, 1228, 264, 19372, 5245,
  13, 400, 309, 390, 11, 264, 4319, 2564, 390, 51420], "temperature": 0.0, "avg_logprob":
  -0.14130511965070452, "compression_ratio": 1.5573770491803278, "no_speech_prob":
  0.0016411576652899384}, {"id": 232, "seek": 156480, "start": 1565.44, "end": 1576.24,
  "text": " 2048 tokens, I think. So just tiny, tiny, tiny window. And so a huge focus
  at the time was how to", "tokens": [50396, 945, 13318, 22667, 11, 286, 519, 13,
  407, 445, 5870, 11, 5870, 11, 5870, 4910, 13, 400, 370, 257, 2603, 1879, 412, 264,
  565, 390, 577, 281, 50936], "temperature": 0.0, "avg_logprob": -0.10980531484773844,
  "compression_ratio": 1.6196581196581197, "no_speech_prob": 0.005099656525999308},
  {"id": 233, "seek": 156480, "start": 1576.24, "end": 1582.1599999999999, "text":
  " take all the things that we thought might be useful and squeeze it down into this
  tiny space,", "tokens": [50936, 747, 439, 264, 721, 300, 321, 1194, 1062, 312, 4420,
  293, 13578, 309, 760, 666, 341, 5870, 1901, 11, 51232], "temperature": 0.0, "avg_logprob":
  -0.10980531484773844, "compression_ratio": 1.6196581196581197, "no_speech_prob":
  0.005099656525999308}, {"id": 234, "seek": 156480, "start": 1582.1599999999999,
  "end": 1588.24, "text": " just, you know, actually make sure you''ve nailed it.
  Because not only do you have to fit the", "tokens": [51232, 445, 11, 291, 458, 11,
  767, 652, 988, 291, 600, 30790, 309, 13, 1436, 406, 787, 360, 291, 362, 281, 3318,
  264, 51536], "temperature": 0.0, "avg_logprob": -0.10980531484773844, "compression_ratio":
  1.6196581196581197, "no_speech_prob": 0.005099656525999308}, {"id": 235, "seek":
  156480, "start": 1588.8799999999999, "end": 1594.48, "text": " prompt into this
  2048 tokens, but whatever the completions are, that''s, you know, that they''re",
  "tokens": [51568, 12391, 666, 341, 945, 13318, 22667, 11, 457, 2035, 264, 1557,
  626, 366, 11, 300, 311, 11, 291, 458, 11, 300, 436, 434, 51848], "temperature":
  0.0, "avg_logprob": -0.10980531484773844, "compression_ratio": 1.6196581196581197,
  "no_speech_prob": 0.005099656525999308}, {"id": 236, "seek": 159448, "start": 1594.48,
  "end": 1600.4, "text": " sharing the same windows. You can move that line up and
  down, but it''s always in 2048. So there wasn''t,", "tokens": [50364, 5414, 264,
  912, 9309, 13, 509, 393, 1286, 300, 1622, 493, 293, 760, 11, 457, 309, 311, 1009,
  294, 945, 13318, 13, 407, 456, 2067, 380, 11, 50660], "temperature": 0.0, "avg_logprob":
  -0.24075753081078624, "compression_ratio": 1.613821138211382, "no_speech_prob":
  0.0001363903866149485}, {"id": 237, "seek": 159448, "start": 1600.4, "end": 1608.96,
  "text": " there wasn''t. The ingredients were pretty simple. The file that you''re
  looking at is obviously the", "tokens": [50660, 456, 2067, 380, 13, 440, 6952, 645,
  1238, 2199, 13, 440, 3991, 300, 291, 434, 1237, 412, 307, 2745, 264, 51088], "temperature":
  0.0, "avg_logprob": -0.24075753081078624, "compression_ratio": 1.613821138211382,
  "no_speech_prob": 0.0001363903866149485}, {"id": 238, "seek": 159448, "start": 1608.96,
  "end": 1614.64, "text": " most important thing. If the file is long, which I''ll
  often I''ll log in to that 48, then the text", "tokens": [51088, 881, 1021, 551,
  13, 759, 264, 3991, 307, 938, 11, 597, 286, 603, 2049, 286, 603, 3565, 294, 281,
  300, 11174, 11, 550, 264, 2487, 51372], "temperature": 0.0, "avg_logprob": -0.24075753081078624,
  "compression_ratio": 1.613821138211382, "no_speech_prob": 0.0001363903866149485},
  {"id": 239, "seek": 159448, "start": 1614.64, "end": 1623.3600000000001, "text":
  " right above the cursor is an important thing. There are some initial work with
  like the, they''re", "tokens": [51372, 558, 3673, 264, 28169, 307, 364, 1021, 551,
  13, 821, 366, 512, 5883, 589, 365, 411, 264, 11, 436, 434, 51808], "temperature":
  0.0, "avg_logprob": -0.24075753081078624, "compression_ratio": 1.613821138211382,
  "no_speech_prob": 0.0001363903866149485}, {"id": 240, "seek": 162336, "start": 1623.36,
  "end": 1627.1999999999998, "text": " still called fill in the middle models, which
  they don''t need this anymore because all the models", "tokens": [50364, 920, 1219,
  2836, 294, 264, 2808, 5245, 11, 597, 436, 500, 380, 643, 341, 3602, 570, 439, 264,
  5245, 50556], "temperature": 0.0, "avg_logprob": -0.21598333653395738, "compression_ratio":
  1.7536764705882353, "no_speech_prob": 0.004661222919821739}, {"id": 241, "seek":
  162336, "start": 1627.1999999999998, "end": 1632.1599999999999, "text": " are so
  free. You don''t need a specialized model for this. But you could, you know, you
  could say the", "tokens": [50556, 366, 370, 1737, 13, 509, 500, 380, 643, 257, 19813,
  2316, 337, 341, 13, 583, 291, 727, 11, 291, 458, 11, 291, 727, 584, 264, 50804],
  "temperature": 0.0, "avg_logprob": -0.21598333653395738, "compression_ratio": 1.7536764705882353,
  "no_speech_prob": 0.004661222919821739}, {"id": 242, "seek": 162336, "start": 1632.1599999999999,
  "end": 1636.56, "text": " prefix and the suffix, and it would do a good job about
  filling in the middle. So the suffix was", "tokens": [50804, 46969, 293, 264, 3889,
  970, 11, 293, 309, 576, 360, 257, 665, 1691, 466, 10623, 294, 264, 2808, 13, 407,
  264, 3889, 970, 390, 51024], "temperature": 0.0, "avg_logprob": -0.21598333653395738,
  "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.004661222919821739},
  {"id": 243, "seek": 162336, "start": 1636.56, "end": 1642.3999999999999, "text":
  " also an important part of the context. Where do you stop this thing? And then
  as the model,", "tokens": [51024, 611, 364, 1021, 644, 295, 264, 4319, 13, 2305,
  360, 291, 1590, 341, 551, 30, 400, 550, 382, 264, 2316, 11, 51316], "temperature":
  0.0, "avg_logprob": -0.21598333653395738, "compression_ratio": 1.7536764705882353,
  "no_speech_prob": 0.004661222919821739}, {"id": 244, "seek": 162336, "start": 1643.12,
  "end": 1648.0, "text": " crew is a context-based crew a little bit, we can start
  sticking in extra things. And so,", "tokens": [51352, 7260, 307, 257, 4319, 12,
  6032, 7260, 257, 707, 857, 11, 321, 393, 722, 13465, 294, 2857, 721, 13, 400, 370,
  11, 51596], "temperature": 0.0, "avg_logprob": -0.21598333653395738, "compression_ratio":
  1.7536764705882353, "no_speech_prob": 0.004661222919821739}, {"id": 245, "seek":
  164800, "start": 1648.8, "end": 1653.92, "text": " you know, you start with little
  bitty things. These models were not trained on,", "tokens": [50404, 291, 458, 11,
  291, 722, 365, 707, 272, 10016, 721, 13, 1981, 5245, 645, 406, 8895, 322, 11, 50660],
  "temperature": 0.0, "avg_logprob": -0.17921436916698108, "compression_ratio": 1.638095238095238,
  "no_speech_prob": 0.0006231710431165993}, {"id": 246, "seek": 164800, "start": 1656.48,
  "end": 1662.4, "text": " these models were trained on code, but they didn''t necessarily
  have the context around the code.", "tokens": [50788, 613, 5245, 645, 8895, 322,
  3089, 11, 457, 436, 994, 380, 4725, 362, 264, 4319, 926, 264, 3089, 13, 51084],
  "temperature": 0.0, "avg_logprob": -0.17921436916698108, "compression_ratio": 1.638095238095238,
  "no_speech_prob": 0.0006231710431165993}, {"id": 247, "seek": 164800, "start": 1662.4,
  "end": 1666.08, "text": " So the first easy thing to stick in is you could do a
  shabang at the top,", "tokens": [51084, 407, 264, 700, 1858, 551, 281, 2897, 294,
  307, 291, 727, 360, 257, 402, 455, 656, 412, 264, 1192, 11, 51268], "temperature":
  0.0, "avg_logprob": -0.17921436916698108, "compression_ratio": 1.638095238095238,
  "no_speech_prob": 0.0006231710431165993}, {"id": 248, "seek": 164800, "start": 1666.08,
  "end": 1674.4, "text": " protecting a comment that says, here''s the path for this,
  this file. And that gives the model", "tokens": [51268, 12316, 257, 2871, 300, 1619,
  11, 510, 311, 264, 3100, 337, 341, 11, 341, 3991, 13, 400, 300, 2709, 264, 2316,
  51684], "temperature": 0.0, "avg_logprob": -0.17921436916698108, "compression_ratio":
  1.638095238095238, "no_speech_prob": 0.0006231710431165993}, {"id": 249, "seek":
  167440, "start": 1674.4, "end": 1680.24, "text": " context about where this lives
  in the context of everything else. A big breakthrough that", "tokens": [50364, 4319,
  466, 689, 341, 2909, 294, 264, 4319, 295, 1203, 1646, 13, 316, 955, 22397, 300,
  50656], "temperature": 0.0, "avg_logprob": -0.22268356244588636, "compression_ratio":
  1.526530612244898, "no_speech_prob": 0.0016933761071413755}, {"id": 250, "seek":
  167440, "start": 1681.8400000000001, "end": 1688.88, "text": " Albert Dealer, Mike
  Coother, pioneered was the neighboring tab stuff. And I think this is all", "tokens":
  [50736, 20812, 1346, 17148, 11, 6602, 3066, 802, 11, 19761, 4073, 390, 264, 31521,
  4421, 1507, 13, 400, 286, 519, 341, 307, 439, 51088], "temperature": 0.0, "avg_logprob":
  -0.22268356244588636, "compression_ratio": 1.526530612244898, "no_speech_prob":
  0.0016933761071413755}, {"id": 251, "seek": 167440, "start": 1689.6000000000001,
  "end": 1695.1200000000001, "text": " common sense these days. But basically, when
  you, as a human, are using an IDE, you open up the", "tokens": [51124, 2689, 2020,
  613, 1708, 13, 583, 1936, 11, 562, 291, 11, 382, 257, 1952, 11, 366, 1228, 364,
  40930, 11, 291, 1269, 493, 264, 51400], "temperature": 0.0, "avg_logprob": -0.22268356244588636,
  "compression_ratio": 1.526530612244898, "no_speech_prob": 0.0016933761071413755},
  {"id": 252, "seek": 167440, "start": 1695.1200000000001, "end": 1701.2, "text":
  " file you''re working on. But you also open up other files for reference. So, duh,
  why don''t we,", "tokens": [51400, 3991, 291, 434, 1364, 322, 13, 583, 291, 611,
  1269, 493, 661, 7098, 337, 6408, 13, 407, 11, 43763, 11, 983, 500, 380, 321, 11,
  51704], "temperature": 0.0, "avg_logprob": -0.22268356244588636, "compression_ratio":
  1.526530612244898, "no_speech_prob": 0.0016933761071413755}, {"id": 253, "seek":
  170120, "start": 1701.2, "end": 1705.44, "text": " you know, do that ourselves.
  And the initial implementations of this that, you know, probably got", "tokens":
  [50364, 291, 458, 11, 360, 300, 4175, 13, 400, 264, 5883, 4445, 763, 295, 341, 300,
  11, 291, 458, 11, 1391, 658, 50576], "temperature": 0.0, "avg_logprob": -0.1722081164096264,
  "compression_ratio": 1.6488888888888888, "no_speech_prob": 0.0063216807320714},
  {"id": 254, "seek": 170120, "start": 1705.44, "end": 1711.04, "text": " not better
  at this point. It was simple. It was like, look at the text right around the cursor.",
  "tokens": [50576, 406, 1101, 412, 341, 935, 13, 467, 390, 2199, 13, 467, 390, 411,
  11, 574, 412, 264, 2487, 558, 926, 264, 28169, 13, 50856], "temperature": 0.0, "avg_logprob":
  -0.1722081164096264, "compression_ratio": 1.6488888888888888, "no_speech_prob":
  0.0063216807320714}, {"id": 255, "seek": 170120, "start": 1711.04, "end": 1718.0800000000002,
  "text": " And then search these files for similar text. And in your timing, 2048
  token space,", "tokens": [50856, 400, 550, 3164, 613, 7098, 337, 2531, 2487, 13,
  400, 294, 428, 10822, 11, 945, 13318, 14862, 1901, 11, 51208], "temperature": 0.0,
  "avg_logprob": -0.1722081164096264, "compression_ratio": 1.6488888888888888, "no_speech_prob":
  0.0063216807320714}, {"id": 256, "seek": 170120, "start": 1718.0800000000002, "end":
  1723.1200000000001, "text": " you have any room for any of these snippets, then
  you can chunk other stuff into the context.", "tokens": [51208, 291, 362, 604, 1808,
  337, 604, 295, 613, 35623, 1385, 11, 550, 291, 393, 16635, 661, 1507, 666, 264,
  4319, 13, 51460], "temperature": 0.0, "avg_logprob": -0.1722081164096264, "compression_ratio":
  1.6488888888888888, "no_speech_prob": 0.0063216807320714}, {"id": 257, "seek": 172312,
  "start": 1723.6, "end": 1731.84, "text": " You have to be careful how you present
  that. You can''t just, you know, have random scraps of text", "tokens": [50388,
  509, 362, 281, 312, 5026, 577, 291, 1974, 300, 13, 509, 393, 380, 445, 11, 291,
  458, 11, 362, 4974, 45204, 295, 2487, 50800], "temperature": 0.0, "avg_logprob":
  -0.19006152905915913, "compression_ratio": 1.6891891891891893, "no_speech_prob":
  0.011846904642879963}, {"id": 258, "seek": 172312, "start": 1732.8799999999999,
  "end": 1738.1599999999999, "text": " that are like, you know, partial function implementations.
  Because that will prime the model to", "tokens": [50852, 300, 366, 411, 11, 291,
  458, 11, 14641, 2445, 4445, 763, 13, 1436, 300, 486, 5835, 264, 2316, 281, 51116],
  "temperature": 0.0, "avg_logprob": -0.19006152905915913, "compression_ratio": 1.6891891891891893,
  "no_speech_prob": 0.011846904642879963}, {"id": 259, "seek": 172312, "start": 1739.04,
  "end": 1743.6799999999998, "text": " implement partial functions. Like, it''ll,
  you know, it''ll just iterate the same gross pattern.", "tokens": [51160, 4445,
  14641, 6828, 13, 1743, 11, 309, 603, 11, 291, 458, 11, 309, 603, 445, 44497, 264,
  912, 11367, 5102, 13, 51392], "temperature": 0.0, "avg_logprob": -0.19006152905915913,
  "compression_ratio": 1.6891891891891893, "no_speech_prob": 0.011846904642879963},
  {"id": 260, "seek": 172312, "start": 1743.6799999999998, "end": 1750.32, "text":
  " It seems above. So you do things that make it look more like code. You say, here
  is an", "tokens": [51392, 467, 2544, 3673, 13, 407, 291, 360, 721, 300, 652, 309,
  574, 544, 411, 3089, 13, 509, 584, 11, 510, 307, 364, 51724], "temperature": 0.0,
  "avg_logprob": -0.19006152905915913, "compression_ratio": 1.6891891891891893, "no_speech_prob":
  0.011846904642879963}, {"id": 261, "seek": 175032, "start": 1750.32, "end": 1757.28,
  "text": " interesting, you skip a code from this file in the comment so that it''s
  still, you know,", "tokens": [50364, 1880, 11, 291, 10023, 257, 3089, 490, 341,
  3991, 294, 264, 2871, 370, 300, 309, 311, 920, 11, 291, 458, 11, 50712], "temperature":
  0.0, "avg_logprob": -0.17527279955275515, "compression_ratio": 1.5854700854700854,
  "no_speech_prob": 0.0009059800067916512}, {"id": 262, "seek": 175032, "start": 1757.28,
  "end": 1764.56, "text": " importantly, so it''s still valid syntax at the end of
  it. And voila, the rest of this history,", "tokens": [50712, 8906, 11, 370, 309,
  311, 920, 7363, 28431, 412, 264, 917, 295, 309, 13, 400, 45565, 11, 264, 1472, 295,
  341, 2503, 11, 51076], "temperature": 0.0, "avg_logprob": -0.17527279955275515,
  "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.0009059800067916512},
  {"id": 263, "seek": 175032, "start": 1764.56, "end": 1773.4399999999998, "text":
  " we came out with a really impactful product that no one had seen anything like
  it before. And", "tokens": [51076, 321, 1361, 484, 365, 257, 534, 30842, 1674, 300,
  572, 472, 632, 1612, 1340, 411, 309, 949, 13, 400, 51520], "temperature": 0.0, "avg_logprob":
  -0.17527279955275515, "compression_ratio": 1.5854700854700854, "no_speech_prob":
  0.0009059800067916512}, {"id": 264, "seek": 175032, "start": 1773.4399999999998,
  "end": 1779.6799999999998, "text": " it''s certainly changed the way I code. I''m
  much quicker and probably dumber at the same time.", "tokens": [51520, 309, 311,
  3297, 3105, 264, 636, 286, 3089, 13, 286, 478, 709, 16255, 293, 1391, 274, 4182,
  412, 264, 912, 565, 13, 51832], "temperature": 0.0, "avg_logprob": -0.17527279955275515,
  "compression_ratio": 1.5854700854700854, "no_speech_prob": 0.0009059800067916512},
  {"id": 265, "seek": 178032, "start": 1781.12, "end": 1784.3999999999999, "text":
  " Yeah, it''s been an interesting experience.", "tokens": [50404, 865, 11, 309,
  311, 668, 364, 1880, 1752, 13, 50568], "temperature": 0.0, "avg_logprob": -0.2303636683974155,
  "compression_ratio": 1.6515151515151516, "no_speech_prob": 0.028755618259310722},
  {"id": 266, "seek": 178032, "start": 1784.3999999999999, "end": 1790.96, "text":
  " Oh, maybe more smart because you get to do more things, right? Like you can, I
  guess you can,", "tokens": [50568, 876, 11, 1310, 544, 4069, 570, 291, 483, 281,
  360, 544, 721, 11, 558, 30, 1743, 291, 393, 11, 286, 2041, 291, 393, 11, 50896],
  "temperature": 0.0, "avg_logprob": -0.2303636683974155, "compression_ratio": 1.6515151515151516,
  "no_speech_prob": 0.028755618259310722}, {"id": 267, "seek": 178032, "start": 1790.96,
  "end": 1796.72, "text": " you can get hired, like you can achieve, you know, larger
  heights and then, like experiment", "tokens": [50896, 291, 393, 483, 13144, 11,
  411, 291, 393, 4584, 11, 291, 458, 11, 4833, 25930, 293, 550, 11, 411, 5120, 51184],
  "temperature": 0.0, "avg_logprob": -0.2303636683974155, "compression_ratio": 1.6515151515151516,
  "no_speech_prob": 0.028755618259310722}, {"id": 268, "seek": 178032, "start": 1796.72,
  "end": 1802.56, "text": " way, you need to experiment, right? And not where it feels
  maybe more mundane. As long as the code", "tokens": [51184, 636, 11, 291, 643, 281,
  5120, 11, 558, 30, 400, 406, 689, 309, 3417, 1310, 544, 43497, 13, 1018, 938, 382,
  264, 3089, 51476], "temperature": 0.0, "avg_logprob": -0.2303636683974155, "compression_ratio":
  1.6515151515151516, "no_speech_prob": 0.028755618259310722}, {"id": 269, "seek":
  180256, "start": 1802.56, "end": 1808.8799999999999, "text": " works and like, I
  don''t know, there are no security holes in it and stuff like that, which", "tokens":
  [50364, 1985, 293, 411, 11, 286, 500, 380, 458, 11, 456, 366, 572, 3825, 8118, 294,
  309, 293, 1507, 411, 300, 11, 597, 50680], "temperature": 0.0, "avg_logprob": -0.14798387568047705,
  "compression_ratio": 1.5416666666666667, "no_speech_prob": 0.1485767811536789},
  {"id": 270, "seek": 180256, "start": 1809.9199999999998, "end": 1814.8, "text":
  " would need to be checked separately, I guess. Anyway, that''s very interesting.
  But to close", "tokens": [50732, 576, 643, 281, 312, 10033, 14759, 11, 286, 2041,
  13, 5684, 11, 300, 311, 588, 1880, 13, 583, 281, 1998, 50976], "temperature": 0.0,
  "avg_logprob": -0.14798387568047705, "compression_ratio": 1.5416666666666667, "no_speech_prob":
  0.1485767811536789}, {"id": 271, "seek": 180256, "start": 1814.8, "end": 1820.0,
  "text": " up the loop there, like I''m just trying to understand, you said you focused
  on keyword search,", "tokens": [50976, 493, 264, 6367, 456, 11, 411, 286, 478, 445,
  1382, 281, 1223, 11, 291, 848, 291, 5178, 322, 20428, 3164, 11, 51236], "temperature":
  0.0, "avg_logprob": -0.14798387568047705, "compression_ratio": 1.5416666666666667,
  "no_speech_prob": 0.1485767811536789}, {"id": 272, "seek": 180256, "start": 1820.0,
  "end": 1825.2, "text": " right? So you, you owned the elastic search sort of pipeline.
  Can you, if you''re comfortable", "tokens": [51236, 558, 30, 407, 291, 11, 291,
  11684, 264, 17115, 3164, 1333, 295, 15517, 13, 1664, 291, 11, 498, 291, 434, 4619,
  51496], "temperature": 0.0, "avg_logprob": -0.14798387568047705, "compression_ratio":
  1.5416666666666667, "no_speech_prob": 0.1485767811536789}, {"id": 273, "seek": 182520,
  "start": 1825.2, "end": 1834.8, "text": " disclosing that, like, would that index
  the visible code in the ID somehow so that you can,", "tokens": [50364, 17092, 6110,
  300, 11, 411, 11, 576, 300, 8186, 264, 8974, 3089, 294, 264, 7348, 6063, 370, 300,
  291, 393, 11, 50844], "temperature": 0.0, "avg_logprob": -0.21798915247763356, "compression_ratio":
  1.56, "no_speech_prob": 0.0045714848674833775}, {"id": 274, "seek": 182520, "start":
  1834.8, "end": 1838.24, "text": " or what was the role of that in the whole chain,
  hope, pipeline?", "tokens": [50844, 420, 437, 390, 264, 3090, 295, 300, 294, 264,
  1379, 5021, 11, 1454, 11, 15517, 30, 51016], "temperature": 0.0, "avg_logprob":
  -0.21798915247763356, "compression_ratio": 1.56, "no_speech_prob": 0.0045714848674833775},
  {"id": 275, "seek": 182520, "start": 1840.8, "end": 1846.48, "text": " You''re asking
  a lot of questions that don''t quite seek well on my actual experience. Let me see,",
  "tokens": [51144, 509, 434, 3365, 257, 688, 295, 1651, 300, 500, 380, 1596, 8075,
  731, 322, 452, 3539, 1752, 13, 961, 385, 536, 11, 51428], "temperature": 0.0, "avg_logprob":
  -0.21798915247763356, "compression_ratio": 1.56, "no_speech_prob": 0.0045714848674833775},
  {"id": 276, "seek": 182520, "start": 1846.48, "end": 1851.6000000000001, "text":
  " if I can take your question and you take it just a little bit. When I came to
  GitHub, I worked on", "tokens": [51428, 498, 286, 393, 747, 428, 1168, 293, 291,
  747, 309, 445, 257, 707, 857, 13, 1133, 286, 1361, 281, 23331, 11, 286, 2732, 322,
  51684], "temperature": 0.0, "avg_logprob": -0.21798915247763356, "compression_ratio":
  1.56, "no_speech_prob": 0.0045714848674833775}, {"id": 277, "seek": 185160, "start":
  1851.6, "end": 1858.24, "text": " code search, which was keyword, like, school search
  for the entire code corpus.", "tokens": [50364, 3089, 3164, 11, 597, 390, 20428,
  11, 411, 11, 1395, 3164, 337, 264, 2302, 3089, 1181, 31624, 13, 50696], "temperature":
  0.0, "avg_logprob": -0.17702709544788708, "compression_ratio": 1.662037037037037,
  "no_speech_prob": 0.0016760448925197124}, {"id": 278, "seek": 185160, "start": 1859.76,
  "end": 1865.28, "text": " And that was really cool work. But that has since moved
  to that, they''ve rebuilt the", "tokens": [50772, 400, 300, 390, 534, 1627, 589,
  13, 583, 300, 575, 1670, 4259, 281, 300, 11, 436, 600, 38532, 264, 51048], "temperature":
  0.0, "avg_logprob": -0.17702709544788708, "compression_ratio": 1.662037037037037,
  "no_speech_prob": 0.0016760448925197124}, {"id": 279, "seek": 185160, "start": 1865.28,
  "end": 1873.28, "text": " whole system yet again. And it''s a really amazing engine,
  the proprietary engine that''s effectively", "tokens": [51048, 1379, 1185, 1939,
  797, 13, 400, 309, 311, 257, 534, 2243, 2848, 11, 264, 38992, 2848, 300, 311, 8659,
  51448], "temperature": 0.0, "avg_logprob": -0.17702709544788708, "compression_ratio":
  1.662037037037037, "no_speech_prob": 0.0016760448925197124}, {"id": 280, "seek":
  185160, "start": 1874.0, "end": 1880.8799999999999, "text": " grip at fantastically
  massive scale. But that said, that code engine, the one that I built in,", "tokens":
  [51484, 12007, 412, 4115, 22808, 5994, 4373, 13, 583, 300, 848, 11, 300, 3089, 2848,
  11, 264, 472, 300, 286, 3094, 294, 11, 51828], "temperature": 0.0, "avg_logprob":
  -0.17702709544788708, "compression_ratio": 1.662037037037037, "no_speech_prob":
  0.0016760448925197124}, {"id": 281, "seek": 188088, "start": 1880.88, "end": 1887.1200000000001,
  "text": " even the one that came after it, are not the things that are most beneficial
  for some of the", "tokens": [50364, 754, 264, 472, 300, 1361, 934, 309, 11, 366,
  406, 264, 721, 300, 366, 881, 14072, 337, 512, 295, 264, 50676], "temperature":
  0.0, "avg_logprob": -0.2272092718827097, "compression_ratio": 1.5921052631578947,
  "no_speech_prob": 0.00038533390033990145}, {"id": 282, "seek": 188088, "start":
  1887.1200000000001, "end": 1891.6000000000001, "text": " applications that KhoPy
  that has in the editor. And they do different things for that.", "tokens": [50676,
  5821, 300, 591, 1289, 47, 88, 300, 575, 294, 264, 9839, 13, 400, 436, 360, 819,
  721, 337, 300, 13, 50900], "temperature": 0.0, "avg_logprob": -0.2272092718827097,
  "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.00038533390033990145},
  {"id": 283, "seek": 188088, "start": 1892.96, "end": 1901.5200000000002, "text":
  " They''re, for example, if you''re on the web app side, there are things, now I
  need even in the", "tokens": [50968, 814, 434, 11, 337, 1365, 11, 498, 291, 434,
  322, 264, 3670, 724, 1252, 11, 456, 366, 721, 11, 586, 286, 643, 754, 294, 264,
  51396], "temperature": 0.0, "avg_logprob": -0.2272092718827097, "compression_ratio":
  1.5921052631578947, "no_speech_prob": 0.00038533390033990145}, {"id": 284, "seek":
  188088, "start": 1901.5200000000002, "end": 1908.48, "text": " ID, I''m remembering
  stuff from six months ago, they do just in time like vector embedding", "tokens":
  [51396, 7348, 11, 286, 478, 20719, 1507, 490, 2309, 2493, 2057, 11, 436, 360, 445,
  294, 565, 411, 8062, 12240, 3584, 51744], "temperature": 0.0, "avg_logprob": -0.2272092718827097,
  "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.00038533390033990145},
  {"id": 285, "seek": 190848, "start": 1908.48, "end": 1915.84, "text": " vector storage
  and stuff. Vectors are a lot better for certain types of code search where you''re",
  "tokens": [50364, 8062, 6725, 293, 1507, 13, 691, 557, 830, 366, 257, 688, 1101,
  337, 1629, 3467, 295, 3089, 3164, 689, 291, 434, 50732], "temperature": 0.0, "avg_logprob":
  -0.15836210250854493, "compression_ratio": 1.7567567567567568, "no_speech_prob":
  0.001108266762457788}, {"id": 286, "seek": 190848, "start": 1915.84, "end": 1922.16,
  "text": " finding code that is about something. Whereas, lexical search is a lot
  better when you''re finding code", "tokens": [50732, 5006, 3089, 300, 307, 466,
  746, 13, 13813, 11, 476, 87, 804, 3164, 307, 257, 688, 1101, 562, 291, 434, 5006,
  3089, 51048], "temperature": 0.0, "avg_logprob": -0.15836210250854493, "compression_ratio":
  1.7567567567567568, "no_speech_prob": 0.001108266762457788}, {"id": 287, "seek":
  190848, "start": 1922.16, "end": 1929.92, "text": " that matches this exact string.
  And I think everyone in code outside of code, everyone everywhere", "tokens": [51048,
  300, 10676, 341, 1900, 6798, 13, 400, 286, 519, 1518, 294, 3089, 2380, 295, 3089,
  11, 1518, 5315, 51436], "temperature": 0.0, "avg_logprob": -0.15836210250854493,
  "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.001108266762457788},
  {"id": 288, "seek": 190848, "start": 1929.92, "end": 1936.48, "text": " is still
  kind of wrestling with this. There''s no one data structure that does all that stuff",
  "tokens": [51436, 307, 920, 733, 295, 19274, 365, 341, 13, 821, 311, 572, 472, 1412,
  3877, 300, 775, 439, 300, 1507, 51764], "temperature": 0.0, "avg_logprob": -0.15836210250854493,
  "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.001108266762457788},
  {"id": 289, "seek": 193648, "start": 1936.56, "end": 1942.8, "text": " ideally.
  And I think we were wrestling with that inside KhoPylet as well.", "tokens": [50368,
  22915, 13, 400, 286, 519, 321, 645, 19274, 365, 300, 1854, 591, 1289, 47, 88, 2631,
  382, 731, 13, 50680], "temperature": 0.0, "avg_logprob": -0.19335080218571488, "compression_ratio":
  1.5598290598290598, "no_speech_prob": 0.02517448365688324}, {"id": 290, "seek":
  193648, "start": 1943.52, "end": 1947.28, "text": " Yeah, but I guess, yeah, I understood
  your point and I probably missed that in your explanation", "tokens": [50716, 865,
  11, 457, 286, 2041, 11, 1338, 11, 286, 7320, 428, 935, 293, 286, 1391, 6721, 300,
  294, 428, 10835, 50904], "temperature": 0.0, "avg_logprob": -0.19335080218571488,
  "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.02517448365688324},
  {"id": 291, "seek": 193648, "start": 1947.28, "end": 1952.08, "text": " that you
  worked on code search and not on the generation. That''s why in code search, you
  did use", "tokens": [50904, 300, 291, 2732, 322, 3089, 3164, 293, 406, 322, 264,
  5125, 13, 663, 311, 983, 294, 3089, 3164, 11, 291, 630, 764, 51144], "temperature":
  0.0, "avg_logprob": -0.19335080218571488, "compression_ratio": 1.5598290598290598,
  "no_speech_prob": 0.02517448365688324}, {"id": 292, "seek": 193648, "start": 1952.08,
  "end": 1958.0, "text": " the elastic search index. But like what I was imagining
  and I''m completely clueless in this topic,", "tokens": [51144, 264, 17115, 3164,
  8186, 13, 583, 411, 437, 286, 390, 27798, 293, 286, 478, 2584, 596, 3483, 442, 294,
  341, 4829, 11, 51440], "temperature": 0.0, "avg_logprob": -0.19335080218571488,
  "compression_ratio": 1.5598290598290598, "no_speech_prob": 0.02517448365688324},
  {"id": 293, "seek": 195800, "start": 1958.0, "end": 1967.52, "text": " is that by
  the virtue of LLM being trained on bunch of code, let''s say open source code that",
  "tokens": [50364, 307, 300, 538, 264, 20816, 295, 441, 43, 44, 885, 8895, 322, 3840,
  295, 3089, 11, 718, 311, 584, 1269, 4009, 3089, 300, 50840], "temperature": 0.0,
  "avg_logprob": -0.13879903625039494, "compression_ratio": 1.588235294117647, "no_speech_prob":
  0.0033970868680626154}, {"id": 294, "seek": 195800, "start": 1967.52, "end": 1975.44,
  "text": " you can train on license wise, if the user is asking something that reminds
  the code that had", "tokens": [50840, 291, 393, 3847, 322, 10476, 10829, 11, 498,
  264, 4195, 307, 3365, 746, 300, 12025, 264, 3089, 300, 632, 51236], "temperature":
  0.0, "avg_logprob": -0.13879903625039494, "compression_ratio": 1.588235294117647,
  "no_speech_prob": 0.0033970868680626154}, {"id": 295, "seek": 195800, "start": 1975.44,
  "end": 1984.24, "text": " written before, wouldn''t it make sense to try to find
  that code and kind of somehow", "tokens": [51236, 3720, 949, 11, 2759, 380, 309,
  652, 2020, 281, 853, 281, 915, 300, 3089, 293, 733, 295, 6063, 51676], "temperature":
  0.0, "avg_logprob": -0.13879903625039494, "compression_ratio": 1.588235294117647,
  "no_speech_prob": 0.0033970868680626154}, {"id": 296, "seek": 198424, "start": 1984.8,
  "end": 1993.44, "text": " you know, rag on it with LLM or is it completely different
  than how you did it?", "tokens": [50392, 291, 458, 11, 17539, 322, 309, 365, 441,
  43, 44, 420, 307, 309, 2584, 819, 813, 577, 291, 630, 309, 30, 50824], "temperature":
  0.0, "avg_logprob": -0.38978754679361977, "compression_ratio": 1.4673913043478262,
  "no_speech_prob": 0.03426869958639145}, {"id": 297, "seek": 198424, "start": 1996.0,
  "end": 2003.92, "text": " The like at this point, we''ve moved to much, it''s you
  know, as of May my left, they''ve moved to", "tokens": [50952, 440, 411, 412, 341,
  935, 11, 321, 600, 4259, 281, 709, 11, 309, 311, 291, 458, 11, 382, 295, 1891, 452,
  1411, 11, 436, 600, 4259, 281, 51348], "temperature": 0.0, "avg_logprob": -0.38978754679361977,
  "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.03426869958639145},
  {"id": 298, "seek": 198424, "start": 2003.92, "end": 2012.32, "text": " much larger
  models. And then the models themselves have read not only all the code and GitHub,",
  "tokens": [51348, 709, 4833, 5245, 13, 400, 550, 264, 5245, 2969, 362, 1401, 406,
  787, 439, 264, 3089, 293, 23331, 11, 51768], "temperature": 0.0, "avg_logprob":
  -0.38978754679361977, "compression_ratio": 1.4673913043478262, "no_speech_prob":
  0.03426869958639145}, {"id": 299, "seek": 201232, "start": 2012.32, "end": 2017.76,
  "text": " but also it''s read the internet five times or something. So they read
  all the blog posts about code.", "tokens": [50364, 457, 611, 309, 311, 1401, 264,
  4705, 1732, 1413, 420, 746, 13, 407, 436, 1401, 439, 264, 6968, 12300, 466, 3089,
  13, 50636], "temperature": 0.0, "avg_logprob": -0.19740371704101561, "compression_ratio":
  1.7857142857142858, "no_speech_prob": 0.0029065452981740236}, {"id": 300, "seek":
  201232, "start": 2020.24, "end": 2026.0, "text": " It''s amazing, right? It''s what
  times you live in. So whenever you''re typing something and it kind", "tokens":
  [50760, 467, 311, 2243, 11, 558, 30, 467, 311, 437, 1413, 291, 1621, 294, 13, 407,
  5699, 291, 434, 18444, 746, 293, 309, 733, 51048], "temperature": 0.0, "avg_logprob":
  -0.19740371704101561, "compression_ratio": 1.7857142857142858, "no_speech_prob":
  0.0029065452981740236}, {"id": 301, "seek": 201232, "start": 2026.0, "end": 2033.6,
  "text": " of smells like something it''s a thing before, it doesn''t, it doesn''t
  necessarily need rag to go get", "tokens": [51048, 295, 10036, 411, 746, 309, 311,
  257, 551, 949, 11, 309, 1177, 380, 11, 309, 1177, 380, 4725, 643, 17539, 281, 352,
  483, 51428], "temperature": 0.0, "avg_logprob": -0.19740371704101561, "compression_ratio":
  1.7857142857142858, "no_speech_prob": 0.0029065452981740236}, {"id": 302, "seek":
  201232, "start": 2033.6, "end": 2037.6799999999998, "text": " you know, common motifs,
  common, you know, here''s what you''re doing, here''s what I think you''re doing",
  "tokens": [51428, 291, 458, 11, 2689, 2184, 18290, 11, 2689, 11, 291, 458, 11, 510,
  311, 437, 291, 434, 884, 11, 510, 311, 437, 286, 519, 291, 434, 884, 51632], "temperature":
  0.0, "avg_logprob": -0.19740371704101561, "compression_ratio": 1.7857142857142858,
  "no_speech_prob": 0.0029065452981740236}, {"id": 303, "seek": 203768, "start": 2037.76,
  "end": 2042.4, "text": " a code right now and it can piece code together from all
  the code it''s ever learned from and extract", "tokens": [50368, 257, 3089, 558,
  586, 293, 309, 393, 2522, 3089, 1214, 490, 439, 264, 3089, 309, 311, 1562, 3264,
  490, 293, 8947, 50600], "temperature": 0.0, "avg_logprob": -0.1648193935178361,
  "compression_ratio": 1.6752136752136753, "no_speech_prob": 0.0017650466179475188},
  {"id": 304, "seek": 203768, "start": 2042.4, "end": 2051.36, "text": " late outside
  of it. But if it is and you know, this is me talking about how maybe I would build
  a", "tokens": [50600, 3469, 2380, 295, 309, 13, 583, 498, 309, 307, 293, 291, 458,
  11, 341, 307, 385, 1417, 466, 577, 1310, 286, 576, 1322, 257, 51048], "temperature":
  0.0, "avg_logprob": -0.1648193935178361, "compression_ratio": 1.6752136752136753,
  "no_speech_prob": 0.0017650466179475188}, {"id": 305, "seek": 203768, "start": 2051.36,
  "end": 2059.04, "text": " co-pilot. At some point I guess, you know, you need to
  see if it''s if the user''s typing code that", "tokens": [51048, 598, 12, 79, 31516,
  13, 1711, 512, 935, 286, 2041, 11, 291, 458, 11, 291, 643, 281, 536, 498, 309, 311,
  498, 264, 4195, 311, 18444, 3089, 300, 51432], "temperature": 0.0, "avg_logprob":
  -0.1648193935178361, "compression_ratio": 1.6752136752136753, "no_speech_prob":
  0.0017650466179475188}, {"id": 306, "seek": 203768, "start": 2059.04, "end": 2065.6800000000003,
  "text": " is so similar to code in this code base that it''s worth bringing it in.
  And we kind of did that", "tokens": [51432, 307, 370, 2531, 281, 3089, 294, 341,
  3089, 3096, 300, 309, 311, 3163, 5062, 309, 294, 13, 400, 321, 733, 295, 630, 300,
  51764], "temperature": 0.0, "avg_logprob": -0.1648193935178361, "compression_ratio":
  1.6752136752136753, "no_speech_prob": 0.0017650466179475188}, {"id": 307, "seek":
  206568, "start": 2065.7599999999998, "end": 2070.8799999999997, "text": " in a rudimentary
  way with the neighboring tabs. You''ve already got the tabs open. And that ended",
  "tokens": [50368, 294, 257, 32109, 2328, 822, 636, 365, 264, 31521, 20743, 13, 509,
  600, 1217, 658, 264, 20743, 1269, 13, 400, 300, 4590, 50624], "temperature": 0.0,
  "avg_logprob": -0.1338464135993017, "compression_ratio": 1.5235602094240839, "no_speech_prob":
  0.0007026331732049584}, {"id": 308, "seek": 206568, "start": 2070.8799999999997,
  "end": 2081.2, "text": " up being super useful. I think there''s probably a kind
  of a decreased efficacy, there''s work for this,", "tokens": [50624, 493, 885, 1687,
  4420, 13, 286, 519, 456, 311, 1391, 257, 733, 295, 257, 24436, 33492, 11, 456, 311,
  589, 337, 341, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1338464135993017,
  "compression_ratio": 1.5235602094240839, "no_speech_prob": 0.0007026331732049584},
  {"id": 309, "seek": 206568, "start": 2083.2, "end": 2090.3999999999996, "text":
  " where if you''re doing a rag search over the entire code base, probably the code
  that you''re", "tokens": [51240, 689, 498, 291, 434, 884, 257, 17539, 3164, 670,
  264, 2302, 3089, 3096, 11, 1391, 264, 3089, 300, 291, 434, 51600], "temperature":
  0.0, "avg_logprob": -0.1338464135993017, "compression_ratio": 1.5235602094240839,
  "no_speech_prob": 0.0007026331732049584}, {"id": 310, "seek": 209040, "start": 2090.4,
  "end": 2095.92, "text": " going to find is already code that''s open in the tabs
  right beside them. So maybe it''s useful to", "tokens": [50364, 516, 281, 915, 307,
  1217, 3089, 300, 311, 1269, 294, 264, 20743, 558, 15726, 552, 13, 407, 1310, 309,
  311, 4420, 281, 50640], "temperature": 0.0, "avg_logprob": -0.19042962096458257,
  "compression_ratio": 1.4851485148514851, "no_speech_prob": 0.007279460318386555},
  {"id": 311, "seek": 209040, "start": 2095.92, "end": 2105.76, "text": " do that
  maybe it''s not. But I don''t know. Yeah, interesting. I think code is like, as
  you said, it''s the", "tokens": [50640, 360, 300, 1310, 309, 311, 406, 13, 583,
  286, 500, 380, 458, 13, 865, 11, 1880, 13, 286, 519, 3089, 307, 411, 11, 382, 291,
  848, 11, 309, 311, 264, 51132], "temperature": 0.0, "avg_logprob": -0.19042962096458257,
  "compression_ratio": 1.4851485148514851, "no_speech_prob": 0.007279460318386555},
  {"id": 312, "seek": 209040, "start": 2105.76, "end": 2112.64, "text": " first successful
  LLM application. Probably some companies will say, no, no, no, Dr. Boog''s was the",
  "tokens": [51132, 700, 4406, 441, 43, 44, 3861, 13, 9210, 512, 3431, 486, 584, 11,
  572, 11, 572, 11, 572, 11, 2491, 13, 3286, 664, 311, 390, 264, 51476], "temperature":
  0.0, "avg_logprob": -0.19042962096458257, "compression_ratio": 1.4851485148514851,
  "no_speech_prob": 0.007279460318386555}, {"id": 313, "seek": 211264, "start": 2112.64,
  "end": 2120.08, "text": " first successful LLM application. But I, but I, there
  were some, maybe it was the first successful", "tokens": [50364, 700, 4406, 441,
  43, 44, 3861, 13, 583, 286, 11, 457, 286, 11, 456, 645, 512, 11, 1310, 309, 390,
  264, 700, 4406, 50736], "temperature": 0.0, "avg_logprob": -0.2961770645295731,
  "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.012393930926918983},
  {"id": 314, "seek": 211264, "start": 2120.08, "end": 2125.92, "text": " neural search
  application. And then co-pilot was the first LLM application, successful LLM application.",
  "tokens": [50736, 18161, 3164, 3861, 13, 400, 550, 598, 12, 79, 31516, 390, 264,
  700, 441, 43, 44, 3861, 11, 4406, 441, 43, 44, 3861, 13, 51028], "temperature":
  0.0, "avg_logprob": -0.2961770645295731, "compression_ratio": 1.8333333333333333,
  "no_speech_prob": 0.012393930926918983}, {"id": 315, "seek": 211264, "start": 2125.92,
  "end": 2133.6, "text": " And there''s plan nine. Yeah, there was another company
  that was out there actually before us,", "tokens": [51028, 400, 456, 311, 1393,
  4949, 13, 865, 11, 456, 390, 1071, 2237, 300, 390, 484, 456, 767, 949, 505, 11,
  51412], "temperature": 0.0, "avg_logprob": -0.2961770645295731, "compression_ratio":
  1.8333333333333333, "no_speech_prob": 0.012393930926918983}, {"id": 316, "seek":
  211264, "start": 2133.6, "end": 2138.24, "text": " but they just didn''t have quite
  the same, they weren''t only my Microsoft at the time, that probably", "tokens":
  [51412, 457, 436, 445, 994, 380, 362, 1596, 264, 912, 11, 436, 4999, 380, 787, 452,
  8116, 412, 264, 565, 11, 300, 1391, 51644], "temperature": 0.0, "avg_logprob": -0.2961770645295731,
  "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.012393930926918983},
  {"id": 317, "seek": 213824, "start": 2138.3199999999997, "end": 2148.56, "text":
  " helped a bit. Yeah, budget wise. I''m guessing. Yeah. Yeah, but I still, I still
  feel like it feels like", "tokens": [50368, 4254, 257, 857, 13, 865, 11, 4706, 10829,
  13, 286, 478, 17939, 13, 865, 13, 865, 11, 457, 286, 920, 11, 286, 920, 841, 411,
  309, 3417, 411, 50880], "temperature": 0.0, "avg_logprob": -0.22762206582462086,
  "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.03456605225801468},
  {"id": 318, "seek": 213824, "start": 2148.56, "end": 2156.3199999999997, "text":
  " magic, right? Like, judgey PT also felt magic and scary in the beginning. Like
  when I saw it for the", "tokens": [50880, 5585, 11, 558, 30, 1743, 11, 6995, 88,
  35460, 611, 2762, 5585, 293, 6958, 294, 264, 2863, 13, 1743, 562, 286, 1866, 309,
  337, 264, 51268], "temperature": 0.0, "avg_logprob": -0.22762206582462086, "compression_ratio":
  1.5555555555555556, "no_speech_prob": 0.03456605225801468}, {"id": 319, "seek":
  213824, "start": 2156.3199999999997, "end": 2162.24, "text": " first time and I
  saw it produce code, I thought that my job is done, even though I was not a programmer",
  "tokens": [51268, 700, 565, 293, 286, 1866, 309, 5258, 3089, 11, 286, 1194, 300,
  452, 1691, 307, 1096, 11, 754, 1673, 286, 390, 406, 257, 32116, 51564], "temperature":
  0.0, "avg_logprob": -0.22762206582462086, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.03456605225801468}, {"id": 320, "seek": 216224, "start": 2162.3199999999997,
  "end": 2170.9599999999996, "text": " anymore by then, but I felt the existential,
  well, not crisis of fear that basically many of us,", "tokens": [50368, 3602, 538,
  550, 11, 457, 286, 2762, 264, 37133, 11, 731, 11, 406, 5869, 295, 4240, 300, 1936,
  867, 295, 505, 11, 50800], "temperature": 0.0, "avg_logprob": -0.16170352113013173,
  "compression_ratio": 1.6125, "no_speech_prob": 0.02611222118139267}, {"id": 321,
  "seek": 216224, "start": 2171.6, "end": 2178.0, "text": " and especially junior
  developers, like probably not needed anymore. But then as I was, you know,", "tokens":
  [50832, 293, 2318, 16195, 8849, 11, 411, 1391, 406, 2978, 3602, 13, 583, 550, 382,
  286, 390, 11, 291, 458, 11, 51152], "temperature": 0.0, "avg_logprob": -0.16170352113013173,
  "compression_ratio": 1.6125, "no_speech_prob": 0.02611222118139267}, {"id": 322,
  "seek": 216224, "start": 2178.0, "end": 2184.9599999999996, "text": " overcoming
  my fear and I was like, now let me try this thing. It''s probably a toy. I found
  some,", "tokens": [51152, 38047, 452, 4240, 293, 286, 390, 411, 11, 586, 718, 385,
  853, 341, 551, 13, 467, 311, 1391, 257, 12058, 13, 286, 1352, 512, 11, 51500], "temperature":
  0.0, "avg_logprob": -0.16170352113013173, "compression_ratio": 1.6125, "no_speech_prob":
  0.02611222118139267}, {"id": 323, "seek": 216224, "start": 2184.9599999999996, "end":
  2189.04, "text": " what I explained, you know, some edge cases, which just doesn''t
  work. It goes in loops. And so I", "tokens": [51500, 437, 286, 8825, 11, 291, 458,
  11, 512, 4691, 3331, 11, 597, 445, 1177, 380, 589, 13, 467, 1709, 294, 16121, 13,
  400, 370, 286, 51704], "temperature": 0.0, "avg_logprob": -0.16170352113013173,
  "compression_ratio": 1.6125, "no_speech_prob": 0.02611222118139267}, {"id": 324,
  "seek": 218904, "start": 2189.04, "end": 2195.2, "text": " was like, okay, it seems
  like another tool under my belt. So I better master it and not,", "tokens": [50364,
  390, 411, 11, 1392, 11, 309, 2544, 411, 1071, 2290, 833, 452, 10750, 13, 407, 286,
  1101, 4505, 309, 293, 406, 11, 50672], "temperature": 0.0, "avg_logprob": -0.16682620578342014,
  "compression_ratio": 1.668141592920354, "no_speech_prob": 0.01401131134480238},
  {"id": 325, "seek": 218904, "start": 2196.32, "end": 2204.96, "text": " you know,
  walk away from it. But the code generation still feels like magic because you can
  explain,", "tokens": [50728, 291, 458, 11, 1792, 1314, 490, 309, 13, 583, 264, 3089,
  5125, 920, 3417, 411, 5585, 570, 291, 393, 2903, 11, 51160], "temperature": 0.0,
  "avg_logprob": -0.16682620578342014, "compression_ratio": 1.668141592920354, "no_speech_prob":
  0.01401131134480238}, {"id": 326, "seek": 218904, "start": 2204.96, "end": 2210.24,
  "text": " like you can use tap tab and like on a method signature complete, complete
  something or on the", "tokens": [51160, 411, 291, 393, 764, 5119, 4421, 293, 411,
  322, 257, 3170, 13397, 3566, 11, 3566, 746, 420, 322, 264, 51424], "temperature":
  0.0, "avg_logprob": -0.16682620578342014, "compression_ratio": 1.668141592920354,
  "no_speech_prob": 0.01401131134480238}, {"id": 327, "seek": 218904, "start": 2210.24,
  "end": 2214.72, "text": " comment complete something, but you could also write natural
  language, right? You could say,", "tokens": [51424, 2871, 3566, 746, 11, 457, 291,
  727, 611, 2464, 3303, 2856, 11, 558, 30, 509, 727, 584, 11, 51648], "temperature":
  0.0, "avg_logprob": -0.16682620578342014, "compression_ratio": 1.668141592920354,
  "no_speech_prob": 0.01401131134480238}, {"id": 328, "seek": 221472, "start": 2215.3599999999997,
  "end": 2220.64, "text": " generate test cases for me or something like that, right?
  And then it will understand it and", "tokens": [50396, 8460, 1500, 3331, 337, 385,
  420, 746, 411, 300, 11, 558, 30, 400, 550, 309, 486, 1223, 309, 293, 50660], "temperature":
  0.0, "avg_logprob": -0.1816089201946648, "compression_ratio": 1.7422222222222221,
  "no_speech_prob": 0.005565757397562265}, {"id": 329, "seek": 221472, "start": 2220.64,
  "end": 2226.9599999999996, "text": " will read your code and will reason about it
  and produce the test cases. I mean, that feels really", "tokens": [50660, 486, 1401,
  428, 3089, 293, 486, 1778, 466, 309, 293, 5258, 264, 1500, 3331, 13, 286, 914, 11,
  300, 3417, 534, 50976], "temperature": 0.0, "avg_logprob": -0.1816089201946648,
  "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.005565757397562265},
  {"id": 330, "seek": 221472, "start": 2226.9599999999996, "end": 2234.24, "text":
  " magical. It''s the time we''re wandering into right now is going to feel like
  magic for a while until", "tokens": [50976, 12066, 13, 467, 311, 264, 565, 321,
  434, 26396, 666, 558, 586, 307, 516, 281, 841, 411, 5585, 337, 257, 1339, 1826,
  51340], "temperature": 0.0, "avg_logprob": -0.1816089201946648, "compression_ratio":
  1.7422222222222221, "no_speech_prob": 0.005565757397562265}, {"id": 331, "seek":
  221472, "start": 2234.24, "end": 2239.12, "text": " we''ve got to get used to the
  exponent, it''s just going to keep going up and going up more, going up.", "tokens":
  [51340, 321, 600, 658, 281, 483, 1143, 281, 264, 37871, 11, 309, 311, 445, 516,
  281, 1066, 516, 493, 293, 516, 493, 544, 11, 516, 493, 13, 51584], "temperature":
  0.0, "avg_logprob": -0.1816089201946648, "compression_ratio": 1.7422222222222221,
  "no_speech_prob": 0.005565757397562265}, {"id": 332, "seek": 223912, "start": 2239.6,
  "end": 2247.6, "text": " But you know, I''ve had those existential pains myself,
  but then I realized when I start using", "tokens": [50388, 583, 291, 458, 11, 286,
  600, 632, 729, 37133, 29774, 2059, 11, 457, 550, 286, 5334, 562, 286, 722, 1228,
  50788], "temperature": 0.0, "avg_logprob": -0.1469578656283292, "compression_ratio":
  1.7241379310344827, "no_speech_prob": 0.006377519108355045}, {"id": 333, "seek":
  223912, "start": 2247.6, "end": 2255.12, "text": " these new tools the way that
  they want me to use them, I have superpowers. I think what we''re actually,", "tokens":
  [50788, 613, 777, 3873, 264, 636, 300, 436, 528, 385, 281, 764, 552, 11, 286, 362,
  1687, 47953, 13, 286, 519, 437, 321, 434, 767, 11, 51164], "temperature": 0.0, "avg_logprob":
  -0.1469578656283292, "compression_ratio": 1.7241379310344827, "no_speech_prob":
  0.006377519108355045}, {"id": 334, "seek": 223912, "start": 2255.12, "end": 2259.8399999999997,
  "text": " you got to have the right mindset. If your mindset is like, oh, my cobalt
  job is over, you might be", "tokens": [51164, 291, 658, 281, 362, 264, 558, 12543,
  13, 759, 428, 12543, 307, 411, 11, 1954, 11, 452, 598, 2645, 83, 1691, 307, 670,
  11, 291, 1062, 312, 51400], "temperature": 0.0, "avg_logprob": -0.1469578656283292,
  "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.006377519108355045},
  {"id": 335, "seek": 223912, "start": 2259.8399999999997, "end": 2267.04, "text":
  " right, your cobalt job is probably over. But if your mindset is like, oh, wow,
  I can do things I never", "tokens": [51400, 558, 11, 428, 598, 2645, 83, 1691, 307,
  1391, 670, 13, 583, 498, 428, 12543, 307, 411, 11, 1954, 11, 6076, 11, 286, 393,
  360, 721, 286, 1128, 51760], "temperature": 0.0, "avg_logprob": -0.1469578656283292,
  "compression_ratio": 1.7241379310344827, "no_speech_prob": 0.006377519108355045},
  {"id": 336, "seek": 226704, "start": 2267.04, "end": 2274.88, "text": " could do
  before. I, John Berryman, put together the HTML from my website and built a react
  app", "tokens": [50364, 727, 360, 949, 13, 286, 11, 2619, 34084, 1601, 11, 829,
  1214, 264, 17995, 490, 452, 3144, 293, 3094, 257, 4515, 724, 50756], "temperature":
  0.0, "avg_logprob": -0.17659191170124092, "compression_ratio": 1.5139442231075697,
  "no_speech_prob": 0.002981428988277912}, {"id": 337, "seek": 226704, "start": 2274.88,
  "end": 2280.24, "text": " in this like, like I thought I''d have to have a PhD to
  do something like that. But it''s amazing.", "tokens": [50756, 294, 341, 411, 11,
  411, 286, 1194, 286, 1116, 362, 281, 362, 257, 14476, 281, 360, 746, 411, 300, 13,
  583, 309, 311, 2243, 13, 51024], "temperature": 0.0, "avg_logprob": -0.17659191170124092,
  "compression_ratio": 1.5139442231075697, "no_speech_prob": 0.002981428988277912},
  {"id": 338, "seek": 226704, "start": 2281.04, "end": 2288.08, "text": " And what
  you''re seeing is an emergence of a new group of people that are, they call us the
  AI", "tokens": [51064, 400, 437, 291, 434, 2577, 307, 364, 36211, 295, 257, 777,
  1594, 295, 561, 300, 366, 11, 436, 818, 505, 264, 7318, 51416], "temperature": 0.0,
  "avg_logprob": -0.17659191170124092, "compression_ratio": 1.5139442231075697, "no_speech_prob":
  0.002981428988277912}, {"id": 339, "seek": 226704, "start": 2288.08, "end": 2294.96,
  "text": " natives, AI native development. And I''ve heard, you know, code composers
  rather than like just", "tokens": [51416, 47964, 11, 7318, 8470, 3250, 13, 400,
  286, 600, 2198, 11, 291, 458, 11, 3089, 43872, 2831, 813, 411, 445, 51760], "temperature":
  0.0, "avg_logprob": -0.17659191170124092, "compression_ratio": 1.5139442231075697,
  "no_speech_prob": 0.002981428988277912}, {"id": 340, "seek": 229496, "start": 2294.96,
  "end": 2302.8, "text": " coders. And you have people that are technically savvy.
  You can''t, you have to have, you know,", "tokens": [50364, 17656, 433, 13, 400,
  291, 362, 561, 300, 366, 12120, 47506, 13, 509, 393, 380, 11, 291, 362, 281, 362,
  11, 291, 458, 11, 50756], "temperature": 0.0, "avg_logprob": -0.22497196917263967,
  "compression_ratio": 1.670995670995671, "no_speech_prob": 0.006318431347608566},
  {"id": 341, "seek": 229496, "start": 2302.8, "end": 2306.96, "text": " some ability
  to, to recode still at this point, to debuck some stuff like you were talking about.",
  "tokens": [50756, 512, 3485, 281, 11, 281, 319, 22332, 920, 412, 341, 935, 11, 281,
  3001, 1134, 512, 1507, 411, 291, 645, 1417, 466, 13, 50964], "temperature": 0.0,
  "avg_logprob": -0.22497196917263967, "compression_ratio": 1.670995670995671, "no_speech_prob":
  0.006318431347608566}, {"id": 342, "seek": 229496, "start": 2307.52, "end": 2314.4,
  "text": " But they all go out at a screen, do this thing for me. And they have,
  it just takes a little bit", "tokens": [50992, 583, 436, 439, 352, 484, 412, 257,
  2568, 11, 360, 341, 551, 337, 385, 13, 400, 436, 362, 11, 309, 445, 2516, 257, 707,
  857, 51336], "temperature": 0.0, "avg_logprob": -0.22497196917263967, "compression_ratio":
  1.670995670995671, "no_speech_prob": 0.006318431347608566}, {"id": 343, "seek":
  229496, "start": 2314.4, "end": 2320.16, "text": " of experience to learn how to
  shout at the screen in the right way. You got to, you know, you''ve", "tokens":
  [51336, 295, 1752, 281, 1466, 577, 281, 8043, 412, 264, 2568, 294, 264, 558, 636,
  13, 509, 658, 281, 11, 291, 458, 11, 291, 600, 51624], "temperature": 0.0, "avg_logprob":
  -0.22497196917263967, "compression_ratio": 1.670995670995671, "no_speech_prob":
  0.006318431347608566}, {"id": 344, "seek": 232016, "start": 2320.64, "end": 2325.92,
  "text": " you still got to have the human ability to, you have to think about how
  this is structured,", "tokens": [50388, 291, 920, 658, 281, 362, 264, 1952, 3485,
  281, 11, 291, 362, 281, 519, 466, 577, 341, 307, 18519, 11, 50652], "temperature":
  0.0, "avg_logprob": -0.1668251714398784, "compression_ratio": 1.6736111111111112,
  "no_speech_prob": 0.0022325259633362293}, {"id": 345, "seek": 232016, "start": 2325.92,
  "end": 2331.2, "text": " how to modularize stuff. There, there is a craft to it
  still. But you, you can start building up", "tokens": [50652, 577, 281, 31111, 1125,
  1507, 13, 821, 11, 456, 307, 257, 8448, 281, 309, 920, 13, 583, 291, 11, 291, 393,
  722, 2390, 493, 50916], "temperature": 0.0, "avg_logprob": -0.1668251714398784,
  "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.0022325259633362293},
  {"id": 346, "seek": 232016, "start": 2331.2, "end": 2336.48, "text": " pieces. Even
  if you''re not technically savvy, if you''ve been building it in chunks when one
  of these", "tokens": [50916, 3755, 13, 2754, 498, 291, 434, 406, 12120, 47506, 11,
  498, 291, 600, 668, 2390, 309, 294, 24004, 562, 472, 295, 613, 51180], "temperature":
  0.0, "avg_logprob": -0.1668251714398784, "compression_ratio": 1.6736111111111112,
  "no_speech_prob": 0.0022325259633362293}, {"id": 347, "seek": 232016, "start": 2336.48,
  "end": 2340.7999999999997, "text": " pieces messes up gloriously and you''ve got
  your floating point numbers that I don''t work in", "tokens": [51180, 3755, 2082,
  279, 493, 26623, 8994, 293, 291, 600, 658, 428, 12607, 935, 3547, 300, 286, 500,
  380, 589, 294, 51396], "temperature": 0.0, "avg_logprob": -0.1668251714398784, "compression_ratio":
  1.6736111111111112, "no_speech_prob": 0.0022325259633362293}, {"id": 348, "seek":
  232016, "start": 2340.7999999999997, "end": 2346.3199999999997, "text": " out like
  your example, then at least you can say, I''m going to delete back to here. I''m
  going to try", "tokens": [51396, 484, 411, 428, 1365, 11, 550, 412, 1935, 291, 393,
  584, 11, 286, 478, 516, 281, 12097, 646, 281, 510, 13, 286, 478, 516, 281, 853,
  51672], "temperature": 0.0, "avg_logprob": -0.1668251714398784, "compression_ratio":
  1.6736111111111112, "no_speech_prob": 0.0022325259633362293}, {"id": 349, "seek":
  234632, "start": 2346.48, "end": 2352.32, "text": " a different route. See if I
  can just bump it out of this. And often you can. And people in every", "tokens":
  [50372, 257, 819, 7955, 13, 3008, 498, 286, 393, 445, 9961, 309, 484, 295, 341,
  13, 400, 2049, 291, 393, 13, 400, 561, 294, 633, 50664], "temperature": 0.0, "avg_logprob":
  -0.18462447400362986, "compression_ratio": 1.6416666666666666, "no_speech_prob":
  0.013191438280045986}, {"id": 350, "seek": 234632, "start": 2352.32, "end": 2360.8,
  "text": " walk of life are are much more effective and efficient at creating. And
  it''s, you know, you don''t get", "tokens": [50664, 1792, 295, 993, 366, 366, 709,
  544, 4942, 293, 7148, 412, 4084, 13, 400, 309, 311, 11, 291, 458, 11, 291, 500,
  380, 483, 51088], "temperature": 0.0, "avg_logprob": -0.18462447400362986, "compression_ratio":
  1.6416666666666666, "no_speech_prob": 0.013191438280045986}, {"id": 351, "seek":
  234632, "start": 2360.8, "end": 2366.6400000000003, "text": " this, you don''t always
  get to solve the nitpahee little, you know, if you really love debugging and", "tokens":
  [51088, 341, 11, 291, 500, 380, 1009, 483, 281, 5039, 264, 10900, 79, 545, 1653,
  707, 11, 291, 458, 11, 498, 291, 534, 959, 45592, 293, 51380], "temperature": 0.0,
  "avg_logprob": -0.18462447400362986, "compression_ratio": 1.6416666666666666, "no_speech_prob":
  0.013191438280045986}, {"id": 352, "seek": 234632, "start": 2366.6400000000003,
  "end": 2373.36, "text": " writing tests, I''m sorry. I think that''s your days might
  be numbered. But if you love creating,", "tokens": [51380, 3579, 6921, 11, 286,
  478, 2597, 13, 286, 519, 300, 311, 428, 1708, 1062, 312, 40936, 13, 583, 498, 291,
  959, 4084, 11, 51716], "temperature": 0.0, "avg_logprob": -0.18462447400362986,
  "compression_ratio": 1.6416666666666666, "no_speech_prob": 0.013191438280045986},
  {"id": 353, "seek": 237336, "start": 2373.36, "end": 2378.2400000000002, "text":
  " that''s I think we''re approaching a new golden age and it''s exponential. We''re
  going to keep", "tokens": [50364, 300, 311, 286, 519, 321, 434, 14908, 257, 777,
  9729, 3205, 293, 309, 311, 21510, 13, 492, 434, 516, 281, 1066, 50608], "temperature":
  0.0, "avg_logprob": -0.1968018737020372, "compression_ratio": 1.5737704918032787,
  "no_speech_prob": 0.009339823387563229}, {"id": 354, "seek": 237336, "start": 2378.2400000000002,
  "end": 2384.2400000000002, "text": " approaching new golden ages for a while. Yeah,
  I think in my career, if I can reflect a little bit,", "tokens": [50608, 14908,
  777, 9729, 12357, 337, 257, 1339, 13, 865, 11, 286, 519, 294, 452, 3988, 11, 498,
  286, 393, 5031, 257, 707, 857, 11, 50908], "temperature": 0.0, "avg_logprob": -0.1968018737020372,
  "compression_ratio": 1.5737704918032787, "no_speech_prob": 0.009339823387563229},
  {"id": 355, "seek": 237336, "start": 2384.2400000000002, "end": 2391.6800000000003,
  "text": " I, I love creating much more for sure. But then back then, we didn''t
  have a lamp, so didn''t have", "tokens": [50908, 286, 11, 286, 959, 4084, 709, 544,
  337, 988, 13, 583, 550, 646, 550, 11, 321, 994, 380, 362, 257, 12684, 11, 370, 994,
  380, 362, 51280], "temperature": 0.0, "avg_logprob": -0.1968018737020372, "compression_ratio":
  1.5737704918032787, "no_speech_prob": 0.009339823387563229}, {"id": 356, "seek":
  239168, "start": 2391.68, "end": 2400.8799999999997, "text": " compilates. We had
  to do pay a programming, right? And that was our command. Yeah. And but the,", "tokens":
  [50364, 715, 388, 1024, 13, 492, 632, 281, 360, 1689, 257, 9410, 11, 558, 30, 400,
  300, 390, 527, 5622, 13, 865, 13, 400, 457, 264, 11, 50824], "temperature": 0.0,
  "avg_logprob": -0.2759245258488067, "compression_ratio": 1.508108108108108, "no_speech_prob":
  0.026053477078676224}, {"id": 357, "seek": 239168, "start": 2400.8799999999997,
  "end": 2409.04, "text": " the, the, that notion that you just said about creativity,
  I think that drove much more", "tokens": [50824, 264, 11, 264, 11, 300, 10710, 300,
  291, 445, 848, 466, 12915, 11, 286, 519, 300, 13226, 709, 544, 51232], "temperature":
  0.0, "avg_logprob": -0.2759245258488067, "compression_ratio": 1.508108108108108,
  "no_speech_prob": 0.026053477078676224}, {"id": 358, "seek": 239168, "start": 2410.16,
  "end": 2417.3599999999997, "text": " forward than us going into the rabbit down
  the rabbit holes, you know, of debugging that thing.", "tokens": [51288, 2128, 813,
  505, 516, 666, 264, 19509, 760, 264, 19509, 8118, 11, 291, 458, 11, 295, 45592,
  300, 551, 13, 51648], "temperature": 0.0, "avg_logprob": -0.2759245258488067, "compression_ratio":
  1.508108108108108, "no_speech_prob": 0.026053477078676224}, {"id": 359, "seek":
  241736, "start": 2417.36, "end": 2422.7200000000003, "text": " However important
  that thing was, you know, of course, you need to debug and so on. But it didn''t
  feel,", "tokens": [50364, 2908, 1021, 300, 551, 390, 11, 291, 458, 11, 295, 1164,
  11, 291, 643, 281, 24083, 293, 370, 322, 13, 583, 309, 994, 380, 841, 11, 50632],
  "temperature": 0.0, "avg_logprob": -0.2384022813502366, "compression_ratio": 1.6981818181818182,
  "no_speech_prob": 0.012571687810122967}, {"id": 360, "seek": 241736, "start": 2423.52,
  "end": 2428.08, "text": " like you, you would just feel exhausted after that. You
  know, like, yeah, I fixed that bug. Finally,", "tokens": [50672, 411, 291, 11, 291,
  576, 445, 841, 17992, 934, 300, 13, 509, 458, 11, 411, 11, 1338, 11, 286, 6806,
  300, 7426, 13, 6288, 11, 50900], "temperature": 0.0, "avg_logprob": -0.2384022813502366,
  "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.012571687810122967},
  {"id": 361, "seek": 241736, "start": 2428.08, "end": 2434.1600000000003, "text":
  " I squashed it. Move on because you, you want to build stuff, right? You, and I
  think it was it,", "tokens": [50900, 286, 2339, 12219, 309, 13, 10475, 322, 570,
  291, 11, 291, 528, 281, 1322, 1507, 11, 558, 30, 509, 11, 293, 286, 519, 309, 390,
  309, 11, 51204], "temperature": 0.0, "avg_logprob": -0.2384022813502366, "compression_ratio":
  1.6981818181818182, "no_speech_prob": 0.012571687810122967}, {"id": 362, "seek":
  241736, "start": 2434.7200000000003, "end": 2440.56, "text": " the extra who said,
  if debugging is the process of removing, finding and removing bugs, then", "tokens":
  [51232, 264, 2857, 567, 848, 11, 498, 45592, 307, 264, 1399, 295, 12720, 11, 5006,
  293, 12720, 15120, 11, 550, 51524], "temperature": 0.0, "avg_logprob": -0.2384022813502366,
  "compression_ratio": 1.6981818181818182, "no_speech_prob": 0.012571687810122967},
  {"id": 363, "seek": 241736, "start": 2441.2000000000003, "end": 2446.7200000000003,
  "text": " programming must be the process of introducing bugs. And so that''s right.",
  "tokens": [51556, 9410, 1633, 312, 264, 1399, 295, 15424, 15120, 13, 400, 370, 300,
  311, 558, 13, 51832], "temperature": 0.0, "avg_logprob": -0.2384022813502366, "compression_ratio":
  1.6981818181818182, "no_speech_prob": 0.012571687810122967}, {"id": 364, "seek":
  244736, "start": 2447.36, "end": 2449.04, "text": " Yeah. That''s a vicious circle.
  Yeah.", "tokens": [50364, 865, 13, 663, 311, 257, 30093, 6329, 13, 865, 13, 50448],
  "temperature": 0.0, "avg_logprob": -0.2122407219626687, "compression_ratio": 1.5670498084291187,
  "no_speech_prob": 0.003264091443270445}, {"id": 365, "seek": 244736, "start": 2451.36,
  "end": 2456.96, "text": " You, you already touched on that topic a bit earlier about
  artifacts. I''ve read your blog posts,", "tokens": [50564, 509, 11, 291, 1217, 9828,
  322, 300, 4829, 257, 857, 3071, 466, 24617, 13, 286, 600, 1401, 428, 6968, 12300,
  11, 50844], "temperature": 0.0, "avg_logprob": -0.2122407219626687, "compression_ratio":
  1.5670498084291187, "no_speech_prob": 0.003264091443270445}, {"id": 366, "seek":
  244736, "start": 2456.96, "end": 2463.2000000000003, "text": " which will, will
  definitely link, link in and I, I got inspired by that. I have to say,", "tokens":
  [50844, 597, 486, 11, 486, 2138, 2113, 11, 2113, 294, 293, 286, 11, 286, 658, 7547,
  538, 300, 13, 286, 362, 281, 584, 11, 51156], "temperature": 0.0, "avg_logprob":
  -0.2122407219626687, "compression_ratio": 1.5670498084291187, "no_speech_prob":
  0.003264091443270445}, {"id": 367, "seek": 244736, "start": 2463.84, "end": 2469.04,
  "text": " because oftentimes when I go to the set applications, you know, chat,
  GPT or perplexity,", "tokens": [51188, 570, 18349, 562, 286, 352, 281, 264, 992,
  5821, 11, 291, 458, 11, 5081, 11, 26039, 51, 420, 680, 18945, 507, 11, 51448], "temperature":
  0.0, "avg_logprob": -0.2122407219626687, "compression_ratio": 1.5670498084291187,
  "no_speech_prob": 0.003264091443270445}, {"id": 368, "seek": 244736, "start": 2469.04,
  "end": 2476.8, "text": " what have you, and you have a longer conversation there,
  it is hard to then sort of trace back and", "tokens": [51448, 437, 362, 291, 11,
  293, 291, 362, 257, 2854, 3761, 456, 11, 309, 307, 1152, 281, 550, 1333, 295, 13508,
  646, 293, 51836], "temperature": 0.0, "avg_logprob": -0.2122407219626687, "compression_ratio":
  1.5670498084291187, "no_speech_prob": 0.003264091443270445}, {"id": 369, "seek":
  247680, "start": 2476.88, "end": 2482.0800000000004, "text": " think, okay, I branched
  here and, okay, what was my thinking again? What did I produce at that", "tokens":
  [50368, 519, 11, 1392, 11, 286, 9819, 292, 510, 293, 11, 1392, 11, 437, 390, 452,
  1953, 797, 30, 708, 630, 286, 5258, 412, 300, 50628], "temperature": 0.0, "avg_logprob":
  -0.13843217830068058, "compression_ratio": 1.6167400881057268, "no_speech_prob":
  0.008451452478766441}, {"id": 370, "seek": 247680, "start": 2482.0800000000004,
  "end": 2488.0800000000004, "text": " point? There is nothing to hold on to except
  scrolling back and forth. And that''s what you", "tokens": [50628, 935, 30, 821,
  307, 1825, 281, 1797, 322, 281, 3993, 29053, 646, 293, 5220, 13, 400, 300, 311,
  437, 291, 50928], "temperature": 0.0, "avg_logprob": -0.13843217830068058, "compression_ratio":
  1.6167400881057268, "no_speech_prob": 0.008451452478766441}, {"id": 371, "seek":
  247680, "start": 2488.0800000000004, "end": 2493.84, "text": " really put. And I
  want you to open, like, you basically proposed something new, I believe.", "tokens":
  [50928, 534, 829, 13, 400, 286, 528, 291, 281, 1269, 11, 411, 11, 291, 1936, 10348,
  746, 777, 11, 286, 1697, 13, 51216], "temperature": 0.0, "avg_logprob": -0.13843217830068058,
  "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.008451452478766441},
  {"id": 372, "seek": 247680, "start": 2494.7200000000003, "end": 2501.6800000000003,
  "text": " I wonder if you are the creator of this or like, in any case, you carry
  this idea forward.", "tokens": [51260, 286, 2441, 498, 291, 366, 264, 14181, 295,
  341, 420, 411, 11, 294, 604, 1389, 11, 291, 3985, 341, 1558, 2128, 13, 51608], "temperature":
  0.0, "avg_logprob": -0.13843217830068058, "compression_ratio": 1.6167400881057268,
  "no_speech_prob": 0.008451452478766441}, {"id": 373, "seek": 250168, "start": 2501.68,
  "end": 2504.64, "text": " Can you explain what do you mean by artifacts?", "tokens":
  [50364, 1664, 291, 2903, 437, 360, 291, 914, 538, 24617, 30, 50512], "temperature":
  0.0, "avg_logprob": -0.17453065732630288, "compression_ratio": 1.5566037735849056,
  "no_speech_prob": 0.005332770757377148}, {"id": 374, "seek": 250168, "start": 2505.8399999999997,
  "end": 2513.12, "text": " I will carry the idea forward. I think there is what we''re
  seeing is some convergence around", "tokens": [50572, 286, 486, 3985, 264, 1558,
  2128, 13, 286, 519, 456, 307, 437, 321, 434, 2577, 307, 512, 32181, 926, 50936],
  "temperature": 0.0, "avg_logprob": -0.17453065732630288, "compression_ratio": 1.5566037735849056,
  "no_speech_prob": 0.005332770757377148}, {"id": 375, "seek": 250168, "start": 2513.12,
  "end": 2521.52, "text": " the notion that put into my blog post. For example, with
  anthropics, artifacts, so that they,", "tokens": [50936, 264, 10710, 300, 829, 666,
  452, 6968, 2183, 13, 1171, 1365, 11, 365, 22727, 1167, 11, 24617, 11, 370, 300,
  436, 11, 51356], "temperature": 0.0, "avg_logprob": -0.17453065732630288, "compression_ratio":
  1.5566037735849056, "no_speech_prob": 0.005332770757377148}, {"id": 376, "seek":
  250168, "start": 2521.52, "end": 2526.8799999999997, "text": " they splash something
  that I think is getting at what I''m talking about. But if you dig a little", "tokens":
  [51356, 436, 25757, 746, 300, 286, 519, 307, 1242, 412, 437, 286, 478, 1417, 466,
  13, 583, 498, 291, 2528, 257, 707, 51624], "temperature": 0.0, "avg_logprob": -0.17453065732630288,
  "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.005332770757377148},
  {"id": 377, "seek": 252688, "start": 2526.88, "end": 2533.84, "text": " at the end,
  it''s not quite what I''m talking about. Whenever you engaged in a conversation
  with", "tokens": [50364, 412, 264, 917, 11, 309, 311, 406, 1596, 437, 286, 478,
  1417, 466, 13, 14159, 291, 8237, 294, 257, 3761, 365, 50712], "temperature": 0.0,
  "avg_logprob": -0.23977725982666015, "compression_ratio": 1.6041666666666667, "no_speech_prob":
  0.0029198743868619204}, {"id": 378, "seek": 252688, "start": 2535.28, "end": 2542.4,
  "text": " an assistant, LM experience, they just want to chat. And so we''ve done
  good over time by giving", "tokens": [50784, 364, 10994, 11, 441, 44, 1752, 11,
  436, 445, 528, 281, 5081, 13, 400, 370, 321, 600, 1096, 665, 670, 565, 538, 2902,
  51140], "temperature": 0.0, "avg_logprob": -0.23977725982666015, "compression_ratio":
  1.6041666666666667, "no_speech_prob": 0.0029198743868619204}, {"id": 379, "seek":
  252688, "start": 2542.4, "end": 2547.36, "text": " them like tools. So now it''s
  rather than just like being your therapist, they can go inducing for", "tokens":
  [51140, 552, 411, 3873, 13, 407, 586, 309, 311, 2831, 813, 445, 411, 885, 428, 19830,
  11, 436, 393, 352, 13716, 2175, 337, 51388], "temperature": 0.0, "avg_logprob":
  -0.23977725982666015, "compression_ratio": 1.6041666666666667, "no_speech_prob":
  0.0029198743868619204}, {"id": 380, "seek": 252688, "start": 2547.36, "end": 2553.04,
  "text": " you. So that''s nice. But still, it''s a linear flow. And whenever you''re
  talking about something,", "tokens": [51388, 291, 13, 407, 300, 311, 1481, 13, 583,
  920, 11, 309, 311, 257, 8213, 3095, 13, 400, 5699, 291, 434, 1417, 466, 746, 11,
  51672], "temperature": 0.0, "avg_logprob": -0.23977725982666015, "compression_ratio":
  1.6041666666666667, "no_speech_prob": 0.0029198743868619204}, {"id": 381, "seek":
  255304, "start": 2553.92, "end": 2560.32, "text": " it flows back into the backstroll.
  Most of the time, when you are getting work done, you''re getting", "tokens": [50408,
  309, 12867, 646, 666, 264, 646, 372, 3970, 13, 4534, 295, 264, 565, 11, 562, 291,
  366, 1242, 589, 1096, 11, 291, 434, 1242, 50728], "temperature": 0.0, "avg_logprob":
  -0.2807449722290039, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.007474581710994244},
  {"id": 382, "seek": 255304, "start": 2560.32, "end": 2566.96, "text": " work done
  on something. And artifact, there is a staple, I really wanted to call it a", "tokens":
  [50728, 589, 1096, 322, 746, 13, 400, 34806, 11, 456, 307, 257, 32361, 11, 286,
  534, 1415, 281, 818, 309, 257, 51060], "temperature": 0.0, "avg_logprob": -0.2807449722290039,
  "compression_ratio": 1.742081447963801, "no_speech_prob": 0.007474581710994244},
  {"id": 383, "seek": 255304, "start": 2566.96, "end": 2573.12, "text": " staple object
  of discourse because it isn''t object. It''s staple because it may change. And it''s",
  "tokens": [51060, 32361, 2657, 295, 23938, 570, 309, 1943, 380, 2657, 13, 467, 311,
  32361, 570, 309, 815, 1319, 13, 400, 309, 311, 51368], "temperature": 0.0, "avg_logprob":
  -0.2807449722290039, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.007474581710994244},
  {"id": 384, "seek": 255304, "start": 2573.12, "end": 2579.52, "text": " it''s the
  object of the discourse. But artifacts is not just easy to say. But this is what
  we deal with.", "tokens": [51368, 309, 311, 264, 2657, 295, 264, 23938, 13, 583,
  24617, 307, 406, 445, 1858, 281, 584, 13, 583, 341, 307, 437, 321, 2028, 365, 13,
  51688], "temperature": 0.0, "avg_logprob": -0.2807449722290039, "compression_ratio":
  1.742081447963801, "no_speech_prob": 0.007474581710994244}, {"id": 385, "seek":
  257952, "start": 2580.24, "end": 2584.48, "text": " Whenever we''re paraprogramming
  on something, it''s me and you looking at this piece of code,", "tokens": [50400,
  14159, 321, 434, 36992, 340, 1342, 2810, 322, 746, 11, 309, 311, 385, 293, 291,
  1237, 412, 341, 2522, 295, 3089, 11, 50612], "temperature": 0.0, "avg_logprob":
  -0.14734056260850695, "compression_ratio": 1.7216117216117217, "no_speech_prob":
  0.02397976815700531}, {"id": 386, "seek": 257952, "start": 2584.48, "end": 2588.32,
  "text": " and you make a recommendation about this. And I say, that''s good. We
  go back and forth.", "tokens": [50612, 293, 291, 652, 257, 11879, 466, 341, 13,
  400, 286, 584, 11, 300, 311, 665, 13, 492, 352, 646, 293, 5220, 13, 50804], "temperature":
  0.0, "avg_logprob": -0.14734056260850695, "compression_ratio": 1.7216117216117217,
  "no_speech_prob": 0.02397976815700531}, {"id": 387, "seek": 257952, "start": 2588.88,
  "end": 2595.12, "text": " And anything that you can imagine can be addressed like
  that. The situation becomes a particular", "tokens": [50832, 400, 1340, 300, 291,
  393, 3811, 393, 312, 13847, 411, 300, 13, 440, 2590, 3643, 257, 1729, 51144], "temperature":
  0.0, "avg_logprob": -0.14734056260850695, "compression_ratio": 1.7216117216117217,
  "no_speech_prob": 0.02397976815700531}, {"id": 388, "seek": 257952, "start": 2595.92,
  "end": 2603.52, "text": " point yet, when every you''re dealing with multiple artifacts.
  So if you''re saying, I really like", "tokens": [51184, 935, 1939, 11, 562, 633,
  291, 434, 6260, 365, 3866, 24617, 13, 407, 498, 291, 434, 1566, 11, 286, 534, 411,
  51564], "temperature": 0.0, "avg_logprob": -0.14734056260850695, "compression_ratio":
  1.7216117216117217, "no_speech_prob": 0.02397976815700531}, {"id": 389, "seek":
  257952, "start": 2603.52, "end": 2608.8, "text": " this thing over here. And I wonder
  how it would fit in with this thing over here. You''re having,", "tokens": [51564,
  341, 551, 670, 510, 13, 400, 286, 2441, 577, 309, 576, 3318, 294, 365, 341, 551,
  670, 510, 13, 509, 434, 1419, 11, 51828], "temperature": 0.0, "avg_logprob": -0.14734056260850695,
  "compression_ratio": 1.7216117216117217, "no_speech_prob": 0.02397976815700531},
  {"id": 390, "seek": 260880, "start": 2608.8, "end": 2613.36, "text": " as a human,
  you''re having to refer to more than one thing that exists outside of this linear",
  "tokens": [50364, 382, 257, 1952, 11, 291, 434, 1419, 281, 2864, 281, 544, 813,
  472, 551, 300, 8198, 2380, 295, 341, 8213, 50592], "temperature": 0.0, "avg_logprob":
  -0.1477616917003285, "compression_ratio": 1.6077586206896552, "no_speech_prob":
  0.0016633168561384082}, {"id": 391, "seek": 260880, "start": 2613.36, "end": 2620.1600000000003,
  "text": " conversation. And you''re talking about how they relate to one another.
  And so the blog post,", "tokens": [50592, 3761, 13, 400, 291, 434, 1417, 466, 577,
  436, 10961, 281, 472, 1071, 13, 400, 370, 264, 6968, 2183, 11, 50932], "temperature":
  0.0, "avg_logprob": -0.1477616917003285, "compression_ratio": 1.6077586206896552,
  "no_speech_prob": 0.0016633168561384082}, {"id": 392, "seek": 260880, "start": 2620.1600000000003,
  "end": 2625.1200000000003, "text": " which I hope you guys all read, arterislabs.com,
  we''ll do this again in a second, right?", "tokens": [50932, 597, 286, 1454, 291,
  1074, 439, 1401, 11, 30455, 271, 75, 17243, 13, 1112, 11, 321, 603, 360, 341, 797,
  294, 257, 1150, 11, 558, 30, 51180], "temperature": 0.0, "avg_logprob": -0.1477616917003285,
  "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.0016633168561384082},
  {"id": 393, "seek": 260880, "start": 2626.0, "end": 2636.0800000000004, "text":
  " It gets into what I think of as an artifact. It talks about how to build a prompt
  so that you have", "tokens": [51224, 467, 2170, 666, 437, 286, 519, 295, 382, 364,
  34806, 13, 467, 6686, 466, 577, 281, 1322, 257, 12391, 370, 300, 291, 362, 51728],
  "temperature": 0.0, "avg_logprob": -0.1477616917003285, "compression_ratio": 1.6077586206896552,
  "no_speech_prob": 0.0016633168561384082}, {"id": 394, "seek": 263608, "start": 2636.08,
  "end": 2642.64, "text": " space for this linear conversation. But you also draw
  the models attention to a chunk at the top,", "tokens": [50364, 1901, 337, 341,
  8213, 3761, 13, 583, 291, 611, 2642, 264, 5245, 3202, 281, 257, 16635, 412, 264,
  1192, 11, 50692], "temperature": 0.0, "avg_logprob": -0.14141454299290976, "compression_ratio":
  1.6425531914893616, "no_speech_prob": 0.001587264589034021}, {"id": 395, "seek":
  263608, "start": 2642.64, "end": 2646.72, "text": " usually, you might put it all
  through, you might put it at the bottom to have an experiment with it.", "tokens":
  [50692, 2673, 11, 291, 1062, 829, 309, 439, 807, 11, 291, 1062, 829, 309, 412, 264,
  2767, 281, 362, 364, 5120, 365, 309, 13, 50896], "temperature": 0.0, "avg_logprob":
  -0.14141454299290976, "compression_ratio": 1.6425531914893616, "no_speech_prob":
  0.001587264589034021}, {"id": 396, "seek": 263608, "start": 2646.72, "end": 2652.24,
  "text": " But a static chunk, which is like, here is all the things on the table
  that people can refer to.", "tokens": [50896, 583, 257, 13437, 16635, 11, 597, 307,
  411, 11, 510, 307, 439, 264, 721, 322, 264, 3199, 300, 561, 393, 2864, 281, 13,
  51172], "temperature": 0.0, "avg_logprob": -0.14141454299290976, "compression_ratio":
  1.6425531914893616, "no_speech_prob": 0.001587264589034021}, {"id": 397, "seek":
  263608, "start": 2653.04, "end": 2662.3199999999997, "text": " Each object, each
  artifact, has importantly an ID to be referred to. And I''ve noticed that", "tokens":
  [51212, 6947, 2657, 11, 1184, 34806, 11, 575, 8906, 364, 7348, 281, 312, 10839,
  281, 13, 400, 286, 600, 5694, 300, 51676], "temperature": 0.0, "avg_logprob": -0.14141454299290976,
  "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.001587264589034021},
  {"id": 398, "seek": 266232, "start": 2662.8, "end": 2670.0, "text": " these models
  do really well with arbitrary hexadecimal IDs. So I''ll just give them a random
  ID.", "tokens": [50388, 613, 5245, 360, 534, 731, 365, 23211, 23291, 762, 66, 10650,
  48212, 13, 407, 286, 603, 445, 976, 552, 257, 4974, 7348, 13, 50748], "temperature":
  0.0, "avg_logprob": -0.1527665891145405, "compression_ratio": 1.6033755274261603,
  "no_speech_prob": 0.004685103893280029}, {"id": 399, "seek": 266232, "start": 2670.7200000000003,
  "end": 2674.48, "text": " But they''re really good at referring to those and not
  like, you know, they don''t seem to hallucinate", "tokens": [50784, 583, 436, 434,
  534, 665, 412, 13761, 281, 729, 293, 406, 411, 11, 291, 458, 11, 436, 500, 380,
  1643, 281, 35212, 13923, 50972], "temperature": 0.0, "avg_logprob": -0.1527665891145405,
  "compression_ratio": 1.6033755274261603, "no_speech_prob": 0.004685103893280029},
  {"id": 400, "seek": 266232, "start": 2674.96, "end": 2683.1200000000003, "text":
  " these IDs, which surprised me. And so if you have a prompt with these artifacts
  at the top,", "tokens": [50996, 613, 48212, 11, 597, 6100, 385, 13, 400, 370, 498,
  291, 362, 257, 12391, 365, 613, 24617, 412, 264, 1192, 11, 51404], "temperature":
  0.0, "avg_logprob": -0.1527665891145405, "compression_ratio": 1.6033755274261603,
  "no_speech_prob": 0.004685103893280029}, {"id": 401, "seek": 266232, "start": 2683.1200000000003,
  "end": 2687.84, "text": " and you have a system message that explains to the model
  how to interact with these things,", "tokens": [51404, 293, 291, 362, 257, 1185,
  3636, 300, 13948, 281, 264, 2316, 577, 281, 4648, 365, 613, 721, 11, 51640], "temperature":
  0.0, "avg_logprob": -0.1527665891145405, "compression_ratio": 1.6033755274261603,
  "no_speech_prob": 0.004685103893280029}, {"id": 402, "seek": 268784, "start": 2688.4,
  "end": 2694.56, "text": " then my experience is that they obey the instructions
  really well. They talk", "tokens": [50392, 550, 452, 1752, 307, 300, 436, 19297,
  264, 9415, 534, 731, 13, 814, 751, 50700], "temperature": 0.0, "avg_logprob": -0.13474469184875487,
  "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.004789172206073999},
  {"id": 403, "seek": 268784, "start": 2696.96, "end": 2703.6000000000004, "text":
  " humans are used to using pronouns and names and nicknames and, you know, other
  pointers", "tokens": [50820, 6255, 366, 1143, 281, 1228, 35883, 293, 5288, 293,
  15416, 77, 1632, 293, 11, 291, 458, 11, 661, 44548, 51152], "temperature": 0.0,
  "avg_logprob": -0.13474469184875487, "compression_ratio": 1.6431924882629108, "no_speech_prob":
  0.004789172206073999}, {"id": 404, "seek": 268784, "start": 2704.2400000000002,
  "end": 2710.8, "text": " that refer to the real thing. And these models having read
  all the human text that they could", "tokens": [51184, 300, 2864, 281, 264, 957,
  551, 13, 400, 613, 5245, 1419, 1401, 439, 264, 1952, 2487, 300, 436, 727, 51512],
  "temperature": 0.0, "avg_logprob": -0.13474469184875487, "compression_ratio": 1.6431924882629108,
  "no_speech_prob": 0.004789172206073999}, {"id": 405, "seek": 268784, "start": 2710.8,
  "end": 2717.52, "text": " get their hands on, the internet five times, they also
  understand what you mean about using", "tokens": [51512, 483, 641, 2377, 322, 11,
  264, 4705, 1732, 1413, 11, 436, 611, 1223, 437, 291, 914, 466, 1228, 51848], "temperature":
  0.0, "avg_logprob": -0.13474469184875487, "compression_ratio": 1.6431924882629108,
  "no_speech_prob": 0.004789172206073999}, {"id": 406, "seek": 271752, "start": 2718.0,
  "end": 2722.8, "text": " using pronouns and stuff. So you can say, you know, dear
  model, there''s this thing called artifact.", "tokens": [50388, 1228, 35883, 293,
  1507, 13, 407, 291, 393, 584, 11, 291, 458, 11, 6875, 2316, 11, 456, 311, 341, 551,
  1219, 34806, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2086505709954028,
  "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.0008889439632184803},
  {"id": 407, "seek": 271752, "start": 2722.8, "end": 2732.16, "text": " They have
  these IDs. When you refer to them, then use these, use anchors like in HML because",
  "tokens": [50628, 814, 362, 613, 48212, 13, 1133, 291, 2864, 281, 552, 11, 550,
  764, 613, 11, 764, 12723, 830, 411, 294, 389, 12683, 570, 51096], "temperature":
  0.0, "avg_logprob": -0.2086505709954028, "compression_ratio": 1.5991735537190082,
  "no_speech_prob": 0.0008889439632184803}, {"id": 408, "seek": 271752, "start": 2732.16,
  "end": 2737.84, "text": " they''ve seen a lot of those. And in the href tag, refer
  to it. And here''s an example. And they just,", "tokens": [51096, 436, 600, 1612,
  257, 688, 295, 729, 13, 400, 294, 264, 276, 33115, 6162, 11, 2864, 281, 309, 13,
  400, 510, 311, 364, 1365, 13, 400, 436, 445, 11, 51380], "temperature": 0.0, "avg_logprob":
  -0.2086505709954028, "compression_ratio": 1.5991735537190082, "no_speech_prob":
  0.0008889439632184803}, {"id": 409, "seek": 271752, "start": 2739.52, "end": 2744.16,
  "text": " I haven''t done any like formal like, you know, reinforced testing, but
  in my experience with,", "tokens": [51464, 286, 2378, 380, 1096, 604, 411, 9860,
  411, 11, 291, 458, 11, 31365, 4997, 11, 457, 294, 452, 1752, 365, 11, 51696], "temperature":
  0.0, "avg_logprob": -0.2086505709954028, "compression_ratio": 1.5991735537190082,
  "no_speech_prob": 0.0008889439632184803}, {"id": 410, "seek": 274416, "start": 2744.24,
  "end": 2749.68, "text": " they just haven''t gone wrong. They are comfortable referring
  to these things. And it provides a really", "tokens": [50368, 436, 445, 2378, 380,
  2780, 2085, 13, 814, 366, 4619, 13761, 281, 613, 721, 13, 400, 309, 6417, 257, 534,
  50640], "temperature": 0.0, "avg_logprob": -0.11439032554626465, "compression_ratio":
  1.7885304659498207, "no_speech_prob": 0.0037125013768672943}, {"id": 411, "seek":
  274416, "start": 2750.3199999999997, "end": 2756.96, "text": " slick experience,
  I think, for the user. The user at the end of this conversation is looking at a",
  "tokens": [50672, 37406, 1752, 11, 286, 519, 11, 337, 264, 4195, 13, 440, 4195,
  412, 264, 917, 295, 341, 3761, 307, 1237, 412, 257, 51004], "temperature": 0.0,
  "avg_logprob": -0.11439032554626465, "compression_ratio": 1.7885304659498207, "no_speech_prob":
  0.0037125013768672943}, {"id": 412, "seek": 274416, "start": 2756.96, "end": 2760.96,
  "text": " conversation that they don''t have to scroll back up to. They''re looking
  at artifacts on the right.", "tokens": [51004, 3761, 300, 436, 500, 380, 362, 281,
  11369, 646, 493, 281, 13, 814, 434, 1237, 412, 24617, 322, 264, 558, 13, 51204],
  "temperature": 0.0, "avg_logprob": -0.11439032554626465, "compression_ratio": 1.7885304659498207,
  "no_speech_prob": 0.0037125013768672943}, {"id": 413, "seek": 274416, "start": 2760.96,
  "end": 2766.64, "text": " And they can, they can grab the ones they need. The artifacts
  themselves, you know, the application", "tokens": [51204, 400, 436, 393, 11, 436,
  393, 4444, 264, 2306, 436, 643, 13, 440, 24617, 2969, 11, 291, 458, 11, 264, 3861,
  51488], "temperature": 0.0, "avg_logprob": -0.11439032554626465, "compression_ratio":
  1.7885304659498207, "no_speech_prob": 0.0037125013768672943}, {"id": 414, "seek":
  274416, "start": 2766.64, "end": 2770.56, "text": " developer, you''re in charge
  of how you want to present these things. If it''s its text, you can just", "tokens":
  [51488, 10754, 11, 291, 434, 294, 4602, 295, 577, 291, 528, 281, 1974, 613, 721,
  13, 759, 309, 311, 1080, 2487, 11, 291, 393, 445, 51684], "temperature": 0.0, "avg_logprob":
  -0.11439032554626465, "compression_ratio": 1.7885304659498207, "no_speech_prob":
  0.0037125013768672943}, {"id": 415, "seek": 277056, "start": 2770.56, "end": 2775.2,
  "text": " make it text. But if it''s like a home listing, you know, in the background,
  it can really be", "tokens": [50364, 652, 309, 2487, 13, 583, 498, 309, 311, 411,
  257, 1280, 22161, 11, 291, 458, 11, 294, 264, 3678, 11, 309, 393, 534, 312, 50596],
  "temperature": 0.0, "avg_logprob": -0.1703711918422154, "compression_ratio": 1.7720588235294117,
  "no_speech_prob": 0.0019141300581395626}, {"id": 416, "seek": 277056, "start": 2775.2,
  "end": 2781.68, "text": " represented by, you know, JSON. But you present the user,
  you know, picture the home and the, you know,", "tokens": [50596, 10379, 538, 11,
  291, 458, 11, 31828, 13, 583, 291, 1974, 264, 4195, 11, 291, 458, 11, 3036, 264,
  1280, 293, 264, 11, 291, 458, 11, 50920], "temperature": 0.0, "avg_logprob": -0.1703711918422154,
  "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0019141300581395626},
  {"id": 417, "seek": 277056, "start": 2781.68, "end": 2787.36, "text": " scrollable
  tab and maybe a scheduling button. You can do all these rich things with artifacts",
  "tokens": [50920, 11369, 712, 4421, 293, 1310, 257, 29055, 2960, 13, 509, 393, 360,
  439, 613, 4593, 721, 365, 24617, 51204], "temperature": 0.0, "avg_logprob": -0.1703711918422154,
  "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0019141300581395626},
  {"id": 418, "seek": 277056, "start": 2787.92, "end": 2791.44, "text": " that you
  can''t do if you''re just having a chit chat conversation and it''s all just scrolling",
  "tokens": [51232, 300, 291, 393, 380, 360, 498, 291, 434, 445, 1419, 257, 417, 270,
  5081, 3761, 293, 309, 311, 439, 445, 11369, 278, 51408], "temperature": 0.0, "avg_logprob":
  -0.1703711918422154, "compression_ratio": 1.7720588235294117, "no_speech_prob":
  0.0019141300581395626}, {"id": 419, "seek": 277056, "start": 2791.44, "end": 2798.48,
  "text": " back into the back. So I think it''s a cool enough idea. I think there''s
  some indications that it''s", "tokens": [51408, 646, 666, 264, 646, 13, 407, 286,
  519, 309, 311, 257, 1627, 1547, 1558, 13, 286, 519, 456, 311, 512, 44450, 300, 309,
  311, 51760], "temperature": 0.0, "avg_logprob": -0.1703711918422154, "compression_ratio":
  1.7720588235294117, "no_speech_prob": 0.0019141300581395626}, {"id": 420, "seek":
  279848, "start": 2798.48, "end": 2804.4, "text": " coming into existence with, you
  know, Anthropocardic factor, GPT, OpenAI''s Canvas.", "tokens": [50364, 1348, 666,
  9123, 365, 11, 291, 458, 11, 12727, 1513, 905, 515, 299, 5952, 11, 26039, 51, 11,
  7238, 48698, 311, 25725, 13, 50660], "temperature": 0.0, "avg_logprob": -0.2982587192369544,
  "compression_ratio": 1.51528384279476, "no_speech_prob": 0.003516318742185831},
  {"id": 421, "seek": 279848, "start": 2808.0, "end": 2812.4, "text": " Persure is
  actually implicitly doing a really good job with somehow they''re doing this.",
  "tokens": [50840, 14006, 540, 307, 767, 26947, 356, 884, 257, 534, 665, 1691, 365,
  6063, 436, 434, 884, 341, 13, 51060], "temperature": 0.0, "avg_logprob": -0.2982587192369544,
  "compression_ratio": 1.51528384279476, "no_speech_prob": 0.003516318742185831},
  {"id": 422, "seek": 279848, "start": 2813.12, "end": 2818.64, "text": " So it''ll
  come into reality, I think, at some point. It just gives you an idea.", "tokens":
  [51096, 407, 309, 603, 808, 666, 4103, 11, 286, 519, 11, 412, 512, 935, 13, 467,
  445, 2709, 291, 364, 1558, 13, 51372], "temperature": 0.0, "avg_logprob": -0.2982587192369544,
  "compression_ratio": 1.51528384279476, "no_speech_prob": 0.003516318742185831},
  {"id": 423, "seek": 279848, "start": 2818.64, "end": 2824.88, "text": " Yeah. It
  feels like it structures the interaction with the element. It doesn''t feel like
  you lost", "tokens": [51372, 865, 13, 467, 3417, 411, 309, 9227, 264, 9285, 365,
  264, 4478, 13, 467, 1177, 380, 841, 411, 291, 2731, 51684], "temperature": 0.0,
  "avg_logprob": -0.2982587192369544, "compression_ratio": 1.51528384279476, "no_speech_prob":
  0.003516318742185831}, {"id": 424, "seek": 282488, "start": 2824.88, "end": 2829.92,
  "text": " your time in a way that you, like, it''s like you need to summarize it
  for your conversation, right?", "tokens": [50364, 428, 565, 294, 257, 636, 300,
  291, 11, 411, 11, 309, 311, 411, 291, 643, 281, 20858, 309, 337, 428, 3761, 11,
  558, 30, 50616], "temperature": 0.0, "avg_logprob": -0.21321314175923664, "compression_ratio":
  1.700374531835206, "no_speech_prob": 0.0097951740026474}, {"id": 425, "seek": 282488,
  "start": 2829.92, "end": 2834.96, "text": " To go back and like tell you what was
  important, right? But how does it all know what is important?", "tokens": [50616,
  1407, 352, 646, 293, 411, 980, 291, 437, 390, 1021, 11, 558, 30, 583, 577, 775,
  309, 439, 458, 437, 307, 1021, 30, 50868], "temperature": 0.0, "avg_logprob": -0.21321314175923664,
  "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0097951740026474}, {"id":
  426, "seek": 282488, "start": 2834.96, "end": 2839.2000000000003, "text": " You
  know, but you already forgot. And so if you have this artifacts, you can refer to
  them.", "tokens": [50868, 509, 458, 11, 457, 291, 1217, 5298, 13, 400, 370, 498,
  291, 362, 341, 24617, 11, 291, 393, 2864, 281, 552, 13, 51080], "temperature": 0.0,
  "avg_logprob": -0.21321314175923664, "compression_ratio": 1.700374531835206, "no_speech_prob":
  0.0097951740026474}, {"id": 427, "seek": 282488, "start": 2841.2000000000003, "end":
  2848.1600000000003, "text": " But it''s interesting that I think these artifacts
  can you use them? And by the way, I don''t know,", "tokens": [51180, 583, 309, 311,
  1880, 300, 286, 519, 613, 24617, 393, 291, 764, 552, 30, 400, 538, 264, 636, 11,
  286, 500, 380, 458, 11, 51528], "temperature": 0.0, "avg_logprob": -0.21321314175923664,
  "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0097951740026474}, {"id":
  428, "seek": 282488, "start": 2848.1600000000003, "end": 2850.8, "text": " if you
  can demo something quickly, I saw a demo on your website.", "tokens": [51528, 498,
  291, 393, 10723, 746, 2661, 11, 286, 1866, 257, 10723, 322, 428, 3144, 13, 51660],
  "temperature": 0.0, "avg_logprob": -0.21321314175923664, "compression_ratio": 1.700374531835206,
  "no_speech_prob": 0.0097951740026474}, {"id": 429, "seek": 285080, "start": 2851.76,
  "end": 2859.36, "text": " All right. So this is how do you go to my website? Oh,
  and you know, check this out.", "tokens": [50412, 1057, 558, 13, 407, 341, 307,
  577, 360, 291, 352, 281, 452, 3144, 30, 876, 11, 293, 291, 458, 11, 1520, 341, 484,
  13, 50792], "temperature": 0.0, "avg_logprob": -0.30674143040433843, "compression_ratio":
  1.4881516587677726, "no_speech_prob": 0.11062902212142944}, {"id": 430, "seek":
  285080, "start": 2859.36, "end": 2867.2000000000003, "text": " This website was
  me and like chat GPT and cursor just kind of hanging out, teaching me some HTML.",
  "tokens": [50792, 639, 3144, 390, 385, 293, 411, 5081, 26039, 51, 293, 28169, 445,
  733, 295, 8345, 484, 11, 4571, 385, 512, 17995, 13, 51184], "temperature": 0.0,
  "avg_logprob": -0.30674143040433843, "compression_ratio": 1.4881516587677726, "no_speech_prob":
  0.11062902212142944}, {"id": 431, "seek": 285080, "start": 2867.2000000000003, "end":
  2873.76, "text": " But yeah, you go to my blog. Wait a second. Wait a second. You''ve
  built this site with an LLM.", "tokens": [51184, 583, 1338, 11, 291, 352, 281, 452,
  6968, 13, 3802, 257, 1150, 13, 3802, 257, 1150, 13, 509, 600, 3094, 341, 3621, 365,
  364, 441, 43, 44, 13, 51512], "temperature": 0.0, "avg_logprob": -0.30674143040433843,
  "compression_ratio": 1.4881516587677726, "no_speech_prob": 0.11062902212142944},
  {"id": 432, "seek": 285080, "start": 2875.04, "end": 2877.44, "text": " Correct?
  Yeah. That''s what you said.", "tokens": [51576, 12753, 30, 865, 13, 663, 311, 437,
  291, 848, 13, 51696], "temperature": 0.0, "avg_logprob": -0.30674143040433843, "compression_ratio":
  1.4881516587677726, "no_speech_prob": 0.11062902212142944}, {"id": 433, "seek":
  287744, "start": 2877.44, "end": 2882.4, "text": " Well, it was me and a large language
  ball. It wouldn''t be just saying build a website.", "tokens": [50364, 1042, 11,
  309, 390, 385, 293, 257, 2416, 2856, 2594, 13, 467, 2759, 380, 312, 445, 1566, 1322,
  257, 3144, 13, 50612], "temperature": 0.0, "avg_logprob": -0.27813111490278103,
  "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.025666004046797752},
  {"id": 434, "seek": 287744, "start": 2882.4, "end": 2886.08, "text": " Of course.
  It''s going to, it''s the, it''s what''s going to happen in our future.", "tokens":
  [50612, 2720, 1164, 13, 467, 311, 516, 281, 11, 309, 311, 264, 11, 309, 311, 437,
  311, 516, 281, 1051, 294, 527, 2027, 13, 50796], "temperature": 0.0, "avg_logprob":
  -0.27813111490278103, "compression_ratio": 1.7601476014760147, "no_speech_prob":
  0.025666004046797752}, {"id": 435, "seek": 287744, "start": 2886.08, "end": 2890.32,
  "text": " It was everything is going to be a conversation working on this with a
  large language ball.", "tokens": [50796, 467, 390, 1203, 307, 516, 281, 312, 257,
  3761, 1364, 322, 341, 365, 257, 2416, 2856, 2594, 13, 51008], "temperature": 0.0,
  "avg_logprob": -0.27813111490278103, "compression_ratio": 1.7601476014760147, "no_speech_prob":
  0.025666004046797752}, {"id": 436, "seek": 287744, "start": 2890.32, "end": 2895.12,
  "text": " It''s a beautiful website. I have to say, yeah, amazing. And the logo.",
  "tokens": [51008, 467, 311, 257, 2238, 3144, 13, 286, 362, 281, 584, 11, 1338, 11,
  2243, 13, 400, 264, 9699, 13, 51248], "temperature": 0.0, "avg_logprob": -0.27813111490278103,
  "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.025666004046797752},
  {"id": 437, "seek": 287744, "start": 2895.12, "end": 2901.6, "text": " Even the
  sniffy little logo was generated. AI. Oh, amazing. Okay. This is ridiculous.", "tokens":
  [51248, 2754, 264, 31101, 88, 707, 9699, 390, 10833, 13, 7318, 13, 876, 11, 2243,
  13, 1033, 13, 639, 307, 11083, 13, 51572], "temperature": 0.0, "avg_logprob": -0.27813111490278103,
  "compression_ratio": 1.7601476014760147, "no_speech_prob": 0.025666004046797752},
  {"id": 438, "seek": 287744, "start": 2901.6, "end": 2904.0, "text": " I''m going
  to take up just a little bit of your time. It''s okay.", "tokens": [51572, 286,
  478, 516, 281, 747, 493, 445, 257, 707, 857, 295, 428, 565, 13, 467, 311, 1392,
  13, 51692], "temperature": 0.0, "avg_logprob": -0.27813111490278103, "compression_ratio":
  1.7601476014760147, "no_speech_prob": 0.025666004046797752}, {"id": 439, "seek":
  290400, "start": 2904.96, "end": 2910.56, "text": " Oh, it''s fine. This logo right
  here. Check out how many cool things out. There''s,", "tokens": [50412, 876, 11,
  309, 311, 2489, 13, 639, 9699, 558, 510, 13, 6881, 484, 577, 867, 1627, 721, 484,
  13, 821, 311, 11, 50692], "temperature": 0.0, "avg_logprob": -0.20681597636296198,
  "compression_ratio": 1.6079295154185023, "no_speech_prob": 0.012884764932096004},
  {"id": 440, "seek": 290400, "start": 2910.56, "end": 2914.72, "text": " there''s
  a bunch of little bits in here. And then I''ll make give you a quiz so you can find
  the last", "tokens": [50692, 456, 311, 257, 3840, 295, 707, 9239, 294, 510, 13,
  400, 550, 286, 603, 652, 976, 291, 257, 15450, 370, 291, 393, 915, 264, 1036, 50900],
  "temperature": 0.0, "avg_logprob": -0.20681597636296198, "compression_ratio": 1.6079295154185023,
  "no_speech_prob": 0.012884764932096004}, {"id": 441, "seek": 290400, "start": 2916.4,
  "end": 2922.48, "text": " thing hiding in this. Arcturus is a star in the Northern
  hemisphere. It''s a navigational", "tokens": [50984, 551, 10596, 294, 341, 13, 1587,
  349, 374, 301, 307, 257, 3543, 294, 264, 14335, 38453, 13, 467, 311, 257, 7407,
  1478, 51288], "temperature": 0.0, "avg_logprob": -0.20681597636296198, "compression_ratio":
  1.6079295154185023, "no_speech_prob": 0.012884764932096004}, {"id": 442, "seek":
  290400, "start": 2922.48, "end": 2929.44, "text": " star. It''s a brightest star.
  And it means guardian of the bear. And so with my cubo logo here,", "tokens": [51288,
  3543, 13, 467, 311, 257, 36271, 3543, 13, 400, 309, 1355, 30355, 295, 264, 6155,
  13, 400, 370, 365, 452, 10057, 78, 9699, 510, 11, 51636], "temperature": 0.0, "avg_logprob":
  -0.20681597636296198, "compression_ratio": 1.6079295154185023, "no_speech_prob":
  0.012884764932096004}, {"id": 443, "seek": 292944, "start": 2929.44, "end": 2934.4,
  "text": " you''ve got the a you got the bear. The a is kind of serves. It''s a little
  looks like", "tokens": [50364, 291, 600, 658, 264, 257, 291, 658, 264, 6155, 13,
  440, 257, 307, 733, 295, 13451, 13, 467, 311, 257, 707, 1542, 411, 50612], "temperature":
  0.0, "avg_logprob": -0.24754694529942103, "compression_ratio": 1.6487603305785123,
  "no_speech_prob": 0.0011738804168999195}, {"id": 444, "seek": 292944, "start": 2934.4,
  "end": 2939.12, "text": " guardian. The bears represent of the big hairy problem.
  That''s powerful. And but I''m going to,", "tokens": [50612, 30355, 13, 440, 17276,
  2906, 295, 264, 955, 42346, 1154, 13, 663, 311, 4005, 13, 400, 457, 286, 478, 516,
  281, 11, 50848], "temperature": 0.0, "avg_logprob": -0.24754694529942103, "compression_ratio":
  1.6487603305785123, "no_speech_prob": 0.0011738804168999195}, {"id": 445, "seek":
  292944, "start": 2939.12, "end": 2943.52, "text": " I''m going to help you out.
  The stars are all for for pointed. It''s navigational.", "tokens": [50848, 286,
  478, 516, 281, 854, 291, 484, 13, 440, 6105, 366, 439, 337, 337, 10932, 13, 467,
  311, 7407, 1478, 13, 51068], "temperature": 0.0, "avg_logprob": -0.24754694529942103,
  "compression_ratio": 1.6487603305785123, "no_speech_prob": 0.0011738804168999195},
  {"id": 446, "seek": 292944, "start": 2944.64, "end": 2951.76, "text": " There''s
  one more little uh, uh, Easter egg in this that I didn''t notice until I finished
  building it.", "tokens": [51124, 821, 311, 472, 544, 707, 2232, 11, 2232, 11, 9403,
  3777, 294, 341, 300, 286, 994, 380, 3449, 1826, 286, 4335, 2390, 309, 13, 51480],
  "temperature": 0.0, "avg_logprob": -0.24754694529942103, "compression_ratio": 1.6487603305785123,
  "no_speech_prob": 0.0011738804168999195}, {"id": 447, "seek": 292944, "start": 2951.76,
  "end": 2953.36, "text": " I didn''t design it. It just emerged.", "tokens": [51480,
  286, 994, 380, 1715, 309, 13, 467, 445, 20178, 13, 51560], "temperature": 0.0, "avg_logprob":
  -0.24754694529942103, "compression_ratio": 1.6487603305785123, "no_speech_prob":
  0.0011738804168999195}, {"id": 448, "seek": 295336, "start": 2954.08, "end": 2958.88,
  "text": " And if, yeah, I''ll start doing it.", "tokens": [50400, 400, 498, 11,
  1338, 11, 286, 603, 722, 884, 309, 13, 50640], "temperature": 0.0, "avg_logprob":
  -0.2758225003878276, "compression_ratio": 1.6648936170212767, "no_speech_prob":
  0.036097023636102676}, {"id": 449, "seek": 295336, "start": 2961.6, "end": 2967.28,
  "text": " If you''re a good computer scientist, especially, oh yeah, then yeah,
  yeah, a star search,", "tokens": [50776, 759, 291, 434, 257, 665, 3820, 12662, 11,
  2318, 11, 1954, 1338, 11, 550, 1338, 11, 1338, 11, 257, 3543, 3164, 11, 51060],
  "temperature": 0.0, "avg_logprob": -0.2758225003878276, "compression_ratio": 1.6648936170212767,
  "no_speech_prob": 0.036097023636102676}, {"id": 450, "seek": 295336, "start": 2967.28,
  "end": 2973.04, "text": " a star search. You got it. You got it. You got it. I didn''t
  even think about it. I just thought", "tokens": [51060, 257, 3543, 3164, 13, 509,
  658, 309, 13, 509, 658, 309, 13, 509, 658, 309, 13, 286, 994, 380, 754, 519, 466,
  309, 13, 286, 445, 1194, 51348], "temperature": 0.0, "avg_logprob": -0.2758225003878276,
  "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.036097023636102676},
  {"id": 451, "seek": 295336, "start": 2973.04, "end": 2977.92, "text": " this needs
  kind of a star over here. And I looked at it and it''s a star, which is, you know,",
  "tokens": [51348, 341, 2203, 733, 295, 257, 3543, 670, 510, 13, 400, 286, 2956,
  412, 309, 293, 309, 311, 257, 3543, 11, 597, 307, 11, 291, 458, 11, 51592], "temperature":
  0.0, "avg_logprob": -0.2758225003878276, "compression_ratio": 1.6648936170212767,
  "no_speech_prob": 0.036097023636102676}, {"id": 452, "seek": 297792, "start": 2978.0,
  "end": 2982.16, "text": " optimal, near optimal navigation of the difficult domain
  ahead.", "tokens": [50368, 16252, 11, 2651, 16252, 17346, 295, 264, 2252, 9274,
  2286, 13, 50576], "temperature": 0.0, "avg_logprob": -0.4031072096391158, "compression_ratio":
  1.5089285714285714, "no_speech_prob": 0.01714983582496643}, {"id": 453, "seek":
  297792, "start": 2983.6, "end": 2989.6, "text": " LMS is a good at creating Easter
  eggs then. Yeah, very terrible jazz. Yeah.", "tokens": [50648, 441, 10288, 307,
  257, 665, 412, 4084, 9403, 6466, 550, 13, 865, 11, 588, 6237, 15066, 13, 865, 13,
  50948], "temperature": 0.0, "avg_logprob": -0.4031072096391158, "compression_ratio":
  1.5089285714285714, "no_speech_prob": 0.01714983582496643}, {"id": 454, "seek":
  297792, "start": 2991.44, "end": 2998.08, "text": " So anyway, sorry, sorry for
  the, also the stars that was, I mean, these stars are amazing as well.", "tokens":
  [51040, 407, 4033, 11, 2597, 11, 2597, 337, 264, 11, 611, 264, 6105, 300, 390, 11,
  286, 914, 11, 613, 6105, 366, 2243, 382, 731, 13, 51372], "temperature": 0.0, "avg_logprob":
  -0.4031072096391158, "compression_ratio": 1.5089285714285714, "no_speech_prob":
  0.01714983582496643}, {"id": 455, "seek": 297792, "start": 2998.08, "end": 3005.36,
  "text": " You can just stare at them, right? And Marvel, they move, they look a
  bit like snowflakes sometimes", "tokens": [51372, 509, 393, 445, 22432, 412, 552,
  11, 558, 30, 400, 13837, 11, 436, 1286, 11, 436, 574, 257, 857, 411, 44124, 3419,
  2171, 51736], "temperature": 0.0, "avg_logprob": -0.4031072096391158, "compression_ratio":
  1.5089285714285714, "no_speech_prob": 0.01714983582496643}, {"id": 456, "seek":
  300536, "start": 3005.44, "end": 3012.1600000000003, "text": " as well. Yep, they
  do. All right, so thank you for the digression.", "tokens": [50368, 382, 731, 13,
  7010, 11, 436, 360, 13, 1057, 558, 11, 370, 1309, 291, 337, 264, 2528, 2775, 13,
  50704], "temperature": 0.0, "avg_logprob": -0.16619229524031928, "compression_ratio":
  1.6885245901639345, "no_speech_prob": 0.0031185667030513287}, {"id": 457, "seek":
  300536, "start": 3013.52, "end": 3018.56, "text": " We''re looking through my blog
  and we''re looking through, uh, cut the chit chat with artifacts.", "tokens": [50772,
  492, 434, 1237, 807, 452, 6968, 293, 321, 434, 1237, 807, 11, 2232, 11, 1723, 264,
  417, 270, 5081, 365, 24617, 13, 51024], "temperature": 0.0, "avg_logprob": -0.16619229524031928,
  "compression_ratio": 1.6885245901639345, "no_speech_prob": 0.0031185667030513287},
  {"id": 458, "seek": 300536, "start": 3018.56, "end": 3022.6400000000003, "text":
  " One thing I''m trying to do recently with my blog, and I hope you guys will, you
  know,", "tokens": [51024, 1485, 551, 286, 478, 1382, 281, 360, 3938, 365, 452, 6968,
  11, 293, 286, 1454, 291, 1074, 486, 11, 291, 458, 11, 51228], "temperature": 0.0,
  "avg_logprob": -0.16619229524031928, "compression_ratio": 1.6885245901639345, "no_speech_prob":
  0.0031185667030513287}, {"id": 459, "seek": 300536, "start": 3022.6400000000003,
  "end": 3027.6, "text": " there''s plenty of place where you can, uh, like, subscribe
  for this. I''m trying to put in", "tokens": [51228, 456, 311, 7140, 295, 1081, 689,
  291, 393, 11, 2232, 11, 411, 11, 3022, 337, 341, 13, 286, 478, 1382, 281, 829, 294,
  51476], "temperature": 0.0, "avg_logprob": -0.16619229524031928, "compression_ratio":
  1.6885245901639345, "no_speech_prob": 0.0031185667030513287}, {"id": 460, "seek":
  300536, "start": 3028.8, "end": 3033.76, "text": " plenty of examples. And here''s
  the kind of built-in example of it working.", "tokens": [51536, 7140, 295, 5110,
  13, 400, 510, 311, 264, 733, 295, 3094, 12, 259, 1365, 295, 309, 1364, 13, 51784],
  "temperature": 0.0, "avg_logprob": -0.16619229524031928, "compression_ratio": 1.6885245901639345,
  "no_speech_prob": 0.0031185667030513287}, {"id": 461, "seek": 303536, "start": 3035.6800000000003,
  "end": 3042.96, "text": " Let''s see. You know what, this, we might very well edit
  this out, but I''m going to go down to", "tokens": [50380, 961, 311, 536, 13, 509,
  458, 437, 11, 341, 11, 321, 1062, 588, 731, 8129, 341, 484, 11, 457, 286, 478, 516,
  281, 352, 760, 281, 50744], "temperature": 0.0, "avg_logprob": -0.193627709740991,
  "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0037873743567615747},
  {"id": 462, "seek": 303536, "start": 3042.96, "end": 3051.52, "text": " the now
  you try a bit right here. Oh, if this is, uh, in a naive approach, uh, let''s say
  that I''m", "tokens": [50744, 264, 586, 291, 853, 257, 857, 558, 510, 13, 876, 11,
  498, 341, 307, 11, 2232, 11, 294, 257, 29052, 3109, 11, 2232, 11, 718, 311, 584,
  300, 286, 478, 51172], "temperature": 0.0, "avg_logprob": -0.193627709740991, "compression_ratio":
  1.6483050847457628, "no_speech_prob": 0.0037873743567615747}, {"id": 463, "seek":
  303536, "start": 3051.52, "end": 3058.2400000000002, "text": " building like a real
  estate, uh, helper assistant. I help real estate agents. And the real estate agent",
  "tokens": [51172, 2390, 411, 257, 957, 9749, 11, 2232, 11, 36133, 10994, 13, 286,
  854, 957, 9749, 12554, 13, 400, 264, 957, 9749, 9461, 51508], "temperature": 0.0,
  "avg_logprob": -0.193627709740991, "compression_ratio": 1.6483050847457628, "no_speech_prob":
  0.0037873743567615747}, {"id": 464, "seek": 303536, "start": 3058.2400000000002,
  "end": 3063.1200000000003, "text": " says, I want to put together an email for client
  about, and I''m listed on Oak Street. Can you", "tokens": [51508, 1619, 11, 286,
  528, 281, 829, 1214, 364, 3796, 337, 6423, 466, 11, 293, 286, 478, 10052, 322, 19692,
  7638, 13, 1664, 291, 51752], "temperature": 0.0, "avg_logprob": -0.193627709740991,
  "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.0037873743567615747},
  {"id": 465, "seek": 306312, "start": 3063.12, "end": 3069.12, "text": " hold a listing?
  And so the thing has some tools built in. Uh, it''s got a get listing tool. And
  so", "tokens": [50364, 1797, 257, 22161, 30, 400, 370, 264, 551, 575, 512, 3873,
  3094, 294, 13, 4019, 11, 309, 311, 658, 257, 483, 22161, 2290, 13, 400, 370, 50664],
  "temperature": 0.0, "avg_logprob": -0.13347744260515484, "compression_ratio": 1.9254901960784314,
  "no_speech_prob": 0.003623353084549308}, {"id": 466, "seek": 306312, "start": 3069.12,
  "end": 3076.4, "text": " you can see all the garbage that puts in there. And it''s
  got this listing, um, but like, I don''t,", "tokens": [50664, 291, 393, 536, 439,
  264, 14150, 300, 8137, 294, 456, 13, 400, 309, 311, 658, 341, 22161, 11, 1105, 11,
  457, 411, 11, 286, 500, 380, 11, 51028], "temperature": 0.0, "avg_logprob": -0.13347744260515484,
  "compression_ratio": 1.9254901960784314, "no_speech_prob": 0.003623353084549308},
  {"id": 467, "seek": 306312, "start": 3076.4, "end": 3080.7999999999997, "text":
  " I''ve got the listing. It says it''s got the listing. Somehow all this garbage,
  there''s a listing,", "tokens": [51028, 286, 600, 658, 264, 22161, 13, 467, 1619,
  309, 311, 658, 264, 22161, 13, 28357, 439, 341, 14150, 11, 456, 311, 257, 22161,
  11, 51248], "temperature": 0.0, "avg_logprob": -0.13347744260515484, "compression_ratio":
  1.9254901960784314, "no_speech_prob": 0.003623353084549308}, {"id": 468, "seek":
  306312, "start": 3080.7999999999997, "end": 3086.64, "text": " but I don''t know
  what the listing''s really about, um, and so I could ask about it, but then it''s,",
  "tokens": [51248, 457, 286, 500, 380, 458, 437, 264, 22161, 311, 534, 466, 11, 1105,
  11, 293, 370, 286, 727, 1029, 466, 309, 11, 457, 550, 309, 311, 11, 51540], "temperature":
  0.0, "avg_logprob": -0.13347744260515484, "compression_ratio": 1.9254901960784314,
  "no_speech_prob": 0.003623353084549308}, {"id": 469, "seek": 306312, "start": 3086.64,
  "end": 3090.7999999999997, "text": " it''s a filter. I don''t have the thing that
  came from the database. I have this weird filter in front", "tokens": [51540, 309,
  311, 257, 6608, 13, 286, 500, 380, 362, 264, 551, 300, 1361, 490, 264, 8149, 13,
  286, 362, 341, 3657, 6608, 294, 1868, 51748], "temperature": 0.0, "avg_logprob":
  -0.13347744260515484, "compression_ratio": 1.9254901960784314, "no_speech_prob":
  0.003623353084549308}, {"id": 470, "seek": 309080, "start": 3090.8, "end": 3096.4,
  "text": " of it. Uh, can you pull an email template and draft a new email in another
  tool that it has?", "tokens": [50364, 295, 309, 13, 4019, 11, 393, 291, 2235, 364,
  3796, 12379, 293, 11206, 257, 777, 3796, 294, 1071, 2290, 300, 309, 575, 30, 50644],
  "temperature": 0.0, "avg_logprob": -0.3757033348083496, "compression_ratio": 1.2713178294573644,
  "no_speech_prob": 0.0034126616083085537}, {"id": 471, "seek": 309080, "start": 3105.36,
  "end": 3108.1600000000003, "text": " I guess it''s going to take its sweet time
  to do it. Oh, of course.", "tokens": [51092, 286, 2041, 309, 311, 516, 281, 747,
  1080, 3844, 565, 281, 360, 309, 13, 876, 11, 295, 1164, 13, 51232], "temperature":
  0.0, "avg_logprob": -0.3757033348083496, "compression_ratio": 1.2713178294573644,
  "no_speech_prob": 0.0034126616083085537}, {"id": 472, "seek": 309080, "start": 3109.6000000000004,
  "end": 3109.84, "text": " Hmm.", "tokens": [51304, 8239, 13, 51316], "temperature":
  0.0, "avg_logprob": -0.3757033348083496, "compression_ratio": 1.2713178294573644,
  "no_speech_prob": 0.0034126616083085537}, {"id": 473, "seek": 310984, "start": 3110.0,
  "end": 3110.8, "text": " Hmm.", "tokens": [50372, 8239, 13, 50412], "temperature":
  0.0, "avg_logprob": -0.34123297660581525, "compression_ratio": 1.5487179487179488,
  "no_speech_prob": 0.00704128947108984}, {"id": 474, "seek": 310984, "start": 3117.04,
  "end": 3124.6400000000003, "text": " Okay. Um, so it drafts, it drafts an email,
  but oh, look, I''ve forgotten this, the, the buyer''s name.", "tokens": [50724,
  1033, 13, 3301, 11, 370, 309, 11206, 82, 11, 309, 11206, 82, 364, 3796, 11, 457,
  1954, 11, 574, 11, 286, 600, 11832, 341, 11, 264, 11, 264, 24645, 311, 1315, 13,
  51104], "temperature": 0.0, "avg_logprob": -0.34123297660581525, "compression_ratio":
  1.5487179487179488, "no_speech_prob": 0.00704128947108984}, {"id": 475, "seek":
  310984, "start": 3124.6400000000003, "end": 3130.1600000000003, "text": " So this
  is one version of the email that is relevant to this thing right here. Uh, but,
  you know,", "tokens": [51104, 407, 341, 307, 472, 3037, 295, 264, 3796, 300, 307,
  7340, 281, 341, 551, 558, 510, 13, 4019, 11, 457, 11, 291, 458, 11, 51380], "temperature":
  0.0, "avg_logprob": -0.34123297660581525, "compression_ratio": 1.5487179487179488,
  "no_speech_prob": 0.00704128947108984}, {"id": 476, "seek": 310984, "start": 3130.1600000000003,
  "end": 3134.88, "text": " I''ve forgotten to tell you his name is Tim Cersei and
  my company''s name is Artie Tristral Estate.", "tokens": [51380, 286, 600, 11832,
  281, 980, 291, 702, 1315, 307, 7172, 26402, 43665, 293, 452, 2237, 311, 1315, 307,
  5735, 414, 1765, 468, 2155, 48097, 13, 51616], "temperature": 0.0, "avg_logprob":
  -0.34123297660581525, "compression_ratio": 1.5487179487179488, "no_speech_prob":
  0.00704128947108984}, {"id": 477, "seek": 313488, "start": 3135.84, "end": 3141.84,
  "text": " Uh, it goes back to this and so it fills it in and then I''m left at the
  end of the conversation,", "tokens": [50412, 4019, 11, 309, 1709, 646, 281, 341,
  293, 370, 309, 22498, 309, 294, 293, 550, 286, 478, 1411, 412, 264, 917, 295, 264,
  3761, 11, 50712], "temperature": 0.0, "avg_logprob": -0.12002897974270493, "compression_ratio":
  1.7527272727272727, "no_speech_prob": 0.012477729469537735}, {"id": 478, "seek":
  313488, "start": 3141.84, "end": 3146.8, "text": " you know, copy and paste in this
  out. If this is what I want, I''m going to paste this in the user''s", "tokens":
  [50712, 291, 458, 11, 5055, 293, 9163, 294, 341, 484, 13, 759, 341, 307, 437, 286,
  528, 11, 286, 478, 516, 281, 9163, 341, 294, 264, 4195, 311, 50960], "temperature":
  0.0, "avg_logprob": -0.12002897974270493, "compression_ratio": 1.7527272727272727,
  "no_speech_prob": 0.012477729469537735}, {"id": 479, "seek": 313488, "start": 3146.8,
  "end": 3151.2000000000003, "text": " email and be really embarrassed when it''s
  got this little string at the top because I''ve copied that", "tokens": [50960,
  3796, 293, 312, 534, 16843, 562, 309, 311, 658, 341, 707, 6798, 412, 264, 1192,
  570, 286, 600, 25365, 300, 51180], "temperature": 0.0, "avg_logprob": -0.12002897974270493,
  "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.012477729469537735},
  {"id": 480, "seek": 313488, "start": 3151.2000000000003, "end": 3156.56, "text":
  " out. And if I wanted to do anything else like modify the template or do anything,
  it''s,", "tokens": [51180, 484, 13, 400, 498, 286, 1415, 281, 360, 1340, 1646, 411,
  16927, 264, 12379, 420, 360, 1340, 11, 309, 311, 11, 51448], "temperature": 0.0,
  "avg_logprob": -0.12002897974270493, "compression_ratio": 1.7527272727272727, "no_speech_prob":
  0.012477729469537735}, {"id": 481, "seek": 313488, "start": 3157.28, "end": 3161.6,
  "text": " it''s, it''s just, it''s not there for me. All right. So let''s, let''s
  do a similar experience with,", "tokens": [51484, 309, 311, 11, 309, 311, 445, 11,
  309, 311, 406, 456, 337, 385, 13, 1057, 558, 13, 407, 718, 311, 11, 718, 311, 360,
  257, 2531, 1752, 365, 11, 51700], "temperature": 0.0, "avg_logprob": -0.12002897974270493,
  "compression_ratio": 1.7527272727272727, "no_speech_prob": 0.012477729469537735},
  {"id": 482, "seek": 316160, "start": 3162.08, "end": 3167.36, "text": " with this.
  I want to, uh, again, pull out that listing per, for Oak Street. Oh, I have an interesting.",
  "tokens": [50388, 365, 341, 13, 286, 528, 281, 11, 2232, 11, 797, 11, 2235, 484,
  300, 22161, 680, 11, 337, 19692, 7638, 13, 876, 11, 286, 362, 364, 1880, 13, 50652],
  "temperature": 0.0, "avg_logprob": -0.26006824269014245, "compression_ratio": 1.4567307692307692,
  "no_speech_prob": 0.005329612176865339}, {"id": 483, "seek": 316160, "start": 3174.08,
  "end": 3179.12, "text": " All right. So in this time, I''m still showing that it,
  it knows how to use tools, but every time it", "tokens": [50988, 1057, 558, 13,
  407, 294, 341, 565, 11, 286, 478, 920, 4099, 300, 309, 11, 309, 3255, 577, 281,
  764, 3873, 11, 457, 633, 565, 309, 51240], "temperature": 0.0, "avg_logprob": -0.26006824269014245,
  "compression_ratio": 1.4567307692307692, "no_speech_prob": 0.005329612176865339},
  {"id": 484, "seek": 316160, "start": 3179.12, "end": 3185.44, "text": " tries to
  spit out this like JSON stuff, it''s actually getting substituted in with an HRF
  that points", "tokens": [51240, 9898, 281, 22127, 484, 341, 411, 31828, 1507, 11,
  309, 311, 767, 1242, 26441, 4866, 294, 365, 364, 389, 49, 37, 300, 2793, 51556],
  "temperature": 0.0, "avg_logprob": -0.26006824269014245, "compression_ratio": 1.4567307692307692,
  "no_speech_prob": 0.005329612176865339}, {"id": 485, "seek": 318544, "start": 3185.52,
  "end": 3191.6, "text": " to it. And what is it point to where you click on it and
  it automatically loads, uh, this", "tokens": [50368, 281, 309, 13, 400, 437, 307,
  309, 935, 281, 689, 291, 2052, 322, 309, 293, 309, 6772, 12668, 11, 2232, 11, 341,
  50672], "temperature": 0.0, "avg_logprob": -0.2031742654195646, "compression_ratio":
  1.6379310344827587, "no_speech_prob": 0.016665233299136162}, {"id": 486, "seek":
  318544, "start": 3191.6, "end": 3196.56, "text": " scar right here. Now, um, I didn''t
  take time to make a real pretty interface, but you can,", "tokens": [50672, 10569,
  558, 510, 13, 823, 11, 1105, 11, 286, 994, 380, 747, 565, 281, 652, 257, 957, 1238,
  9226, 11, 457, 291, 393, 11, 50920], "temperature": 0.0, "avg_logprob": -0.2031742654195646,
  "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.016665233299136162},
  {"id": 487, "seek": 318544, "start": 3196.56, "end": 3200.7200000000003, "text":
  " imagine this is JSON. You can make this look like anything you want to. You can
  make it link out", "tokens": [50920, 3811, 341, 307, 31828, 13, 509, 393, 652, 341,
  574, 411, 1340, 291, 528, 281, 13, 509, 393, 652, 309, 2113, 484, 51128], "temperature":
  0.0, "avg_logprob": -0.2031742654195646, "compression_ratio": 1.6379310344827587,
  "no_speech_prob": 0.016665233299136162}, {"id": 488, "seek": 318544, "start": 3200.7200000000003,
  "end": 3205.52, "text": " to the database and do all sorts of things. All right.
  I''m going to put together that email template", "tokens": [51128, 281, 264, 8149,
  293, 360, 439, 7527, 295, 721, 13, 1057, 558, 13, 286, 478, 516, 281, 829, 1214,
  300, 3796, 12379, 51368], "temperature": 0.0, "avg_logprob": -0.2031742654195646,
  "compression_ratio": 1.6379310344827587, "no_speech_prob": 0.016665233299136162},
  {"id": 489, "seek": 318544, "start": 3205.52, "end": 3211.84, "text": " again. Yeah.
  I guess especially when you build a dedicated LAM application, right? You know what",
  "tokens": [51368, 797, 13, 865, 13, 286, 2041, 2318, 562, 291, 1322, 257, 8374,
  441, 2865, 3861, 11, 558, 30, 509, 458, 437, 51684], "temperature": 0.0, "avg_logprob":
  -0.2031742654195646, "compression_ratio": 1.6379310344827587, "no_speech_prob":
  0.016665233299136162}, {"id": 490, "seek": 321184, "start": 3211.84, "end": 3217.04,
  "text": " type of, what types of objects you''re going to be interacting with and
  you can build the, you know,", "tokens": [50364, 2010, 295, 11, 437, 3467, 295,
  6565, 291, 434, 516, 281, 312, 18017, 365, 293, 291, 393, 1322, 264, 11, 291, 458,
  11, 50624], "temperature": 0.0, "avg_logprob": -0.2142804219172551, "compression_ratio":
  1.7376425855513309, "no_speech_prob": 0.0027152097318321466}, {"id": 491, "seek":
  321184, "start": 3217.76, "end": 3221.84, "text": " I can do UI around those, right?
  But yeah, a very flexible,", "tokens": [50660, 286, 393, 360, 15682, 926, 729, 11,
  558, 30, 583, 1338, 11, 257, 588, 11358, 11, 50864], "temperature": 0.0, "avg_logprob":
  -0.2142804219172551, "compression_ratio": 1.7376425855513309, "no_speech_prob":
  0.0027152097318321466}, {"id": 492, "seek": 321184, "start": 3221.84, "end": 3226.08,
  "text": " manable interface. The interface is whatever the user needs it to be potentially.
  Yeah. All right.", "tokens": [50864, 587, 712, 9226, 13, 440, 9226, 307, 2035, 264,
  4195, 2203, 309, 281, 312, 7263, 13, 865, 13, 1057, 558, 13, 51076], "temperature":
  0.0, "avg_logprob": -0.2142804219172551, "compression_ratio": 1.7376425855513309,
  "no_speech_prob": 0.0027152097318321466}, {"id": 493, "seek": 321184, "start": 3226.08,
  "end": 3233.28, "text": " So it''s, uh, it''s, uh, it''s, uh, got this customized
  email draft. Now, uh, you know, I was looking", "tokens": [51076, 407, 309, 311,
  11, 2232, 11, 309, 311, 11, 2232, 11, 309, 311, 11, 2232, 11, 658, 341, 30581, 3796,
  11206, 13, 823, 11, 2232, 11, 291, 458, 11, 286, 390, 1237, 51436], "temperature":
  0.0, "avg_logprob": -0.2142804219172551, "compression_ratio": 1.7376425855513309,
  "no_speech_prob": 0.0027152097318321466}, {"id": 494, "seek": 321184, "start": 3233.28,
  "end": 3237.84, "text": " here, there''s no email draft here, but there is here
  on, on the side of the screen. And you can see", "tokens": [51436, 510, 11, 456,
  311, 572, 3796, 11206, 510, 11, 457, 456, 307, 510, 322, 11, 322, 264, 1252, 295,
  264, 2568, 13, 400, 291, 393, 536, 51664], "temperature": 0.0, "avg_logprob": -0.2142804219172551,
  "compression_ratio": 1.7376425855513309, "no_speech_prob": 0.0027152097318321466},
  {"id": 495, "seek": 323784, "start": 3237.84, "end": 3245.6800000000003, "text":
  " unfortunately, I forgot, uh, to stick in the user''s names. So, uh, let''s see.
  Here''s the template", "tokens": [50364, 7015, 11, 286, 5298, 11, 2232, 11, 281,
  2897, 294, 264, 4195, 311, 5288, 13, 407, 11, 2232, 11, 718, 311, 536, 13, 1692,
  311, 264, 12379, 50756], "temperature": 0.0, "avg_logprob": -0.12056072737819465,
  "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.0036527165211737156},
  {"id": 496, "seek": 323784, "start": 3245.6800000000003, "end": 3250.0, "text":
  " that it used. We didn''t see that in the last example. You can see how it wants
  to put together", "tokens": [50756, 300, 309, 1143, 13, 492, 994, 380, 536, 300,
  294, 264, 1036, 1365, 13, 509, 393, 536, 577, 309, 2738, 281, 829, 1214, 50972],
  "temperature": 0.0, "avg_logprob": -0.12056072737819465, "compression_ratio": 1.7647058823529411,
  "no_speech_prob": 0.0036527165211737156}, {"id": 497, "seek": 323784, "start": 3250.0,
  "end": 3256.48, "text": " stuff. You can see how it actually put together stuff.
  And whenever I said I forgot his name,", "tokens": [50972, 1507, 13, 509, 393, 536,
  577, 309, 767, 829, 1214, 1507, 13, 400, 5699, 286, 848, 286, 5298, 702, 1315, 11,
  51296], "temperature": 0.0, "avg_logprob": -0.12056072737819465, "compression_ratio":
  1.7647058823529411, "no_speech_prob": 0.0036527165211737156}, {"id": 498, "seek":
  323784, "start": 3256.48, "end": 3260.88, "text": " it said, Oh, okay, I''ve updated
  that artifact for you. So you don''t have like multiple versions", "tokens": [51296,
  309, 848, 11, 876, 11, 1392, 11, 286, 600, 10588, 300, 34806, 337, 291, 13, 407,
  291, 500, 380, 362, 411, 3866, 9606, 51516], "temperature": 0.0, "avg_logprob":
  -0.12056072737819465, "compression_ratio": 1.7647058823529411, "no_speech_prob":
  0.0036527165211737156}, {"id": 499, "seek": 323784, "start": 3260.88, "end": 3266.48,
  "text": " scaling up. You just got this. And you could do even interesting things
  like I could say, uh, you", "tokens": [51516, 21589, 493, 13, 509, 445, 658, 341,
  13, 400, 291, 727, 360, 754, 1880, 721, 411, 286, 727, 584, 11, 2232, 11, 291, 51796],
  "temperature": 0.0, "avg_logprob": -0.12056072737819465, "compression_ratio": 1.7647058823529411,
  "no_speech_prob": 0.0036527165211737156}, {"id": 500, "seek": 326648, "start": 3266.96,
  "end": 3273.12, "text": " know, this is much better if I say gone bare, you man
  right here. And that is now part of the artifact", "tokens": [50388, 458, 11, 341,
  307, 709, 1101, 498, 286, 584, 2780, 6949, 11, 291, 587, 558, 510, 13, 400, 300,
  307, 586, 644, 295, 264, 34806, 50696], "temperature": 0.0, "avg_logprob": -0.22544487061039095,
  "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.001434390083886683},
  {"id": 501, "seek": 326648, "start": 3273.12, "end": 3278.16, "text": " that the
  assistant sees. Uh, it''s, it''s in that artifact section at the top of the prompt.",
  "tokens": [50696, 300, 264, 10994, 8194, 13, 4019, 11, 309, 311, 11, 309, 311, 294,
  300, 34806, 3541, 412, 264, 1192, 295, 264, 12391, 13, 50948], "temperature": 0.0,
  "avg_logprob": -0.22544487061039095, "compression_ratio": 1.6942446043165467, "no_speech_prob":
  0.001434390083886683}, {"id": 502, "seek": 326648, "start": 3278.64, "end": 3282.8,
  "text": " You can have it say, please change my email prompt forever to say something
  out of like this.", "tokens": [50972, 509, 393, 362, 309, 584, 11, 1767, 1319, 452,
  3796, 12391, 5680, 281, 584, 746, 484, 295, 411, 341, 13, 51180], "temperature":
  0.0, "avg_logprob": -0.22544487061039095, "compression_ratio": 1.6942446043165467,
  "no_speech_prob": 0.001434390083886683}, {"id": 503, "seek": 326648, "start": 3282.8,
  "end": 3287.76, "text": " And you can work on this and say that back to the day.
  It just, oh, it opens up a lot of", "tokens": [51180, 400, 291, 393, 589, 322, 341,
  293, 584, 300, 646, 281, 264, 786, 13, 467, 445, 11, 1954, 11, 309, 9870, 493, 257,
  688, 295, 51428], "temperature": 0.0, "avg_logprob": -0.22544487061039095, "compression_ratio":
  1.6942446043165467, "no_speech_prob": 0.001434390083886683}, {"id": 504, "seek":
  326648, "start": 3288.56, "end": 3294.2400000000002, "text": " possibilities for
  a user experience that is easier. Because when we get work done, we get work", "tokens":
  [51468, 12178, 337, 257, 4195, 1752, 300, 307, 3571, 13, 1436, 562, 321, 483, 589,
  1096, 11, 321, 483, 589, 51752], "temperature": 0.0, "avg_logprob": -0.22544487061039095,
  "compression_ratio": 1.6942446043165467, "no_speech_prob": 0.001434390083886683},
  {"id": 505, "seek": 329424, "start": 3294.72, "end": 3301.68, "text": " on things,
  not just check. Yeah. Your reason, your reason around artifacts and you work with
  them", "tokens": [50388, 322, 721, 11, 406, 445, 1520, 13, 865, 13, 2260, 1778,
  11, 428, 1778, 926, 24617, 293, 291, 589, 365, 552, 50736], "temperature": 0.0,
  "avg_logprob": -0.1835279228273502, "compression_ratio": 1.7946768060836502, "no_speech_prob":
  0.021373102441430092}, {"id": 506, "seek": 329424, "start": 3301.68, "end": 3306.72,
  "text": " like as if they were physical objects almost, right? You can take away
  this thing with you and", "tokens": [50736, 411, 382, 498, 436, 645, 4001, 6565,
  1920, 11, 558, 30, 509, 393, 747, 1314, 341, 551, 365, 291, 293, 50988], "temperature":
  0.0, "avg_logprob": -0.1835279228273502, "compression_ratio": 1.7946768060836502,
  "no_speech_prob": 0.021373102441430092}, {"id": 507, "seek": 329424, "start": 3306.72,
  "end": 3312.9599999999996, "text": " go proceed with your task. Yep. You refer to
  them. You modify them. We use them to do things.", "tokens": [50988, 352, 8991,
  365, 428, 5633, 13, 7010, 13, 509, 2864, 281, 552, 13, 509, 16927, 552, 13, 492,
  764, 552, 281, 360, 721, 13, 51300], "temperature": 0.0, "avg_logprob": -0.1835279228273502,
  "compression_ratio": 1.7946768060836502, "no_speech_prob": 0.021373102441430092},
  {"id": 508, "seek": 329424, "start": 3313.9199999999996, "end": 3318.56, "text":
  " And you could, I''m guessing. I''m really guessing. I''m new to this topic. You
  could maybe even", "tokens": [51348, 400, 291, 727, 11, 286, 478, 17939, 13, 286,
  478, 534, 17939, 13, 286, 478, 777, 281, 341, 4829, 13, 509, 727, 1310, 754, 51580],
  "temperature": 0.0, "avg_logprob": -0.1835279228273502, "compression_ratio": 1.7946768060836502,
  "no_speech_prob": 0.021373102441430092}, {"id": 509, "seek": 329424, "start": 3318.56,
  "end": 3322.7999999999997, "text": " condition the model on these things, right?
  You could say given this artifact, I want to do", "tokens": [51580, 4188, 264, 2316,
  322, 613, 721, 11, 558, 30, 509, 727, 584, 2212, 341, 34806, 11, 286, 528, 281,
  360, 51792], "temperature": 0.0, "avg_logprob": -0.1835279228273502, "compression_ratio":
  1.7946768060836502, "no_speech_prob": 0.021373102441430092}, {"id": 510, "seek":
  332280, "start": 3322.8, "end": 3329.04, "text": " something else with it, like
  rewrite some parts. You know, would that work? I mean,", "tokens": [50364, 746,
  1646, 365, 309, 11, 411, 28132, 512, 3166, 13, 509, 458, 11, 576, 300, 589, 30,
  286, 914, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1401319806537931, "compression_ratio":
  1.7944664031620554, "no_speech_prob": 0.002486889250576496}, {"id": 511, "seek":
  332280, "start": 3331.04, "end": 3335.36, "text": " this kind of sky is the limit.
  Uh, it''s, it''s kind of been a fun thing to think about. But", "tokens": [50776,
  341, 733, 295, 5443, 307, 264, 4948, 13, 4019, 11, 309, 311, 11, 309, 311, 733,
  295, 668, 257, 1019, 551, 281, 519, 466, 13, 583, 50992], "temperature": 0.0, "avg_logprob":
  -0.1401319806537931, "compression_ratio": 1.7944664031620554, "no_speech_prob":
  0.002486889250576496}, {"id": 512, "seek": 332280, "start": 3336.5600000000004,
  "end": 3340.96, "text": " you could have typed artifacts. And then when you have
  a certain type of artifact, you could", "tokens": [51052, 291, 727, 362, 33941,
  24617, 13, 400, 550, 562, 291, 362, 257, 1629, 2010, 295, 34806, 11, 291, 727, 51272],
  "temperature": 0.0, "avg_logprob": -0.1401319806537931, "compression_ratio": 1.7944664031620554,
  "no_speech_prob": 0.002486889250576496}, {"id": 513, "seek": 332280, "start": 3341.52,
  "end": 3347.28, "text": " introduce the tools. Uh, so like, you know, if we need
  to modify this artifact artifact, we can,", "tokens": [51300, 5366, 264, 3873, 13,
  4019, 11, 370, 411, 11, 291, 458, 11, 498, 321, 643, 281, 16927, 341, 34806, 34806,
  11, 321, 393, 11, 51588], "temperature": 0.0, "avg_logprob": -0.1401319806537931,
  "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.002486889250576496},
  {"id": 514, "seek": 332280, "start": 3347.28, "end": 3351.1200000000003, "text":
  " we can know how to deal with it. You can have, it''s kind of what I did with my
  next post,", "tokens": [51588, 321, 393, 458, 577, 281, 2028, 365, 309, 13, 509,
  393, 362, 11, 309, 311, 733, 295, 437, 286, 630, 365, 452, 958, 2183, 11, 51780],
  "temperature": 0.0, "avg_logprob": -0.1401319806537931, "compression_ratio": 1.7944664031620554,
  "no_speech_prob": 0.002486889250576496}, {"id": 515, "seek": 335112, "start": 3351.68,
  "end": 3355.8399999999997, "text": " the roaming rag. You can have artifacts that
  are like accordions. They''re, they''re bigger than", "tokens": [50392, 264, 42680,
  17539, 13, 509, 393, 362, 24617, 300, 366, 411, 18640, 626, 13, 814, 434, 11, 436,
  434, 3801, 813, 50600], "temperature": 0.0, "avg_logprob": -0.1719035179384293,
  "compression_ratio": 1.772563176895307, "no_speech_prob": 0.004317078739404678},
  {"id": 516, "seek": 335112, "start": 3355.8399999999997, "end": 3363.2, "text":
  " fit in the prompt. But you can say, you know, here''s summarized outline everything
  in every piece", "tokens": [50600, 3318, 294, 264, 12391, 13, 583, 291, 393, 584,
  11, 291, 458, 11, 510, 311, 14611, 1602, 16387, 1203, 294, 633, 2522, 50968], "temperature":
  0.0, "avg_logprob": -0.1719035179384293, "compression_ratio": 1.772563176895307,
  "no_speech_prob": 0.004317078739404678}, {"id": 517, "seek": 335112, "start": 3363.2,
  "end": 3368.08, "text": " of that summary, uh, the model effectively can click on
  it and expand it. And it''s just another", "tokens": [50968, 295, 300, 12691, 11,
  2232, 11, 264, 2316, 8659, 393, 2052, 322, 309, 293, 5268, 309, 13, 400, 309, 311,
  445, 1071, 51212], "temperature": 0.0, "avg_logprob": -0.1719035179384293, "compression_ratio":
  1.772563176895307, "no_speech_prob": 0.004317078739404678}, {"id": 518, "seek":
  335112, "start": 3368.08, "end": 3374.24, "text": " ID and, you know, a tool expand
  the section. So it can read docs that are bigger than fits in its", "tokens": [51212,
  7348, 293, 11, 291, 458, 11, 257, 2290, 5268, 264, 3541, 13, 407, 309, 393, 1401,
  45623, 300, 366, 3801, 813, 9001, 294, 1080, 51520], "temperature": 0.0, "avg_logprob":
  -0.1719035179384293, "compression_ratio": 1.772563176895307, "no_speech_prob": 0.004317078739404678},
  {"id": 519, "seek": 335112, "start": 3374.24, "end": 3380.0, "text": " context.
  There''s just a lot of neat things that I think you can do with artifacts at the
  starting point.", "tokens": [51520, 4319, 13, 821, 311, 445, 257, 688, 295, 10654,
  721, 300, 286, 519, 291, 393, 360, 365, 24617, 412, 264, 2891, 935, 13, 51808],
  "temperature": 0.0, "avg_logprob": -0.1719035179384293, "compression_ratio": 1.772563176895307,
  "no_speech_prob": 0.004317078739404678}, {"id": 520, "seek": 338000, "start": 3380.72,
  "end": 3386.32, "text": " It''s very interesting. Don''t you think that just one
  thought across my mind is that when we", "tokens": [50400, 467, 311, 588, 1880,
  13, 1468, 380, 291, 519, 300, 445, 472, 1194, 2108, 452, 1575, 307, 300, 562, 321,
  50680], "temperature": 0.0, "avg_logprob": -0.15813912285698783, "compression_ratio":
  1.61864406779661, "no_speech_prob": 0.0071645695716142654}, {"id": 521, "seek":
  338000, "start": 3386.32, "end": 3392.4, "text": " transitioned from static web
  to like web 2.0, I guess, so what is what was it called when you can", "tokens":
  [50680, 47346, 490, 13437, 3670, 281, 411, 3670, 568, 13, 15, 11, 286, 2041, 11,
  370, 437, 307, 437, 390, 309, 1219, 562, 291, 393, 50984], "temperature": 0.0, "avg_logprob":
  -0.15813912285698783, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.0071645695716142654},
  {"id": 522, "seek": 338000, "start": 3392.4, "end": 3397.44, "text": " actually
  modify things on the web, right? You could send a comment, you could, you could
  do stuff.", "tokens": [50984, 767, 16927, 721, 322, 264, 3670, 11, 558, 30, 509,
  727, 2845, 257, 2871, 11, 291, 727, 11, 291, 727, 360, 1507, 13, 51236], "temperature":
  0.0, "avg_logprob": -0.15813912285698783, "compression_ratio": 1.61864406779661,
  "no_speech_prob": 0.0071645695716142654}, {"id": 523, "seek": 338000, "start": 3398.24,
  "end": 3405.2, "text": " Uh, now it feels like we''ve transitioned into the new
  phase when we do the same to the ideas.", "tokens": [51276, 4019, 11, 586, 309,
  3417, 411, 321, 600, 47346, 666, 264, 777, 5574, 562, 321, 360, 264, 912, 281, 264,
  3487, 13, 51624], "temperature": 0.0, "avg_logprob": -0.15813912285698783, "compression_ratio":
  1.61864406779661, "no_speech_prob": 0.0071645695716142654}, {"id": 524, "seek":
  340520, "start": 3405.8399999999997, "end": 3413.2, "text": " We like exchange ideas
  and we can like modify them, you know, prior on them, prompt with them,", "tokens":
  [50396, 492, 411, 7742, 3487, 293, 321, 393, 411, 16927, 552, 11, 291, 458, 11,
  4059, 322, 552, 11, 12391, 365, 552, 11, 50764], "temperature": 0.0, "avg_logprob":
  -0.15131521224975586, "compression_ratio": 1.6056338028169015, "no_speech_prob":
  0.019538797438144684}, {"id": 525, "seek": 340520, "start": 3414.24, "end": 3420.0,
  "text": " uh, take away a store. So it becomes more on the concept level.", "tokens":
  [50816, 2232, 11, 747, 1314, 257, 3531, 13, 407, 309, 3643, 544, 322, 264, 3410,
  1496, 13, 51104], "temperature": 0.0, "avg_logprob": -0.15131521224975586, "compression_ratio":
  1.6056338028169015, "no_speech_prob": 0.019538797438144684}, {"id": 526, "seek":
  340520, "start": 3422.24, "end": 3427.2799999999997, "text": " I think everything''s
  going to get really weird, uh, going forward. I think we''ve been used to", "tokens":
  [51216, 286, 519, 1203, 311, 516, 281, 483, 534, 3657, 11, 2232, 11, 516, 2128,
  13, 286, 519, 321, 600, 668, 1143, 281, 51468], "temperature": 0.0, "avg_logprob":
  -0.15131521224975586, "compression_ratio": 1.6056338028169015, "no_speech_prob":
  0.019538797438144684}, {"id": 527, "seek": 340520, "start": 3427.2799999999997,
  "end": 3431.6, "text": " going to the internet and going to web pages. And even
  if we could interact a little bit,", "tokens": [51468, 516, 281, 264, 4705, 293,
  516, 281, 3670, 7183, 13, 400, 754, 498, 321, 727, 4648, 257, 707, 857, 11, 51684],
  "temperature": 0.0, "avg_logprob": -0.15131521224975586, "compression_ratio": 1.6056338028169015,
  "no_speech_prob": 0.019538797438144684}, {"id": 528, "seek": 343160, "start": 3431.68,
  "end": 3436.72, "text": " it''s nothing like you''re about to see. I wonder if a
  lot of the internet experiences,", "tokens": [50368, 309, 311, 1825, 411, 291, 434,
  466, 281, 536, 13, 286, 2441, 498, 257, 688, 295, 264, 4705, 5235, 11, 50620], "temperature":
  0.0, "avg_logprob": -0.1799073259369666, "compression_ratio": 1.7992424242424243,
  "no_speech_prob": 0.002159588737413287}, {"id": 529, "seek": 343160, "start": 3436.72,
  "end": 3442.0, "text": " you know, they''re worried about all the text going away,
  uh, because like we were, we''d run out,", "tokens": [50620, 291, 458, 11, 436,
  434, 5804, 466, 439, 264, 2487, 516, 1314, 11, 2232, 11, 570, 411, 321, 645, 11,
  321, 1116, 1190, 484, 11, 50884], "temperature": 0.0, "avg_logprob": -0.1799073259369666,
  "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.002159588737413287},
  {"id": 530, "seek": 343160, "start": 3442.0, "end": 3446.64, "text": " run out of
  the text, the internet, training these giant giant models. Maybe the future of the",
  "tokens": [50884, 1190, 484, 295, 264, 2487, 11, 264, 4705, 11, 3097, 613, 7410,
  7410, 5245, 13, 2704, 264, 2027, 295, 264, 51116], "temperature": 0.0, "avg_logprob":
  -0.1799073259369666, "compression_ratio": 1.7992424242424243, "no_speech_prob":
  0.002159588737413287}, {"id": 531, "seek": 343160, "start": 3446.64, "end": 3453.44,
  "text": " internet is going to be replaced by just conversations. The, you''re going
  to go to a place that is a", "tokens": [51116, 4705, 307, 516, 281, 312, 10772,
  538, 445, 7315, 13, 440, 11, 291, 434, 516, 281, 352, 281, 257, 1081, 300, 307,
  257, 51456], "temperature": 0.0, "avg_logprob": -0.1799073259369666, "compression_ratio":
  1.7992424242424243, "no_speech_prob": 0.002159588737413287}, {"id": 532, "seek":
  343160, "start": 3453.44, "end": 3458.72, "text": " sensible, you know, starting
  point, but the whole website is going to become whatever reality you", "tokens":
  [51456, 25380, 11, 291, 458, 11, 2891, 935, 11, 457, 264, 1379, 3144, 307, 516,
  281, 1813, 2035, 4103, 291, 51720], "temperature": 0.0, "avg_logprob": -0.1799073259369666,
  "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.002159588737413287},
  {"id": 533, "seek": 345872, "start": 3458.72, "end": 3464.48, "text": " need it
  to be at the time. And I have no idea how we harvest the text of that train of future
  models.", "tokens": [50364, 643, 309, 281, 312, 412, 264, 565, 13, 400, 286, 362,
  572, 1558, 577, 321, 11917, 264, 2487, 295, 300, 3847, 295, 2027, 5245, 13, 50652],
  "temperature": 0.0, "avg_logprob": -0.28321976788276065, "compression_ratio": 1.5153846153846153,
  "no_speech_prob": 0.01129200030118227}, {"id": 534, "seek": 345872, "start": 3464.48,
  "end": 3470.64, "text": " It might be crazy, but I think I think we''re getting
  ready for a future we cannot possibly predict.", "tokens": [50652, 467, 1062, 312,
  3219, 11, 457, 286, 519, 286, 519, 321, 434, 1242, 1919, 337, 257, 2027, 321, 2644,
  6264, 6069, 13, 50960], "temperature": 0.0, "avg_logprob": -0.28321976788276065,
  "compression_ratio": 1.5153846153846153, "no_speech_prob": 0.01129200030118227},
  {"id": 535, "seek": 345872, "start": 3471.3599999999997, "end": 3475.7599999999998,
  "text": " Yeah, and I think spam will be replaced by slope, right? I don''t know
  if you heard of this,", "tokens": [50996, 865, 11, 293, 286, 519, 24028, 486, 312,
  10772, 538, 13525, 11, 558, 30, 286, 500, 380, 458, 498, 291, 2198, 295, 341, 11,
  51216], "temperature": 0.0, "avg_logprob": -0.28321976788276065, "compression_ratio":
  1.5153846153846153, "no_speech_prob": 0.01129200030118227}, {"id": 536, "seek":
  345872, "start": 3476.8799999999997, "end": 3487.52, "text": " YouTube. No, slow,
  slow, slow is, uh, SLOP. So it''s basically an unverified output of an LLAM model.",
  "tokens": [51272, 3088, 13, 883, 11, 2964, 11, 2964, 11, 2964, 307, 11, 2232, 11,
  22999, 12059, 13, 407, 309, 311, 1936, 364, 517, 331, 2587, 5598, 295, 364, 441,
  43, 2865, 2316, 13, 51804], "temperature": 0.0, "avg_logprob": -0.28321976788276065,
  "compression_ratio": 1.5153846153846153, "no_speech_prob": 0.01129200030118227},
  {"id": 537, "seek": 348752, "start": 3487.52, "end": 3493.52, "text": " So something
  that got produced back to your question, you don''t, you have no idea if it''s true
  or not,", "tokens": [50364, 407, 746, 300, 658, 7126, 646, 281, 428, 1168, 11, 291,
  500, 380, 11, 291, 362, 572, 1558, 498, 309, 311, 2074, 420, 406, 11, 50664], "temperature":
  0.0, "avg_logprob": -0.18153953552246094, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.013459110632538795}, {"id": 538, "seek": 348752, "start": 3493.52,
  "end": 3498.96, "text": " you go and paste it somewhere in the web and then LLAM
  goes and scraps it and learns from it.", "tokens": [50664, 291, 352, 293, 9163,
  309, 4079, 294, 264, 3670, 293, 550, 441, 43, 2865, 1709, 293, 45204, 309, 293,
  27152, 490, 309, 13, 50936], "temperature": 0.0, "avg_logprob": -0.18153953552246094,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.013459110632538795},
  {"id": 539, "seek": 348752, "start": 3499.68, "end": 3506.48, "text": " So you spam
  the model. And so there is a call out. If this feedback effect. Yeah, exactly.",
  "tokens": [50972, 407, 291, 24028, 264, 2316, 13, 400, 370, 456, 307, 257, 818,
  484, 13, 759, 341, 5824, 1802, 13, 865, 11, 2293, 13, 51312], "temperature": 0.0,
  "avg_logprob": -0.18153953552246094, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.013459110632538795}, {"id": 540, "seek": 348752, "start": 3506.48, "end": 3513.04,
  "text": " And there is a call out that, hey, let''s not spam or let''s not post
  slope on the web because that will", "tokens": [51312, 400, 456, 307, 257, 818,
  484, 300, 11, 4177, 11, 718, 311, 406, 24028, 420, 718, 311, 406, 2183, 13525, 322,
  264, 3670, 570, 300, 486, 51640], "temperature": 0.0, "avg_logprob": -0.18153953552246094,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.013459110632538795},
  {"id": 541, "seek": 351304, "start": 3513.04, "end": 3521.84, "text": " bite us
  because we are moving so far ahead in the LLAM. And who is obeying that recommendation?",
  "tokens": [50364, 7988, 505, 570, 321, 366, 2684, 370, 1400, 2286, 294, 264, 441,
  43, 2865, 13, 400, 567, 307, 36346, 1840, 300, 11879, 30, 50804], "temperature":
  0.0, "avg_logprob": -0.18967651006743663, "compression_ratio": 1.5695364238410596,
  "no_speech_prob": 0.032904285937547684}, {"id": 542, "seek": 351304, "start": 3521.84,
  "end": 3527.2799999999997, "text": " Exactly. Probably not the companies that need
  content produced. Yeah. Yeah. The moment you say,", "tokens": [50804, 7587, 13,
  9210, 406, 264, 3431, 300, 643, 2701, 7126, 13, 865, 13, 865, 13, 440, 1623, 291,
  584, 11, 51076], "temperature": 0.0, "avg_logprob": -0.18967651006743663, "compression_ratio":
  1.5695364238410596, "no_speech_prob": 0.032904285937547684}, {"id": 543, "seek":
  351304, "start": 3527.2799999999997, "end": 3532.08, "text": " don''t do something,
  there will be a bunch of people saying, oh, let''s try. That sounds like fine.",
  "tokens": [51076, 500, 380, 360, 746, 11, 456, 486, 312, 257, 3840, 295, 561, 1566,
  11, 1954, 11, 718, 311, 853, 13, 663, 3263, 411, 2489, 13, 51316], "temperature":
  0.0, "avg_logprob": -0.18967651006743663, "compression_ratio": 1.5695364238410596,
  "no_speech_prob": 0.032904285937547684}, {"id": 544, "seek": 351304, "start": 3532.08,
  "end": 3537.12, "text": " Oh, that''s a good idea. And then we need to invent a
  solution for that. Hey, Jonathan, it was", "tokens": [51316, 876, 11, 300, 311,
  257, 665, 1558, 13, 400, 550, 321, 643, 281, 7962, 257, 3827, 337, 300, 13, 1911,
  11, 15471, 11, 309, 390, 51568], "temperature": 0.0, "avg_logprob": -0.18967651006743663,
  "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.032904285937547684},
  {"id": 545, "seek": 351304, "start": 3537.12, "end": 3542.0, "text": " really exciting.
  And I''ve known it like a ton by talking to you. I feel like we can record", "tokens":
  [51568, 534, 4670, 13, 400, 286, 600, 2570, 309, 411, 257, 2952, 538, 1417, 281,
  291, 13, 286, 841, 411, 321, 393, 2136, 51812], "temperature": 0.0, "avg_logprob":
  -0.18967651006743663, "compression_ratio": 1.5695364238410596, "no_speech_prob":
  0.032904285937547684}, {"id": 546, "seek": 354200, "start": 3542.0, "end": 3549.36,
  "text": " probably like like three months style episode, you know, four or five
  hours before we get exhausted.", "tokens": [50364, 1391, 411, 411, 1045, 2493, 3758,
  3500, 11, 291, 458, 11, 1451, 420, 1732, 2496, 949, 321, 483, 17992, 13, 50732],
  "temperature": 0.0, "avg_logprob": -0.25740442396719243, "compression_ratio": 1.5549738219895288,
  "no_speech_prob": 0.04945411905646324}, {"id": 547, "seek": 354200, "start": 3549.36,
  "end": 3557.84, "text": " But I also wanted to give you a chance to, you know, go
  on stage and sort of and talk about your book", "tokens": [50732, 583, 286, 611,
  1415, 281, 976, 291, 257, 2931, 281, 11, 291, 458, 11, 352, 322, 3233, 293, 1333,
  295, 293, 751, 466, 428, 1446, 51156], "temperature": 0.0, "avg_logprob": -0.25740442396719243,
  "compression_ratio": 1.5549738219895288, "no_speech_prob": 0.04945411905646324},
  {"id": 548, "seek": 354200, "start": 3558.8, "end": 3565.84, "text": " way. Like,
  why do you think everyone needs to read it? I want to read it. If I get a chance
  to", "tokens": [51204, 636, 13, 1743, 11, 983, 360, 291, 519, 1518, 2203, 281, 1401,
  309, 30, 286, 528, 281, 1401, 309, 13, 759, 286, 483, 257, 2931, 281, 51556], "temperature":
  0.0, "avg_logprob": -0.25740442396719243, "compression_ratio": 1.5549738219895288,
  "no_speech_prob": 0.04945411905646324}, {"id": 549, "seek": 356584, "start": 3566.4,
  "end": 3572.4, "text": " get my hands on it, hopefully soon. Everyone needs to read
  it because every time I make a sale,", "tokens": [50392, 483, 452, 2377, 322, 309,
  11, 4696, 2321, 13, 5198, 2203, 281, 1401, 309, 570, 633, 565, 286, 652, 257, 8680,
  11, 50692], "temperature": 0.0, "avg_logprob": -0.21144647418328053, "compression_ratio":
  1.6385542168674698, "no_speech_prob": 0.02142273634672165}, {"id": 550, "seek":
  356584, "start": 3572.4, "end": 3577.84, "text": " I get one cup of coffee. So that''s
  why everyone needs to read it. Of course. Yeah, that''s a good reason.", "tokens":
  [50692, 286, 483, 472, 4414, 295, 4982, 13, 407, 300, 311, 983, 1518, 2203, 281,
  1401, 309, 13, 2720, 1164, 13, 865, 11, 300, 311, 257, 665, 1778, 13, 50964], "temperature":
  0.0, "avg_logprob": -0.21144647418328053, "compression_ratio": 1.6385542168674698,
  "no_speech_prob": 0.02142273634672165}, {"id": 551, "seek": 356584, "start": 3579.84,
  "end": 3587.36, "text": " But then also, yeah, go ahead. No, I wanted also you to
  give you a chance to talk about your company.", "tokens": [51064, 583, 550, 611,
  11, 1338, 11, 352, 2286, 13, 883, 11, 286, 1415, 611, 291, 281, 976, 291, 257, 2931,
  281, 751, 466, 428, 2237, 13, 51440], "temperature": 0.0, "avg_logprob": -0.21144647418328053,
  "compression_ratio": 1.6385542168674698, "no_speech_prob": 0.02142273634672165},
  {"id": 552, "seek": 356584, "start": 3587.36, "end": 3595.28, "text": " Because
  I know that feeling of starting something new on your own, you call yourself an
  indie consultant,", "tokens": [51440, 1436, 286, 458, 300, 2633, 295, 2891, 746,
  777, 322, 428, 1065, 11, 291, 818, 1803, 364, 33184, 24676, 11, 51836], "temperature":
  0.0, "avg_logprob": -0.21144647418328053, "compression_ratio": 1.6385542168674698,
  "no_speech_prob": 0.02142273634672165}, {"id": 553, "seek": 359528, "start": 3595.28,
  "end": 3604.32, "text": " right? At the same time, you have so much with you and
  your luggage, right? Like you, the knowledge", "tokens": [50364, 558, 30, 1711,
  264, 912, 565, 11, 291, 362, 370, 709, 365, 291, 293, 428, 27744, 11, 558, 30, 1743,
  291, 11, 264, 3601, 50816], "temperature": 0.0, "avg_logprob": -0.1874878908458509,
  "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.006824716459959745},
  {"id": 554, "seek": 359528, "start": 3604.32, "end": 3610.7200000000003, "text":
  " of the experience. And so why not share it in a different way through your company.
  But I wanted", "tokens": [50816, 295, 264, 1752, 13, 400, 370, 983, 406, 2073, 309,
  294, 257, 819, 636, 807, 428, 2237, 13, 583, 286, 1415, 51136], "temperature": 0.0,
  "avg_logprob": -0.1874878908458509, "compression_ratio": 1.5128205128205128, "no_speech_prob":
  0.006824716459959745}, {"id": 555, "seek": 359528, "start": 3610.7200000000003,
  "end": 3619.52, "text": " to learn a bit more. What is your vision for the company?
  What do you think you will offer like in", "tokens": [51136, 281, 1466, 257, 857,
  544, 13, 708, 307, 428, 5201, 337, 264, 2237, 30, 708, 360, 291, 519, 291, 486,
  2626, 411, 294, 51576], "temperature": 0.0, "avg_logprob": -0.1874878908458509,
  "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.006824716459959745},
  {"id": 556, "seek": 361952, "start": 3619.52, "end": 3624.8, "text": " midterm?
  Where do you create the value for the customers? And maybe there will be some customers",
  "tokens": [50364, 2062, 7039, 30, 2305, 360, 291, 1884, 264, 2158, 337, 264, 4581,
  30, 400, 1310, 456, 486, 312, 512, 4581, 50628], "temperature": 0.0, "avg_logprob":
  -0.14651490960802352, "compression_ratio": 1.718045112781955, "no_speech_prob":
  0.016938241198658943}, {"id": 557, "seek": 361952, "start": 3624.8, "end": 3630.64,
  "text": " listening in this podcast, hopefully. Sure. Well, okay, let''s go through
  both of those then.", "tokens": [50628, 4764, 294, 341, 7367, 11, 4696, 13, 4894,
  13, 1042, 11, 1392, 11, 718, 311, 352, 807, 1293, 295, 729, 550, 13, 50920], "temperature":
  0.0, "avg_logprob": -0.14651490960802352, "compression_ratio": 1.718045112781955,
  "no_speech_prob": 0.016938241198658943}, {"id": 558, "seek": 361952, "start": 3631.7599999999998,
  "end": 3636.4, "text": " I hope I hope everyone reads the book. I hope they enjoy
  it. I hope they learn from it.", "tokens": [50976, 286, 1454, 286, 1454, 1518, 15700,
  264, 1446, 13, 286, 1454, 436, 2103, 309, 13, 286, 1454, 436, 1466, 490, 309, 13,
  51208], "temperature": 0.0, "avg_logprob": -0.14651490960802352, "compression_ratio":
  1.718045112781955, "no_speech_prob": 0.016938241198658943}, {"id": 559, "seek":
  361952, "start": 3638.16, "end": 3642.48, "text": " Working with large language
  models is a very different beast from what you''re used to.", "tokens": [51296,
  18337, 365, 2416, 2856, 5245, 307, 257, 588, 819, 13464, 490, 437, 291, 434, 1143,
  281, 13, 51512], "temperature": 0.0, "avg_logprob": -0.14651490960802352, "compression_ratio":
  1.718045112781955, "no_speech_prob": 0.016938241198658943}, {"id": 560, "seek":
  361952, "start": 3643.04, "end": 3648.88, "text": " I think, you know, three years
  from now, everyone will be a large language model application", "tokens": [51540,
  286, 519, 11, 291, 458, 11, 1045, 924, 490, 586, 11, 1518, 486, 312, 257, 2416,
  2856, 2316, 3861, 51832], "temperature": 0.0, "avg_logprob": -0.14651490960802352,
  "compression_ratio": 1.718045112781955, "no_speech_prob": 0.016938241198658943},
  {"id": 561, "seek": 364888, "start": 3648.88, "end": 3655.6800000000003, "text":
  " developer because they''re becoming so prevalent everywhere. So start early. Get
  your hands dirty,", "tokens": [50364, 10754, 570, 436, 434, 5617, 370, 30652, 5315,
  13, 407, 722, 2440, 13, 3240, 428, 2377, 9360, 11, 50704], "temperature": 0.0, "avg_logprob":
  -0.11855846168720617, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.004083811305463314}, {"id": 562, "seek": 364888, "start": 3655.6800000000003,
  "end": 3661.76, "text": " interact with these things. And my book helps kind of
  take, you know, give you the training wells", "tokens": [50704, 4648, 365, 613,
  721, 13, 400, 452, 1446, 3665, 733, 295, 747, 11, 291, 458, 11, 976, 291, 264, 3097,
  30984, 51008], "temperature": 0.0, "avg_logprob": -0.11855846168720617, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.004083811305463314}, {"id": 563, "seek":
  364888, "start": 3661.76, "end": 3666.32, "text": " at first to understand here
  are a bunch of the problems that you run into. Here''s how here''s", "tokens": [51008,
  412, 700, 281, 1223, 510, 366, 257, 3840, 295, 264, 2740, 300, 291, 1190, 666, 13,
  1692, 311, 577, 510, 311, 51236], "temperature": 0.0, "avg_logprob": -0.11855846168720617,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.004083811305463314},
  {"id": 564, "seek": 364888, "start": 3666.32, "end": 3671.28, "text": " how model
  works. That''s there''s actually a lot of good intuition and just understanding
  the tool", "tokens": [51236, 577, 2316, 1985, 13, 663, 311, 456, 311, 767, 257,
  688, 295, 665, 24002, 293, 445, 3701, 264, 2290, 51484], "temperature": 0.0, "avg_logprob":
  -0.11855846168720617, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.004083811305463314}, {"id": 565, "seek": 364888, "start": 3671.28, "end": 3676.4,
  "text": " that you''re interacting with. Here''s how to organize a prompt. And that''s
  not always easy. You", "tokens": [51484, 300, 291, 434, 18017, 365, 13, 1692, 311,
  577, 281, 13859, 257, 12391, 13, 400, 300, 311, 406, 1009, 1858, 13, 509, 51740],
  "temperature": 0.0, "avg_logprob": -0.11855846168720617, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.004083811305463314}, {"id": 566, "seek": 367640, "start": 3676.4,
  "end": 3681.52, "text": " got to figure out what''s all the stuff you might use.
  And you can''t use all of it because it", "tokens": [50364, 658, 281, 2573, 484,
  437, 311, 439, 264, 1507, 291, 1062, 764, 13, 400, 291, 393, 380, 764, 439, 295,
  309, 570, 309, 50620], "temperature": 0.0, "avg_logprob": -0.1298813819885254, "compression_ratio":
  1.853846153846154, "no_speech_prob": 0.0006139843608252704}, {"id": 567, "seek":
  367640, "start": 3681.52, "end": 3685.44, "text": " doesn''t all fit or because
  you don''t want to wait for the latency. You know, it tells you how to,", "tokens":
  [50620, 1177, 380, 439, 3318, 420, 570, 291, 500, 380, 528, 281, 1699, 337, 264,
  27043, 13, 509, 458, 11, 309, 5112, 291, 577, 281, 11, 50816], "temperature": 0.0,
  "avg_logprob": -0.1298813819885254, "compression_ratio": 1.853846153846154, "no_speech_prob":
  0.0006139843608252704}, {"id": 568, "seek": 367640, "start": 3685.44, "end": 3690.0,
  "text": " you know, fit that into a prompt, present it to the model in a way that
  kind of empathetically,", "tokens": [50816, 291, 458, 11, 3318, 300, 666, 257, 12391,
  11, 1974, 309, 281, 264, 2316, 294, 257, 636, 300, 733, 295, 27155, 22652, 11, 51044],
  "temperature": 0.0, "avg_logprob": -0.1298813819885254, "compression_ratio": 1.853846153846154,
  "no_speech_prob": 0.0006139843608252704}, {"id": 569, "seek": 367640, "start": 3690.0,
  "end": 3695.12, "text": " the model is going to understand the model is not psychic.
  You need to talk to the model as if", "tokens": [51044, 264, 2316, 307, 516, 281,
  1223, 264, 2316, 307, 406, 35406, 13, 509, 643, 281, 751, 281, 264, 2316, 382, 498,
  51300], "temperature": 0.0, "avg_logprob": -0.1298813819885254, "compression_ratio":
  1.853846153846154, "no_speech_prob": 0.0006139843608252704}, {"id": 570, "seek":
  367640, "start": 3695.12, "end": 3700.8, "text": " you''re talking to, you know,
  someone that you''re working with. And then towards the end of the book,", "tokens":
  [51300, 291, 434, 1417, 281, 11, 291, 458, 11, 1580, 300, 291, 434, 1364, 365, 13,
  400, 550, 3030, 264, 917, 295, 264, 1446, 11, 51584], "temperature": 0.0, "avg_logprob":
  -0.1298813819885254, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.0006139843608252704},
  {"id": 571, "seek": 370080, "start": 3700.8, "end": 3706.2400000000002, "text":
  " it gets outside of a single prompt and it talks about, you know, like this tool
  magic word we''ve", "tokens": [50364, 309, 2170, 2380, 295, 257, 2167, 12391, 293,
  309, 6686, 466, 11, 291, 458, 11, 411, 341, 2290, 5585, 1349, 321, 600, 50636],
  "temperature": 0.0, "avg_logprob": -0.1326928734779358, "compression_ratio": 1.7962962962962963,
  "no_speech_prob": 0.0006211837171576917}, {"id": 572, "seek": 370080, "start": 3706.2400000000002,
  "end": 3713.36, "text": " got right now, agency and how to build a assistant behavior
  with tools and how to, you know, build a", "tokens": [50636, 658, 558, 586, 11,
  7934, 293, 577, 281, 1322, 257, 10994, 5223, 365, 3873, 293, 577, 281, 11, 291,
  458, 11, 1322, 257, 50992], "temperature": 0.0, "avg_logprob": -0.1326928734779358,
  "compression_ratio": 1.7962962962962963, "no_speech_prob": 0.0006211837171576917},
  {"id": 573, "seek": 370080, "start": 3713.36, "end": 3718.88, "text": " more sophisticated
  thinking steps with it in review of, you know, what''s happened. And it talks",
  "tokens": [50992, 544, 16950, 1953, 4439, 365, 309, 294, 3131, 295, 11, 291, 458,
  11, 437, 311, 2011, 13, 400, 309, 6686, 51268], "temperature": 0.0, "avg_logprob":
  -0.1326928734779358, "compression_ratio": 1.7962962962962963, "no_speech_prob":
  0.0006211837171576917}, {"id": 574, "seek": 370080, "start": 3718.88, "end": 3724.0800000000004,
  "text": " about workflows, which is another type of agency really about how to,
  you know, take an input,", "tokens": [51268, 466, 43461, 11, 597, 307, 1071, 2010,
  295, 7934, 534, 466, 577, 281, 11, 291, 458, 11, 747, 364, 4846, 11, 51528], "temperature":
  0.0, "avg_logprob": -0.1326928734779358, "compression_ratio": 1.7962962962962963,
  "no_speech_prob": 0.0006211837171576917}, {"id": 575, "seek": 372408, "start": 3724.08,
  "end": 3730.88, "text": " bunch of data, pick it apart, do the right steps to get
  a job done with hopefully not going off", "tokens": [50364, 3840, 295, 1412, 11,
  1888, 309, 4936, 11, 360, 264, 558, 4439, 281, 483, 257, 1691, 1096, 365, 4696,
  406, 516, 766, 50704], "temperature": 0.0, "avg_logprob": -0.11226291839893048,
  "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00508774584159255},
  {"id": 576, "seek": 372408, "start": 3730.88, "end": 3736.56, "text": " track too
  much. We talk a little bit about evaluation and we wrap it up by saying holy cow,",
  "tokens": [50704, 2837, 886, 709, 13, 492, 751, 257, 707, 857, 466, 13344, 293,
  321, 7019, 309, 493, 538, 1566, 10622, 8408, 11, 50988], "temperature": 0.0, "avg_logprob":
  -0.11226291839893048, "compression_ratio": 1.6099585062240664, "no_speech_prob":
  0.00508774584159255}, {"id": 577, "seek": 372408, "start": 3736.56, "end": 3741.7599999999998,
  "text": " look at the future we''re going into. This is going to be amazing. So
  I hope you get a chance to read", "tokens": [50988, 574, 412, 264, 2027, 321, 434,
  516, 666, 13, 639, 307, 516, 281, 312, 2243, 13, 407, 286, 1454, 291, 483, 257,
  2931, 281, 1401, 51248], "temperature": 0.0, "avg_logprob": -0.11226291839893048,
  "compression_ratio": 1.6099585062240664, "no_speech_prob": 0.00508774584159255},
  {"id": 578, "seek": 372408, "start": 3741.7599999999998, "end": 3748.64, "text":
  " the book and I hope you enjoy it. I hope it''s as enjoyable to you as it was painful
  to me to write.", "tokens": [51248, 264, 1446, 293, 286, 1454, 291, 2103, 309, 13,
  286, 1454, 309, 311, 382, 20305, 281, 291, 382, 309, 390, 11697, 281, 385, 281,
  2464, 13, 51592], "temperature": 0.0, "avg_logprob": -0.11226291839893048, "compression_ratio":
  1.6099585062240664, "no_speech_prob": 0.00508774584159255}, {"id": 579, "seek":
  374864, "start": 3749.52, "end": 3758.8799999999997, "text": " And then yes, I am
  out of my own now. I''m an indie consultant at Arturus Labs. I''m specializing",
  "tokens": [50408, 400, 550, 2086, 11, 286, 669, 484, 295, 452, 1065, 586, 13, 286,
  478, 364, 33184, 24676, 412, 5735, 374, 301, 40047, 13, 286, 478, 2121, 3319, 50876],
  "temperature": 0.0, "avg_logprob": -0.1967130777787189, "compression_ratio": 1.616326530612245,
  "no_speech_prob": 0.0049796137027442455}, {"id": 580, "seek": 374864, "start": 3758.8799999999997,
  "end": 3763.92, "text": " in all things just like the book, prompt engineering,
  large language model application development.", "tokens": [50876, 294, 439, 721,
  445, 411, 264, 1446, 11, 12391, 7043, 11, 2416, 2856, 2316, 3861, 3250, 13, 51128],
  "temperature": 0.0, "avg_logprob": -0.1967130777787189, "compression_ratio": 1.616326530612245,
  "no_speech_prob": 0.0049796137027442455}, {"id": 581, "seek": 374864, "start": 3764.96,
  "end": 3771.3599999999997, "text": " I think we''re going into a very different
  world as far as like how you build things. You''ve got to", "tokens": [51180, 286,
  519, 321, 434, 516, 666, 257, 588, 819, 1002, 382, 1400, 382, 411, 577, 291, 1322,
  721, 13, 509, 600, 658, 281, 51500], "temperature": 0.0, "avg_logprob": -0.1967130777787189,
  "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0049796137027442455},
  {"id": 582, "seek": 374864, "start": 3771.3599999999997, "end": 3776.0, "text":
  " build it like we had earlier in this conversation. You''ve got to build these
  components to deal with.", "tokens": [51500, 1322, 309, 411, 321, 632, 3071, 294,
  341, 3761, 13, 509, 600, 658, 281, 1322, 613, 6677, 281, 2028, 365, 13, 51732],
  "temperature": 0.0, "avg_logprob": -0.1967130777787189, "compression_ratio": 1.616326530612245,
  "no_speech_prob": 0.0049796137027442455}, {"id": 583, "seek": 377600, "start": 3776.0,
  "end": 3781.92, "text": " You''ve got to build it web apps to deal with these components
  that are very undependable. I", "tokens": [50364, 509, 600, 658, 281, 1322, 309,
  3670, 7733, 281, 2028, 365, 613, 6677, 300, 366, 588, 674, 4217, 712, 13, 286, 50660],
  "temperature": 0.0, "avg_logprob": -0.16883624778999076, "compression_ratio": 1.5958333333333334,
  "no_speech_prob": 0.0033530977088958025}, {"id": 584, "seek": 377600, "start": 3781.92,
  "end": 3785.92, "text": " do make them as dependable as possible. How do you make
  the user experience where they trust what''s", "tokens": [50660, 360, 652, 552,
  382, 5672, 712, 382, 1944, 13, 1012, 360, 291, 652, 264, 4195, 1752, 689, 436, 3361,
  437, 311, 50860], "temperature": 0.0, "avg_logprob": -0.16883624778999076, "compression_ratio":
  1.5958333333333334, "no_speech_prob": 0.0033530977088958025}, {"id": 585, "seek":
  377600, "start": 3785.92, "end": 3793.04, "text": " happening? And that''s tricky.
  So I offer a whole range of things from just education, going in", "tokens": [50860,
  2737, 30, 400, 300, 311, 12414, 13, 407, 286, 2626, 257, 1379, 3613, 295, 721, 490,
  445, 3309, 11, 516, 294, 51216], "temperature": 0.0, "avg_logprob": -0.16883624778999076,
  "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0033530977088958025},
  {"id": 586, "seek": 377600, "start": 3793.04, "end": 3800.0, "text": " and training
  companies. I like going and working with them to think through what product they''re",
  "tokens": [51216, 293, 3097, 3431, 13, 286, 411, 516, 293, 1364, 365, 552, 281,
  519, 807, 437, 1674, 436, 434, 51564], "temperature": 0.0, "avg_logprob": -0.16883624778999076,
  "compression_ratio": 1.5958333333333334, "no_speech_prob": 0.0033530977088958025},
  {"id": 587, "seek": 380000, "start": 3800.0, "end": 3809.6, "text": " working on
  right now with their next big goal. I can say this is a great idea. You''re on the",
  "tokens": [50364, 1364, 322, 558, 586, 365, 641, 958, 955, 3387, 13, 286, 393, 584,
  341, 307, 257, 869, 1558, 13, 509, 434, 322, 264, 50844], "temperature": 0.0, "avg_logprob":
  -0.1603565621883311, "compression_ratio": 1.5560165975103735, "no_speech_prob":
  0.005064779426902533}, {"id": 588, "seek": 380000, "start": 3809.6, "end": 3814.96,
  "text": " right track. This is not quite feasible, but we can fix it. That''s the
  product type stuff I like", "tokens": [50844, 558, 2837, 13, 639, 307, 406, 1596,
  26648, 11, 457, 321, 393, 3191, 309, 13, 663, 311, 264, 1674, 2010, 1507, 286, 411,
  51112], "temperature": 0.0, "avg_logprob": -0.1603565621883311, "compression_ratio":
  1.5560165975103735, "no_speech_prob": 0.005064779426902533}, {"id": 589, "seek":
  380000, "start": 3814.96, "end": 3822.56, "text": " thinking through. And then as
  we get to a longer engagement, I just love working with these", "tokens": [51112,
  1953, 807, 13, 400, 550, 382, 321, 483, 281, 257, 2854, 8742, 11, 286, 445, 959,
  1364, 365, 613, 51492], "temperature": 0.0, "avg_logprob": -0.1603565621883311,
  "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.005064779426902533},
  {"id": 590, "seek": 380000, "start": 3822.56, "end": 3829.36, "text": " companies,
  especially like startups. Just sit down, pair with them, do transfer of knowledge,",
  "tokens": [51492, 3431, 11, 2318, 411, 28041, 13, 1449, 1394, 760, 11, 6119, 365,
  552, 11, 360, 5003, 295, 3601, 11, 51832], "temperature": 0.0, "avg_logprob": -0.1603565621883311,
  "compression_ratio": 1.5560165975103735, "no_speech_prob": 0.005064779426902533},
  {"id": 591, "seek": 382936, "start": 3829.36, "end": 3835.28, "text": " type stuff.
  It''s just really neat to see what people are up to. A lot of creative ideas right
  now.", "tokens": [50364, 2010, 1507, 13, 467, 311, 445, 534, 10654, 281, 536, 437,
  561, 366, 493, 281, 13, 316, 688, 295, 5880, 3487, 558, 586, 13, 50660], "temperature":
  0.0, "avg_logprob": -0.21692513446418607, "compression_ratio": 1.5701754385964912,
  "no_speech_prob": 0.008672201074659824}, {"id": 592, "seek": 382936, "start": 3836.6400000000003,
  "end": 3839.6, "text": " And then finally, yeah, please make sure you check out
  my website,", "tokens": [50728, 400, 550, 2721, 11, 1338, 11, 1767, 652, 988, 291,
  1520, 484, 452, 3144, 11, 50876], "temperature": 0.0, "avg_logprob": -0.21692513446418607,
  "compression_ratio": 1.5701754385964912, "no_speech_prob": 0.008672201074659824},
  {"id": 593, "seek": 382936, "start": 3839.6, "end": 3849.52, "text": " www.artrisslabs.com.
  I''m going to throw together a lot more blog posts like the one we didn''t", "tokens":
  [50876, 12520, 13, 446, 81, 891, 75, 17243, 13, 1112, 13, 286, 478, 516, 281, 3507,
  1214, 257, 688, 544, 6968, 12300, 411, 264, 472, 321, 994, 380, 51372], "temperature":
  0.0, "avg_logprob": -0.21692513446418607, "compression_ratio": 1.5701754385964912,
  "no_speech_prob": 0.008672201074659824}, {"id": 594, "seek": 382936, "start": 3849.52,
  "end": 3855.92, "text": " know today. I''m trying right now to make sure every blog
  post has something juicy, a piece of code", "tokens": [51372, 458, 965, 13, 286,
  478, 1382, 558, 586, 281, 652, 988, 633, 6968, 2183, 575, 746, 24696, 11, 257, 2522,
  295, 3089, 51692], "temperature": 0.0, "avg_logprob": -0.21692513446418607, "compression_ratio":
  1.5701754385964912, "no_speech_prob": 0.008672201074659824}, {"id": 595, "seek":
  385592, "start": 3856.0, "end": 3860.8, "text": " that actually works and you can
  experience the thing that was running to my mind at the time.", "tokens": [50368,
  300, 767, 1985, 293, 291, 393, 1752, 264, 551, 300, 390, 2614, 281, 452, 1575, 412,
  264, 565, 13, 50608], "temperature": 0.0, "avg_logprob": -0.179567860622032, "compression_ratio":
  1.7309417040358743, "no_speech_prob": 0.17391560971736908}, {"id": 596, "seek":
  385592, "start": 3861.28, "end": 3867.6800000000003, "text": " So try it out. I''m
  really engaging on Twitter. Tell me what you think. And yeah, I''d like to get to",
  "tokens": [50632, 407, 853, 309, 484, 13, 286, 478, 534, 11268, 322, 5794, 13, 5115,
  385, 437, 291, 519, 13, 400, 1338, 11, 286, 1116, 411, 281, 483, 281, 50952], "temperature":
  0.0, "avg_logprob": -0.179567860622032, "compression_ratio": 1.7309417040358743,
  "no_speech_prob": 0.17391560971736908}, {"id": 597, "seek": 385592, "start": 3868.32,
  "end": 3877.04, "text": " to know you guys too. Yeah, amazing, amazing. And I wish
  you all the best with your new adventure,", "tokens": [50984, 281, 458, 291, 1074,
  886, 13, 865, 11, 2243, 11, 2243, 13, 400, 286, 3172, 291, 439, 264, 1151, 365,
  428, 777, 9868, 11, 51420], "temperature": 0.0, "avg_logprob": -0.179567860622032,
  "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.17391560971736908},
  {"id": 598, "seek": 385592, "start": 3877.04, "end": 3883.28, "text": " your new
  venture. And yeah, we will link everything. We will link the book. We will link
  your", "tokens": [51420, 428, 777, 18474, 13, 400, 1338, 11, 321, 486, 2113, 1203,
  13, 492, 486, 2113, 264, 1446, 13, 492, 486, 2113, 428, 51732], "temperature": 0.0,
  "avg_logprob": -0.179567860622032, "compression_ratio": 1.7309417040358743, "no_speech_prob":
  0.17391560971736908}, {"id": 599, "seek": 388328, "start": 3883.28, "end": 3890.0,
  "text": " site and blogs for sure. Thanks so much for spending time with me and
  educating me and", "tokens": [50364, 3621, 293, 31038, 337, 988, 13, 2561, 370,
  709, 337, 6434, 565, 365, 385, 293, 28835, 385, 293, 50700], "temperature": 0.0,
  "avg_logprob": -0.18006372451782227, "compression_ratio": 1.6710526315789473, "no_speech_prob":
  0.06044189631938934}, {"id": 600, "seek": 388328, "start": 3891.2000000000003, "end":
  3897.1200000000003, "text": " keeping up with Mike sometimes, you know, and obvious
  questions. It was really, really a pleasure", "tokens": [50760, 5145, 493, 365,
  6602, 2171, 11, 291, 458, 11, 293, 6322, 1651, 13, 467, 390, 534, 11, 534, 257,
  6834, 51056], "temperature": 0.0, "avg_logprob": -0.18006372451782227, "compression_ratio":
  1.6710526315789473, "no_speech_prob": 0.06044189631938934}, {"id": 601, "seek":
  388328, "start": 3897.1200000000003, "end": 3902.8, "text": " to talk to you. And
  I really, really hope that we can record sometime soon because you seem to be",
  "tokens": [51056, 281, 751, 281, 291, 13, 400, 286, 534, 11, 534, 1454, 300, 321,
  393, 2136, 15053, 2321, 570, 291, 1643, 281, 312, 51340], "temperature": 0.0, "avg_logprob":
  -0.18006372451782227, "compression_ratio": 1.6710526315789473, "no_speech_prob":
  0.06044189631938934}, {"id": 602, "seek": 388328, "start": 3902.8, "end": 3912.4,
  "text": " cooking a lot of ideas. And you take from what I gather, you take really
  practical view of things.", "tokens": [51340, 6361, 257, 688, 295, 3487, 13, 400,
  291, 747, 490, 437, 286, 5448, 11, 291, 747, 534, 8496, 1910, 295, 721, 13, 51820],
  "temperature": 0.0, "avg_logprob": -0.18006372451782227, "compression_ratio": 1.6710526315789473,
  "no_speech_prob": 0.06044189631938934}, {"id": 603, "seek": 391240, "start": 3912.4,
  "end": 3918.7200000000003, "text": " And you''ve been like, and you are an engineer
  and researcher. And so that''s very dear to my heart", "tokens": [50364, 400, 291,
  600, 668, 411, 11, 293, 291, 366, 364, 11403, 293, 21751, 13, 400, 370, 300, 311,
  588, 6875, 281, 452, 1917, 50680], "temperature": 0.0, "avg_logprob": -0.12263016641875844,
  "compression_ratio": 1.4867724867724867, "no_speech_prob": 0.03289446979761124},
  {"id": 604, "seek": 391240, "start": 3918.7200000000003, "end": 3926.48, "text":
  " to see. And I can''t wait to see what you come up with next. Me too. Well, thanks
  so much for", "tokens": [50680, 281, 536, 13, 400, 286, 393, 380, 1699, 281, 536,
  437, 291, 808, 493, 365, 958, 13, 1923, 886, 13, 1042, 11, 3231, 370, 709, 337,
  51068], "temperature": 0.0, "avg_logprob": -0.12263016641875844, "compression_ratio":
  1.4867724867724867, "no_speech_prob": 0.03289446979761124}, {"id": 605, "seek":
  391240, "start": 3926.48, "end": 3932.64, "text": " having me on. It''s been great
  talking to you. So yeah, let''s do this again sometime. Yeah,", "tokens": [51068,
  1419, 385, 322, 13, 467, 311, 668, 869, 1417, 281, 291, 13, 407, 1338, 11, 718,
  311, 360, 341, 797, 15053, 13, 865, 11, 51376], "temperature": 0.0, "avg_logprob":
  -0.12263016641875844, "compression_ratio": 1.4867724867724867, "no_speech_prob":
  0.03289446979761124}, {"id": 606, "seek": 393264, "start": 3932.64, "end": 3934.48,
  "text": " thanks, John. Have a good day.", "tokens": [50368, 3231, 11, 2619, 13,
  3560, 257, 665, 786, 13, 50456], "temperature": 0.0, "avg_logprob": -0.3358224232991536,
  "compression_ratio": 0.7894736842105263, "no_speech_prob": 0.2616181969642639}]'
---

Hello everyone, Vector podcast is back. Season 3. We are wrapping up the season with some really, really juicy episodes. I'm sure you will love this one. I have the privilege of talking to John Barryman today. He is an ex senior machine learning researcher who worked on GitHub Copilot.
Currently, he runs his own consultancy actress labs. I'm sure he will talk more about that. Yeah, welcome, John. Good to be here. How's it going? Awesome. I actually just picked the book of yours and the book that you and Dr. Moll have written together.
I've interviewed Doug a couple of times already on the podcast. He has a lot to say. And I realized you've written this book together. It's my go-to source of wisdom on search. Do you still remember which chapters you covered? Oh my gosh. It's been a long time, I'm sure.
Yeah, if you told me the chapter title, I could probably say whether it is mere Doug. I did all the fun ones that did all hard ones. And we both did chapter one in our own times. I mean, we were all in chapter twice.
I want to read maybe everything, but in the search relevance problems, search under the hood, debugging, relevance problem, tame in tokens, basic multi-field search, how you build relevance function, feed relevance feedback. Yeah, relevance centered enterprise. That's interesting.
And then semantic and personalized search. Wow. Back in when was this published? I think we published that in 2016, I see. Yeah, 2016. Yeah, well, it's been almost 10 years. Yeah, that's the version I have. So you do have semantic search in the end there. Yeah, awesome. Yeah.
But yeah, Joan, it's interesting to introduce yourself to our audience. What's your background? How you got here? What are you up to? Oh, well, I guess that's a long story. I've had a very circuitous path. I started out in aerospace engineering because I like the math.
And as I got into the field, I found that that's a thing that I really liked once the math and was the software. You could do anything with those. And so while everyone was geeking out about satellites and stuff, I thought that was really cool.
But I realized that there's a big, big world out there that you could address the whole thing with software and math. So I breached out and got that book in your hand. My next big adventure was into search. I joined a concerted consultancy in Charlottesville, Virginia, worked with Doug Turnbull.
I did had amazing adventures, hop on planes. And I talked to Zappos, shoe sales and worked with a patent office. And then I got the opportunity to write that book with Doug. So that pushed me all really, really far.
I got the opportunity to start working for some really interesting companies or for Eventbrite and built out their search and recommendation twice. And then I got a chance to parly that into GitHub. So I went to GitHub and built out their last search-based code search infrastructure.
The old search infrastructure had smoke coming out of it. So we came in, rebuilt infrastructure from ground up. And after a while, I was search was fun. But I was always trying to get a little bit back towards math, towards data science tips.
And in about 2021, I got my chance to make the leak to data science. Join data science at GitHub. And from there, it ended up getting the opportunity, just right place, right time to join Copilot. Because that was kind of, you know, ML machine learning type stuff.
And I was in the data science group to that point. And I was, I came on to Copilot after the research team had wrapped up. There was a research team, brilliant people from GitHub next. They said, while look at these large language models, they're going to do amazing things. I think it's time.
And they built this prototype. And then I came in on the team that was there when it was going into production. So how to get this shipped to everyone, how to start improving it, how to measure, you know, what was working and what wasn't not working.
And then from there, I went into chat, Copilot chat. I was working with some of those features inside the web app. And finally, I was like, well, you know, I've got a little bit of knowledge in my head now, time to write another book.
And I connected with one of the research scientists that was on the original team. Albert Ziegler, we wrote the book, Prompt Engineering for Elements. It's about building the Elements applications.
And with that, just published two weeks ago, officially published, I have started out on a new adventure. Yet again, I am running Arturus Labs. I'm an indie consultant.
And I'm focusing on everything, large language models, Prompt Engineering, how to build applications, you know, it's feasibility, evaluations, stuff like that. Kind of anything you want at this point. And it's a blast. Oh, well, fantastic journey. Yeah, thanks for sharing that.
It's very, you know, it says a lot there. You will believe it or not. But I actually advertised your recent books, the Prompt Engineering, to my students on the recent course that we caught up with my former colleagues on LLMs and Generative AI. So I took the chapter on the rag.
And I thought that rag is nothing else than Prompt Engineering, really. Well, yeah, it's interesting. I mean, that's a topic in and of itself. Are we going to open that kind of worms? Of course. Sure.
Yeah, rag is an interesting thing because everyone talks about rag as if it's own entity, that it's a special thing.
But if you like look at it, especially from my background, which has been searched and then large language models, you click look at rag and it is search and then the large language models.
And if you combine them both, then it's really hard to get a good understanding of what's working and what's not working. You just, you know, you throw up the basic chain application, connect the data source. And I guess you just pray that it works.
But really, what it breaks, if you break it down to its components, then you've got a search application and the Prompt Engineering large language application that it overlaps. But a lot of it's kind of downstream.
And if you can look at those two chunks separately, it becomes a lot easier to debug problems. Rather than saying, you know, user asks this question, I got a garbage answer.
You can say the user asks this question, the large language model interpreted it as this search, this search, return these results. And maybe that's the, maybe that's where the problem was. And you can start debugging that. And the search results got interpreted this way.
And maybe you're not presenting it right to the model.
 So always the name of the game with it's probably everything we're going to talk about today is, you know, figured out how to take this giant black box and break it down into components and figure out what is, you know, what's it made of and what possibly is going wrong and put sensors there and actually debugger.
Yeah, you're absolutely right. And in the, in the lecture, I actually longed code from someone, I forgot their name, but I'll make sure to link it. We've built a rag ground up without using any framework whatsoever. You didn't mention Langchain, that's one way of doing it for sure.
But we just really built, you know, naive, can and search and just use the model out of the box, sentence, bird.
And then I've noticed that because we did use dot product there, I've noticed that it would favor longer passages over shorter ones, right? For example, it would pull up an appendix of a AI powered book, AI powered search book.
And I was like, like, you could clearly see that it's missing the point. It's not able to pull up one short sentence where the answer lies. It just pulls something else remotely related. And that's exactly what you said, right? Like you need to start debugging what's going on there.
And you need to start fixing on figuring out maybe change the model, maybe change the chunking. But yeah, I agree. It felt a bit like black box, but less so when you implement it ground up, right? So you don't depend on any framework.
And when you implement it ground up, you find out that it's not all of that complicated. And once you've built every piece of it, like, you know, I mean, you've already seen the black box broken down to its sub pieces. It's not a black box anymore.
So yeah, that's typically since the whole industry now is sorting itself, trying to figure out what tools are useful and what tools are not going to be useful, I often advocate that people start as close to the metal as possible.
Because these models are actually pretty friendly, pretty fun to play with. Don't put layers on top of it that obfuscate, you know, what's actually happening. Yeah, absolutely. I'm really itching to ask you more about now, like your time at GitHub.
But before that, I also want to like a little bit take your, you know, take a look at your approach, how you view your career, right? So you look, you worked on search, but then you ended up in the hottest place in the way, applying all the lamps, right?
And you needed to convert in a way to an ML engineer.
Do you view it that way? And also if you do, how did you prepare yourself to become a machine learning researcher, actually not even an engineer, right? You are focusing on research aspects of things. So you needed to move the needle in the research space.
I don't know if I have a good answer for you. Like if anyone thinks my career has been successful, which in many ways I've done all right, it's been luckily like tripping and falling uphill. Every time I fall down, it's like in the uphill direction. And I don't, I'm the hand of Providence.
And so what do I do with any of these crazy jumps that I make to prepare? Pretty much, I just take the jump. I think I'm going to say how I'm going to prepare for the next jump. I take, I see the jump. And then I jump into it and like almost drown every single time by surviving.
So in this particular case, yeah, the move towards AI researcher, I mean, there's a lot in that, there's a lot of weight in that phrase that maybe I don't necessarily feel in my own career.
By beginning search for so long and by wanting to do data science for so long, I made myself, you know, over time, pretty aware of how things were, you know, just the typical approach to the model.
So I was never caught any of this in school, but you know, you read, you read the right books and you know, go through the right examples. Yeah, so I have gained, I wouldn't say just an absolute comfort with any of this even now.
But you know, familiarity of being around it for periods of this point. And then when I jumped into the large language modeling stuff, it's actually kind of interesting because it's a different type of AI expert than we've had before and maybe an easier entrance for a lot of people.
Much of my career, I have been an engineer and really I still, I predominantly think of myself as an engineering mindset. And so when you come into, you know, large language models, it's actually really approachable.
You don't have to immediately know everything about, you know, what choice of models to use and like, you know, how to train and have the whole outside and evaluate.
And you can just go to work and at first, at least, just experiment and I really encourage people to do this when they're building on, they're on an application. Rather than, you know, thinking about all the evaluations and stuff at front, you'll need, don't worry, you'll get to them.
But just get your hands, hands dirty, start using the, the APIs and build up some intuition and a weird way empathy for these large language models. Yeah, yeah, this is brilliantly said. I just recently listened to the episode of Lex Friedman with the ontropic team.
So the CEO and some of the researchers there. And one of them said, yeah, exactly. And one of them said that you, along the lines of what you just said about empathy towards the model that when you know where model succeeds and where it kind of fails, you learn how to prompt it. Right.
You know, like which risks you will encounter and you should be okay with those, but you don't tilt towards more risky areas, in the west to succeed in some specific thing. So I don't know, I like that.
But what is your take on LLM unpredictability compared to more, if you will, you know, traditional programming per se. Right. So for example, when you, when we used to, when you used to write code, and I don't know, C++ Java, what have you? It was very deterministic in many ways.
Maybe there have been some things non deterministic like runtime and so on, but still you felt like you, you are in control of many things, right. With LLM, it's different. For example, when you ask an LLM to summarize a document for you, and then you ask second time, the answer will be different.
It will be, you know, in subtle ways, it will be different.
And so that also creates, in my mind, some issues around, okay, if I have several users accessing the same document, should they compute the summary on the fly, or should they compute it once and store it and then show the same copy to all of them, right.
But that also means that if the original summary was not good enough for some reason and subsequent versions were better, I will never show those better versions. Right. So like, you start asking all these like multitude of questions, or am I asking the wrong questions? It's such a challenge.
And I don't, yeah, it's it's a period. Right. Like if you're used to doing something with Python, it's going to be the exact same answer every single time. With these models, it's just like, you know, a very finicky person that keeps changing their opinion.
And you ask them the same question twice, and they've forgotten what they just said. Because it's a new session, so they literally don't have them. You're fully just that they start over. I think we're going to see a shift in this is not going to change anytime soon.
Just it's almost as if you kind of plug a fake human into the circuit. It's like it's going to be independent. That's the nature of it. And that nature is not going to change anytime soon. So I think what you're going to see is a modification in the way we build code around these things.
I think the pain point is when you assume that it's going to be as predictable as a code that you're used to.
But once you get over it, you realize that, okay, well, if I just literally had a human in the loop, there's like an API to connect to a human, then I have to be build a user experience that is somehow tolerant to that. And so let's see.
A lot of times people are hoping the first phrase into interacting with these things. They say, here's a specification, build this code, and they expect the answer to just forward. Now, that can fail in one of two big ways. One way is that it's just too complex.
The model you can do chain of thought reasoning, 01 has it built in and it's magic. And it's going to get better. But with any sufficiently large request, complex request, since you're just appending one token at a time, it's just too easy to paint yourself into a corner.
So models will get better and they'll be less and less likely to paint themselves into a corner. But it'll always be the case with sufficient complexity.
The other issue that you run into and why we'll never ever get there is because when I describe something, the domain of possible implementations, possible completions that match that input is so much larger than whatever I have in my head right now.
And so if you have a company that's like, you know, we're going to have like, you say the specification or code and it will just always make the code. It's like, you don't realize into the codes written which you even wanted. You don't, and then you go back and change it.
You don't realize the codes written and written incorrectly, you know, that, oh, that it's doing what I said. That's not what I meant.
So what does this all mean? I think that future implementations have to do a lot to keep the user in the loop and make the experience so that the user doesn't feel like they're just shouting instructions at a thing and then hoping that it works.
But the user has to be interacting with this thing and, you know, converging towards a solution. So you see this in a couple of ways. One way is like with the assistant interface.
And cursor, forgive me, GitHub, per se, the cursor is just a really good example here where you feel like you're chatting with someone that is working with you to, to, on this code. It gets into something I hope we talk about a little bit later.
Art of facts, you know, they're, you're having this conversation here, but you're working on these artifacts. You're working on these things. And these assistants under, you understand what they're looking at.
Whenever they make a recommendation to change something, you understand how it's going to change your code. You are still in control as a human, say yes or no for all this stuff. And that's one way that they keep the users in the loop.
The other way that we keep users in the loop, and I promise I'll shut up soon, is there's a assistant type behavior and then there's like workflows where a human is, it's still in the loop. But there is a human at the beginning that designed a workflow as like a set of steps.
You can't just say look at this website and pull out all the phone numbers, all of the menu items, all of the, you know, the structure content. And always expected to work.
Sometimes it's better to say, let's take this big thing and have a human, a human in this loop is defining all the steps that it's going to take to implement this workflow.
And that way it's still, you can make something that is recoverable, you know, that there's airstates for some of these steps and you can get out of them, pass it back up to a real human.
But yeah, all along the way of saying these things are going to remain hard to predict, but the code that's built around them, I think, is going to become very tolerant of that and by pulling the users into the conversation constantly.
Yeah, so you basically, if I got your idea right, is that you put the user in the driver's seat, right? And the model or whatever LLM app is still, it's kind of like an assistant, as you said, or companion, whatever you want to call it, right?
But you, like, you still, I guess we are still at that point in time when we need to know exactly what we want, right? As users.
And I think we also need to know how to get it out of the model, right?
Because sometimes no matter what you know, it's not somehow achievable, maybe because you don't know how to prompt well or, you know, you just go into the loops, I frequently go there, when I, for example, chat to, I don't know, chat GPT or it could be any other tool.
When it just keeps going and returning to the point which didn't work already, because the alternative doesn't work now. And I'm like, okay, neither work. Like what you propose just doesn't work.
What should I do? But still, I feel like I became much more productive as a, I don't write code every day, you know, for my work anymore, but for leaving. But when I do, I feel like I saved, I don't know, three, five days of my time by using these tools.
But there is still this kind of unpredictable component to it, you know, I'll give you one example, very specific one. So I was building like like simple Python code, which would draw a diagram. And on the x-axis, it would need to put, you know, these values like 1, 1.5, 2, 2.5 and so on.
And so the model made a mistake by rounding all these values to an integer. And so when x-axis, all of a sudden, I saw the same values, right? And the model doesn't have the reasoning component to realize that it made a mistake.
Or at least call it out and say, do you want it this way? Or should I do it another way? I had to correct it because I knew that I needed to cast it to float. But if I didn't know programming, I wouldn't be able to do that, right? I would be stuck right there.
And so that's the level lake of sophistication we are in still, right? If we're talking about code completion, but I wonder what you feel about this? What do you think about code complete?
You did call out cursor as the tool you use the probably more often now, but you did work on that in GitHub, Copilot team.
And what was your sense of its quality and like challenges around it? And in general, how did you approach the task that research challenge? I can speak a little bit of that. There's two ways in which I will be unsatisfactory here. One, I can't get into all the details probably.
And another way is I've been gone since May. So I'm sure that that makes an amazing change since then. But this Copilot completions was one of the first successful applications of large language models.
And outside of the pure model, chat to BT, a large language model as a large language model service. Like this is, this was just the, I guess it was the first. So the implementation was actually fairly simplistic. Basically, they, we weren't using chat models at the time. Those didn't exist.
We were only using completion models. Completion models, basically, I mean, your audience probably knows this, but given the top part of a document, then all the model does. And it's useful to think of the model this way. It simplifies things. All the model does is it picks the next token.
What is the most likely token based on all these words before it? What's the next token? And then you append that one and you did it again and again.
And so the big aha moment that happened probably in 2019, as well before my time on Copilot was, look, I can take this top half the code down to the function. And the answer, you know, the completion that it makes is surprisingly good.
So like maybe it's time to just wrap or wrap up for application around it. And then after that, everybody's learning these lessons at this point. But it's all about the context that you put around it and how you present it so that the model can make sense of it.
At the time that I started with Copilot, we were still using the completion models. And it was, the context itself was 2048 tokens, I think. So just tiny, tiny, tiny window.
And so a huge focus at the time was how to take all the things that we thought might be useful and squeeze it down into this tiny space, just, you know, actually make sure you've nailed it.
Because not only do you have to fit the prompt into this 2048 tokens, but whatever the completions are, that's, you know, that they're sharing the same windows. You can move that line up and down, but it's always in 2048. So there wasn't, there wasn't. The ingredients were pretty simple.
The file that you're looking at is obviously the most important thing. If the file is long, which I'll often I'll log in to that 48, then the text right above the cursor is an important thing.
There are some initial work with like the, they're still called fill in the middle models, which they don't need this anymore because all the models are so free. You don't need a specialized model for this.
But you could, you know, you could say the prefix and the suffix, and it would do a good job about filling in the middle. So the suffix was also an important part of the context.
Where do you stop this thing? And then as the model, crew is a context-based crew a little bit, we can start sticking in extra things. And so, you know, you start with little bitty things.
These models were not trained on, these models were trained on code, but they didn't necessarily have the context around the code. So the first easy thing to stick in is you could do a shabang at the top, protecting a comment that says, here's the path for this, this file.
And that gives the model context about where this lives in the context of everything else. A big breakthrough that Albert Dealer, Mike Coother, pioneered was the neighboring tab stuff. And I think this is all common sense these days.
But basically, when you, as a human, are using an IDE, you open up the file you're working on. But you also open up other files for reference. So, duh, why don't we, you know, do that ourselves. And the initial implementations of this that, you know, probably got not better at this point.
It was simple. It was like, look at the text right around the cursor. And then search these files for similar text. And in your timing, 2048 token space, you have any room for any of these snippets, then you can chunk other stuff into the context. You have to be careful how you present that.
You can't just, you know, have random scraps of text that are like, you know, partial function implementations. Because that will prime the model to implement partial functions. Like, it'll, you know, it'll just iterate the same gross pattern. It seems above.
So you do things that make it look more like code. You say, here is an interesting, you skip a code from this file in the comment so that it's still, you know, importantly, so it's still valid syntax at the end of it.
And voila, the rest of this history, we came out with a really impactful product that no one had seen anything like it before. And it's certainly changed the way I code. I'm much quicker and probably dumber at the same time. Yeah, it's been an interesting experience.
Oh, maybe more smart because you get to do more things, right? Like you can, I guess you can, you can get hired, like you can achieve, you know, larger heights and then, like experiment way, you need to experiment, right? And not where it feels maybe more mundane.
As long as the code works and like, I don't know, there are no security holes in it and stuff like that, which would need to be checked separately, I guess. Anyway, that's very interesting.
But to close up the loop there, like I'm just trying to understand, you said you focused on keyword search, right? So you, you owned the elastic search sort of pipeline.
Can you, if you're comfortable disclosing that, like, would that index the visible code in the ID somehow so that you can, or what was the role of that in the whole chain, hope, pipeline? You're asking a lot of questions that don't quite seek well on my actual experience.
Let me see, if I can take your question and you take it just a little bit. When I came to GitHub, I worked on code search, which was keyword, like, school search for the entire code corpus. And that was really cool work. But that has since moved to that, they've rebuilt the whole system yet again.
And it's a really amazing engine, the proprietary engine that's effectively grip at fantastically massive scale.
But that said, that code engine, the one that I built in, even the one that came after it, are not the things that are most beneficial for some of the applications that KhoPy that has in the editor. And they do different things for that.
They're, for example, if you're on the web app side, there are things, now I need even in the ID, I'm remembering stuff from six months ago, they do just in time like vector embedding vector storage and stuff.
Vectors are a lot better for certain types of code search where you're finding code that is about something. Whereas, lexical search is a lot better when you're finding code that matches this exact string.
And I think everyone in code outside of code, everyone everywhere is still kind of wrestling with this. There's no one data structure that does all that stuff ideally. And I think we were wrestling with that inside KhoPylet as well.
Yeah, but I guess, yeah, I understood your point and I probably missed that in your explanation that you worked on code search and not on the generation. That's why in code search, you did use the elastic search index.
 But like what I was imagining and I'm completely clueless in this topic, is that by the virtue of LLM being trained on bunch of code, let's say open source code that you can train on license wise, if the user is asking something that reminds the code that had written before, wouldn't it make sense to try to find that code and kind of somehow you know, rag on it with LLM or is it completely different than how you did it?
The like at this point, we've moved to much, it's you know, as of May my left, they've moved to much larger models.
And then the models themselves have read not only all the code and GitHub, but also it's read the internet five times or something. So they read all the blog posts about code. It's amazing, right? It's what times you live in.
 So whenever you're typing something and it kind of smells like something it's a thing before, it doesn't, it doesn't necessarily need rag to go get you know, common motifs, common, you know, here's what you're doing, here's what I think you're doing a code right now and it can piece code together from all the code it's ever learned from and extract late outside of it.
But if it is and you know, this is me talking about how maybe I would build a co-pilot. At some point I guess, you know, you need to see if it's if the user's typing code that is so similar to code in this code base that it's worth bringing it in.
And we kind of did that in a rudimentary way with the neighboring tabs. You've already got the tabs open. And that ended up being super useful.
I think there's probably a kind of a decreased efficacy, there's work for this, where if you're doing a rag search over the entire code base, probably the code that you're going to find is already code that's open in the tabs right beside them. So maybe it's useful to do that maybe it's not.
But I don't know. Yeah, interesting. I think code is like, as you said, it's the first successful LLM application. Probably some companies will say, no, no, no, Dr. Boog's was the first successful LLM application.
But I, but I, there were some, maybe it was the first successful neural search application. And then co-pilot was the first LLM application, successful LLM application. And there's plan nine.
Yeah, there was another company that was out there actually before us, but they just didn't have quite the same, they weren't only my Microsoft at the time, that probably helped a bit. Yeah, budget wise. I'm guessing. Yeah.
Yeah, but I still, I still feel like it feels like magic, right? Like, judgey PT also felt magic and scary in the beginning.
Like when I saw it for the first time and I saw it produce code, I thought that my job is done, even though I was not a programmer anymore by then, but I felt the existential, well, not crisis of fear that basically many of us, and especially junior developers, like probably not needed anymore.
But then as I was, you know, overcoming my fear and I was like, now let me try this thing. It's probably a toy. I found some, what I explained, you know, some edge cases, which just doesn't work. It goes in loops. And so I was like, okay, it seems like another tool under my belt.
So I better master it and not, you know, walk away from it.
But the code generation still feels like magic because you can explain, like you can use tap tab and like on a method signature complete, complete something or on the comment complete something, but you could also write natural language, right?
You could say, generate test cases for me or something like that, right? And then it will understand it and will read your code and will reason about it and produce the test cases.
I mean, that feels really magical. It's the time we're wandering into right now is going to feel like magic for a while until we've got to get used to the exponent, it's just going to keep going up and going up more, going up.
But you know, I've had those existential pains myself, but then I realized when I start using these new tools the way that they want me to use them, I have superpowers. I think what we're actually, you got to have the right mindset.
If your mindset is like, oh, my cobalt job is over, you might be right, your cobalt job is probably over. But if your mindset is like, oh, wow, I can do things I never could do before.
I, John Berryman, put together the HTML from my website and built a react app in this like, like I thought I'd have to have a PhD to do something like that. But it's amazing.
And what you're seeing is an emergence of a new group of people that are, they call us the AI natives, AI native development. And I've heard, you know, code composers rather than like just coders. And you have people that are technically savvy.
You can't, you have to have, you know, some ability to, to recode still at this point, to debuck some stuff like you were talking about. But they all go out at a screen, do this thing for me.
And they have, it just takes a little bit of experience to learn how to shout at the screen in the right way. You got to, you know, you've you still got to have the human ability to, you have to think about how this is structured, how to modularize stuff. There, there is a craft to it still.
But you, you can start building up pieces.
Even if you're not technically savvy, if you've been building it in chunks when one of these pieces messes up gloriously and you've got your floating point numbers that I don't work in out like your example, then at least you can say, I'm going to delete back to here.
I'm going to try a different route. See if I can just bump it out of this. And often you can. And people in every walk of life are are much more effective and efficient at creating.
And it's, you know, you don't get this, you don't always get to solve the nitpahee little, you know, if you really love debugging and writing tests, I'm sorry. I think that's your days might be numbered.
But if you love creating, that's I think we're approaching a new golden age and it's exponential. We're going to keep approaching new golden ages for a while. Yeah, I think in my career, if I can reflect a little bit, I, I love creating much more for sure.
But then back then, we didn't have a lamp, so didn't have compilates. We had to do pay a programming, right? And that was our command. Yeah.
And but the, the, the, that notion that you just said about creativity, I think that drove much more forward than us going into the rabbit down the rabbit holes, you know, of debugging that thing. However important that thing was, you know, of course, you need to debug and so on.
But it didn't feel, like you, you would just feel exhausted after that. You know, like, yeah, I fixed that bug. Finally, I squashed it.
Move on because you, you want to build stuff, right? You, and I think it was it, the extra who said, if debugging is the process of removing, finding and removing bugs, then programming must be the process of introducing bugs. And so that's right. Yeah. That's a vicious circle. Yeah.
You, you already touched on that topic a bit earlier about artifacts. I've read your blog posts, which will, will definitely link, link in and I, I got inspired by that.
I have to say, because oftentimes when I go to the set applications, you know, chat, GPT or perplexity, what have you, and you have a longer conversation there, it is hard to then sort of trace back and think, okay, I branched here and, okay, what was my thinking again?
What did I produce at that point? There is nothing to hold on to except scrolling back and forth.
And that's what you really put. And I want you to open, like, you basically proposed something new, I believe. I wonder if you are the creator of this or like, in any case, you carry this idea forward. Can you explain what do you mean by artifacts? I will carry the idea forward.
I think there is what we're seeing is some convergence around the notion that put into my blog post. For example, with anthropics, artifacts, so that they, they splash something that I think is getting at what I'm talking about.
But if you dig a little at the end, it's not quite what I'm talking about. Whenever you engaged in a conversation with an assistant, LM experience, they just want to chat. And so we've done good over time by giving them like tools.
So now it's rather than just like being your therapist, they can go inducing for you. So that's nice. But still, it's a linear flow. And whenever you're talking about something, it flows back into the backstroll.
Most of the time, when you are getting work done, you're getting work done on something. And artifact, there is a staple, I really wanted to call it a staple object of discourse because it isn't object. It's staple because it may change. And it's it's the object of the discourse.
But artifacts is not just easy to say. But this is what we deal with. Whenever we're paraprogramming on something, it's me and you looking at this piece of code, and you make a recommendation about this. And I say, that's good. We go back and forth.
And anything that you can imagine can be addressed like that. The situation becomes a particular point yet, when every you're dealing with multiple artifacts. So if you're saying, I really like this thing over here. And I wonder how it would fit in with this thing over here.
You're having, as a human, you're having to refer to more than one thing that exists outside of this linear conversation. And you're talking about how they relate to one another. And so the blog post, which I hope you guys all read, arterislabs.
com, we'll do this again in a second, right? It gets into what I think of as an artifact. It talks about how to build a prompt so that you have space for this linear conversation.
But you also draw the models attention to a chunk at the top, usually, you might put it all through, you might put it at the bottom to have an experiment with it. But a static chunk, which is like, here is all the things on the table that people can refer to.
Each object, each artifact, has importantly an ID to be referred to. And I've noticed that these models do really well with arbitrary hexadecimal IDs. So I'll just give them a random ID.
But they're really good at referring to those and not like, you know, they don't seem to hallucinate these IDs, which surprised me.
And so if you have a prompt with these artifacts at the top, and you have a system message that explains to the model how to interact with these things, then my experience is that they obey the instructions really well.
They talk humans are used to using pronouns and names and nicknames and, you know, other pointers that refer to the real thing.
And these models having read all the human text that they could get their hands on, the internet five times, they also understand what you mean about using using pronouns and stuff. So you can say, you know, dear model, there's this thing called artifact. They have these IDs.
When you refer to them, then use these, use anchors like in HML because they've seen a lot of those. And in the href tag, refer to it. And here's an example. And they just, I haven't done any like formal like, you know, reinforced testing, but in my experience with, they just haven't gone wrong.
They are comfortable referring to these things. And it provides a really slick experience, I think, for the user. The user at the end of this conversation is looking at a conversation that they don't have to scroll back up to. They're looking at artifacts on the right.
And they can, they can grab the ones they need. The artifacts themselves, you know, the application developer, you're in charge of how you want to present these things. If it's its text, you can just make it text.
But if it's like a home listing, you know, in the background, it can really be represented by, you know, JSON. But you present the user, you know, picture the home and the, you know, scrollable tab and maybe a scheduling button.
You can do all these rich things with artifacts that you can't do if you're just having a chit chat conversation and it's all just scrolling back into the back. So I think it's a cool enough idea.
I think there's some indications that it's coming into existence with, you know, Anthropocardic factor, GPT, OpenAI's Canvas. Persure is actually implicitly doing a really good job with somehow they're doing this. So it'll come into reality, I think, at some point. It just gives you an idea. Yeah.
It feels like it structures the interaction with the element.
It doesn't feel like you lost your time in a way that you, like, it's like you need to summarize it for your conversation, right? To go back and like tell you what was important, right? But how does it all know what is important? You know, but you already forgot.
And so if you have this artifacts, you can refer to them. But it's interesting that I think these artifacts can you use them? And by the way, I don't know, if you can demo something quickly, I saw a demo on your website. All right.
So this is how do you go to my website? Oh, and you know, check this out. This website was me and like chat GPT and cursor just kind of hanging out, teaching me some HTML. But yeah, you go to my blog. Wait a second. Wait a second. You've built this site with an LLM. Correct? Yeah.
That's what you said. Well, it was me and a large language ball. It wouldn't be just saying build a website. Of course. It's going to, it's the, it's what's going to happen in our future. It was everything is going to be a conversation working on this with a large language ball.
It's a beautiful website. I have to say, yeah, amazing. And the logo. Even the sniffy little logo was generated. AI. Oh, amazing. Okay. This is ridiculous. I'm going to take up just a little bit of your time. It's okay. Oh, it's fine. This logo right here. Check out how many cool things out.
There's, there's a bunch of little bits in here. And then I'll make give you a quiz so you can find the last thing hiding in this. Arcturus is a star in the Northern hemisphere. It's a navigational star. It's a brightest star. And it means guardian of the bear.
And so with my cubo logo here, you've got the a you got the bear. The a is kind of serves. It's a little looks like guardian. The bears represent of the big hairy problem. That's powerful. And but I'm going to, I'm going to help you out. The stars are all for for pointed. It's navigational.
There's one more little uh, uh, Easter egg in this that I didn't notice until I finished building it. I didn't design it. It just emerged. And if, yeah, I'll start doing it. If you're a good computer scientist, especially, oh yeah, then yeah, yeah, a star search, a star search. You got it.
You got it. You got it. I didn't even think about it. I just thought this needs kind of a star over here. And I looked at it and it's a star, which is, you know, optimal, near optimal navigation of the difficult domain ahead. LMS is a good at creating Easter eggs then. Yeah, very terrible jazz.
Yeah. So anyway, sorry, sorry for the, also the stars that was, I mean, these stars are amazing as well. You can just stare at them, right? And Marvel, they move, they look a bit like snowflakes sometimes as well. Yep, they do. All right, so thank you for the digression.
We're looking through my blog and we're looking through, uh, cut the chit chat with artifacts. One thing I'm trying to do recently with my blog, and I hope you guys will, you know, there's plenty of place where you can, uh, like, subscribe for this. I'm trying to put in plenty of examples.
And here's the kind of built-in example of it working. Let's see. You know what, this, we might very well edit this out, but I'm going to go down to the now you try a bit right here. Oh, if this is, uh, in a naive approach, uh, let's say that I'm building like a real estate, uh, helper assistant.
I help real estate agents. And the real estate agent says, I want to put together an email for client about, and I'm listed on Oak Street. Can you hold a listing? And so the thing has some tools built in. Uh, it's got a get listing tool. And so you can see all the garbage that puts in there.
And it's got this listing, um, but like, I don't, I've got the listing. It says it's got the listing. Somehow all this garbage, there's a listing, but I don't know what the listing's really about, um, and so I could ask about it, but then it's, it's a filter.
I don't have the thing that came from the database. I have this weird filter in front of it. Uh, can you pull an email template and draft a new email in another tool that it has? I guess it's going to take its sweet time to do it. Oh, of course. Hmm. Hmm. Okay.
Um, so it drafts, it drafts an email, but oh, look, I've forgotten this, the, the buyer's name. So this is one version of the email that is relevant to this thing right here. Uh, but, you know, I've forgotten to tell you his name is Tim Cersei and my company's name is Artie Tristral Estate.
Uh, it goes back to this and so it fills it in and then I'm left at the end of the conversation, you know, copy and paste in this out.
If this is what I want, I'm going to paste this in the user's email and be really embarrassed when it's got this little string at the top because I've copied that out. And if I wanted to do anything else like modify the template or do anything, it's, it's, it's just, it's not there for me.
All right. So let's, let's do a similar experience with, with this. I want to, uh, again, pull out that listing per, for Oak Street. Oh, I have an interesting. All right.
So in this time, I'm still showing that it, it knows how to use tools, but every time it tries to spit out this like JSON stuff, it's actually getting substituted in with an HRF that points to it. And what is it point to where you click on it and it automatically loads, uh, this scar right here.
Now, um, I didn't take time to make a real pretty interface, but you can, imagine this is JSON. You can make this look like anything you want to. You can make it link out to the database and do all sorts of things. All right. I'm going to put together that email template again. Yeah.
I guess especially when you build a dedicated LAM application, right? You know what type of, what types of objects you're going to be interacting with and you can build the, you know, I can do UI around those, right? But yeah, a very flexible, manable interface.
The interface is whatever the user needs it to be potentially. Yeah. All right. So it's, uh, it's, uh, it's, uh, got this customized email draft. Now, uh, you know, I was looking here, there's no email draft here, but there is here on, on the side of the screen.
And you can see unfortunately, I forgot, uh, to stick in the user's names. So, uh, let's see. Here's the template that it used. We didn't see that in the last example. You can see how it wants to put together stuff. You can see how it actually put together stuff.
And whenever I said I forgot his name, it said, Oh, okay, I've updated that artifact for you. So you don't have like multiple versions scaling up. You just got this. And you could do even interesting things like I could say, uh, you know, this is much better if I say gone bare, you man right here.
And that is now part of the artifact that the assistant sees. Uh, it's, it's in that artifact section at the top of the prompt. You can have it say, please change my email prompt forever to say something out of like this. And you can work on this and say that back to the day.
It just, oh, it opens up a lot of possibilities for a user experience that is easier. Because when we get work done, we get work on things, not just check. Yeah.
Your reason, your reason around artifacts and you work with them like as if they were physical objects almost, right? You can take away this thing with you and go proceed with your task. Yep. You refer to them. You modify them. We use them to do things. And you could, I'm guessing.
I'm really guessing. I'm new to this topic. You could maybe even condition the model on these things, right? You could say given this artifact, I want to do something else with it, like rewrite some parts. You know, would that work? I mean, this kind of sky is the limit.
Uh, it's, it's kind of been a fun thing to think about. But you could have typed artifacts. And then when you have a certain type of artifact, you could introduce the tools. Uh, so like, you know, if we need to modify this artifact artifact, we can, we can know how to deal with it.
You can have, it's kind of what I did with my next post, the roaming rag. You can have artifacts that are like accordions. They're, they're bigger than fit in the prompt.
But you can say, you know, here's summarized outline everything in every piece of that summary, uh, the model effectively can click on it and expand it. And it's just another ID and, you know, a tool expand the section. So it can read docs that are bigger than fits in its context.
There's just a lot of neat things that I think you can do with artifacts at the starting point. It's very interesting. Don't you think that just one thought across my mind is that when we transitioned from static web to like web 2.
0, I guess, so what is what was it called when you can actually modify things on the web, right? You could send a comment, you could, you could do stuff. Uh, now it feels like we've transitioned into the new phase when we do the same to the ideas.
We like exchange ideas and we can like modify them, you know, prior on them, prompt with them, uh, take away a store. So it becomes more on the concept level. I think everything's going to get really weird, uh, going forward. I think we've been used to going to the internet and going to web pages.
And even if we could interact a little bit, it's nothing like you're about to see. I wonder if a lot of the internet experiences, you know, they're worried about all the text going away, uh, because like we were, we'd run out, run out of the text, the internet, training these giant giant models.
Maybe the future of the internet is going to be replaced by just conversations. The, you're going to go to a place that is a sensible, you know, starting point, but the whole website is going to become whatever reality you need it to be at the time.
And I have no idea how we harvest the text of that train of future models. It might be crazy, but I think I think we're getting ready for a future we cannot possibly predict. Yeah, and I think spam will be replaced by slope, right? I don't know if you heard of this, YouTube.
No, slow, slow, slow is, uh, SLOP. So it's basically an unverified output of an LLAM model. So something that got produced back to your question, you don't, you have no idea if it's true or not, you go and paste it somewhere in the web and then LLAM goes and scraps it and learns from it.
So you spam the model. And so there is a call out. If this feedback effect. Yeah, exactly. And there is a call out that, hey, let's not spam or let's not post slope on the web because that will bite us because we are moving so far ahead in the LLAM. And who is obeying that recommendation? Exactly.
Probably not the companies that need content produced. Yeah. Yeah. The moment you say, don't do something, there will be a bunch of people saying, oh, let's try. That sounds like fine. Oh, that's a good idea. And then we need to invent a solution for that. Hey, Jonathan, it was really exciting.
And I've known it like a ton by talking to you. I feel like we can record probably like like three months style episode, you know, four or five hours before we get exhausted. But I also wanted to give you a chance to, you know, go on stage and sort of and talk about your book way.
Like, why do you think everyone needs to read it? I want to read it. If I get a chance to get my hands on it, hopefully soon. Everyone needs to read it because every time I make a sale, I get one cup of coffee. So that's why everyone needs to read it. Of course. Yeah, that's a good reason.
But then also, yeah, go ahead. No, I wanted also you to give you a chance to talk about your company.
Because I know that feeling of starting something new on your own, you call yourself an indie consultant, right? At the same time, you have so much with you and your luggage, right? Like you, the knowledge of the experience. And so why not share it in a different way through your company.
But I wanted to learn a bit more. What is your vision for the company? What do you think you will offer like in midterm? Where do you create the value for the customers? And maybe there will be some customers listening in this podcast, hopefully. Sure.
Well, okay, let's go through both of those then. I hope I hope everyone reads the book. I hope they enjoy it. I hope they learn from it. Working with large language models is a very different beast from what you're used to.
I think, you know, three years from now, everyone will be a large language model application developer because they're becoming so prevalent everywhere. So start early. Get your hands dirty, interact with these things.
And my book helps kind of take, you know, give you the training wells at first to understand here are a bunch of the problems that you run into. Here's how here's how model works. That's there's actually a lot of good intuition and just understanding the tool that you're interacting with.
Here's how to organize a prompt. And that's not always easy. You got to figure out what's all the stuff you might use. And you can't use all of it because it doesn't all fit or because you don't want to wait for the latency.
You know, it tells you how to, you know, fit that into a prompt, present it to the model in a way that kind of empathetically, the model is going to understand the model is not psychic. You need to talk to the model as if you're talking to, you know, someone that you're working with.
 And then towards the end of the book, it gets outside of a single prompt and it talks about, you know, like this tool magic word we've got right now, agency and how to build a assistant behavior with tools and how to, you know, build a more sophisticated thinking steps with it in review of, you know, what's happened.
And it talks about workflows, which is another type of agency really about how to, you know, take an input, bunch of data, pick it apart, do the right steps to get a job done with hopefully not going off track too much.
We talk a little bit about evaluation and we wrap it up by saying holy cow, look at the future we're going into. This is going to be amazing. So I hope you get a chance to read the book and I hope you enjoy it. I hope it's as enjoyable to you as it was painful to me to write.
And then yes, I am out of my own now. I'm an indie consultant at Arturus Labs. I'm specializing in all things just like the book, prompt engineering, large language model application development. I think we're going into a very different world as far as like how you build things.
You've got to build it like we had earlier in this conversation. You've got to build these components to deal with. You've got to build it web apps to deal with these components that are very undependable. I do make them as dependable as possible.
How do you make the user experience where they trust what's happening? And that's tricky. So I offer a whole range of things from just education, going in and training companies. I like going and working with them to think through what product they're working on right now with their next big goal.
I can say this is a great idea. You're on the right track. This is not quite feasible, but we can fix it. That's the product type stuff I like thinking through. And then as we get to a longer engagement, I just love working with these companies, especially like startups.
Just sit down, pair with them, do transfer of knowledge, type stuff. It's just really neat to see what people are up to. A lot of creative ideas right now. And then finally, yeah, please make sure you check out my website, www.artrisslabs.com.
I'm going to throw together a lot more blog posts like the one we didn't know today. I'm trying right now to make sure every blog post has something juicy, a piece of code that actually works and you can experience the thing that was running to my mind at the time. So try it out.
I'm really engaging on Twitter. Tell me what you think. And yeah, I'd like to get to to know you guys too. Yeah, amazing, amazing. And I wish you all the best with your new adventure, your new venture. And yeah, we will link everything. We will link the book.
We will link your site and blogs for sure. Thanks so much for spending time with me and educating me and keeping up with Mike sometimes, you know, and obvious questions. It was really, really a pleasure to talk to you.
And I really, really hope that we can record sometime soon because you seem to be cooking a lot of ideas. And you take from what I gather, you take really practical view of things. And you've been like, and you are an engineer and researcher. And so that's very dear to my heart to see.
And I can't wait to see what you come up with next. Me too. Well, thanks so much for having me on. It's been great talking to you. So yeah, let's do this again sometime. Yeah, thanks, John. Have a good day.