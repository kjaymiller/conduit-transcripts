---
description: '<p>YouTube: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=r4HEpyur-OE">https://www.youtube.com/watch?v=r4HEpyur-OE</a></p><p>Vector
  Podcast Live</p><p>Topics:</p><p>00:00 Kick-off introducing co:rise study platform</p><p>03:03
  Grant’s background</p><p>04:58 Principle of 3 C’s in the life of a CTO: Code, Conferences
  and Customers</p><p>07:16 Principle of 3 C’s in the Search Engine development: Content,
  Collaboration and Context</p><p>11:51 Balance between manual tuning in pursuit to
  learn and Machine Learning</p><p>15:42 How to nurture intuition in building search
  engine algorithms</p><p>18:51 How to change the approach of organizations to true
  experimentation</p><p>23:17 Where should one start in approaching the data (like
  click logs) for developing a search engine</p><p>29:36 How to measure the success
  of your search engine</p><p>33:50 The role of manual query rating to improve search
  result relevancy</p><p>36:56 What are the available datasets, tools and algorithms,
  that allow us to build a search engine?</p><p>41:56 Vector search and its role in
  broad search engine development and how the profession is shaping up</p><p>49:01
  The magical question of WHY: what motivates Grant to stay in the space</p><p>52:09
  Announcement from Grant: course discount code DGSEARCH10</p><p>54:55 Questions from
  the audience</p><p>Show notes:</p><p>- Grant’s interview at Berlin Buzzwords 2016:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=Y13gZM5EGdc">https://www.youtube.com/watch?v=Y13gZM5EGdc</a></p><p>-
  “BM25 is so Yesterday: Modern Techniques for Better Search”: <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=CRZfc9lj7Po">https://www.youtube.com/watch?v=CRZfc9lj7Po</a></p><p>-
  “Taming text” - book co-authored by Grant: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.manning.com/books/taming-text">https://www.manning.com/books/taming-text</a></p><p>-
  Search Fundamentals course - <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://corise.com/course/search-fundamentals">https://corise.com/course/search-fundamentals</a></p><p>-
  Search with ML course - <a target="_blank" rel="noopener noreferrer nofollow" href="https://corise.com/course/search-with-machine-learning">https://corise.com/course/search-with-machine-learning</a></p><p>-
  Click Models for Web Search: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://github.com/markovi/PyClick">https://github.com/markovi/PyClick</a></p><p>-
  Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing, book
  by Ron Kohavi et al: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical-ebook/dp/B0845Y3DJV">https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical-ebook/dp/B0845Y3DJV</a></p><p>-
  Quepid, open source tool and free service for query rating and relevancy tuning:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://quepid.com/">https://quepid.com/</a></p><p>-
  Grant’s talk in 2013 where he discussed the need of a vector field in Lucene and
  Solr: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=dCCqauwMWFE">https://www.youtube.com/watch?v=dCCqauwMWFE</a></p><p>-
  Demo of multimodal search with CLIP: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://blog.muves.io/multilingual-and-multimodal-vector-search-with-hardware-acceleration-2091a825de78">https://blog.muves.io/multilingual-and-multimodal-vector-search-with-hardware-acceleration-2091a825de78</a></p><p>-
  Learning to Boost: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=af1dyamySCs">https://www.youtube.com/watch?v=af1dyamySCs</a></p>'
image_url: https://media.rss.com/vector-podcast/20220609_020607_0461c4544521e6be53134d28774b7c4a.jpg
pub_date: Thu, 09 Jun 2022 14:51:07 GMT
title: Grant Ingersoll - Fractional CTO, Leading Search Consultant - Engineering Better
  Search
url: https://rss.com/podcasts/vector-podcast/514832
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 28.400000000000002,
  "text": " Hello there, vector podcast is here. I''m Dimitri Khan and I''ll be hosting
  this session. And just a few words on the logistics.", "tokens": [50364, 2425, 456,
  11, 8062, 7367, 307, 510, 13, 286, 478, 20975, 270, 470, 18136, 293, 286, 603, 312,
  16058, 341, 5481, 13, 400, 445, 257, 1326, 2283, 322, 264, 27420, 13, 51784], "temperature":
  0.0, "avg_logprob": -0.4064546857561384, "compression_ratio": 1.1666666666666667,
  "no_speech_prob": 0.08359098434448242}, {"id": 1, "seek": 2840, "start": 29.36,
  "end": 38.32, "text": " Everyone in the audience feel free to submit your questions
  either through a Q&A panel or directly in the chat and we will try to handle as
  many questions as we can.", "tokens": [50412, 5198, 294, 264, 4034, 841, 1737, 281,
  10315, 428, 1651, 2139, 807, 257, 1249, 5, 32, 4831, 420, 3838, 294, 264, 5081,
  293, 321, 486, 853, 281, 4813, 382, 867, 1651, 382, 321, 393, 13, 50860], "temperature":
  0.0, "avg_logprob": -0.1969124231583033, "compression_ratio": 1.569672131147541,
  "no_speech_prob": 0.06889087706804276}, {"id": 2, "seek": 2840, "start": 39.68,
  "end": 55.519999999999996, "text": " I''ll save you words about core eyes. What''s
  core eyes is a new education platform that transforms the way professionals build
  technical high demand skills through top industry instructors and collective peer
  learning.", "tokens": [50928, 286, 603, 3155, 291, 2283, 466, 4965, 2575, 13, 708,
  311, 4965, 2575, 307, 257, 777, 3309, 3663, 300, 35592, 264, 636, 11954, 1322, 6191,
  1090, 4733, 3942, 807, 1192, 3518, 28367, 293, 12590, 15108, 2539, 13, 51720], "temperature":
  0.0, "avg_logprob": -0.1969124231583033, "compression_ratio": 1.569672131147541,
  "no_speech_prob": 0.06889087706804276}, {"id": 3, "seek": 5552, "start": 56.080000000000005,
  "end": 69.12, "text": " And the format of their courses is innovative mixing live
  instructor sessions with real world projects and fireside chats like this one technique
  with operators who experts in their fields.", "tokens": [50392, 400, 264, 7877,
  295, 641, 7712, 307, 12999, 11983, 1621, 18499, 11081, 365, 957, 1002, 4455, 293,
  15044, 482, 38057, 411, 341, 472, 6532, 365, 19077, 567, 8572, 294, 641, 7909, 13,
  51044], "temperature": 0.0, "avg_logprob": -0.19098338796131648, "compression_ratio":
  1.5142857142857142, "no_speech_prob": 0.014435657300055027}, {"id": 4, "seek": 5552,
  "start": 71.12, "end": 79.68, "text": " I will say a few words about myself as well,
  untraditionally on the podcast, but I think it becomes a tradition now second time.",
  "tokens": [51144, 286, 486, 584, 257, 1326, 2283, 466, 2059, 382, 731, 11, 1701,
  6206, 15899, 322, 264, 7367, 11, 457, 286, 519, 309, 3643, 257, 6994, 586, 1150,
  565, 13, 51572], "temperature": 0.0, "avg_logprob": -0.19098338796131648, "compression_ratio":
  1.5142857142857142, "no_speech_prob": 0.014435657300055027}, {"id": 5, "seek": 7968,
  "start": 80.64, "end": 94.48, "text": " I said and Dimitri Khan I have a PhD in
  natural language processing. I''ve worked at company Alphasense helped to build
  the search stack. I spent like a decade, you know, there.", "tokens": [50412, 286,
  848, 293, 20975, 270, 470, 18136, 286, 362, 257, 14476, 294, 3303, 2856, 9007, 13,
  286, 600, 2732, 412, 2237, 967, 7485, 1288, 4254, 281, 1322, 264, 3164, 8630, 13,
  286, 4418, 411, 257, 10378, 11, 291, 458, 11, 456, 13, 51104], "temperature": 0.0,
  "avg_logprob": -0.28397237141927084, "compression_ratio": 1.4158415841584158, "no_speech_prob":
  0.018019085749983788}, {"id": 6, "seek": 7968, "start": 95.68, "end": 103.68, "text":
  " I''ve been a principal AI scientist at silo AI. It''s a AI consulting gig focusing
  on a number of ML verticals.", "tokens": [51164, 286, 600, 668, 257, 9716, 7318,
  12662, 412, 3425, 78, 7318, 13, 467, 311, 257, 7318, 23682, 8741, 8416, 322, 257,
  1230, 295, 21601, 9429, 82, 13, 51564], "temperature": 0.0, "avg_logprob": -0.28397237141927084,
  "compression_ratio": 1.4158415841584158, "no_speech_prob": 0.018019085749983788},
  {"id": 7, "seek": 10368, "start": 104.64, "end": 109.84, "text": " And recently
  I joined company Tom Tom as a senior product manager working on search.", "tokens":
  [50412, 400, 3938, 286, 6869, 2237, 5041, 5041, 382, 257, 7965, 1674, 6598, 1364,
  322, 3164, 13, 50672], "temperature": 0.0, "avg_logprob": -0.21967054393193494,
  "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04752865061163902},
  {"id": 8, "seek": 10368, "start": 111.60000000000001, "end": 117.76, "text": " I''ve
  also been a contributor and user of Cupid. It''s a query rating tool go check it
  out. It''s an open source tool.", "tokens": [50760, 286, 600, 611, 668, 257, 42859,
  293, 4195, 295, 383, 6127, 13, 467, 311, 257, 14581, 10990, 2290, 352, 1520, 309,
  484, 13, 467, 311, 364, 1269, 4009, 2290, 13, 51068], "temperature": 0.0, "avg_logprob":
  -0.21967054393193494, "compression_ratio": 1.471698113207547, "no_speech_prob":
  0.04752865061163902}, {"id": 9, "seek": 10368, "start": 118.64000000000001, "end":
  125.60000000000001, "text": " So overall I spent like 16 years in developing search
  engines for startups and multinational technology giants.", "tokens": [51112, 407,
  4787, 286, 4418, 411, 3165, 924, 294, 6416, 3164, 12982, 337, 28041, 293, 45872,
  1478, 2899, 31894, 13, 51460], "temperature": 0.0, "avg_logprob": -0.21967054393193494,
  "compression_ratio": 1.471698113207547, "no_speech_prob": 0.04752865061163902},
  {"id": 10, "seek": 12560, "start": 126.24, "end": 134.16, "text": " I also happen
  to be hosting this podcast vector podcast go check it out. I''ll share the link
  in a second.", "tokens": [50396, 286, 611, 1051, 281, 312, 16058, 341, 7367, 8062,
  7367, 352, 1520, 309, 484, 13, 286, 603, 2073, 264, 2113, 294, 257, 1150, 13, 50792],
  "temperature": 0.0, "avg_logprob": -0.1895643570843865, "compression_ratio": 1.587962962962963,
  "no_speech_prob": 0.010211152024567127}, {"id": 11, "seek": 12560, "start": 135.35999999999999,
  "end": 142.88, "text": " And I''m also blogging on medium on on my findings in vector
  search. So you might hear me talking about vector search here and there.", "tokens":
  [50852, 400, 286, 478, 611, 6968, 3249, 322, 6399, 322, 322, 452, 16483, 294, 8062,
  3164, 13, 407, 291, 1062, 1568, 385, 1417, 466, 8062, 3164, 510, 293, 456, 13, 51228],
  "temperature": 0.0, "avg_logprob": -0.1895643570843865, "compression_ratio": 1.587962962962963,
  "no_speech_prob": 0.010211152024567127}, {"id": 12, "seek": 12560, "start": 144.07999999999998,
  "end": 152.56, "text": " And today I''m super super excited to have Grant in your
  soul with me. I''ve known Grant since about 2011.", "tokens": [51288, 400, 965,
  286, 478, 1687, 1687, 2919, 281, 362, 17529, 294, 428, 5133, 365, 385, 13, 286,
  600, 2570, 17529, 1670, 466, 10154, 13, 51712], "temperature": 0.0, "avg_logprob":
  -0.1895643570843865, "compression_ratio": 1.587962962962963, "no_speech_prob": 0.010211152024567127},
  {"id": 13, "seek": 15256, "start": 152.72, "end": 162.32, "text": " Not personally,
  but I''ve seen I''ve seen him on stage on you know Berlin buzzwords conference and
  Lucinda revolution.", "tokens": [50372, 1726, 5665, 11, 457, 286, 600, 1612, 286,
  600, 1612, 796, 322, 3233, 322, 291, 458, 13848, 13036, 13832, 7586, 293, 9593,
  6837, 8894, 13, 50852], "temperature": 0.0, "avg_logprob": -0.2910769271850586,
  "compression_ratio": 1.5645756457564575, "no_speech_prob": 0.011651513166725636},
  {"id": 14, "seek": 15256, "start": 162.32, "end": 168.24, "text": " And he has been
  a long contributor and open source as well. Solary, Lucinda Mahoot and others.",
  "tokens": [50852, 400, 415, 575, 668, 257, 938, 42859, 293, 1269, 4009, 382, 731,
  13, 7026, 822, 11, 9593, 6837, 10104, 6259, 293, 2357, 13, 51148], "temperature":
  0.0, "avg_logprob": -0.2910769271850586, "compression_ratio": 1.5645756457564575,
  "no_speech_prob": 0.011651513166725636}, {"id": 15, "seek": 15256, "start": 169.44,
  "end": 174.8, "text": " And very very effective presenter. I just watched a few
  presentations as a homework for this session.", "tokens": [51208, 400, 588, 588,
  4942, 35594, 13, 286, 445, 6337, 257, 1326, 18964, 382, 257, 14578, 337, 341, 5481,
  13, 51476], "temperature": 0.0, "avg_logprob": -0.2910769271850586, "compression_ratio":
  1.5645756457564575, "no_speech_prob": 0.011651513166725636}, {"id": 16, "seek":
  15256, "start": 175.52, "end": 181.2, "text": " There will be some questions from
  there. But hey Grant, let''s start with an introduction from Indio own words.",
  "tokens": [51512, 821, 486, 312, 512, 1651, 490, 456, 13, 583, 4177, 17529, 11,
  718, 311, 722, 365, 364, 9339, 490, 2333, 1004, 1065, 2283, 13, 51796], "temperature":
  0.0, "avg_logprob": -0.2910769271850586, "compression_ratio": 1.5645756457564575,
  "no_speech_prob": 0.011651513166725636}, {"id": 17, "seek": 18256, "start": 182.56,
  "end": 191.84, "text": " Hey Dmitri and thank you so much for having me on the vector
  podcast and obviously props to co rise here as well for helping sponsor this.",
  "tokens": [50364, 1911, 413, 3508, 470, 293, 1309, 291, 370, 709, 337, 1419, 385,
  322, 264, 8062, 7367, 293, 2745, 26173, 281, 598, 6272, 510, 382, 731, 337, 4315,
  16198, 341, 13, 50828], "temperature": 0.0, "avg_logprob": -0.26167962816026474,
  "compression_ratio": 1.5546218487394958, "no_speech_prob": 0.001603670185431838},
  {"id": 18, "seek": 18256, "start": 192.72, "end": 201.2, "text": " Both Daniel Tungaling
  and I are on the co rise platform and really enjoying our time there. So real quick
  about myself.", "tokens": [50872, 6767, 8033, 314, 1063, 4270, 293, 286, 366, 322,
  264, 598, 6272, 3663, 293, 534, 9929, 527, 565, 456, 13, 407, 957, 1702, 466, 2059,
  13, 51296], "temperature": 0.0, "avg_logprob": -0.26167962816026474, "compression_ratio":
  1.5546218487394958, "no_speech_prob": 0.001603670185431838}, {"id": 19, "seek":
  18256, "start": 201.2, "end": 208.88, "text": " As you said, my name is Grant Ingersoll.
  I guess these days a long standing user and contributor and committer.", "tokens":
  [51296, 1018, 291, 848, 11, 452, 1315, 307, 17529, 682, 9458, 1833, 13, 286, 2041,
  613, 1708, 257, 938, 4877, 4195, 293, 42859, 293, 5599, 391, 13, 51680], "temperature":
  0.0, "avg_logprob": -0.26167962816026474, "compression_ratio": 1.5546218487394958,
  "no_speech_prob": 0.001603670185431838}, {"id": 20, "seek": 20888, "start": 209.35999999999999,
  "end": 221.6, "text": " And generally somebody who participates in the search space,
  if you will, I think I wrote my first Lucine code back in 2004 or so. I guess that
  maybe makes me old.", "tokens": [50388, 400, 5101, 2618, 567, 3421, 1024, 294, 264,
  3164, 1901, 11, 498, 291, 486, 11, 286, 519, 286, 4114, 452, 700, 9593, 533, 3089,
  646, 294, 15817, 420, 370, 13, 286, 2041, 300, 1310, 1669, 385, 1331, 13, 51000],
  "temperature": 0.0, "avg_logprob": -0.14172953528326912, "compression_ratio": 1.5502645502645502,
  "no_speech_prob": 0.005819553509354591}, {"id": 21, "seek": 20888, "start": 222.72,
  "end": 231.68, "text": " As far as my background is I was one of the co founders
  of Lucidworks, which is one of the leading companies in the search space.", "tokens":
  [51056, 1018, 1400, 382, 452, 3678, 307, 286, 390, 472, 295, 264, 598, 25608, 295,
  9593, 327, 18357, 11, 597, 307, 472, 295, 264, 5775, 3431, 294, 264, 3164, 1901,
  13, 51504], "temperature": 0.0, "avg_logprob": -0.14172953528326912, "compression_ratio":
  1.5502645502645502, "no_speech_prob": 0.005819553509354591}, {"id": 22, "seek":
  23168, "start": 232.64000000000001, "end": 240.08, "text": " I then left them in
  2019 to become the chief technology officer at the Wikimedia Foundation.", "tokens":
  [50412, 286, 550, 1411, 552, 294, 6071, 281, 1813, 264, 9588, 2899, 8456, 412, 264,
  23377, 332, 14212, 10335, 13, 50784], "temperature": 0.0, "avg_logprob": -0.18921605278463924,
  "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.01034748274832964},
  {"id": 23, "seek": 23168, "start": 240.88, "end": 246.96, "text": " You probably
  know them better as the nonprofit behind Wikipedia and Wikidata.", "tokens": [50824,
  509, 1391, 458, 552, 1101, 382, 264, 23348, 2261, 28999, 293, 23377, 327, 3274,
  13, 51128], "temperature": 0.0, "avg_logprob": -0.18921605278463924, "compression_ratio":
  1.4193548387096775, "no_speech_prob": 0.01034748274832964}, {"id": 24, "seek": 23168,
  "start": 247.6, "end": 256.96000000000004, "text": " So I was the CTO there for
  two years. And then in August or so of 2021, I took some time off.", "tokens": [51160,
  407, 286, 390, 264, 383, 15427, 456, 337, 732, 924, 13, 400, 550, 294, 6897, 420,
  370, 295, 7201, 11, 286, 1890, 512, 565, 766, 13, 51628], "temperature": 0.0, "avg_logprob":
  -0.18921605278463924, "compression_ratio": 1.4193548387096775, "no_speech_prob":
  0.01034748274832964}, {"id": 25, "seek": 25696, "start": 257.91999999999996, "end":
  265.52, "text": " And then in January of 2022, I went on my own as a consultant
  and an instructor for co rise.", "tokens": [50412, 400, 550, 294, 7061, 295, 20229,
  11, 286, 1437, 322, 452, 1065, 382, 257, 24676, 293, 364, 18499, 337, 598, 6272,
  13, 50792], "temperature": 0.0, "avg_logprob": -0.17949795455075382, "compression_ratio":
  1.5166666666666666, "no_speech_prob": 0.005357579793781042}, {"id": 26, "seek":
  25696, "start": 265.52, "end": 274.79999999999995, "text": " So here we are now.
  I am commonly doing work in what I would call fractional CTO land, which means I
  primarily help companies", "tokens": [50792, 407, 510, 321, 366, 586, 13, 286, 669,
  12719, 884, 589, 294, 437, 286, 576, 818, 17948, 1966, 383, 15427, 2117, 11, 597,
  1355, 286, 10029, 854, 3431, 51256], "temperature": 0.0, "avg_logprob": -0.17949795455075382,
  "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.005357579793781042},
  {"id": 27, "seek": 25696, "start": 275.59999999999997, "end": 284.71999999999997,
  "text": " kind of get their technology stack in order, make decisions about technology,
  higher teams, upgrade teams, do all the things that a CTO would do.", "tokens":
  [51296, 733, 295, 483, 641, 2899, 8630, 294, 1668, 11, 652, 5327, 466, 2899, 11,
  2946, 5491, 11, 11484, 5491, 11, 360, 439, 264, 721, 300, 257, 383, 15427, 576,
  360, 13, 51752], "temperature": 0.0, "avg_logprob": -0.17949795455075382, "compression_ratio":
  1.5166666666666666, "no_speech_prob": 0.005357579793781042}, {"id": 28, "seek":
  28472, "start": 285.28000000000003, "end": 288.88000000000005, "text": " Often for
  small businesses and or startups.", "tokens": [50392, 20043, 337, 1359, 6011, 293,
  420, 28041, 13, 50572], "temperature": 0.0, "avg_logprob": -0.2162195506848787,
  "compression_ratio": 1.6049382716049383, "no_speech_prob": 0.005259320139884949},
  {"id": 29, "seek": 28472, "start": 290.64000000000004, "end": 295.68, "text": "
  And so that''s really my background. Really happy to be here and looking forward
  to the podcast.", "tokens": [50660, 400, 370, 300, 311, 534, 452, 3678, 13, 4083,
  2055, 281, 312, 510, 293, 1237, 2128, 281, 264, 7367, 13, 50912], "temperature":
  0.0, "avg_logprob": -0.2162195506848787, "compression_ratio": 1.6049382716049383,
  "no_speech_prob": 0.005259320139884949}, {"id": 30, "seek": 28472, "start": 296.88000000000005,
  "end": 305.92, "text": " Awesome. Great to have you, really grand. And also, you
  know, finally, I have a chance to ask some questions and chat to you in this cozy
  atmosphere as well.", "tokens": [50972, 10391, 13, 3769, 281, 362, 291, 11, 534,
  2697, 13, 400, 611, 11, 291, 458, 11, 2721, 11, 286, 362, 257, 2931, 281, 1029,
  512, 1651, 293, 5081, 281, 291, 294, 341, 29414, 8018, 382, 731, 13, 51424], "temperature":
  0.0, "avg_logprob": -0.2162195506848787, "compression_ratio": 1.6049382716049383,
  "no_speech_prob": 0.005259320139884949}, {"id": 31, "seek": 28472, "start": 307.36,
  "end": 314.0, "text": " And I wanted to start with a question. So I was watching
  a kind of short interview you gave.", "tokens": [51496, 400, 286, 1415, 281, 722,
  365, 257, 1168, 13, 407, 286, 390, 1976, 257, 733, 295, 2099, 4049, 291, 2729, 13,
  51828], "temperature": 0.0, "avg_logprob": -0.2162195506848787, "compression_ratio":
  1.6049382716049383, "no_speech_prob": 0.005259320139884949}, {"id": 32, "seek":
  31472, "start": 314.88000000000005, "end": 322.40000000000003, "text": " During
  Berlin buzzwords 2016, where you said how you split your time as then CTO", "tokens":
  [50372, 6842, 13848, 13036, 13832, 6549, 11, 689, 291, 848, 577, 291, 7472, 428,
  565, 382, 550, 383, 15427, 50748], "temperature": 0.0, "avg_logprob": -0.2232109815224834,
  "compression_ratio": 1.5818181818181818, "no_speech_prob": 0.0024626140948385},
  {"id": 33, "seek": 31472, "start": 323.6, "end": 329.20000000000005, "text": " of
  I believe, Lucid works. You said that you split your time between three C''s, which
  is writing code,", "tokens": [50808, 295, 286, 1697, 11, 9593, 327, 1985, 13, 509,
  848, 300, 291, 7472, 428, 565, 1296, 1045, 383, 311, 11, 597, 307, 3579, 3089, 11,
  51088], "temperature": 0.0, "avg_logprob": -0.2232109815224834, "compression_ratio":
  1.5818181818181818, "no_speech_prob": 0.0024626140948385}, {"id": 34, "seek": 31472,
  "start": 330.0, "end": 336.64000000000004, "text": " going to conferences and talking
  to customers. Now that you''re independent, is this how you spend your time or did
  you", "tokens": [51128, 516, 281, 22032, 293, 1417, 281, 4581, 13, 823, 300, 291,
  434, 6695, 11, 307, 341, 577, 291, 3496, 428, 565, 420, 630, 291, 51460], "temperature":
  0.0, "avg_logprob": -0.2232109815224834, "compression_ratio": 1.5818181818181818,
  "no_speech_prob": 0.0024626140948385}, {"id": 35, "seek": 31472, "start": 336.64000000000004,
  "end": 338.72, "text": " did you get some new letters of the alphabet?", "tokens":
  [51460, 630, 291, 483, 512, 777, 7825, 295, 264, 23339, 30, 51564], "temperature":
  0.0, "avg_logprob": -0.2232109815224834, "compression_ratio": 1.5818181818181818,
  "no_speech_prob": 0.0024626140948385}, {"id": 36, "seek": 33872, "start": 339.12,
  "end": 339.92, "text": " Yeah.", "tokens": [50384, 865, 13, 50424], "temperature":
  0.0, "avg_logprob": -0.23830691443549262, "compression_ratio": 1.5161290322580645,
  "no_speech_prob": 0.007980392314493656}, {"id": 37, "seek": 33872, "start": 341.12,
  "end": 349.76000000000005, "text": " Yeah, and there''s often in there as well,
  colleagues and co-workers, you know, especially in, you know, the CTO role is kind
  of a funny one, right?", "tokens": [50484, 865, 11, 293, 456, 311, 2049, 294, 456,
  382, 731, 11, 7734, 293, 598, 12, 37101, 11, 291, 458, 11, 2318, 294, 11, 291, 458,
  11, 264, 383, 15427, 3090, 307, 733, 295, 257, 4074, 472, 11, 558, 30, 50916], "temperature":
  0.0, "avg_logprob": -0.23830691443549262, "compression_ratio": 1.5161290322580645,
  "no_speech_prob": 0.007980392314493656}, {"id": 38, "seek": 33872, "start": 350.40000000000003,
  "end": 358.40000000000003, "text": " Depending on the company, it can mean a lot
  of different things. At some companies, CTOs are entirely outward facing.", "tokens":
  [50948, 22539, 322, 264, 2237, 11, 309, 393, 914, 257, 688, 295, 819, 721, 13, 1711,
  512, 3431, 11, 383, 15427, 82, 366, 7696, 26914, 7170, 13, 51348], "temperature":
  0.0, "avg_logprob": -0.23830691443549262, "compression_ratio": 1.5161290322580645,
  "no_speech_prob": 0.007980392314493656}, {"id": 39, "seek": 33872, "start": 358.40000000000003,
  "end": 363.44000000000005, "text": " It''s effectively a sales role or a marketing
  role, right?", "tokens": [51348, 467, 311, 8659, 257, 5763, 3090, 420, 257, 6370,
  3090, 11, 558, 30, 51600], "temperature": 0.0, "avg_logprob": -0.23830691443549262,
  "compression_ratio": 1.5161290322580645, "no_speech_prob": 0.007980392314493656},
  {"id": 40, "seek": 36344, "start": 364.16, "end": 368.16, "text": " You''re out
  evangelizing the product, you''re talking to customers, etc.", "tokens": [50400,
  509, 434, 484, 24546, 3319, 264, 1674, 11, 291, 434, 1417, 281, 4581, 11, 5183,
  13, 50600], "temperature": 0.0, "avg_logprob": -0.13048405017492906, "compression_ratio":
  1.808695652173913, "no_speech_prob": 0.011804792098701}, {"id": 41, "seek": 36344,
  "start": 368.71999999999997, "end": 376.96, "text": " In a startup, the CTO is often
  the primary engineer. If you''re a two person startup and you''re just", "tokens":
  [50628, 682, 257, 18578, 11, 264, 383, 15427, 307, 2049, 264, 6194, 11403, 13, 759,
  291, 434, 257, 732, 954, 18578, 293, 291, 434, 445, 51040], "temperature": 0.0,
  "avg_logprob": -0.13048405017492906, "compression_ratio": 1.808695652173913, "no_speech_prob":
  0.011804792098701}, {"id": 42, "seek": 36344, "start": 376.96, "end": 381.52, "text":
  " getting off the ground, you probably have the CTO title if you''re the technical
  one in that startup,", "tokens": [51040, 1242, 766, 264, 2727, 11, 291, 1391, 362,
  264, 383, 15427, 4876, 498, 291, 434, 264, 6191, 472, 294, 300, 18578, 11, 51268],
  "temperature": 0.0, "avg_logprob": -0.13048405017492906, "compression_ratio": 1.808695652173913,
  "no_speech_prob": 0.011804792098701}, {"id": 43, "seek": 36344, "start": 381.52,
  "end": 383.92, "text": " and you''re probably writing all the code, right?", "tokens":
  [51268, 293, 291, 434, 1391, 3579, 439, 264, 3089, 11, 558, 30, 51388], "temperature":
  0.0, "avg_logprob": -0.13048405017492906, "compression_ratio": 1.808695652173913,
  "no_speech_prob": 0.011804792098701}, {"id": 44, "seek": 36344, "start": 385.52,
  "end": 390.4, "text": " In other places, you''re running your engineering team,
  and you may not be writing as much code,", "tokens": [51468, 682, 661, 3190, 11,
  291, 434, 2614, 428, 7043, 1469, 11, 293, 291, 815, 406, 312, 3579, 382, 709, 3089,
  11, 51712], "temperature": 0.0, "avg_logprob": -0.13048405017492906, "compression_ratio":
  1.808695652173913, "no_speech_prob": 0.011804792098701}, {"id": 45, "seek": 39040,
  "start": 390.4, "end": 396.15999999999997, "text": " but you''re responsible for
  the team. I guess over my years, I''ve worn all of those hats.", "tokens": [50364,
  457, 291, 434, 6250, 337, 264, 1469, 13, 286, 2041, 670, 452, 924, 11, 286, 600,
  15254, 439, 295, 729, 20549, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14257277382744682,
  "compression_ratio": 1.682170542635659, "no_speech_prob": 0.003562831087037921},
  {"id": 46, "seek": 39040, "start": 397.03999999999996, "end": 403.76, "text": "
  I''ve been out doing conferences and evangelizing. I''ve done a lot of sales work,
  especially later on at", "tokens": [50696, 286, 600, 668, 484, 884, 22032, 293,
  24546, 3319, 13, 286, 600, 1096, 257, 688, 295, 5763, 589, 11, 2318, 1780, 322,
  412, 51032], "temperature": 0.0, "avg_logprob": -0.14257277382744682, "compression_ratio":
  1.682170542635659, "no_speech_prob": 0.003562831087037921}, {"id": 47, "seek": 39040,
  "start": 403.76, "end": 411.03999999999996, "text": " Lucidworks, I did a lot of
  sales work as the company evolved and grew. When I was at Wikimedia, it was all",
  "tokens": [51032, 9593, 327, 18357, 11, 286, 630, 257, 688, 295, 5763, 589, 382,
  264, 2237, 14178, 293, 6109, 13, 1133, 286, 390, 412, 23377, 332, 14212, 11, 309,
  390, 439, 51396], "temperature": 0.0, "avg_logprob": -0.14257277382744682, "compression_ratio":
  1.682170542635659, "no_speech_prob": 0.003562831087037921}, {"id": 48, "seek": 39040,
  "start": 411.03999999999996, "end": 419.2, "text": " pretty much internal running
  the technology team, making, you know, helping making technology decisions, all
  of those kinds of things.", "tokens": [51396, 1238, 709, 6920, 2614, 264, 2899,
  1469, 11, 1455, 11, 291, 458, 11, 4315, 1455, 2899, 5327, 11, 439, 295, 729, 3685,
  295, 721, 13, 51804], "temperature": 0.0, "avg_logprob": -0.14257277382744682, "compression_ratio":
  1.682170542635659, "no_speech_prob": 0.003562831087037921}, {"id": 49, "seek": 42040,
  "start": 420.88, "end": 426.4, "text": " So I wouldn''t necessarily say it''s changed
  much. I still do write some code, but not as much as", "tokens": [50388, 407, 286,
  2759, 380, 4725, 584, 309, 311, 3105, 709, 13, 286, 920, 360, 2464, 512, 3089, 11,
  457, 406, 382, 709, 382, 50664], "temperature": 0.0, "avg_logprob": -0.15673101562814615,
  "compression_ratio": 1.532, "no_speech_prob": 0.016913631930947304}, {"id": 50,
  "seek": 42040, "start": 427.2, "end": 434.15999999999997, "text": " as I used to,
  I guess, when I was a full-time engineer. But yeah, it still roughly falls into
  those", "tokens": [50704, 382, 286, 1143, 281, 11, 286, 2041, 11, 562, 286, 390,
  257, 1577, 12, 3766, 11403, 13, 583, 1338, 11, 309, 920, 9810, 8804, 666, 729, 51052],
  "temperature": 0.0, "avg_logprob": -0.15673101562814615, "compression_ratio": 1.532,
  "no_speech_prob": 0.016913631930947304}, {"id": 51, "seek": 42040, "start": 434.15999999999997,
  "end": 443.03999999999996, "text": " categories. Yeah, and I mean, like having been
  a student on your course, I''ve really enjoyed", "tokens": [51052, 10479, 13, 865,
  11, 293, 286, 914, 11, 411, 1419, 668, 257, 3107, 322, 428, 1164, 11, 286, 600,
  534, 4626, 51496], "temperature": 0.0, "avg_logprob": -0.15673101562814615, "compression_ratio":
  1.532, "no_speech_prob": 0.016913631930947304}, {"id": 52, "seek": 42040, "start":
  443.03999999999996, "end": 448.88, "text": " so much code that you''ve written to
  support this infrastructure of building the search engine.", "tokens": [51496, 370,
  709, 3089, 300, 291, 600, 3720, 281, 1406, 341, 6896, 295, 2390, 264, 3164, 2848,
  13, 51788], "temperature": 0.0, "avg_logprob": -0.15673101562814615, "compression_ratio":
  1.532, "no_speech_prob": 0.016913631930947304}, {"id": 53, "seek": 44888, "start":
  448.88, "end": 454.4, "text": " And I mean, you are still highly technical person,
  so I wouldn''t discount that. And I mean, this is", "tokens": [50364, 400, 286,
  914, 11, 291, 366, 920, 5405, 6191, 954, 11, 370, 286, 2759, 380, 11635, 300, 13,
  400, 286, 914, 11, 341, 307, 50640], "temperature": 0.0, "avg_logprob": -0.1724810269799563,
  "compression_ratio": 1.5853658536585367, "no_speech_prob": 0.004058916121721268},
  {"id": 54, "seek": 44888, "start": 454.4, "end": 460.15999999999997, "text": " something
  that is dear to my heart as well for me being an engineer, to talk to like-minded
  person.", "tokens": [50640, 746, 300, 307, 6875, 281, 452, 1917, 382, 731, 337,
  385, 885, 364, 11403, 11, 281, 751, 281, 411, 12, 23310, 954, 13, 50928], "temperature":
  0.0, "avg_logprob": -0.1724810269799563, "compression_ratio": 1.5853658536585367,
  "no_speech_prob": 0.004058916121721268}, {"id": 55, "seek": 44888, "start": 461.76,
  "end": 468.71999999999997, "text": " And in this segment year, in the same conference,
  2017, you gave an excellent talk title,", "tokens": [51008, 400, 294, 341, 9469,
  1064, 11, 294, 264, 912, 7586, 11, 6591, 11, 291, 2729, 364, 7103, 751, 4876, 11,
  51356], "temperature": 0.0, "avg_logprob": -0.1724810269799563, "compression_ratio":
  1.5853658536585367, "no_speech_prob": 0.004058916121721268}, {"id": 56, "seek":
  44888, "start": 468.71999999999997, "end": 475.52, "text": " BM25, is so yesterday,
  modern techniques for better search. And what''s funny, and I''m going to share",
  "tokens": [51356, 15901, 6074, 11, 307, 370, 5186, 11, 4363, 7512, 337, 1101, 3164,
  13, 400, 437, 311, 4074, 11, 293, 286, 478, 516, 281, 2073, 51696], "temperature":
  0.0, "avg_logprob": -0.1724810269799563, "compression_ratio": 1.5853658536585367,
  "no_speech_prob": 0.004058916121721268}, {"id": 57, "seek": 47552, "start": 475.59999999999997,
  "end": 480.79999999999995, "text": " the link as well. But what''s funny is that
  I don''t know if you noticed it yourself, but you again have", "tokens": [50368,
  264, 2113, 382, 731, 13, 583, 437, 311, 4074, 307, 300, 286, 500, 380, 458, 498,
  291, 5694, 309, 1803, 11, 457, 291, 797, 362, 50628], "temperature": 0.0, "avg_logprob":
  -0.13290157318115234, "compression_ratio": 1.6219512195121952, "no_speech_prob":
  0.01171757560223341}, {"id": 58, "seek": 47552, "start": 480.79999999999995, "end":
  486.4, "text": " three C''s in there. I wonder if you did it on purpose. What you
  have there as building blocks of", "tokens": [50628, 1045, 383, 311, 294, 456, 13,
  286, 2441, 498, 291, 630, 309, 322, 4334, 13, 708, 291, 362, 456, 382, 2390, 8474,
  295, 50908], "temperature": 0.0, "avg_logprob": -0.13290157318115234, "compression_ratio":
  1.6219512195121952, "no_speech_prob": 0.01171757560223341}, {"id": 59, "seek": 47552,
  "start": 486.4, "end": 492.79999999999995, "text": " this kind of journey of building
  a search engine. So the first one is content. And you piggyback on", "tokens": [50908,
  341, 733, 295, 4671, 295, 2390, 257, 3164, 2848, 13, 407, 264, 700, 472, 307, 2701,
  13, 400, 291, 39349, 3207, 322, 51228], "temperature": 0.0, "avg_logprob": -0.13290157318115234,
  "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.01171757560223341},
  {"id": 60, "seek": 47552, "start": 492.79999999999995, "end": 499.2, "text": " solar
  capabilities, but in general, it could be any search engine out there with rules
  for content,", "tokens": [51228, 7936, 10862, 11, 457, 294, 2674, 11, 309, 727,
  312, 604, 3164, 2848, 484, 456, 365, 4474, 337, 2701, 11, 51548], "temperature":
  0.0, "avg_logprob": -0.13290157318115234, "compression_ratio": 1.6219512195121952,
  "no_speech_prob": 0.01171757560223341}, {"id": 61, "seek": 49920, "start": 499.2,
  "end": 504.08, "text": " like with boosting, manual boosting, you know, lending
  pages, and so on. The second C", "tokens": [50364, 411, 365, 43117, 11, 9688, 43117,
  11, 291, 458, 11, 29823, 7183, 11, 293, 370, 322, 13, 440, 1150, 383, 50608], "temperature":
  0.0, "avg_logprob": -0.14259627713995465, "compression_ratio": 1.703971119133574,
  "no_speech_prob": 0.005128503777086735}, {"id": 62, "seek": 49920, "start": 505.03999999999996,
  "end": 510.64, "text": " is collaboration. So that''s like the way you put it, it''s
  collective intelligence to predict", "tokens": [50656, 307, 9363, 13, 407, 300,
  311, 411, 264, 636, 291, 829, 309, 11, 309, 311, 12590, 7599, 281, 6069, 50936],
  "temperature": 0.0, "avg_logprob": -0.14259627713995465, "compression_ratio": 1.703971119133574,
  "no_speech_prob": 0.005128503777086735}, {"id": 63, "seek": 49920, "start": 510.64,
  "end": 515.36, "text": " user behavior based on like historical aggregated data.
  And this is where I think recommenders", "tokens": [50936, 4195, 5223, 2361, 322,
  411, 8584, 16743, 770, 1412, 13, 400, 341, 307, 689, 286, 519, 2748, 433, 51172],
  "temperature": 0.0, "avg_logprob": -0.14259627713995465, "compression_ratio": 1.703971119133574,
  "no_speech_prob": 0.005128503777086735}, {"id": 64, "seek": 49920, "start": 515.36,
  "end": 522.64, "text": " come in, popularity, signals, and so on. And last but not
  least, you have context, which is when you", "tokens": [51172, 808, 294, 11, 19301,
  11, 12354, 11, 293, 370, 322, 13, 400, 1036, 457, 406, 1935, 11, 291, 362, 4319,
  11, 597, 307, 562, 291, 51536], "temperature": 0.0, "avg_logprob": -0.14259627713995465,
  "compression_ratio": 1.703971119133574, "no_speech_prob": 0.005128503777086735},
  {"id": 65, "seek": 49920, "start": 522.64, "end": 527.52, "text": " ask questions,
  who are you, where are you, you know, what I have you done previously. And this
  is", "tokens": [51536, 1029, 1651, 11, 567, 366, 291, 11, 689, 366, 291, 11, 291,
  458, 11, 437, 286, 362, 291, 1096, 8046, 13, 400, 341, 307, 51780], "temperature":
  0.0, "avg_logprob": -0.14259627713995465, "compression_ratio": 1.703971119133574,
  "no_speech_prob": 0.005128503777086735}, {"id": 66, "seek": 52752, "start": 527.76,
  "end": 533.12, "text": " when you start doing market and user segmentation and venture
  into personalization and so on,", "tokens": [50376, 562, 291, 722, 884, 2142, 293,
  4195, 9469, 399, 293, 18474, 666, 2973, 2144, 293, 370, 322, 11, 50644], "temperature":
  0.0, "avg_logprob": -0.13416558343010979, "compression_ratio": 1.6992753623188406,
  "no_speech_prob": 0.0010685856686905026}, {"id": 67, "seek": 52752, "start": 533.84,
  "end": 539.04, "text": " would you say that you view the search engine journey and
  development the same way today, or", "tokens": [50680, 576, 291, 584, 300, 291,
  1910, 264, 3164, 2848, 4671, 293, 3250, 264, 912, 636, 965, 11, 420, 50940], "temperature":
  0.0, "avg_logprob": -0.13416558343010979, "compression_ratio": 1.6992753623188406,
  "no_speech_prob": 0.0010685856686905026}, {"id": 68, "seek": 52752, "start": 539.04,
  "end": 544.4, "text": " have you have you changed your perspective? I really need
  to check and get a little more creative.", "tokens": [50940, 362, 291, 362, 291,
  3105, 428, 4585, 30, 286, 534, 643, 281, 1520, 293, 483, 257, 707, 544, 5880, 13,
  51208], "temperature": 0.0, "avg_logprob": -0.13416558343010979, "compression_ratio":
  1.6992753623188406, "no_speech_prob": 0.0010685856686905026}, {"id": 69, "seek":
  52752, "start": 544.4, "end": 551.4399999999999, "text": " I think I''m using the
  letter C there too many times in a row, but I mean, I think a lot of", "tokens":
  [51208, 286, 519, 286, 478, 1228, 264, 5063, 383, 456, 886, 867, 1413, 294, 257,
  5386, 11, 457, 286, 914, 11, 286, 519, 257, 688, 295, 51560], "temperature": 0.0,
  "avg_logprob": -0.13416558343010979, "compression_ratio": 1.6992753623188406, "no_speech_prob":
  0.0010685856686905026}, {"id": 70, "seek": 52752, "start": 551.4399999999999, "end":
  556.48, "text": " that still stands pretty true. Regardless of the engine you''re
  using or whether you''re using", "tokens": [51560, 300, 920, 7382, 1238, 2074, 13,
  25148, 295, 264, 2848, 291, 434, 1228, 420, 1968, 291, 434, 1228, 51812], "temperature":
  0.0, "avg_logprob": -0.13416558343010979, "compression_ratio": 1.6992753623188406,
  "no_speech_prob": 0.0010685856686905026}, {"id": 71, "seek": 55648, "start": 556.48,
  "end": 561.84, "text": " deep learning techniques or not, like, you know, at the
  end of the day, you''re trying to", "tokens": [50364, 2452, 2539, 7512, 420, 406,
  11, 411, 11, 291, 458, 11, 412, 264, 917, 295, 264, 786, 11, 291, 434, 1382, 281,
  50632], "temperature": 0.0, "avg_logprob": -0.1766209602355957, "compression_ratio":
  1.6196581196581197, "no_speech_prob": 0.0007912431028671563}, {"id": 72, "seek":
  55648, "start": 563.12, "end": 569.44, "text": " match users to information that
  will help them make better decisions or be more informed, right.", "tokens": [50696,
  2995, 5022, 281, 1589, 300, 486, 854, 552, 652, 1101, 5327, 420, 312, 544, 11740,
  11, 558, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1766209602355957, "compression_ratio":
  1.6196581196581197, "no_speech_prob": 0.0007912431028671563}, {"id": 73, "seek":
  55648, "start": 570.16, "end": 576.08, "text": " And you know, these days, I would
  probably add in one more. I''m trying to think of how I could be", "tokens": [51048,
  400, 291, 458, 11, 613, 1708, 11, 286, 576, 1391, 909, 294, 472, 544, 13, 286, 478,
  1382, 281, 519, 295, 577, 286, 727, 312, 51344], "temperature": 0.0, "avg_logprob":
  -0.1766209602355957, "compression_ratio": 1.6196581196581197, "no_speech_prob":
  0.0007912431028671563}, {"id": 74, "seek": 55648, "start": 576.08, "end": 581.9200000000001,
  "text": " witty and make it into another C, but you know, in working with Daniel
  Tongueleg on this class,", "tokens": [51344, 261, 10016, 293, 652, 309, 666, 1071,
  383, 11, 457, 291, 458, 11, 294, 1364, 365, 8033, 26946, 622, 306, 70, 322, 341,
  1508, 11, 51636], "temperature": 0.0, "avg_logprob": -0.1766209602355957, "compression_ratio":
  1.6196581196581197, "no_speech_prob": 0.0007912431028671563}, {"id": 75, "seek":
  58192, "start": 582.0, "end": 587.36, "text": " one of the things that is just absolutely
  wowed me is the query understanding aspects of it.", "tokens": [50368, 472, 295,
  264, 721, 300, 307, 445, 3122, 6076, 292, 385, 307, 264, 14581, 3701, 7270, 295,
  309, 13, 50636], "temperature": 0.0, "avg_logprob": -0.11495535233441521, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.0020068620797246695}, {"id": 76, "seek":
  58192, "start": 587.36, "end": 594.16, "text": " And so maybe you could put that
  into the context category if you wanted. But, you know,", "tokens": [50636, 400,
  370, 1310, 291, 727, 829, 300, 666, 264, 4319, 7719, 498, 291, 1415, 13, 583, 11,
  291, 458, 11, 50976], "temperature": 0.0, "avg_logprob": -0.11495535233441521, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.0020068620797246695}, {"id": 77, "seek":
  58192, "start": 594.16, "end": 599.28, "text": " realistically speaking that that
  work you can do, especially in large scale environments where you", "tokens": [50976,
  40734, 4124, 300, 300, 589, 291, 393, 360, 11, 2318, 294, 2416, 4373, 12388, 689,
  291, 51232], "temperature": 0.0, "avg_logprob": -0.11495535233441521, "compression_ratio":
  1.6666666666666667, "no_speech_prob": 0.0020068620797246695}, {"id": 78, "seek":
  58192, "start": 599.28, "end": 606.48, "text": " have a lot of queries, to really
  understand what users are asking or intending to ask when they", "tokens": [51232,
  362, 257, 688, 295, 24109, 11, 281, 534, 1223, 437, 5022, 366, 3365, 420, 560, 2029,
  281, 1029, 562, 436, 51592], "temperature": 0.0, "avg_logprob": -0.11495535233441521,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0020068620797246695},
  {"id": 79, "seek": 60648, "start": 606.5600000000001, "end": 612.8000000000001,
  "text": " put in a query. So I would probably throw that in there if I didn''t include
  that back then. And", "tokens": [50368, 829, 294, 257, 14581, 13, 407, 286, 576,
  1391, 3507, 300, 294, 456, 498, 286, 994, 380, 4090, 300, 646, 550, 13, 400, 50680],
  "temperature": 0.0, "avg_logprob": -0.11644958314441499, "compression_ratio": 1.7383512544802868,
  "no_speech_prob": 0.0006392719224095345}, {"id": 80, "seek": 60648, "start": 612.8000000000001,
  "end": 618.4, "text": " so like I said, maybe that''s part of your content or your
  or your context is the actual query,", "tokens": [50680, 370, 411, 286, 848, 11,
  1310, 300, 311, 644, 295, 428, 2701, 420, 428, 420, 428, 4319, 307, 264, 3539, 14581,
  11, 50960], "temperature": 0.0, "avg_logprob": -0.11644958314441499, "compression_ratio":
  1.7383512544802868, "no_speech_prob": 0.0006392719224095345}, {"id": 81, "seek":
  60648, "start": 618.4, "end": 625.6, "text": " a user or this, the set of queries
  that a user is asking. You know, but I still think a lot of that", "tokens": [50960,
  257, 4195, 420, 341, 11, 264, 992, 295, 24109, 300, 257, 4195, 307, 3365, 13, 509,
  458, 11, 457, 286, 920, 519, 257, 688, 295, 300, 51320], "temperature": 0.0, "avg_logprob":
  -0.11644958314441499, "compression_ratio": 1.7383512544802868, "no_speech_prob":
  0.0006392719224095345}, {"id": 82, "seek": 60648, "start": 625.6, "end": 631.52,
  "text": " stands at the conceptual level, right, is you have to have some, you know,
  if you think about it,", "tokens": [51320, 7382, 412, 264, 24106, 1496, 11, 558,
  11, 307, 291, 362, 281, 362, 512, 11, 291, 458, 11, 498, 291, 519, 466, 309, 11,
  51616], "temperature": 0.0, "avg_logprob": -0.11644958314441499, "compression_ratio":
  1.7383512544802868, "no_speech_prob": 0.0006392719224095345}, {"id": 83, "seek":
  60648, "start": 631.52, "end": 636.0, "text": " this is the vector podcast, right.
  All of this stuff we''re building vectors and then essentially", "tokens": [51616,
  341, 307, 264, 8062, 7367, 11, 558, 13, 1057, 295, 341, 1507, 321, 434, 2390, 18875,
  293, 550, 4476, 51840], "temperature": 0.0, "avg_logprob": -0.11644958314441499,
  "compression_ratio": 1.7383512544802868, "no_speech_prob": 0.0006392719224095345},
  {"id": 84, "seek": 63600, "start": 636.0, "end": 642.08, "text": " calculating this
  fancy version of a cosine similarity between them. And at the end of the day,",
  "tokens": [50364, 28258, 341, 10247, 3037, 295, 257, 23565, 32194, 1296, 552, 13,
  400, 412, 264, 917, 295, 264, 786, 11, 50668], "temperature": 0.0, "avg_logprob":
  -0.11144391606363017, "compression_ratio": 1.7534246575342465, "no_speech_prob":
  0.0005059881368651986}, {"id": 85, "seek": 63600, "start": 642.08, "end": 649.6,
  "text": " all of these techniques we''re doing are effectively how can we shape
  those vectors so that things", "tokens": [50668, 439, 295, 613, 7512, 321, 434,
  884, 366, 8659, 577, 393, 321, 3909, 729, 18875, 370, 300, 721, 51044], "temperature":
  0.0, "avg_logprob": -0.11144391606363017, "compression_ratio": 1.7534246575342465,
  "no_speech_prob": 0.0005059881368651986}, {"id": 86, "seek": 63600, "start": 649.6,
  "end": 656.56, "text": " that are meant to be closer together, show up closer together
  and things that are not as related", "tokens": [51044, 300, 366, 4140, 281, 312,
  4966, 1214, 11, 855, 493, 4966, 1214, 293, 721, 300, 366, 406, 382, 4077, 51392],
  "temperature": 0.0, "avg_logprob": -0.11144391606363017, "compression_ratio": 1.7534246575342465,
  "no_speech_prob": 0.0005059881368651986}, {"id": 87, "seek": 63600, "start": 657.36,
  "end": 662.48, "text": " the cosine is further apart, right. Like at the end of
  the day, like that math doesn''t change,", "tokens": [51432, 264, 23565, 307, 3052,
  4936, 11, 558, 13, 1743, 412, 264, 917, 295, 264, 786, 11, 411, 300, 5221, 1177,
  380, 1319, 11, 51688], "temperature": 0.0, "avg_logprob": -0.11144391606363017,
  "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0005059881368651986},
  {"id": 88, "seek": 66248, "start": 662.48, "end": 667.36, "text": " yet all these
  techniques, whether it''s deep learning, et cetera, are all about creating those",
  "tokens": [50364, 1939, 439, 613, 7512, 11, 1968, 309, 311, 2452, 2539, 11, 1030,
  11458, 11, 366, 439, 466, 4084, 729, 50608], "temperature": 0.0, "avg_logprob":
  -0.08283464532149465, "compression_ratio": 1.9631147540983607, "no_speech_prob":
  0.001296419301070273}, {"id": 89, "seek": 66248, "start": 667.36, "end": 673.36,
  "text": " vectors and doing that calculation, right. And so by understanding your
  content, you''re shifting", "tokens": [50608, 18875, 293, 884, 300, 17108, 11, 558,
  13, 400, 370, 538, 3701, 428, 2701, 11, 291, 434, 17573, 50908], "temperature":
  0.0, "avg_logprob": -0.08283464532149465, "compression_ratio": 1.9631147540983607,
  "no_speech_prob": 0.001296419301070273}, {"id": 90, "seek": 66248, "start": 673.36,
  "end": 678.24, "text": " those vectors, you''re transforming them in the space,
  you''re adding synonyms, you''re adding embeddings,", "tokens": [50908, 729, 18875,
  11, 291, 434, 27210, 552, 294, 264, 1901, 11, 291, 434, 5127, 5451, 2526, 2592,
  11, 291, 434, 5127, 12240, 29432, 11, 51152], "temperature": 0.0, "avg_logprob":
  -0.08283464532149465, "compression_ratio": 1.9631147540983607, "no_speech_prob":
  0.001296419301070273}, {"id": 91, "seek": 66248, "start": 678.24, "end": 683.44,
  "text": " all of those kinds of things, you''re adding proper nouns, you''re you''re
  doing noun phrases,", "tokens": [51152, 439, 295, 729, 3685, 295, 721, 11, 291,
  434, 5127, 2296, 48184, 11, 291, 434, 291, 434, 884, 23307, 20312, 11, 51412], "temperature":
  0.0, "avg_logprob": -0.08283464532149465, "compression_ratio": 1.9631147540983607,
  "no_speech_prob": 0.001296419301070273}, {"id": 92, "seek": 66248, "start": 683.44,
  "end": 688.4, "text": " et cetera. By understanding your context, you''re able to
  ask better queries, right, which is", "tokens": [51412, 1030, 11458, 13, 3146, 3701,
  428, 4319, 11, 291, 434, 1075, 281, 1029, 1101, 24109, 11, 558, 11, 597, 307, 51660],
  "temperature": 0.0, "avg_logprob": -0.08283464532149465, "compression_ratio": 1.9631147540983607,
  "no_speech_prob": 0.001296419301070273}, {"id": 93, "seek": 68840, "start": 688.4,
  "end": 695.92, "text": " shifting the query vector, right. And by using popularity,
  et cetera, you''re also then shifting", "tokens": [50364, 17573, 264, 14581, 8062,
  11, 558, 13, 400, 538, 1228, 19301, 11, 1030, 11458, 11, 291, 434, 611, 550, 17573,
  50740], "temperature": 0.0, "avg_logprob": -0.10276345339688388, "compression_ratio":
  1.7756653992395437, "no_speech_prob": 0.002730551641434431}, {"id": 94, "seek":
  68840, "start": 695.92, "end": 700.9599999999999, "text": " those vectors by essentially
  adding more weight to things that are more popular, right.", "tokens": [50740, 729,
  18875, 538, 4476, 5127, 544, 3364, 281, 721, 300, 366, 544, 3743, 11, 558, 13, 50992],
  "temperature": 0.0, "avg_logprob": -0.10276345339688388, "compression_ratio": 1.7756653992395437,
  "no_speech_prob": 0.002730551641434431}, {"id": 95, "seek": 68840, "start": 702.0,
  "end": 705.76, "text": " You know, so at the end of the day, like, yeah, I would
  say I''d still stand by that with the", "tokens": [51044, 509, 458, 11, 370, 412,
  264, 917, 295, 264, 786, 11, 411, 11, 1338, 11, 286, 576, 584, 286, 1116, 920, 1463,
  538, 300, 365, 264, 51232], "temperature": 0.0, "avg_logprob": -0.10276345339688388,
  "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.002730551641434431},
  {"id": 96, "seek": 68840, "start": 705.76, "end": 712.56, "text": " caveat is really
  bringing forward the query understanding aspect of it. Yeah, I think query", "tokens":
  [51232, 43012, 307, 534, 5062, 2128, 264, 14581, 3701, 4171, 295, 309, 13, 865,
  11, 286, 519, 14581, 51572], "temperature": 0.0, "avg_logprob": -0.10276345339688388,
  "compression_ratio": 1.7756653992395437, "no_speech_prob": 0.002730551641434431},
  {"id": 97, "seek": 68840, "start": 712.56, "end": 717.52, "text": " understanding,
  you put it brilliantly, it''s like really an exciting space and we actually recorded",
  "tokens": [51572, 3701, 11, 291, 829, 309, 8695, 42580, 11, 309, 311, 411, 534,
  364, 4670, 1901, 293, 321, 767, 8287, 51820], "temperature": 0.0, "avg_logprob":
  -0.10276345339688388, "compression_ratio": 1.7756653992395437, "no_speech_prob":
  0.002730551641434431}, {"id": 98, "seek": 71752, "start": 717.68, "end": 723.92,
  "text": " a podcast as well with Daniel Tankilank, where he explained a lot of it,
  he also blogged a lot about it.", "tokens": [50372, 257, 7367, 382, 731, 365, 8033,
  28746, 388, 657, 11, 689, 415, 8825, 257, 688, 295, 309, 11, 415, 611, 6968, 3004,
  257, 688, 466, 309, 13, 50684], "temperature": 0.0, "avg_logprob": -0.22521983107475385,
  "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.002179196337237954},
  {"id": 99, "seek": 71752, "start": 723.92, "end": 733.36, "text": " So go check
  it out. And like in that same presentation, like when you demoed the capabilities
  of", "tokens": [50684, 407, 352, 1520, 309, 484, 13, 400, 411, 294, 300, 912, 5860,
  11, 411, 562, 291, 10723, 292, 264, 10862, 295, 51156], "temperature": 0.0, "avg_logprob":
  -0.22521983107475385, "compression_ratio": 1.5128205128205128, "no_speech_prob":
  0.002179196337237954}, {"id": 100, "seek": 71752, "start": 733.36, "end": 739.68,
  "text": " Lucidworks platform, where you played a lot with different like ranking
  strategies, basically", "tokens": [51156, 9593, 327, 18357, 3663, 11, 689, 291,
  3737, 257, 688, 365, 819, 411, 17833, 9029, 11, 1936, 51472], "temperature": 0.0,
  "avg_logprob": -0.22521983107475385, "compression_ratio": 1.5128205128205128, "no_speech_prob":
  0.002179196337237954}, {"id": 101, "seek": 73968, "start": 740.16, "end": 747.04,
  "text": " like you pre-trained some of them and you you were able to switch live,
  I felt like you you are", "tokens": [50388, 411, 291, 659, 12, 17227, 2001, 512,
  295, 552, 293, 291, 291, 645, 1075, 281, 3679, 1621, 11, 286, 2762, 411, 291, 291,
  366, 50732], "temperature": 0.0, "avg_logprob": -0.17073892809681057, "compression_ratio":
  1.6317991631799162, "no_speech_prob": 0.0033825526479631662}, {"id": 102, "seek":
  73968, "start": 747.04, "end": 753.76, "text": " a tinkerer as well. You enjoy really
  going deep down into the what search engine can do and what", "tokens": [50732,
  257, 256, 40467, 260, 382, 731, 13, 509, 2103, 534, 516, 2452, 760, 666, 264, 437,
  3164, 2848, 393, 360, 293, 437, 51068], "temperature": 0.0, "avg_logprob": -0.17073892809681057,
  "compression_ratio": 1.6317991631799162, "no_speech_prob": 0.0033825526479631662},
  {"id": 103, "seek": 73968, "start": 753.76, "end": 760.8, "text": " you you can
  extract from the data. And my question is, where do you see the balance between
  kind of like", "tokens": [51068, 291, 291, 393, 8947, 490, 264, 1412, 13, 400, 452,
  1168, 307, 11, 689, 360, 291, 536, 264, 4772, 1296, 733, 295, 411, 51420], "temperature":
  0.0, "avg_logprob": -0.17073892809681057, "compression_ratio": 1.6317991631799162,
  "no_speech_prob": 0.0033825526479631662}, {"id": 104, "seek": 73968, "start": 760.8,
  "end": 766.4799999999999, "text": " doing this in a more manual fashion, where you
  actually educate yourself, right, versus like", "tokens": [51420, 884, 341, 294,
  257, 544, 9688, 6700, 11, 689, 291, 767, 16092, 1803, 11, 558, 11, 5717, 411, 51704],
  "temperature": 0.0, "avg_logprob": -0.17073892809681057, "compression_ratio": 1.6317991631799162,
  "no_speech_prob": 0.0033825526479631662}, {"id": 105, "seek": 76648, "start": 766.48,
  "end": 771.84, "text": " throwing it to a machine learning model? Yeah, it''s a
  great question. I mean, I think", "tokens": [50364, 10238, 309, 281, 257, 3479,
  2539, 2316, 30, 865, 11, 309, 311, 257, 869, 1168, 13, 286, 914, 11, 286, 519, 50632],
  "temperature": 0.0, "avg_logprob": -0.19382127901402915, "compression_ratio": 1.721189591078067,
  "no_speech_prob": 0.005030888132750988}, {"id": 106, "seek": 76648, "start": 772.5600000000001,
  "end": 779.12, "text": " you know, obviously, and I see my former colleague and
  co-founder Eric Hatcher is on, I mean,", "tokens": [50668, 291, 458, 11, 2745, 11,
  293, 286, 536, 452, 5819, 13532, 293, 598, 12, 33348, 9336, 389, 852, 260, 307,
  322, 11, 286, 914, 11, 50996], "temperature": 0.0, "avg_logprob": -0.19382127901402915,
  "compression_ratio": 1.721189591078067, "no_speech_prob": 0.005030888132750988},
  {"id": 107, "seek": 76648, "start": 779.12, "end": 784.72, "text": " he used to
  always say it depends and I''d say it depends here of course as well, which is,
  you know,", "tokens": [50996, 415, 1143, 281, 1009, 584, 309, 5946, 293, 286, 1116,
  584, 309, 5946, 510, 295, 1164, 382, 731, 11, 597, 307, 11, 291, 458, 11, 51276],
  "temperature": 0.0, "avg_logprob": -0.19382127901402915, "compression_ratio": 1.721189591078067,
  "no_speech_prob": 0.005030888132750988}, {"id": 108, "seek": 76648, "start": 784.72,
  "end": 789.76, "text": " I mean, there''s there''s some situations where you just
  you don''t have enough data for machine learning,", "tokens": [51276, 286, 914,
  11, 456, 311, 456, 311, 512, 6851, 689, 291, 445, 291, 500, 380, 362, 1547, 1412,
  337, 3479, 2539, 11, 51528], "temperature": 0.0, "avg_logprob": -0.19382127901402915,
  "compression_ratio": 1.721189591078067, "no_speech_prob": 0.005030888132750988},
  {"id": 109, "seek": 76648, "start": 789.76, "end": 796.16, "text": " right? So by
  default, you are going to be manually tuning the situation, right?", "tokens": [51528,
  558, 30, 407, 538, 7576, 11, 291, 366, 516, 281, 312, 16945, 15164, 264, 2590, 11,
  558, 30, 51848], "temperature": 0.0, "avg_logprob": -0.19382127901402915, "compression_ratio":
  1.721189591078067, "no_speech_prob": 0.005030888132750988}, {"id": 110, "seek":
  79616, "start": 796.16, "end": 801.28, "text": " You you see that a lot in enterprise
  systems, especially smaller enterprise systems or in", "tokens": [50364, 509, 291,
  536, 300, 257, 688, 294, 14132, 3652, 11, 2318, 4356, 14132, 3652, 420, 294, 50620],
  "temperature": 0.0, "avg_logprob": -0.1034354541612708, "compression_ratio": 1.6228813559322033,
  "no_speech_prob": 0.0005666203214786947}, {"id": 111, "seek": 79616, "start": 801.92,
  "end": 808.64, "text": " niche applications where, you know, effectively search
  just needs to be good enough. Maybe you''re not", "tokens": [50652, 19956, 5821,
  689, 11, 291, 458, 11, 8659, 3164, 445, 2203, 281, 312, 665, 1547, 13, 2704, 291,
  434, 406, 50988], "temperature": 0.0, "avg_logprob": -0.1034354541612708, "compression_ratio":
  1.6228813559322033, "no_speech_prob": 0.0005666203214786947}, {"id": 112, "seek":
  79616, "start": 808.64, "end": 815.6, "text": " monetizing search. And so you don''t,
  you know, you just kind of need it to be reasonably good,", "tokens": [50988, 15556,
  3319, 3164, 13, 400, 370, 291, 500, 380, 11, 291, 458, 11, 291, 445, 733, 295, 643,
  309, 281, 312, 23551, 665, 11, 51336], "temperature": 0.0, "avg_logprob": -0.1034354541612708,
  "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0005666203214786947},
  {"id": 113, "seek": 79616, "start": 815.6, "end": 822.3199999999999, "text": " right?
  It''s a feature in a much broader set of features that users are going to engage
  with. And", "tokens": [51336, 558, 30, 467, 311, 257, 4111, 294, 257, 709, 13227,
  992, 295, 4122, 300, 5022, 366, 516, 281, 4683, 365, 13, 400, 51672], "temperature":
  0.0, "avg_logprob": -0.1034354541612708, "compression_ratio": 1.6228813559322033,
  "no_speech_prob": 0.0005666203214786947}, {"id": 114, "seek": 82232, "start": 822.32,
  "end": 827.36, "text": " so, you know, where and how you would use machine learning
  in those situations, you know, you may", "tokens": [50364, 370, 11, 291, 458, 11,
  689, 293, 577, 291, 576, 764, 3479, 2539, 294, 729, 6851, 11, 291, 458, 11, 291,
  815, 50616], "temperature": 0.0, "avg_logprob": -0.14121672571921834, "compression_ratio":
  1.8142857142857143, "no_speech_prob": 0.00045651980326510966}, {"id": 115, "seek":
  82232, "start": 827.36, "end": 834.5600000000001, "text": " or may not. In the situations
  where you have lots and lots of data, lots of users, you''re probably", "tokens":
  [50616, 420, 815, 406, 13, 682, 264, 6851, 689, 291, 362, 3195, 293, 3195, 295,
  1412, 11, 3195, 295, 5022, 11, 291, 434, 1391, 50976], "temperature": 0.0, "avg_logprob":
  -0.14121672571921834, "compression_ratio": 1.8142857142857143, "no_speech_prob":
  0.00045651980326510966}, {"id": 116, "seek": 82232, "start": 834.5600000000001,
  "end": 840.8000000000001, "text": " monetizing search, whether that''s via e-commerce
  or or web search or ads or whatever, like, you know,", "tokens": [50976, 15556,
  3319, 3164, 11, 1968, 300, 311, 5766, 308, 12, 26926, 420, 420, 3670, 3164, 420,
  10342, 420, 2035, 11, 411, 11, 291, 458, 11, 51288], "temperature": 0.0, "avg_logprob":
  -0.14121672571921834, "compression_ratio": 1.8142857142857143, "no_speech_prob":
  0.00045651980326510966}, {"id": 117, "seek": 82232, "start": 840.8000000000001,
  "end": 846.08, "text": " I think machine learning makes a lot more sense there and
  and it''s a lot easier to", "tokens": [51288, 286, 519, 3479, 2539, 1669, 257, 688,
  544, 2020, 456, 293, 293, 309, 311, 257, 688, 3571, 281, 51552], "temperature":
  0.0, "avg_logprob": -0.14121672571921834, "compression_ratio": 1.8142857142857143,
  "no_speech_prob": 0.00045651980326510966}, {"id": 118, "seek": 84608, "start": 846.96,
  "end": 853.5200000000001, "text": " run these types of experiments that allow you
  to tinker not just with the hand-ranked models,", "tokens": [50408, 1190, 613, 3467,
  295, 12050, 300, 2089, 291, 281, 256, 40467, 406, 445, 365, 264, 1011, 12, 20479,
  292, 5245, 11, 50736], "temperature": 0.0, "avg_logprob": -0.10000874598821004,
  "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.002620015060529113},
  {"id": 119, "seek": 84608, "start": 853.5200000000001, "end": 859.6, "text": " which
  I think hand-ranking still has its place, right? Because they help you form intuition
  about", "tokens": [50736, 597, 286, 519, 1011, 12, 20479, 278, 920, 575, 1080, 1081,
  11, 558, 30, 1436, 436, 854, 291, 1254, 24002, 466, 51040], "temperature": 0.0,
  "avg_logprob": -0.10000874598821004, "compression_ratio": 1.6853448275862069, "no_speech_prob":
  0.002620015060529113}, {"id": 120, "seek": 84608, "start": 859.6, "end": 865.6800000000001,
  "text": " what is in your data, right? And that intuition is really important even
  in a machine learning world", "tokens": [51040, 437, 307, 294, 428, 1412, 11, 558,
  30, 400, 300, 24002, 307, 534, 1021, 754, 294, 257, 3479, 2539, 1002, 51344], "temperature":
  0.0, "avg_logprob": -0.10000874598821004, "compression_ratio": 1.6853448275862069,
  "no_speech_prob": 0.002620015060529113}, {"id": 121, "seek": 84608, "start": 865.6800000000001,
  "end": 870.48, "text": " because, you know, at the end of the day, even with machine
  learning, while you can still try out,", "tokens": [51344, 570, 11, 291, 458, 11,
  412, 264, 917, 295, 264, 786, 11, 754, 365, 3479, 2539, 11, 1339, 291, 393, 920,
  853, 484, 11, 51584], "temperature": 0.0, "avg_logprob": -0.10000874598821004, "compression_ratio":
  1.6853448275862069, "no_speech_prob": 0.002620015060529113}, {"id": 122, "seek":
  87048, "start": 870.48, "end": 877.2, "text": " you can try out a lot more features
  and approaches, you still have limited time, right? And so,", "tokens": [50364,
  291, 393, 853, 484, 257, 688, 544, 4122, 293, 11587, 11, 291, 920, 362, 5567, 565,
  11, 558, 30, 400, 370, 11, 50700], "temperature": 0.0, "avg_logprob": -0.12090349733159783,
  "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.0012513004476204515},
  {"id": 123, "seek": 87048, "start": 877.2, "end": 883.36, "text": " you still have
  to have some intuition about what''s going to work and I think there''s no substitute",
  "tokens": [50700, 291, 920, 362, 281, 362, 512, 24002, 466, 437, 311, 516, 281,
  589, 293, 286, 519, 456, 311, 572, 15802, 51008], "temperature": 0.0, "avg_logprob":
  -0.12090349733159783, "compression_ratio": 1.6916299559471366, "no_speech_prob":
  0.0012513004476204515}, {"id": 124, "seek": 87048, "start": 883.36, "end": 889.52,
  "text": " for that intuition helping guide you into what matters, like, so for instance,
  in a learning", "tokens": [51008, 337, 300, 24002, 4315, 5934, 291, 666, 437, 7001,
  11, 411, 11, 370, 337, 5197, 11, 294, 257, 2539, 51316], "temperature": 0.0, "avg_logprob":
  -0.12090349733159783, "compression_ratio": 1.6916299559471366, "no_speech_prob":
  0.0012513004476204515}, {"id": 125, "seek": 87048, "start": 889.52, "end": 896.16,
  "text": " to rank scenario where you''re actually learning a ranking model, you
  still are often building up", "tokens": [51316, 281, 6181, 9005, 689, 291, 434,
  767, 2539, 257, 17833, 2316, 11, 291, 920, 366, 2049, 2390, 493, 51648], "temperature":
  0.0, "avg_logprob": -0.12090349733159783, "compression_ratio": 1.6916299559471366,
  "no_speech_prob": 0.0012513004476204515}, {"id": 126, "seek": 89616, "start": 896.16,
  "end": 902.56, "text": " those systems using the features of your data. So you have
  to know what those features are and", "tokens": [50364, 729, 3652, 1228, 264, 4122,
  295, 428, 1412, 13, 407, 291, 362, 281, 458, 437, 729, 4122, 366, 293, 50684], "temperature":
  0.0, "avg_logprob": -0.14002721206001614, "compression_ratio": 1.5822784810126582,
  "no_speech_prob": 0.0009849191410467029}, {"id": 127, "seek": 89616, "start": 902.56,
  "end": 909.04, "text": " one of the nice things is like with tools like Lucine-based
  engines like OpenSearch or Solar or", "tokens": [50684, 472, 295, 264, 1481, 721,
  307, 411, 365, 3873, 411, 9593, 533, 12, 6032, 12982, 411, 7238, 10637, 1178, 420,
  22385, 420, 51008], "temperature": 0.0, "avg_logprob": -0.14002721206001614, "compression_ratio":
  1.5822784810126582, "no_speech_prob": 0.0009849191410467029}, {"id": 128, "seek":
  89616, "start": 909.04, "end": 914.3199999999999, "text": " Elastic, I''m sure Vespa
  has the same kind of thing, you can go and play around with those,", "tokens": [51008,
  2699, 2750, 11, 286, 478, 988, 691, 279, 4306, 575, 264, 912, 733, 295, 551, 11,
  291, 393, 352, 293, 862, 926, 365, 729, 11, 51272], "temperature": 0.0, "avg_logprob":
  -0.14002721206001614, "compression_ratio": 1.5822784810126582, "no_speech_prob":
  0.0009849191410467029}, {"id": 129, "seek": 89616, "start": 914.3199999999999, "end":
  921.1999999999999, "text": " you can create your own function queries that allow
  you to roughly try out different formulas", "tokens": [51272, 291, 393, 1884, 428,
  1065, 2445, 24109, 300, 2089, 291, 281, 9810, 853, 484, 819, 30546, 51616], "temperature":
  0.0, "avg_logprob": -0.14002721206001614, "compression_ratio": 1.5822784810126582,
  "no_speech_prob": 0.0009849191410467029}, {"id": 130, "seek": 92120, "start": 921.2,
  "end": 926.32, "text": " for ranking and then you can go and turn those things into
  machine learning models, right?", "tokens": [50364, 337, 17833, 293, 550, 291, 393,
  352, 293, 1261, 729, 721, 666, 3479, 2539, 5245, 11, 558, 30, 50620], "temperature":
  0.0, "avg_logprob": -0.1272864124991677, "compression_ratio": 1.6475770925110131,
  "no_speech_prob": 0.003484126413241029}, {"id": 131, "seek": 92120, "start": 926.32,
  "end": 931.12, "text": " That learn a much more effective function than what you
  could come up with, right? So,", "tokens": [50620, 663, 1466, 257, 709, 544, 4942,
  2445, 813, 437, 291, 727, 808, 493, 365, 11, 558, 30, 407, 11, 50860], "temperature":
  0.0, "avg_logprob": -0.1272864124991677, "compression_ratio": 1.6475770925110131,
  "no_speech_prob": 0.003484126413241029}, {"id": 132, "seek": 92120, "start": 932.08,
  "end": 937.2, "text": " I think even in this world of large data sets and machine
  learning, you''re still going to have", "tokens": [50908, 286, 519, 754, 294, 341,
  1002, 295, 2416, 1412, 6352, 293, 3479, 2539, 11, 291, 434, 920, 516, 281, 362,
  51164], "temperature": 0.0, "avg_logprob": -0.1272864124991677, "compression_ratio":
  1.6475770925110131, "no_speech_prob": 0.003484126413241029}, {"id": 133, "seek":
  92120, "start": 938.1600000000001, "end": 946.5600000000001, "text": " to build
  intuition, right? Yeah, absolutely. And like in your own experience and in the experience
  of", "tokens": [51212, 281, 1322, 24002, 11, 558, 30, 865, 11, 3122, 13, 400, 411,
  294, 428, 1065, 1752, 293, 294, 264, 1752, 295, 51632], "temperature": 0.0, "avg_logprob":
  -0.1272864124991677, "compression_ratio": 1.6475770925110131, "no_speech_prob":
  0.003484126413241029}, {"id": 134, "seek": 94656, "start": 946.7199999999999, "end":
  954.4799999999999, "text": " the teams that you supported, how do you nurture this
  intuition? Like, do you read books? Do you constantly", "tokens": [50372, 264, 5491,
  300, 291, 8104, 11, 577, 360, 291, 41451, 341, 24002, 30, 1743, 11, 360, 291, 1401,
  3642, 30, 1144, 291, 6460, 50760], "temperature": 0.0, "avg_logprob": -0.21449580945466695,
  "compression_ratio": 1.6055776892430278, "no_speech_prob": 0.004052360542118549},
  {"id": 135, "seek": 94656, "start": 954.4799999999999, "end": 960.3199999999999,
  "text": " experiment? And also like when it comes in, you know, to understanding
  fundamentals of search,", "tokens": [50760, 5120, 30, 400, 611, 411, 562, 309, 1487,
  294, 11, 291, 458, 11, 281, 3701, 29505, 295, 3164, 11, 51052], "temperature": 0.0,
  "avg_logprob": -0.21449580945466695, "compression_ratio": 1.6055776892430278, "no_speech_prob":
  0.004052360542118549}, {"id": 136, "seek": 94656, "start": 960.3199999999999, "end":
  967.8399999999999, "text": " let''s say knowing how TFIDF formula composed or you
  have 25, what are the trade-offs versus sort of", "tokens": [51052, 718, 311, 584,
  5276, 577, 40964, 2777, 37, 8513, 18204, 420, 291, 362, 3552, 11, 437, 366, 264,
  4923, 12, 19231, 5717, 1333, 295, 51428], "temperature": 0.0, "avg_logprob": -0.21449580945466695,
  "compression_ratio": 1.6055776892430278, "no_speech_prob": 0.004052360542118549},
  {"id": 137, "seek": 94656, "start": 967.8399999999999, "end": 973.3599999999999,
  "text": " like going and actually experimenting and trying out things, you know,
  where do you see that balance", "tokens": [51428, 411, 516, 293, 767, 29070, 293,
  1382, 484, 721, 11, 291, 458, 11, 689, 360, 291, 536, 300, 4772, 51704], "temperature":
  0.0, "avg_logprob": -0.21449580945466695, "compression_ratio": 1.6055776892430278,
  "no_speech_prob": 0.004052360542118549}, {"id": 138, "seek": 97336, "start": 973.36,
  "end": 980.48, "text": " as well for yourself maybe and also for the teams around
  you? Yeah, I mean, I think everybody", "tokens": [50364, 382, 731, 337, 1803, 1310,
  293, 611, 337, 264, 5491, 926, 291, 30, 865, 11, 286, 914, 11, 286, 519, 2201, 50720],
  "temperature": 0.0, "avg_logprob": -0.13456870118776956, "compression_ratio": 1.6475770925110131,
  "no_speech_prob": 0.0009662438533268869}, {"id": 139, "seek": 97336, "start": 980.48,
  "end": 985.2, "text": " will have their own, you know, kind of depending on where
  you come from, right? Like if,", "tokens": [50720, 486, 362, 641, 1065, 11, 291,
  458, 11, 733, 295, 5413, 322, 689, 291, 808, 490, 11, 558, 30, 1743, 498, 11, 50956],
  "temperature": 0.0, "avg_logprob": -0.13456870118776956, "compression_ratio": 1.6475770925110131,
  "no_speech_prob": 0.0009662438533268869}, {"id": 140, "seek": 97336, "start": 985.84,
  "end": 990.88, "text": " you know, like if you have, if you''ve done deep academic
  work, you''re probably going to have a", "tokens": [50988, 291, 458, 11, 411, 498,
  291, 362, 11, 498, 291, 600, 1096, 2452, 7778, 589, 11, 291, 434, 1391, 516, 281,
  362, 257, 51240], "temperature": 0.0, "avg_logprob": -0.13456870118776956, "compression_ratio":
  1.6475770925110131, "no_speech_prob": 0.0009662438533268869}, {"id": 141, "seek":
  97336, "start": 990.88, "end": 997.12, "text": " lot more understanding of the math
  and the theoretical side of it. And then you''re going to have", "tokens": [51240,
  688, 544, 3701, 295, 264, 5221, 293, 264, 20864, 1252, 295, 309, 13, 400, 550, 291,
  434, 516, 281, 362, 51552], "temperature": 0.0, "avg_logprob": -0.13456870118776956,
  "compression_ratio": 1.6475770925110131, "no_speech_prob": 0.0009662438533268869},
  {"id": 142, "seek": 99712, "start": 997.12, "end": 1003.36, "text": " to develop
  the intuition of real world data, right? How messy it is, how clunky it is, how",
  "tokens": [50364, 281, 1499, 264, 24002, 295, 957, 1002, 1412, 11, 558, 30, 1012,
  16191, 309, 307, 11, 577, 596, 25837, 309, 307, 11, 577, 50676], "temperature":
  0.0, "avg_logprob": -0.11430580447418522, "compression_ratio": 1.628691983122363,
  "no_speech_prob": 0.0020228272769600153}, {"id": 143, "seek": 99712, "start": 1004.16,
  "end": 1009.6, "text": " full of junk and spam, et cetera, right? Because a lot
  of times when you''re dealing with academic", "tokens": [50716, 1577, 295, 19109,
  293, 24028, 11, 1030, 11458, 11, 558, 30, 1436, 257, 688, 295, 1413, 562, 291, 434,
  6260, 365, 7778, 50988], "temperature": 0.0, "avg_logprob": -0.11430580447418522,
  "compression_ratio": 1.628691983122363, "no_speech_prob": 0.0020228272769600153},
  {"id": 144, "seek": 99712, "start": 1009.6, "end": 1015.12, "text": " data sets,
  they''re pretty clean, right? Relatively speaking, they still of course have their
  own", "tokens": [50988, 1412, 6352, 11, 436, 434, 1238, 2541, 11, 558, 30, 8738,
  19020, 4124, 11, 436, 920, 295, 1164, 362, 641, 1065, 51264], "temperature": 0.0,
  "avg_logprob": -0.11430580447418522, "compression_ratio": 1.628691983122363, "no_speech_prob":
  0.0020228272769600153}, {"id": 145, "seek": 99712, "start": 1015.12, "end": 1020.64,
  "text": " set of garbage and nuances in them. Whereas if you''re an engineer and
  you''re coming at it from like,", "tokens": [51264, 992, 295, 14150, 293, 38775,
  294, 552, 13, 13813, 498, 291, 434, 364, 11403, 293, 291, 434, 1348, 412, 309, 490,
  411, 11, 51540], "temperature": 0.0, "avg_logprob": -0.11430580447418522, "compression_ratio":
  1.628691983122363, "no_speech_prob": 0.0020228272769600153}, {"id": 146, "seek":
  102064, "start": 1020.64, "end": 1027.2, "text": " hey, you know, often what I see
  with engineers is they come at it from a quantitative standpoint", "tokens": [50364,
  4177, 11, 291, 458, 11, 2049, 437, 286, 536, 365, 11955, 307, 436, 808, 412, 309,
  490, 257, 27778, 15827, 50692], "temperature": 0.0, "avg_logprob": -0.13971944288773971,
  "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0005663756164722145},
  {"id": 147, "seek": 102064, "start": 1027.2, "end": 1031.68, "text": " of like,
  I want to make sure this is scalable and reliable. So they''re solving for the",
  "tokens": [50692, 295, 411, 11, 286, 528, 281, 652, 988, 341, 307, 38481, 293, 12924,
  13, 407, 436, 434, 12606, 337, 264, 50916], "temperature": 0.0, "avg_logprob": -0.13971944288773971,
  "compression_ratio": 1.5732217573221758, "no_speech_prob": 0.0005663756164722145},
  {"id": 148, "seek": 102064, "start": 1032.48, "end": 1041.44, "text": " hardening
  of the system problem first. And then they often will develop the the relevant side
  of it", "tokens": [50956, 1152, 4559, 295, 264, 1185, 1154, 700, 13, 400, 550, 436,
  2049, 486, 1499, 264, 264, 7340, 1252, 295, 309, 51404], "temperature": 0.0, "avg_logprob":
  -0.13971944288773971, "compression_ratio": 1.5732217573221758, "no_speech_prob":
  0.0005663756164722145}, {"id": 149, "seek": 102064, "start": 1041.44, "end": 1047.36,
  "text": " or the the understanding of the data second. Now again, broad generalizations
  there because,", "tokens": [51404, 420, 264, 264, 3701, 295, 264, 1412, 1150, 13,
  823, 797, 11, 4152, 2674, 14455, 456, 570, 11, 51700], "temperature": 0.0, "avg_logprob":
  -0.13971944288773971, "compression_ratio": 1.5732217573221758, "no_speech_prob":
  0.0005663756164722145}, {"id": 150, "seek": 104736, "start": 1047.9199999999998,
  "end": 1053.12, "text": " you know, folks have all kinds of different backgrounds.
  But you know, so like as a leader in", "tokens": [50392, 291, 458, 11, 4024, 362,
  439, 3685, 295, 819, 17336, 13, 583, 291, 458, 11, 370, 411, 382, 257, 5263, 294,
  50652], "temperature": 0.0, "avg_logprob": -0.1250316738027387, "compression_ratio":
  1.7985074626865671, "no_speech_prob": 0.0008158011478371918}, {"id": 151, "seek":
  104736, "start": 1053.6, "end": 1059.84, "text": " somebody who does, you know,
  manages people in this space, like I would often just work with you", "tokens":
  [50676, 2618, 567, 775, 11, 291, 458, 11, 22489, 561, 294, 341, 1901, 11, 411, 286,
  576, 2049, 445, 589, 365, 291, 50988], "temperature": 0.0, "avg_logprob": -0.1250316738027387,
  "compression_ratio": 1.7985074626865671, "no_speech_prob": 0.0008158011478371918},
  {"id": 152, "seek": 104736, "start": 1059.84, "end": 1064.8799999999999, "text":
  " depending on what your background and understanding and intuition is. And then,
  you know, try to help", "tokens": [50988, 5413, 322, 437, 428, 3678, 293, 3701,
  293, 24002, 307, 13, 400, 550, 11, 291, 458, 11, 853, 281, 854, 51240], "temperature":
  0.0, "avg_logprob": -0.1250316738027387, "compression_ratio": 1.7985074626865671,
  "no_speech_prob": 0.0008158011478371918}, {"id": 153, "seek": 104736, "start": 1064.8799999999999,
  "end": 1071.04, "text": " you complement whatever it is you''re missing there, right?
  Like I think you have to have an", "tokens": [51240, 291, 17103, 2035, 309, 307,
  291, 434, 5361, 456, 11, 558, 30, 1743, 286, 519, 291, 362, 281, 362, 364, 51548],
  "temperature": 0.0, "avg_logprob": -0.1250316738027387, "compression_ratio": 1.7985074626865671,
  "no_speech_prob": 0.0008158011478371918}, {"id": 154, "seek": 104736, "start": 1071.04,
  "end": 1077.04, "text": " understanding of how these engines work. I''ve often seen
  folks who don''t have an understanding of", "tokens": [51548, 3701, 295, 577, 613,
  12982, 589, 13, 286, 600, 2049, 1612, 4024, 567, 500, 380, 362, 364, 3701, 295,
  51848], "temperature": 0.0, "avg_logprob": -0.1250316738027387, "compression_ratio":
  1.7985074626865671, "no_speech_prob": 0.0008158011478371918}, {"id": 155, "seek":
  107704, "start": 1077.04, "end": 1083.2, "text": " all the capabilities of these
  modern search engines recreate the wheel, right? Like they''re reinventing", "tokens":
  [50364, 439, 264, 10862, 295, 613, 4363, 3164, 12982, 25833, 264, 5589, 11, 558,
  30, 1743, 436, 434, 33477, 278, 50672], "temperature": 0.0, "avg_logprob": -0.10110753232782538,
  "compression_ratio": 1.742081447963801, "no_speech_prob": 0.00016552692977711558},
  {"id": 156, "seek": 107704, "start": 1083.2, "end": 1089.52, "text": " the wheel
  because they they''re coming from this first principles of the math that they learn",
  "tokens": [50672, 264, 5589, 570, 436, 436, 434, 1348, 490, 341, 700, 9156, 295,
  264, 5221, 300, 436, 1466, 50988], "temperature": 0.0, "avg_logprob": -0.10110753232782538,
  "compression_ratio": 1.742081447963801, "no_speech_prob": 0.00016552692977711558},
  {"id": 157, "seek": 107704, "start": 1089.52, "end": 1095.12, "text": " at the academic
  level. And then, but they don''t necessarily know how that applies to real data",
  "tokens": [50988, 412, 264, 7778, 1496, 13, 400, 550, 11, 457, 436, 500, 380, 4725,
  458, 577, 300, 13165, 281, 957, 1412, 51268], "temperature": 0.0, "avg_logprob":
  -0.10110753232782538, "compression_ratio": 1.742081447963801, "no_speech_prob":
  0.00016552692977711558}, {"id": 158, "seek": 107704, "start": 1095.12, "end": 1100.08,
  "text": " in the real world. Whereas a lot of these, you know, modern search engines,
  because they are,", "tokens": [51268, 294, 264, 957, 1002, 13, 13813, 257, 688,
  295, 613, 11, 291, 458, 11, 4363, 3164, 12982, 11, 570, 436, 366, 11, 51516], "temperature":
  0.0, "avg_logprob": -0.10110753232782538, "compression_ratio": 1.742081447963801,
  "no_speech_prob": 0.00016552692977711558}, {"id": 159, "seek": 110008, "start":
  1100.08, "end": 1108.6399999999999, "text": " they grew up in large scale, you know,
  publicly traded high volume spaces. They''ve really been", "tokens": [50364, 436,
  6109, 493, 294, 2416, 4373, 11, 291, 458, 11, 14843, 27157, 1090, 5523, 7673, 13,
  814, 600, 534, 668, 50792], "temperature": 0.0, "avg_logprob": -0.14634986357255417,
  "compression_ratio": 1.5867768595041323, "no_speech_prob": 0.0004984450642950833},
  {"id": 160, "seek": 110008, "start": 1108.6399999999999, "end": 1114.24, "text":
  " hardened on the engineering side and they really know how to deal with all the
  nuances of real", "tokens": [50792, 42605, 322, 264, 7043, 1252, 293, 436, 534,
  458, 577, 281, 2028, 365, 439, 264, 38775, 295, 957, 51072], "temperature": 0.0,
  "avg_logprob": -0.14634986357255417, "compression_ratio": 1.5867768595041323, "no_speech_prob":
  0.0004984450642950833}, {"id": 161, "seek": 110008, "start": 1114.24, "end": 1121.76,
  "text": " world data, right? And so, you by learning those kinds of things, you
  will be much more effective at", "tokens": [51072, 1002, 1412, 11, 558, 30, 400,
  370, 11, 291, 538, 2539, 729, 3685, 295, 721, 11, 291, 486, 312, 709, 544, 4942,
  412, 51448], "temperature": 0.0, "avg_logprob": -0.14634986357255417, "compression_ratio":
  1.5867768595041323, "no_speech_prob": 0.0004984450642950833}, {"id": 162, "seek":
  110008, "start": 1121.76, "end": 1129.04, "text": " the at bringing to bear your
  intuitions and understandings from whichever background that is.", "tokens": [51448,
  264, 412, 5062, 281, 6155, 428, 16224, 626, 293, 1223, 1109, 490, 24123, 3678, 300,
  307, 13, 51812], "temperature": 0.0, "avg_logprob": -0.14634986357255417, "compression_ratio":
  1.5867768595041323, "no_speech_prob": 0.0004984450642950833}, {"id": 163, "seek":
  112904, "start": 1129.04, "end": 1133.92, "text": " I don''t know if that makes
  sense or not. Yeah, no, absolutely. Yeah, actually in the same", "tokens": [50364,
  286, 500, 380, 458, 498, 300, 1669, 2020, 420, 406, 13, 865, 11, 572, 11, 3122,
  13, 865, 11, 767, 294, 264, 912, 50608], "temperature": 0.0, "avg_logprob": -0.16189155907466493,
  "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.005156954284757376},
  {"id": 164, "seek": 112904, "start": 1133.92, "end": 1140.32, "text": " presentation,
  you also said like, you''ve seen cases where you you come into helper company and",
  "tokens": [50608, 5860, 11, 291, 611, 848, 411, 11, 291, 600, 1612, 3331, 689, 291,
  291, 808, 666, 36133, 2237, 293, 50928], "temperature": 0.0, "avg_logprob": -0.16189155907466493,
  "compression_ratio": 1.6468531468531469, "no_speech_prob": 0.005156954284757376},
  {"id": 165, "seek": 112904, "start": 1141.04, "end": 1147.92, "text": " they they
  point you to sort of like a data, the ace almost of 10,000 rules. And so you you
  said", "tokens": [50964, 436, 436, 935, 291, 281, 1333, 295, 411, 257, 1412, 11,
  264, 17117, 1920, 295, 1266, 11, 1360, 4474, 13, 400, 370, 291, 291, 848, 51308],
  "temperature": 0.0, "avg_logprob": -0.16189155907466493, "compression_ratio": 1.6468531468531469,
  "no_speech_prob": 0.005156954284757376}, {"id": 166, "seek": 112904, "start": 1147.92,
  "end": 1151.44, "text": " they have that in principle, you could just remove solar
  or whatever search engine you have and", "tokens": [51308, 436, 362, 300, 294, 8665,
  11, 291, 727, 445, 4159, 7936, 420, 2035, 3164, 2848, 291, 362, 293, 51484], "temperature":
  0.0, "avg_logprob": -0.16189155907466493, "compression_ratio": 1.6468531468531469,
  "no_speech_prob": 0.005156954284757376}, {"id": 167, "seek": 112904, "start": 1151.44,
  "end": 1156.56, "text": " just use those rules to retrieve documents, right? But
  when you go and ask specific questions,", "tokens": [51484, 445, 764, 729, 4474,
  281, 30254, 8512, 11, 558, 30, 583, 562, 291, 352, 293, 1029, 2685, 1651, 11, 51740],
  "temperature": 0.0, "avg_logprob": -0.16189155907466493, "compression_ratio": 1.6468531468531469,
  "no_speech_prob": 0.005156954284757376}, {"id": 168, "seek": 115656, "start": 1156.56,
  "end": 1162.56, "text": " what what what this rule does? The answer that you you
  illustrated was well, it was created by", "tokens": [50364, 437, 437, 437, 341,
  4978, 775, 30, 440, 1867, 300, 291, 291, 33875, 390, 731, 11, 309, 390, 2942, 538,
  50664], "temperature": 0.0, "avg_logprob": -0.1486204497668208, "compression_ratio":
  1.56, "no_speech_prob": 0.003055430017411709}, {"id": 169, "seek": 115656, "start":
  1162.56, "end": 1169.28, "text": " Joy, you know, and he quit five years ago. So
  he then said it makes sense. So we keep it. So how do", "tokens": [50664, 15571,
  11, 291, 458, 11, 293, 415, 10366, 1732, 924, 2057, 13, 407, 415, 550, 848, 309,
  1669, 2020, 13, 407, 321, 1066, 309, 13, 407, 577, 360, 51000], "temperature": 0.0,
  "avg_logprob": -0.1486204497668208, "compression_ratio": 1.56, "no_speech_prob":
  0.003055430017411709}, {"id": 170, "seek": 115656, "start": 1169.28, "end": 1175.2,
  "text": " you go about convincing the organization or teams to change their perception
  and sort of like become", "tokens": [51000, 291, 352, 466, 24823, 264, 4475, 420,
  5491, 281, 1319, 641, 12860, 293, 1333, 295, 411, 1813, 51296], "temperature": 0.0,
  "avg_logprob": -0.1486204497668208, "compression_ratio": 1.56, "no_speech_prob":
  0.003055430017411709}, {"id": 171, "seek": 115656, "start": 1175.2, "end": 1182.24,
  "text": " more flexible and move into this flywheel of experiments? Yeah, it''s
  hard. And again, I think,", "tokens": [51296, 544, 11358, 293, 1286, 666, 341, 3603,
  22830, 295, 12050, 30, 865, 11, 309, 311, 1152, 13, 400, 797, 11, 286, 519, 11,
  51648], "temperature": 0.0, "avg_logprob": -0.1486204497668208, "compression_ratio":
  1.56, "no_speech_prob": 0.003055430017411709}, {"id": 172, "seek": 118224, "start":
  1182.24, "end": 1187.6, "text": " you know, I mean, you have to look at incentives
  and first principles there, right? Like,", "tokens": [50364, 291, 458, 11, 286,
  914, 11, 291, 362, 281, 574, 412, 23374, 293, 700, 9156, 456, 11, 558, 30, 1743,
  11, 50632], "temperature": 0.0, "avg_logprob": -0.10238174998432124, "compression_ratio":
  1.6824034334763949, "no_speech_prob": 0.0018538926960900426}, {"id": 173, "seek":
  118224, "start": 1188.72, "end": 1195.44, "text": " again, if you''re in this boat
  of like searches, just a feature, there may or may not be any incentive.", "tokens":
  [50688, 797, 11, 498, 291, 434, 294, 341, 6582, 295, 411, 26701, 11, 445, 257, 4111,
  11, 456, 815, 420, 815, 406, 312, 604, 22346, 13, 51024], "temperature": 0.0, "avg_logprob":
  -0.10238174998432124, "compression_ratio": 1.6824034334763949, "no_speech_prob":
  0.0018538926960900426}, {"id": 174, "seek": 118224, "start": 1195.44, "end": 1200.64,
  "text": " But if you''re in this boat of like, hey, search is a really critical
  aspect of what we do. Our users", "tokens": [51024, 583, 498, 291, 434, 294, 341,
  6582, 295, 411, 11, 4177, 11, 3164, 307, 257, 534, 4924, 4171, 295, 437, 321, 360,
  13, 2621, 5022, 51284], "temperature": 0.0, "avg_logprob": -0.10238174998432124,
  "compression_ratio": 1.6824034334763949, "no_speech_prob": 0.0018538926960900426},
  {"id": 175, "seek": 118224, "start": 1200.64, "end": 1209.2, "text": " use it all
  the time. It''s key to revenue. It''s key to timeliness or it''s, you know, people''s
  lives", "tokens": [51284, 764, 309, 439, 264, 565, 13, 467, 311, 2141, 281, 9324,
  13, 467, 311, 2141, 281, 524, 25307, 420, 309, 311, 11, 291, 458, 11, 561, 311,
  2909, 51712], "temperature": 0.0, "avg_logprob": -0.10238174998432124, "compression_ratio":
  1.6824034334763949, "no_speech_prob": 0.0018538926960900426}, {"id": 176, "seek":
  120920, "start": 1209.2, "end": 1216.96, "text": " are on the line, et cetera. You''re
  going to invest in making sure searches as capable as possible.", "tokens": [50364,
  366, 322, 264, 1622, 11, 1030, 11458, 13, 509, 434, 516, 281, 1963, 294, 1455, 988,
  26701, 382, 8189, 382, 1944, 13, 50752], "temperature": 0.0, "avg_logprob": -0.07385156105975715,
  "compression_ratio": 1.5983935742971886, "no_speech_prob": 0.0013425301294773817},
  {"id": 177, "seek": 120920, "start": 1216.96, "end": 1224.16, "text": " Those folks
  usually don''t take much convincing once you can show them a better way, right?
  They''re", "tokens": [50752, 3950, 4024, 2673, 500, 380, 747, 709, 24823, 1564,
  291, 393, 855, 552, 257, 1101, 636, 11, 558, 30, 814, 434, 51112], "temperature":
  0.0, "avg_logprob": -0.07385156105975715, "compression_ratio": 1.5983935742971886,
  "no_speech_prob": 0.0013425301294773817}, {"id": 178, "seek": 120920, "start": 1224.16,
  "end": 1230.88, "text": " often already frustrated by the sheer number of rules
  that they have. And so one of the things that", "tokens": [51112, 2049, 1217, 15751,
  538, 264, 23061, 1230, 295, 4474, 300, 436, 362, 13, 400, 370, 472, 295, 264, 721,
  300, 51448], "temperature": 0.0, "avg_logprob": -0.07385156105975715, "compression_ratio":
  1.5983935742971886, "no_speech_prob": 0.0013425301294773817}, {"id": 179, "seek":
  120920, "start": 1230.88, "end": 1234.88, "text": " can often work in those situations,
  I think is, you know, you can start to just learn the, you know,", "tokens": [51448,
  393, 2049, 589, 294, 729, 6851, 11, 286, 519, 307, 11, 291, 458, 11, 291, 393, 722,
  281, 445, 1466, 264, 11, 291, 458, 11, 51648], "temperature": 0.0, "avg_logprob":
  -0.07385156105975715, "compression_ratio": 1.5983935742971886, "no_speech_prob":
  0.0013425301294773817}, {"id": 180, "seek": 123488, "start": 1234.88, "end": 1240.0800000000002,
  "text": " a lot of these machine learning systems will actually learn the set of
  rules, right? And so if you", "tokens": [50364, 257, 688, 295, 613, 3479, 2539,
  3652, 486, 767, 1466, 264, 992, 295, 4474, 11, 558, 30, 400, 370, 498, 291, 50624],
  "temperature": 0.0, "avg_logprob": -0.10293541635785784, "compression_ratio": 1.8029739776951672,
  "no_speech_prob": 0.0004807735385838896}, {"id": 181, "seek": 123488, "start": 1240.0800000000002,
  "end": 1244.48, "text": " want, you can just start to learn the rules by the fact
  that you''re gathering your queries and", "tokens": [50624, 528, 11, 291, 393, 445,
  722, 281, 1466, 264, 4474, 538, 264, 1186, 300, 291, 434, 13519, 428, 24109, 293,
  50844], "temperature": 0.0, "avg_logprob": -0.10293541635785784, "compression_ratio":
  1.8029739776951672, "no_speech_prob": 0.0004807735385838896}, {"id": 182, "seek":
  123488, "start": 1244.48, "end": 1250.0, "text": " your click logs and you''re looking
  at the engagements users are having with the system, with the rules", "tokens":
  [50844, 428, 2052, 20820, 293, 291, 434, 1237, 412, 264, 44978, 5022, 366, 1419,
  365, 264, 1185, 11, 365, 264, 4474, 51120], "temperature": 0.0, "avg_logprob": -0.10293541635785784,
  "compression_ratio": 1.8029739776951672, "no_speech_prob": 0.0004807735385838896},
  {"id": 183, "seek": 123488, "start": 1250.0, "end": 1255.6000000000001, "text":
  " in place. And then over time, you know, that will learn it. That the harder part
  often is", "tokens": [51120, 294, 1081, 13, 400, 550, 670, 565, 11, 291, 458, 11,
  300, 486, 1466, 309, 13, 663, 264, 6081, 644, 2049, 307, 51400], "temperature":
  0.0, "avg_logprob": -0.10293541635785784, "compression_ratio": 1.8029739776951672,
  "no_speech_prob": 0.0004807735385838896}, {"id": 184, "seek": 123488, "start": 1257.2,
  "end": 1264.4, "text": " getting that last part, which is true experimentation whereby
  they actually have a system in place", "tokens": [51480, 1242, 300, 1036, 644, 11,
  597, 307, 2074, 37142, 36998, 436, 767, 362, 257, 1185, 294, 1081, 51840], "temperature":
  0.0, "avg_logprob": -0.10293541635785784, "compression_ratio": 1.8029739776951672,
  "no_speech_prob": 0.0004807735385838896}, {"id": 185, "seek": 126440, "start": 1264.4,
  "end": 1272.0800000000002, "text": " for running multi-variant experiments or AB
  tests, right? And they can actually try out different", "tokens": [50364, 337, 2614,
  4825, 12, 34033, 394, 12050, 420, 13838, 6921, 11, 558, 30, 400, 436, 393, 767,
  853, 484, 819, 50748], "temperature": 0.0, "avg_logprob": -0.08641707036913056,
  "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.0005219281883910298},
  {"id": 186, "seek": 126440, "start": 1272.0800000000002, "end": 1279.0400000000002,
  "text": " approaches and see which one wins and see which one''s most effective
  and then go with that from,", "tokens": [50748, 11587, 293, 536, 597, 472, 10641,
  293, 536, 597, 472, 311, 881, 4942, 293, 550, 352, 365, 300, 490, 11, 51096], "temperature":
  0.0, "avg_logprob": -0.08641707036913056, "compression_ratio": 1.6302521008403361,
  "no_speech_prob": 0.0005219281883910298}, {"id": 187, "seek": 126440, "start": 1279.0400000000002,
  "end": 1284.4, "text": " you know, until the next one beats it, right? That''s a
  fair amount of engineering work to get in place.", "tokens": [51096, 291, 458, 11,
  1826, 264, 958, 472, 16447, 309, 11, 558, 30, 663, 311, 257, 3143, 2372, 295, 7043,
  589, 281, 483, 294, 1081, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08641707036913056,
  "compression_ratio": 1.6302521008403361, "no_speech_prob": 0.0005219281883910298},
  {"id": 188, "seek": 126440, "start": 1284.4, "end": 1290.4, "text": " It''s also
  a fair amount of math to do in order to make sure it''s appropriate. These days,",
  "tokens": [51364, 467, 311, 611, 257, 3143, 2372, 295, 5221, 281, 360, 294, 1668,
  281, 652, 988, 309, 311, 6854, 13, 1981, 1708, 11, 51664], "temperature": 0.0, "avg_logprob":
  -0.08641707036913056, "compression_ratio": 1.6302521008403361, "no_speech_prob":
  0.0005219281883910298}, {"id": 189, "seek": 129040, "start": 1290.4, "end": 1294.8000000000002,
  "text": " there are systems and tools that allow you to do it, but if you want to
  homegrown it, you know,", "tokens": [50364, 456, 366, 3652, 293, 3873, 300, 2089,
  291, 281, 360, 309, 11, 457, 498, 291, 528, 281, 1280, 38413, 309, 11, 291, 458,
  11, 50584], "temperature": 0.0, "avg_logprob": -0.10876838197099402, "compression_ratio":
  1.6875, "no_speech_prob": 0.003533940063789487}, {"id": 190, "seek": 129040, "start":
  1294.8000000000002, "end": 1300.96, "text": " that can take a lot of work. So getting
  people to be in that mindset, especially in", "tokens": [50584, 300, 393, 747, 257,
  688, 295, 589, 13, 407, 1242, 561, 281, 312, 294, 300, 12543, 11, 2318, 294, 50892],
  "temperature": 0.0, "avg_logprob": -0.10876838197099402, "compression_ratio": 1.6875,
  "no_speech_prob": 0.003533940063789487}, {"id": 191, "seek": 129040, "start": 1302.4,
  "end": 1308.16, "text": " environments or company cultures where like there''s pride
  in being right, you know, you sometimes", "tokens": [50964, 12388, 420, 2237, 12951,
  689, 411, 456, 311, 10936, 294, 885, 558, 11, 291, 458, 11, 291, 2171, 51252], "temperature":
  0.0, "avg_logprob": -0.10876838197099402, "compression_ratio": 1.6875, "no_speech_prob":
  0.003533940063789487}, {"id": 192, "seek": 129040, "start": 1308.16, "end": 1313.76,
  "text": " see that in a lot of companies where it''s like whoever''s the boss has
  to be right kind of situation.", "tokens": [51252, 536, 300, 294, 257, 688, 295,
  3431, 689, 309, 311, 411, 11387, 311, 264, 5741, 575, 281, 312, 558, 733, 295, 2590,
  13, 51532], "temperature": 0.0, "avg_logprob": -0.10876838197099402, "compression_ratio":
  1.6875, "no_speech_prob": 0.003533940063789487}, {"id": 193, "seek": 131376, "start":
  1314.32, "end": 1320.8, "text": " Those types of companies are always going to struggle
  with experiment mindsets because, you know,", "tokens": [50392, 3950, 3467, 295,
  3431, 366, 1009, 516, 281, 7799, 365, 5120, 9634, 1385, 570, 11, 291, 458, 11, 50716],
  "temperature": 0.0, "avg_logprob": -0.2014251756079403, "compression_ratio": 1.6536796536796536,
  "no_speech_prob": 0.010579102672636509}, {"id": 194, "seek": 131376, "start": 1320.8,
  "end": 1327.44, "text": " they reward, quote unquote, being right as opposed to,
  quote unquote, you know, rewarding", "tokens": [50716, 436, 7782, 11, 6513, 37557,
  11, 885, 558, 382, 8851, 281, 11, 6513, 37557, 11, 291, 458, 11, 20063, 51048],
  "temperature": 0.0, "avg_logprob": -0.2014251756079403, "compression_ratio": 1.6536796536796536,
  "no_speech_prob": 0.010579102672636509}, {"id": 195, "seek": 131376, "start": 1328.16,
  "end": 1333.92, "text": " longer term growth and incremental improvements with the
  occasional failures, right? So you really", "tokens": [51084, 2854, 1433, 4599,
  293, 35759, 13797, 365, 264, 31644, 20774, 11, 558, 30, 407, 291, 534, 51372], "temperature":
  0.0, "avg_logprob": -0.2014251756079403, "compression_ratio": 1.6536796536796536,
  "no_speech_prob": 0.010579102672636509}, {"id": 196, "seek": 131376, "start": 1333.92,
  "end": 1341.28, "text": " have to look at company culture first and potentially
  reset that and then build and bake in the", "tokens": [51372, 362, 281, 574, 412,
  2237, 3713, 700, 293, 7263, 14322, 300, 293, 550, 1322, 293, 16562, 294, 264, 51740],
  "temperature": 0.0, "avg_logprob": -0.2014251756079403, "compression_ratio": 1.6536796536796536,
  "no_speech_prob": 0.010579102672636509}, {"id": 197, "seek": 134128, "start": 1342.0,
  "end": 1349.2, "text": " the necessary engineering work to make experiments work.
  Yeah, absolutely. I agree to that same thought", "tokens": [50400, 264, 4818, 7043,
  589, 281, 652, 12050, 589, 13, 865, 11, 3122, 13, 286, 3986, 281, 300, 912, 1194,
  50760], "temperature": 0.0, "avg_logprob": -0.18434673885129532, "compression_ratio":
  1.6642857142857144, "no_speech_prob": 0.005494780372828245}, {"id": 198, "seek":
  134128, "start": 1349.2, "end": 1354.8799999999999, "text": " that, you know, without
  failures, you cannot really breed the culture of creating cool new stuff", "tokens":
  [50760, 300, 11, 291, 458, 11, 1553, 20774, 11, 291, 2644, 534, 18971, 264, 3713,
  295, 4084, 1627, 777, 1507, 51044], "temperature": 0.0, "avg_logprob": -0.18434673885129532,
  "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.005494780372828245},
  {"id": 199, "seek": 134128, "start": 1354.8799999999999, "end": 1360.24, "text":
  " because you basically cannot unleash yourself to go and mess with your code base,
  right?", "tokens": [51044, 570, 291, 1936, 2644, 49814, 1803, 281, 352, 293, 2082,
  365, 428, 3089, 3096, 11, 558, 30, 51312], "temperature": 0.0, "avg_logprob": -0.18434673885129532,
  "compression_ratio": 1.6642857142857144, "no_speech_prob": 0.005494780372828245},
  {"id": 200, "seek": 134128, "start": 1361.04, "end": 1364.16, "text": " And do things
  and create new stuff. So like, you need to be brave for sure.", "tokens": [51352,
  400, 360, 721, 293, 1884, 777, 1507, 13, 407, 411, 11, 291, 643, 281, 312, 12653,
  337, 988, 13, 51508], "temperature": 0.0, "avg_logprob": -0.18434673885129532, "compression_ratio":
  1.6642857142857144, "no_speech_prob": 0.005494780372828245}, {"id": 201, "seek":
  134128, "start": 1364.6399999999999, "end": 1370.3999999999999, "text": " Well,
  as I think the front of mind Ted Dunning said, the cool thing about experimentation
  frameworks", "tokens": [51532, 1042, 11, 382, 286, 519, 264, 1868, 295, 1575, 14985,
  11959, 773, 848, 11, 264, 1627, 551, 466, 37142, 29834, 51820], "temperature": 0.0,
  "avg_logprob": -0.18434673885129532, "compression_ratio": 1.6642857142857144, "no_speech_prob":
  0.005494780372828245}, {"id": 202, "seek": 137040, "start": 1370.4, "end": 1377.1200000000001,
  "text": " is you get to be wrong and that''s okay, right? Like you''re actually
  right by the fact that you''re wrong.", "tokens": [50364, 307, 291, 483, 281, 312,
  2085, 293, 300, 311, 1392, 11, 558, 30, 1743, 291, 434, 767, 558, 538, 264, 1186,
  300, 291, 434, 2085, 13, 50700], "temperature": 0.0, "avg_logprob": -0.114143683531574,
  "compression_ratio": 1.7256637168141593, "no_speech_prob": 0.003049850929528475},
  {"id": 203, "seek": 137040, "start": 1378.0, "end": 1386.3200000000002, "text":
  " You''re because you''re right in the long run, right? Yes. Even if any given experiment
  is flat or bad,", "tokens": [50744, 509, 434, 570, 291, 434, 558, 294, 264, 938,
  1190, 11, 558, 30, 1079, 13, 2754, 498, 604, 2212, 5120, 307, 4962, 420, 1578, 11,
  51160], "temperature": 0.0, "avg_logprob": -0.114143683531574, "compression_ratio":
  1.7256637168141593, "no_speech_prob": 0.003049850929528475}, {"id": 204, "seek":
  137040, "start": 1386.3200000000002, "end": 1391.68, "text": " right? But overall,
  you know, in the long run, you''re going to win out because you''re going to just,",
  "tokens": [51160, 558, 30, 583, 4787, 11, 291, 458, 11, 294, 264, 938, 1190, 11,
  291, 434, 516, 281, 1942, 484, 570, 291, 434, 516, 281, 445, 11, 51428], "temperature":
  0.0, "avg_logprob": -0.114143683531574, "compression_ratio": 1.7256637168141593,
  "no_speech_prob": 0.003049850929528475}, {"id": 205, "seek": 137040, "start": 1391.68,
  "end": 1397.6000000000001, "text": " it''s easier and easier for you to add in a
  new approach. Yeah, absolutely. I think", "tokens": [51428, 309, 311, 3571, 293,
  3571, 337, 291, 281, 909, 294, 257, 777, 3109, 13, 865, 11, 3122, 13, 286, 519,
  51724], "temperature": 0.0, "avg_logprob": -0.114143683531574, "compression_ratio":
  1.7256637168141593, "no_speech_prob": 0.003049850929528475}, {"id": 206, "seek":
  139760, "start": 1398.3999999999999, "end": 1405.04, "text": " that Turnbull also
  said, like, you know, how you basically accumulate this bruises, right? So you''re
  like,", "tokens": [50404, 300, 7956, 37290, 611, 848, 11, 411, 11, 291, 458, 11,
  577, 291, 1936, 33384, 341, 25267, 3598, 11, 558, 30, 407, 291, 434, 411, 11, 50736],
  "temperature": 0.0, "avg_logprob": -0.2441332222211479, "compression_ratio": 1.6554621848739495,
  "no_speech_prob": 0.01411152258515358}, {"id": 207, "seek": 139760, "start": 1405.04,
  "end": 1412.0, "text": " Oscar tissue as some other people say. So I think without
  doing things, you can''t without failing as well,", "tokens": [50736, 20718, 12404,
  382, 512, 661, 561, 584, 13, 407, 286, 519, 1553, 884, 721, 11, 291, 393, 380, 1553,
  18223, 382, 731, 11, 51084], "temperature": 0.0, "avg_logprob": -0.2441332222211479,
  "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.01411152258515358},
  {"id": 208, "seek": 139760, "start": 1412.0, "end": 1417.12, "text": " you can''t
  learn. So I totally agree to that. But still for those who are still learning,",
  "tokens": [51084, 291, 393, 380, 1466, 13, 407, 286, 3879, 3986, 281, 300, 13, 583,
  920, 337, 729, 567, 366, 920, 2539, 11, 51340], "temperature": 0.0, "avg_logprob":
  -0.2441332222211479, "compression_ratio": 1.6554621848739495, "no_speech_prob":
  0.01411152258515358}, {"id": 209, "seek": 139760, "start": 1417.12, "end": 1422.1599999999999,
  "text": " you know, and we are discussing, to some extent, the courses that you
  couldn''t be teaching,", "tokens": [51340, 291, 458, 11, 293, 321, 366, 10850, 11,
  281, 512, 8396, 11, 264, 7712, 300, 291, 2809, 380, 312, 4571, 11, 51592], "temperature":
  0.0, "avg_logprob": -0.2441332222211479, "compression_ratio": 1.6554621848739495,
  "no_speech_prob": 0.01411152258515358}, {"id": 210, "seek": 142216, "start": 1422.72,
  "end": 1428.64, "text": " you know, where do you start? Like, let''s say you have
  some data, right? You have some click logs", "tokens": [50392, 291, 458, 11, 689,
  360, 291, 722, 30, 1743, 11, 718, 311, 584, 291, 362, 512, 1412, 11, 558, 30, 509,
  362, 512, 2052, 20820, 50688], "temperature": 0.0, "avg_logprob": -0.155118648822491,
  "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.025838572531938553},
  {"id": 211, "seek": 142216, "start": 1430.4, "end": 1435.44, "text": " within your
  organization or maybe you found some data set. Where do you start? How do you go
  about", "tokens": [50776, 1951, 428, 4475, 420, 1310, 291, 1352, 512, 1412, 992,
  13, 2305, 360, 291, 722, 30, 1012, 360, 291, 352, 466, 51028], "temperature": 0.0,
  "avg_logprob": -0.155118648822491, "compression_ratio": 1.7162162162162162, "no_speech_prob":
  0.025838572531938553}, {"id": 212, "seek": 142216, "start": 1435.44, "end": 1441.44,
  "text": " dissecting that data set? What do you do with it as next steps and what
  to avoid maybe and", "tokens": [51028, 48332, 278, 300, 1412, 992, 30, 708, 360,
  291, 360, 365, 309, 382, 958, 4439, 293, 437, 281, 5042, 1310, 293, 51328], "temperature":
  0.0, "avg_logprob": -0.155118648822491, "compression_ratio": 1.7162162162162162,
  "no_speech_prob": 0.025838572531938553}, {"id": 213, "seek": 142216, "start": 1442.3200000000002,
  "end": 1448.5600000000002, "text": " what good things to know to keep in mind? Yeah,
  I mean, I think, you know, first off, I mean,", "tokens": [51372, 437, 665, 721,
  281, 458, 281, 1066, 294, 1575, 30, 865, 11, 286, 914, 11, 286, 519, 11, 291, 458,
  11, 700, 766, 11, 286, 914, 11, 51684], "temperature": 0.0, "avg_logprob": -0.155118648822491,
  "compression_ratio": 1.7162162162162162, "no_speech_prob": 0.025838572531938553},
  {"id": 214, "seek": 144856, "start": 1449.52, "end": 1454.24, "text": " a lot of
  companies aren''t even at all that great at actually collecting and managing their
  query", "tokens": [50412, 257, 688, 295, 3431, 3212, 380, 754, 412, 439, 300, 869,
  412, 767, 12510, 293, 11642, 641, 14581, 50648], "temperature": 0.0, "avg_logprob":
  -0.08285161150180227, "compression_ratio": 1.7375886524822695, "no_speech_prob":
  0.014973311685025692}, {"id": 215, "seek": 144856, "start": 1454.24, "end": 1460.24,
  "text": " logs, right? So if you''re, if you''ve got a search engine up and running
  and you want to improve it,", "tokens": [50648, 20820, 11, 558, 30, 407, 498, 291,
  434, 11, 498, 291, 600, 658, 257, 3164, 2848, 493, 293, 2614, 293, 291, 528, 281,
  3470, 309, 11, 50948], "temperature": 0.0, "avg_logprob": -0.08285161150180227,
  "compression_ratio": 1.7375886524822695, "no_speech_prob": 0.014973311685025692},
  {"id": 216, "seek": 144856, "start": 1460.24, "end": 1463.52, "text": " I mean,
  I think the first thing you have to start to do is again, it kind of goes back to
  this", "tokens": [50948, 286, 914, 11, 286, 519, 264, 700, 551, 291, 362, 281, 722,
  281, 360, 307, 797, 11, 309, 733, 295, 1709, 646, 281, 341, 51112], "temperature":
  0.0, "avg_logprob": -0.08285161150180227, "compression_ratio": 1.7375886524822695,
  "no_speech_prob": 0.014973311685025692}, {"id": 217, "seek": 144856, "start": 1463.52,
  "end": 1470.0, "text": " first principles. Like, if I''m not measuring things that
  help me understand what users are doing,", "tokens": [51112, 700, 9156, 13, 1743,
  11, 498, 286, 478, 406, 13389, 721, 300, 854, 385, 1223, 437, 5022, 366, 884, 11,
  51436], "temperature": 0.0, "avg_logprob": -0.08285161150180227, "compression_ratio":
  1.7375886524822695, "no_speech_prob": 0.014973311685025692}, {"id": 218, "seek":
  144856, "start": 1470.0, "end": 1474.96, "text": " and that''s the first step, right?
  Like, make sure you''re able to process your query logs and capture", "tokens":
  [51436, 293, 300, 311, 264, 700, 1823, 11, 558, 30, 1743, 11, 652, 988, 291, 434,
  1075, 281, 1399, 428, 14581, 20820, 293, 7983, 51684], "temperature": 0.0, "avg_logprob":
  -0.08285161150180227, "compression_ratio": 1.7375886524822695, "no_speech_prob":
  0.014973311685025692}, {"id": 219, "seek": 147496, "start": 1474.96, "end": 1480.96,
  "text": " things like session history and what users clicked on, what they saw.
  A lot of companies will only", "tokens": [50364, 721, 411, 5481, 2503, 293, 437,
  5022, 23370, 322, 11, 437, 436, 1866, 13, 316, 688, 295, 3431, 486, 787, 50664],
  "temperature": 0.0, "avg_logprob": -0.0838387648264567, "compression_ratio": 1.8458646616541354,
  "no_speech_prob": 0.015847446396946907}, {"id": 220, "seek": 147496, "start": 1480.96,
  "end": 1487.04, "text": " measure what was clicked on, but they actually don''t
  measure what was seen by the user or at least", "tokens": [50664, 3481, 437, 390,
  23370, 322, 11, 457, 436, 767, 500, 380, 3481, 437, 390, 1612, 538, 264, 4195, 420,
  412, 1935, 50968], "temperature": 0.0, "avg_logprob": -0.0838387648264567, "compression_ratio":
  1.8458646616541354, "no_speech_prob": 0.015847446396946907}, {"id": 221, "seek":
  147496, "start": 1487.04, "end": 1492.64, "text": " inferred to be seen by the user.
  And that can be a big loss because a lot of these machine learning", "tokens": [50968,
  13596, 986, 281, 312, 1612, 538, 264, 4195, 13, 400, 300, 393, 312, 257, 955, 4470,
  570, 257, 688, 295, 613, 3479, 2539, 51248], "temperature": 0.0, "avg_logprob":
  -0.0838387648264567, "compression_ratio": 1.8458646616541354, "no_speech_prob":
  0.015847446396946907}, {"id": 222, "seek": 147496, "start": 1492.64, "end": 1499.28,
  "text": " systems, you need to know what wasn''t chosen just as much as you need
  to know what was chosen,", "tokens": [51248, 3652, 11, 291, 643, 281, 458, 437,
  2067, 380, 8614, 445, 382, 709, 382, 291, 643, 281, 458, 437, 390, 8614, 11, 51580],
  "temperature": 0.0, "avg_logprob": -0.0838387648264567, "compression_ratio": 1.8458646616541354,
  "no_speech_prob": 0.015847446396946907}, {"id": 223, "seek": 147496, "start": 1499.28,
  "end": 1504.0, "text": " right? So really make sure you''ve got the instrumentation
  of your system in place. And guess what?", "tokens": [51580, 558, 30, 407, 534,
  652, 988, 291, 600, 658, 264, 7198, 399, 295, 428, 1185, 294, 1081, 13, 400, 2041,
  437, 30, 51816], "temperature": 0.0, "avg_logprob": -0.0838387648264567, "compression_ratio":
  1.8458646616541354, "no_speech_prob": 0.015847446396946907}, {"id": 224, "seek":
  150400, "start": 1504.0, "end": 1510.0, "text": " A search engine is a great place
  to store all of that data as well, right? As elastic as proven out", "tokens": [50364,
  316, 3164, 2848, 307, 257, 869, 1081, 281, 3531, 439, 295, 300, 1412, 382, 731,
  11, 558, 30, 1018, 17115, 382, 12785, 484, 50664], "temperature": 0.0, "avg_logprob":
  -0.10025016784667969, "compression_ratio": 1.7259786476868328, "no_speech_prob":
  0.002184606157243252}, {"id": 225, "seek": 150400, "start": 1510.0, "end": 1515.76,
  "text": " with their using search for logs and spawn as well, right? And so make
  sure you''re captioning all", "tokens": [50664, 365, 641, 1228, 3164, 337, 20820,
  293, 17088, 382, 731, 11, 558, 30, 400, 370, 652, 988, 291, 434, 31974, 278, 439,
  50952], "temperature": 0.0, "avg_logprob": -0.10025016784667969, "compression_ratio":
  1.7259786476868328, "no_speech_prob": 0.002184606157243252}, {"id": 226, "seek":
  150400, "start": 1515.76, "end": 1520.32, "text": " that stuff. And then again,
  I think this is where your intuition starts to come in. So whenever I get", "tokens":
  [50952, 300, 1507, 13, 400, 550, 797, 11, 286, 519, 341, 307, 689, 428, 24002, 3719,
  281, 808, 294, 13, 407, 5699, 286, 483, 51180], "temperature": 0.0, "avg_logprob":
  -0.10025016784667969, "compression_ratio": 1.7259786476868328, "no_speech_prob":
  0.002184606157243252}, {"id": 227, "seek": 150400, "start": 1520.32, "end": 1526.64,
  "text": " a new data set, a new set of click logs, I start to look at, well, what
  are my most popular queries?", "tokens": [51180, 257, 777, 1412, 992, 11, 257, 777,
  992, 295, 2052, 20820, 11, 286, 722, 281, 574, 412, 11, 731, 11, 437, 366, 452,
  881, 3743, 24109, 30, 51496], "temperature": 0.0, "avg_logprob": -0.10025016784667969,
  "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.002184606157243252},
  {"id": 228, "seek": 150400, "start": 1526.64, "end": 1532.56, "text": " What are
  users asking today? What are they asking overall? What led to zero results?", "tokens":
  [51496, 708, 366, 5022, 3365, 965, 30, 708, 366, 436, 3365, 4787, 30, 708, 4684,
  281, 4018, 3542, 30, 51792], "temperature": 0.0, "avg_logprob": -0.10025016784667969,
  "compression_ratio": 1.7259786476868328, "no_speech_prob": 0.002184606157243252},
  {"id": 229, "seek": 153256, "start": 1533.36, "end": 1538.72, "text": " How often
  are they rewriting their queries like they typed in a query and then they", "tokens":
  [50404, 1012, 2049, 366, 436, 319, 19868, 641, 24109, 411, 436, 33941, 294, 257,
  14581, 293, 550, 436, 50672], "temperature": 0.0, "avg_logprob": -0.13262417080166103,
  "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.003400468034669757},
  {"id": 230, "seek": 153256, "start": 1538.72, "end": 1543.2, "text": " didn''t like
  the results. So they rewrote it. You know, all of these things are pretty easily",
  "tokens": [50672, 994, 380, 411, 264, 3542, 13, 407, 436, 319, 7449, 1370, 309,
  13, 509, 458, 11, 439, 295, 613, 721, 366, 1238, 3612, 50896], "temperature": 0.0,
  "avg_logprob": -0.13262417080166103, "compression_ratio": 1.7196969696969697, "no_speech_prob":
  0.003400468034669757}, {"id": 231, "seek": 153256, "start": 1543.2, "end": 1548.3999999999999,
  "text": " discoverable in query logs, right? So just start digging in and building
  some intuition", "tokens": [50896, 4411, 712, 294, 14581, 20820, 11, 558, 30, 407,
  445, 722, 17343, 294, 293, 2390, 512, 24002, 51156], "temperature": 0.0, "avg_logprob":
  -0.13262417080166103, "compression_ratio": 1.7196969696969697, "no_speech_prob":
  0.003400468034669757}, {"id": 232, "seek": 153256, "start": 1549.2, "end": 1553.52,
  "text": " for those things. So for instance, one of the things when I was back at
  Lucidworks that we would", "tokens": [51196, 337, 729, 721, 13, 407, 337, 5197,
  11, 472, 295, 264, 721, 562, 286, 390, 646, 412, 9593, 327, 18357, 300, 321, 576,
  51412], "temperature": 0.0, "avg_logprob": -0.13262417080166103, "compression_ratio":
  1.7196969696969697, "no_speech_prob": 0.003400468034669757}, {"id": 233, "seek":
  153256, "start": 1554.1599999999999, "end": 1560.1599999999999, "text": " do is
  what we call like head tail analysis or long tail analysis is another thing you
  see in", "tokens": [51444, 360, 307, 437, 321, 818, 411, 1378, 6838, 5215, 420,
  938, 6838, 5215, 307, 1071, 551, 291, 536, 294, 51744], "temperature": 0.0, "avg_logprob":
  -0.13262417080166103, "compression_ratio": 1.7196969696969697, "no_speech_prob":
  0.003400468034669757}, {"id": 234, "seek": 156016, "start": 1560.16, "end": 1564.72,
  "text": " the literature, you know, especially in the e-commerce world where you
  have this power law", "tokens": [50364, 264, 10394, 11, 291, 458, 11, 2318, 294,
  264, 308, 12, 26926, 1002, 689, 291, 362, 341, 1347, 2101, 50592], "temperature":
  0.0, "avg_logprob": -0.11133351486720396, "compression_ratio": 1.695067264573991,
  "no_speech_prob": 0.00028587476117536426}, {"id": 235, "seek": 156016, "start":
  1564.72, "end": 1570.72, "text": " distribution where most people ask the same things
  over and over, but you often have a really", "tokens": [50592, 7316, 689, 881, 561,
  1029, 264, 912, 721, 670, 293, 670, 11, 457, 291, 2049, 362, 257, 534, 50892], "temperature":
  0.0, "avg_logprob": -0.11133351486720396, "compression_ratio": 1.695067264573991,
  "no_speech_prob": 0.00028587476117536426}, {"id": 236, "seek": 156016, "start":
  1570.72, "end": 1576.5600000000002, "text": " long tail. When you analyze the long
  tail in a lot of e-commerce situations, what you often find,", "tokens": [50892,
  938, 6838, 13, 1133, 291, 12477, 264, 938, 6838, 294, 257, 688, 295, 308, 12, 26926,
  6851, 11, 437, 291, 2049, 915, 11, 51184], "temperature": 0.0, "avg_logprob": -0.11133351486720396,
  "compression_ratio": 1.695067264573991, "no_speech_prob": 0.00028587476117536426},
  {"id": 237, "seek": 156016, "start": 1576.5600000000002, "end": 1582.96, "text":
  " for instance, is the long tail is actually pretty highly correlated to the head
  queries, right?", "tokens": [51184, 337, 5197, 11, 307, 264, 938, 6838, 307, 767,
  1238, 5405, 38574, 281, 264, 1378, 24109, 11, 558, 30, 51504], "temperature": 0.0,
  "avg_logprob": -0.11133351486720396, "compression_ratio": 1.695067264573991, "no_speech_prob":
  0.00028587476117536426}, {"id": 238, "seek": 158296, "start": 1583.04, "end": 1588.48,
  "text": " And so developing that intuition of like, you know, why are these long
  tail queries", "tokens": [50368, 400, 370, 6416, 300, 24002, 295, 411, 11, 291,
  458, 11, 983, 366, 613, 938, 6838, 24109, 50640], "temperature": 0.0, "avg_logprob":
  -0.09535997564142401, "compression_ratio": 1.6755555555555555, "no_speech_prob":
  0.0027663810178637505}, {"id": 239, "seek": 158296, "start": 1590.8, "end": 1597.52,
  "text": " working or not working? That can then help you do much better at all of
  your queries, right?", "tokens": [50756, 1364, 420, 406, 1364, 30, 663, 393, 550,
  854, 291, 360, 709, 1101, 412, 439, 295, 428, 24109, 11, 558, 30, 51092], "temperature":
  0.0, "avg_logprob": -0.09535997564142401, "compression_ratio": 1.6755555555555555,
  "no_speech_prob": 0.0027663810178637505}, {"id": 240, "seek": 158296, "start": 1597.52,
  "end": 1602.16, "text": " And so, you know, from those click logs, then you start
  to focus on, well, how do I improve my head", "tokens": [51092, 400, 370, 11, 291,
  458, 11, 490, 729, 2052, 20820, 11, 550, 291, 722, 281, 1879, 322, 11, 731, 11,
  577, 360, 286, 3470, 452, 1378, 51324], "temperature": 0.0, "avg_logprob": -0.09535997564142401,
  "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.0027663810178637505},
  {"id": 241, "seek": 158296, "start": 1602.16, "end": 1608.24, "text": " or my torso
  queries, like the ones that are most common? And then as you go on, then you can
  look at", "tokens": [51324, 420, 452, 34917, 24109, 11, 411, 264, 2306, 300, 366,
  881, 2689, 30, 400, 550, 382, 291, 352, 322, 11, 550, 291, 393, 574, 412, 51628],
  "temperature": 0.0, "avg_logprob": -0.09535997564142401, "compression_ratio": 1.6755555555555555,
  "no_speech_prob": 0.0027663810178637505}, {"id": 242, "seek": 160824, "start": 1608.32,
  "end": 1615.68, "text": " how do I handle long tail queries depending on how important
  they are to you? You know, and from", "tokens": [50368, 577, 360, 286, 4813, 938,
  6838, 24109, 5413, 322, 577, 1021, 436, 366, 281, 291, 30, 509, 458, 11, 293, 490,
  50736], "temperature": 0.0, "avg_logprob": -0.1362801153682968, "compression_ratio":
  1.6470588235294117, "no_speech_prob": 0.0006714849732816219}, {"id": 243, "seek":
  160824, "start": 1615.68, "end": 1620.72, "text": " from that click log, then you
  can start to build either, you know, in some cases, you still might", "tokens":
  [50736, 490, 300, 2052, 3565, 11, 550, 291, 393, 722, 281, 1322, 2139, 11, 291,
  458, 11, 294, 512, 3331, 11, 291, 920, 1062, 50988], "temperature": 0.0, "avg_logprob":
  -0.1362801153682968, "compression_ratio": 1.6470588235294117, "no_speech_prob":
  0.0006714849732816219}, {"id": 244, "seek": 160824, "start": 1620.72, "end": 1627.04,
  "text": " make sense for you to have rules. And then, and then you can also look
  at, you know, like again,", "tokens": [50988, 652, 2020, 337, 291, 281, 362, 4474,
  13, 400, 550, 11, 293, 550, 291, 393, 611, 574, 412, 11, 291, 458, 11, 411, 797,
  11, 51304], "temperature": 0.0, "avg_logprob": -0.1362801153682968, "compression_ratio":
  1.6470588235294117, "no_speech_prob": 0.0006714849732816219}, {"id": 245, "seek":
  160824, "start": 1627.04, "end": 1632.32, "text": " like I would try to look at
  it the problem holistically, what''s going to get me the most bang for my", "tokens":
  [51304, 411, 286, 576, 853, 281, 574, 412, 309, 264, 1154, 4091, 20458, 11, 437,
  311, 516, 281, 483, 385, 264, 881, 8550, 337, 452, 51568], "temperature": 0.0, "avg_logprob":
  -0.1362801153682968, "compression_ratio": 1.6470588235294117, "no_speech_prob":
  0.0006714849732816219}, {"id": 246, "seek": 163232, "start": 1632.32, "end": 1639.4399999999998,
  "text": " buck in terms of where I should spend my time, right? So in the short
  run, rules are probably", "tokens": [50364, 14894, 294, 2115, 295, 689, 286, 820,
  3496, 452, 565, 11, 558, 30, 407, 294, 264, 2099, 1190, 11, 4474, 366, 1391, 50720],
  "temperature": 0.0, "avg_logprob": -0.10916797560874862, "compression_ratio": 1.6952789699570816,
  "no_speech_prob": 0.002710397122427821}, {"id": 247, "seek": 163232, "start": 1639.4399999999998,
  "end": 1646.1599999999999, "text": " easier, but they''re harder to maintain in
  the long run. And of course, you can only manage so many", "tokens": [50720, 3571,
  11, 457, 436, 434, 6081, 281, 6909, 294, 264, 938, 1190, 13, 400, 295, 1164, 11,
  291, 393, 787, 3067, 370, 867, 51056], "temperature": 0.0, "avg_logprob": -0.10916797560874862,
  "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.002710397122427821},
  {"id": 248, "seek": 163232, "start": 1646.1599999999999, "end": 1652.3999999999999,
  "text": " rules on your own and, you know, even with several people, whereas machine
  learning may take more work", "tokens": [51056, 4474, 322, 428, 1065, 293, 11, 291,
  458, 11, 754, 365, 2940, 561, 11, 9735, 3479, 2539, 815, 747, 544, 589, 51368],
  "temperature": 0.0, "avg_logprob": -0.10916797560874862, "compression_ratio": 1.6952789699570816,
  "no_speech_prob": 0.002710397122427821}, {"id": 249, "seek": 163232, "start": 1652.3999999999999,
  "end": 1658.8, "text": " up front, but in the long run is probably easier to maintain.
  Although I do still wonder, you know,", "tokens": [51368, 493, 1868, 11, 457, 294,
  264, 938, 1190, 307, 1391, 3571, 281, 6909, 13, 5780, 286, 360, 920, 2441, 11, 291,
  458, 11, 51688], "temperature": 0.0, "avg_logprob": -0.10916797560874862, "compression_ratio":
  1.6952789699570816, "no_speech_prob": 0.002710397122427821}, {"id": 250, "seek":
  165880, "start": 1658.8, "end": 1662.8, "text": " if we''re going to run into the
  same kind of problems we have with rules with machine learning", "tokens": [50364,
  498, 321, 434, 516, 281, 1190, 666, 264, 912, 733, 295, 2740, 321, 362, 365, 4474,
  365, 3479, 2539, 50564], "temperature": 0.0, "avg_logprob": -0.08125195679841218,
  "compression_ratio": 1.8646616541353382, "no_speech_prob": 0.0008244350901804864},
  {"id": 251, "seek": 165880, "start": 1662.8, "end": 1668.24, "text": " models where
  we have so many different models that are being applied and they''re built by different",
  "tokens": [50564, 5245, 689, 321, 362, 370, 867, 819, 5245, 300, 366, 885, 6456,
  293, 436, 434, 3094, 538, 819, 50836], "temperature": 0.0, "avg_logprob": -0.08125195679841218,
  "compression_ratio": 1.8646616541353382, "no_speech_prob": 0.0008244350901804864},
  {"id": 252, "seek": 165880, "start": 1668.24, "end": 1674.8, "text": " teams and
  they''re applied in different scenarios. And, and next thing you know, you have
  a complexity", "tokens": [50836, 5491, 293, 436, 434, 6456, 294, 819, 15077, 13,
  400, 11, 293, 958, 551, 291, 458, 11, 291, 362, 257, 14024, 51164], "temperature":
  0.0, "avg_logprob": -0.08125195679841218, "compression_ratio": 1.8646616541353382,
  "no_speech_prob": 0.0008244350901804864}, {"id": 253, "seek": 165880, "start": 1674.8,
  "end": 1680.56, "text": " problem on that front as well. But, you know, luckily,
  like with things like machine learning operations", "tokens": [51164, 1154, 322,
  300, 1868, 382, 731, 13, 583, 11, 291, 458, 11, 22880, 11, 411, 365, 721, 411, 3479,
  2539, 7705, 51452], "temperature": 0.0, "avg_logprob": -0.08125195679841218, "compression_ratio":
  1.8646616541353382, "no_speech_prob": 0.0008244350901804864}, {"id": 254, "seek":
  165880, "start": 1680.56, "end": 1687.84, "text": " becoming more of a focus and
  people getting much more rigorous about how they deploy and manage", "tokens": [51452,
  5617, 544, 295, 257, 1879, 293, 561, 1242, 709, 544, 29882, 466, 577, 436, 7274,
  293, 3067, 51816], "temperature": 0.0, "avg_logprob": -0.08125195679841218, "compression_ratio":
  1.8646616541353382, "no_speech_prob": 0.0008244350901804864}, {"id": 255, "seek":
  168784, "start": 1687.84, "end": 1693.1999999999998, "text": " models, I think most
  of those problems will be mitigated in one run, but it still goes back to", "tokens":
  [50364, 5245, 11, 286, 519, 881, 295, 729, 2740, 486, 312, 15699, 770, 294, 472,
  1190, 11, 457, 309, 920, 1709, 646, 281, 50632], "temperature": 0.0, "avg_logprob":
  -0.14867196083068848, "compression_ratio": 1.7607142857142857, "no_speech_prob":
  0.0012617834145203233}, {"id": 256, "seek": 168784, "start": 1693.1999999999998,
  "end": 1700.6399999999999, "text": " the same core principles, which you need to
  have good housekeeping in order to be successful both with", "tokens": [50632, 264,
  912, 4965, 9156, 11, 597, 291, 643, 281, 362, 665, 48033, 294, 1668, 281, 312, 4406,
  1293, 365, 51004], "temperature": 0.0, "avg_logprob": -0.14867196083068848, "compression_ratio":
  1.7607142857142857, "no_speech_prob": 0.0012617834145203233}, {"id": 257, "seek":
  168784, "start": 1700.6399999999999, "end": 1705.6799999999998, "text": " rules
  and with machine learning models. I don''t know if that that was kind of long wind.
  I don''t", "tokens": [51004, 4474, 293, 365, 3479, 2539, 5245, 13, 286, 500, 380,
  458, 498, 300, 300, 390, 733, 295, 938, 2468, 13, 286, 500, 380, 51256], "temperature":
  0.0, "avg_logprob": -0.14867196083068848, "compression_ratio": 1.7607142857142857,
  "no_speech_prob": 0.0012617834145203233}, {"id": 258, "seek": 168784, "start": 1705.6799999999998,
  "end": 1710.72, "text": " know if that answered the question or not. It does, it
  does. I mean, it gives the intuition, especially", "tokens": [51256, 458, 498, 300,
  10103, 264, 1168, 420, 406, 13, 467, 775, 11, 309, 775, 13, 286, 914, 11, 309, 2709,
  264, 24002, 11, 2318, 51508], "temperature": 0.0, "avg_logprob": -0.14867196083068848,
  "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0012617834145203233},
  {"id": 259, "seek": 168784, "start": 1710.72, "end": 1716.08, "text": " where you
  said the connection between, you know, that that was an insight actually to me,
  like", "tokens": [51508, 689, 291, 848, 264, 4984, 1296, 11, 291, 458, 11, 300,
  300, 390, 364, 11269, 767, 281, 385, 11, 411, 51776], "temperature": 0.0, "avg_logprob":
  -0.14867196083068848, "compression_ratio": 1.7607142857142857, "no_speech_prob":
  0.0012617834145203233}, {"id": 260, "seek": 171608, "start": 1716.72, "end": 1722.8799999999999,
  "text": " the connection between head and tail that 50% of tail may correlate with
  your head. And that''s", "tokens": [50396, 264, 4984, 1296, 1378, 293, 6838, 300,
  2625, 4, 295, 6838, 815, 48742, 365, 428, 1378, 13, 400, 300, 311, 50704], "temperature":
  0.0, "avg_logprob": -0.19300092435350605, "compression_ratio": 1.5901639344262295,
  "no_speech_prob": 0.0053496211767196655}, {"id": 261, "seek": 171608, "start": 1722.8799999999999,
  "end": 1729.1999999999998, "text": " amazing. Like 50% of this super hard queries
  could be kind of, you know, removed from that complexity", "tokens": [50704, 2243,
  13, 1743, 2625, 4, 295, 341, 1687, 1152, 24109, 727, 312, 733, 295, 11, 291, 458,
  11, 7261, 490, 300, 14024, 51020], "temperature": 0.0, "avg_logprob": -0.19300092435350605,
  "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0053496211767196655},
  {"id": 262, "seek": 171608, "start": 1729.1999999999998, "end": 1734.6399999999999,
  "text": " space, right? Which is, again, you know, your mileage may vary, right?
  Like it depends on your", "tokens": [51020, 1901, 11, 558, 30, 3013, 307, 11, 797,
  11, 291, 458, 11, 428, 43121, 815, 10559, 11, 558, 30, 1743, 309, 5946, 322, 428,
  51292], "temperature": 0.0, "avg_logprob": -0.19300092435350605, "compression_ratio":
  1.5901639344262295, "no_speech_prob": 0.0053496211767196655}, {"id": 263, "seek":
  171608, "start": 1734.6399999999999, "end": 1741.28, "text": " data set in Europe,
  but you know, like in e-commerce, right? If if I phone 13 or whatever is the", "tokens":
  [51292, 1412, 992, 294, 3315, 11, 457, 291, 458, 11, 411, 294, 308, 12, 26926, 11,
  558, 30, 759, 498, 286, 2593, 3705, 420, 2035, 307, 264, 51624], "temperature":
  0.0, "avg_logprob": -0.19300092435350605, "compression_ratio": 1.5901639344262295,
  "no_speech_prob": 0.0053496211767196655}, {"id": 264, "seek": 174128, "start": 1741.28,
  "end": 1750.16, "text": " head query, there''s probably a tail query that''s, you
  know, silver 64 gigabyte iPhone 13 with case,", "tokens": [50364, 1378, 14581, 11,
  456, 311, 1391, 257, 6838, 14581, 300, 311, 11, 291, 458, 11, 8753, 12145, 8741,
  34529, 7252, 3705, 365, 1389, 11, 50808], "temperature": 0.0, "avg_logprob": -0.10392130264128098,
  "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0016816583229228854},
  {"id": 265, "seek": 174128, "start": 1750.16, "end": 1755.28, "text": " right? Like
  that''s probably a tail query or at least a torso query. And once you have those
  types", "tokens": [50808, 558, 30, 1743, 300, 311, 1391, 257, 6838, 14581, 420,
  412, 1935, 257, 34917, 14581, 13, 400, 1564, 291, 362, 729, 3467, 51064], "temperature":
  0.0, "avg_logprob": -0.10392130264128098, "compression_ratio": 1.7212389380530972,
  "no_speech_prob": 0.0016816583229228854}, {"id": 266, "seek": 174128, "start": 1755.28,
  "end": 1760.72, "text": " of realizations, you can start to link these up. And then
  the cool thing really is that then", "tokens": [51064, 295, 957, 14455, 11, 291,
  393, 722, 281, 2113, 613, 493, 13, 400, 550, 264, 1627, 551, 534, 307, 300, 550,
  51336], "temperature": 0.0, "avg_logprob": -0.10392130264128098, "compression_ratio":
  1.7212389380530972, "no_speech_prob": 0.0016816583229228854}, {"id": 267, "seek":
  174128, "start": 1762.16, "end": 1768.3999999999999, "text": " the things you know
  about the head can apply to those types of tail queries as well. And so you''re",
  "tokens": [51408, 264, 721, 291, 458, 466, 264, 1378, 393, 3079, 281, 729, 3467,
  295, 6838, 24109, 382, 731, 13, 400, 370, 291, 434, 51720], "temperature": 0.0,
  "avg_logprob": -0.10392130264128098, "compression_ratio": 1.7212389380530972, "no_speech_prob":
  0.0016816583229228854}, {"id": 268, "seek": 176840, "start": 1768.4, "end": 1774.64,
  "text": " actually, you might be able to more effectively manage those tail queries,
  even without machine learning", "tokens": [50364, 767, 11, 291, 1062, 312, 1075,
  281, 544, 8659, 3067, 729, 6838, 24109, 11, 754, 1553, 3479, 2539, 50676], "temperature":
  0.0, "avg_logprob": -0.12994774266293174, "compression_ratio": 1.5793650793650793,
  "no_speech_prob": 0.0014774148585274816}, {"id": 269, "seek": 176840, "start": 1774.64,
  "end": 1780.88, "text": " models. Yeah, absolutely. And just a quick reminder to
  our respected audience, feel free to send", "tokens": [50676, 5245, 13, 865, 11,
  3122, 13, 400, 445, 257, 1702, 13548, 281, 527, 20020, 4034, 11, 841, 1737, 281,
  2845, 50988], "temperature": 0.0, "avg_logprob": -0.12994774266293174, "compression_ratio":
  1.5793650793650793, "no_speech_prob": 0.0014774148585274816}, {"id": 270, "seek":
  176840, "start": 1780.88, "end": 1786.0800000000002, "text": " your questions. Otherwise,
  I will ask all the questions myself, which, which of course I have, but,", "tokens":
  [50988, 428, 1651, 13, 10328, 11, 286, 486, 1029, 439, 264, 1651, 2059, 11, 597,
  11, 597, 295, 1164, 286, 362, 11, 457, 11, 51248], "temperature": 0.0, "avg_logprob":
  -0.12994774266293174, "compression_ratio": 1.5793650793650793, "no_speech_prob":
  0.0014774148585274816}, {"id": 271, "seek": 176840, "start": 1786.0800000000002,
  "end": 1792.0, "text": " you know, I''m sure you guys have guys and girls. I''m
  sure you have some interesting cases. We do", "tokens": [51248, 291, 458, 11, 286,
  478, 988, 291, 1074, 362, 1074, 293, 4519, 13, 286, 478, 988, 291, 362, 512, 1880,
  3331, 13, 492, 360, 51544], "temperature": 0.0, "avg_logprob": -0.12994774266293174,
  "compression_ratio": 1.5793650793650793, "no_speech_prob": 0.0014774148585274816},
  {"id": 272, "seek": 179200, "start": 1792.0, "end": 1796.24, "text": " get a few
  questions already, but we will we''ll answer them in the end of this session.",
  "tokens": [50364, 483, 257, 1326, 1651, 1217, 11, 457, 321, 486, 321, 603, 1867,
  552, 294, 264, 917, 295, 341, 5481, 13, 50576], "temperature": 0.0, "avg_logprob":
  -0.16335716454879098, "compression_ratio": 1.6849315068493151, "no_speech_prob":
  0.004379446152597666}, {"id": 273, "seek": 179200, "start": 1797.44, "end": 1804.24,
  "text": " And couple coupling, you know, that process of sort of, you know, crafting
  the signals and", "tokens": [50636, 400, 1916, 37447, 11, 291, 458, 11, 300, 1399,
  295, 1333, 295, 11, 291, 458, 11, 29048, 264, 12354, 293, 50976], "temperature":
  0.0, "avg_logprob": -0.16335716454879098, "compression_ratio": 1.6849315068493151,
  "no_speech_prob": 0.004379446152597666}, {"id": 274, "seek": 179200, "start": 1804.24,
  "end": 1811.28, "text": " training your model and deploying it and ML ops that you
  mentioned. How do you when it comes to", "tokens": [50976, 3097, 428, 2316, 293,
  34198, 309, 293, 21601, 44663, 300, 291, 2835, 13, 1012, 360, 291, 562, 309, 1487,
  281, 51328], "temperature": 0.0, "avg_logprob": -0.16335716454879098, "compression_ratio":
  1.6849315068493151, "no_speech_prob": 0.004379446152597666}, {"id": 275, "seek":
  179200, "start": 1811.28, "end": 1816.32, "text": " measurement, how do you measure?
  How do you make sure that, you know, what happens right now in", "tokens": [51328,
  13160, 11, 577, 360, 291, 3481, 30, 1012, 360, 291, 652, 988, 300, 11, 291, 458,
  11, 437, 2314, 558, 586, 294, 51580], "temperature": 0.0, "avg_logprob": -0.16335716454879098,
  "compression_ratio": 1.6849315068493151, "no_speech_prob": 0.004379446152597666},
  {"id": 276, "seek": 181632, "start": 1816.32, "end": 1822.24, "text": " production
  still makes sense that they don''t need to do any hectic action about, you know,
  okay,", "tokens": [50364, 4265, 920, 1669, 2020, 300, 436, 500, 380, 643, 281, 360,
  604, 415, 15518, 3069, 466, 11, 291, 458, 11, 1392, 11, 50660], "temperature": 0.0,
  "avg_logprob": -0.13573506537904131, "compression_ratio": 1.646808510638298, "no_speech_prob":
  0.002851012861356139}, {"id": 277, "seek": 181632, "start": 1822.24, "end": 1827.52,
  "text": " pulling the model back or something like that. What''s your sense on on
  on that front? And like,", "tokens": [50660, 8407, 264, 2316, 646, 420, 746, 411,
  300, 13, 708, 311, 428, 2020, 322, 322, 322, 300, 1868, 30, 400, 411, 11, 50924],
  "temperature": 0.0, "avg_logprob": -0.13573506537904131, "compression_ratio": 1.646808510638298,
  "no_speech_prob": 0.002851012861356139}, {"id": 278, "seek": 181632, "start": 1827.52,
  "end": 1832.48, "text": " maybe some measurements that you have deployed yourself
  and have been observing every single day", "tokens": [50924, 1310, 512, 15383, 300,
  291, 362, 17826, 1803, 293, 362, 668, 22107, 633, 2167, 786, 51172], "temperature":
  0.0, "avg_logprob": -0.13573506537904131, "compression_ratio": 1.646808510638298,
  "no_speech_prob": 0.002851012861356139}, {"id": 279, "seek": 181632, "start": 1833.2,
  "end": 1840.0, "text": " and relying on it. And again, it depends on your what,
  you know, kind of what domain you work in.", "tokens": [51208, 293, 24140, 322,
  309, 13, 400, 797, 11, 309, 5946, 322, 428, 437, 11, 291, 458, 11, 733, 295, 437,
  9274, 291, 589, 294, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13573506537904131,
  "compression_ratio": 1.646808510638298, "no_speech_prob": 0.002851012861356139},
  {"id": 280, "seek": 184000, "start": 1840.0, "end": 1846.4, "text": " But, you know,
  I mean, there''s there''s lots of literature on how to score and and, you know,",
  "tokens": [50364, 583, 11, 291, 458, 11, 286, 914, 11, 456, 311, 456, 311, 3195,
  295, 10394, 322, 577, 281, 6175, 293, 293, 11, 291, 458, 11, 50684], "temperature":
  0.0, "avg_logprob": -0.1576796220929435, "compression_ratio": 1.7638888888888888,
  "no_speech_prob": 0.0022852637339383364}, {"id": 281, "seek": 184000, "start": 1846.4,
  "end": 1851.76, "text": " test your model. So things like precision and recall where
  you''re looking at what users are", "tokens": [50684, 1500, 428, 2316, 13, 407,
  721, 411, 18356, 293, 9901, 689, 291, 434, 1237, 412, 437, 5022, 366, 50952], "temperature":
  0.0, "avg_logprob": -0.1576796220929435, "compression_ratio": 1.7638888888888888,
  "no_speech_prob": 0.0022852637339383364}, {"id": 282, "seek": 184000, "start": 1851.76,
  "end": 1858.88, "text": " clicking on and whether they''re finding the results,
  things like zero results or often one of the", "tokens": [50952, 9697, 322, 293,
  1968, 436, 434, 5006, 264, 3542, 11, 721, 411, 4018, 3542, 420, 2049, 472, 295,
  264, 51308], "temperature": 0.0, "avg_logprob": -0.1576796220929435, "compression_ratio":
  1.7638888888888888, "no_speech_prob": 0.0022852637339383364}, {"id": 283, "seek":
  184000, "start": 1858.88, "end": 1867.28, "text": " things that I find helpful is
  like what what you would call surprising results where documents are", "tokens":
  [51308, 721, 300, 286, 915, 4961, 307, 411, 437, 437, 291, 576, 818, 8830, 3542,
  689, 8512, 366, 51728], "temperature": 0.0, "avg_logprob": -0.1576796220929435,
  "compression_ratio": 1.7638888888888888, "no_speech_prob": 0.0022852637339383364},
  {"id": 284, "seek": 186728, "start": 1867.28, "end": 1873.84, "text": " occurring
  fairly high up in the results, but they''re not actually garnering the clicks that
  you", "tokens": [50364, 18386, 6457, 1090, 493, 294, 264, 3542, 11, 457, 436, 434,
  406, 767, 25067, 1794, 264, 18521, 300, 291, 50692], "temperature": 0.0, "avg_logprob":
  -0.09422235990825452, "compression_ratio": 1.6122448979591837, "no_speech_prob":
  0.002201144816353917}, {"id": 285, "seek": 186728, "start": 1873.84, "end": 1879.52,
  "text": " would expect given that position. So for instance, you know, I mean, many
  people in search understand", "tokens": [50692, 576, 2066, 2212, 300, 2535, 13,
  407, 337, 5197, 11, 291, 458, 11, 286, 914, 11, 867, 561, 294, 3164, 1223, 50976],
  "temperature": 0.0, "avg_logprob": -0.09422235990825452, "compression_ratio": 1.6122448979591837,
  "no_speech_prob": 0.002201144816353917}, {"id": 286, "seek": 186728, "start": 1879.52,
  "end": 1886.8, "text": " that there''s a position bias that''s just built into all
  of us as humans. We we trust the machine.", "tokens": [50976, 300, 456, 311, 257,
  2535, 12577, 300, 311, 445, 3094, 666, 439, 295, 505, 382, 6255, 13, 492, 321, 3361,
  264, 3479, 13, 51340], "temperature": 0.0, "avg_logprob": -0.09422235990825452,
  "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.002201144816353917},
  {"id": 287, "seek": 186728, "start": 1886.8, "end": 1893.6, "text": " And so we
  click on the first one. Well, if you if you consistently see that a document is
  appearing", "tokens": [51340, 400, 370, 321, 2052, 322, 264, 700, 472, 13, 1042,
  11, 498, 291, 498, 291, 14961, 536, 300, 257, 4166, 307, 19870, 51680], "temperature":
  0.0, "avg_logprob": -0.09422235990825452, "compression_ratio": 1.6122448979591837,
  "no_speech_prob": 0.002201144816353917}, {"id": 288, "seek": 189360, "start": 1893.6,
  "end": 1900.6399999999999, "text": " at say number one or number two in the results,
  but it''s getting way less clicks than say the six or", "tokens": [50364, 412, 584,
  1230, 472, 420, 1230, 732, 294, 264, 3542, 11, 457, 309, 311, 1242, 636, 1570, 18521,
  813, 584, 264, 2309, 420, 50716], "temperature": 0.0, "avg_logprob": -0.14063564936319986,
  "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.004283294081687927},
  {"id": 289, "seek": 189360, "start": 1900.6399999999999, "end": 1907.76, "text":
  " seventh document, that might be an indication to you that that document isn''t
  particularly relevant or", "tokens": [50716, 17875, 4166, 11, 300, 1062, 312, 364,
  18877, 281, 291, 300, 300, 4166, 1943, 380, 4098, 7340, 420, 51072], "temperature":
  0.0, "avg_logprob": -0.14063564936319986, "compression_ratio": 1.5731707317073171,
  "no_speech_prob": 0.004283294081687927}, {"id": 290, "seek": 189360, "start": 1907.76,
  "end": 1913.84, "text": " for whatever reasons users aren''t liking it. So those
  kinds of more subtle metrics can also be", "tokens": [51072, 337, 2035, 4112, 5022,
  3212, 380, 16933, 309, 13, 407, 729, 3685, 295, 544, 13743, 16367, 393, 611, 312,
  51376], "temperature": 0.0, "avg_logprob": -0.14063564936319986, "compression_ratio":
  1.5731707317073171, "no_speech_prob": 0.004283294081687927}, {"id": 291, "seek":
  189360, "start": 1913.84, "end": 1921.6799999999998, "text": " informative. I think,
  you know, if you have a AB experiment, testing framework in place,", "tokens": [51376,
  27759, 13, 286, 519, 11, 291, 458, 11, 498, 291, 362, 257, 13838, 5120, 11, 4997,
  8388, 294, 1081, 11, 51768], "temperature": 0.0, "avg_logprob": -0.14063564936319986,
  "compression_ratio": 1.5731707317073171, "no_speech_prob": 0.004283294081687927},
  {"id": 292, "seek": 192168, "start": 1921.68, "end": 1927.52, "text": " obviously
  you can do all of your metrics around AB testing, you know, start with just giving",
  "tokens": [50364, 2745, 291, 393, 360, 439, 295, 428, 16367, 926, 13838, 4997, 11,
  291, 458, 11, 722, 365, 445, 2902, 50656], "temperature": 0.0, "avg_logprob": -0.13764904936154684,
  "compression_ratio": 1.7074235807860263, "no_speech_prob": 0.0021812322083860636},
  {"id": 293, "seek": 192168, "start": 1927.52, "end": 1933.92, "text": " a certain
  amount of traffic to your new approach and then ramping up as it meets your metrics,",
  "tokens": [50656, 257, 1629, 2372, 295, 6419, 281, 428, 777, 3109, 293, 550, 12428,
  278, 493, 382, 309, 13961, 428, 16367, 11, 50976], "temperature": 0.0, "avg_logprob":
  -0.13764904936154684, "compression_ratio": 1.7074235807860263, "no_speech_prob":
  0.0021812322083860636}, {"id": 294, "seek": 192168, "start": 1933.92, "end": 1939.76,
  "text": " whatever that is or what, you know, your targets are if that''s things
  like add the cards, etc. You can", "tokens": [50976, 2035, 300, 307, 420, 437, 11,
  291, 458, 11, 428, 12911, 366, 498, 300, 311, 721, 411, 909, 264, 5632, 11, 5183,
  13, 509, 393, 51268], "temperature": 0.0, "avg_logprob": -0.13764904936154684, "compression_ratio":
  1.7074235807860263, "no_speech_prob": 0.0021812322083860636}, {"id": 295, "seek":
  192168, "start": 1939.76, "end": 1948.24, "text": " ramp up those those types of
  tests as you as it proves out. There''s obviously there''s things you can", "tokens":
  [51268, 12428, 493, 729, 729, 3467, 295, 6921, 382, 291, 382, 309, 25019, 484, 13,
  821, 311, 2745, 456, 311, 721, 291, 393, 51692], "temperature": 0.0, "avg_logprob":
  -0.13764904936154684, "compression_ratio": 1.7074235807860263, "no_speech_prob":
  0.0021812322083860636}, {"id": 296, "seek": 194824, "start": 1948.24, "end": 1955.76,
  "text": " do offline as well, like especially if you have enough query logs. And
  if your index hasn''t changed", "tokens": [50364, 360, 21857, 382, 731, 11, 411,
  2318, 498, 291, 362, 1547, 14581, 20820, 13, 400, 498, 428, 8186, 6132, 380, 3105,
  50740], "temperature": 0.0, "avg_logprob": -0.1337230470445421, "compression_ratio":
  1.5303030303030303, "no_speech_prob": 0.0010978406062349677}, {"id": 297, "seek":
  194824, "start": 1955.76, "end": 1962.88, "text": " that much, but maybe just the
  approach you''re taking has, then you can you can replay your logs, you can", "tokens":
  [50740, 300, 709, 11, 457, 1310, 445, 264, 3109, 291, 434, 1940, 575, 11, 550, 291,
  393, 291, 393, 23836, 428, 20820, 11, 291, 393, 51096], "temperature": 0.0, "avg_logprob":
  -0.1337230470445421, "compression_ratio": 1.5303030303030303, "no_speech_prob":
  0.0010978406062349677}, {"id": 298, "seek": 194824, "start": 1962.88, "end": 1970.72,
  "text": " test out and you know, effectively simulate what users might click on
  in those scenarios. And then", "tokens": [51096, 1500, 484, 293, 291, 458, 11, 8659,
  27817, 437, 5022, 1062, 2052, 322, 294, 729, 15077, 13, 400, 550, 51488], "temperature":
  0.0, "avg_logprob": -0.1337230470445421, "compression_ratio": 1.5303030303030303,
  "no_speech_prob": 0.0010978406062349677}, {"id": 299, "seek": 197072, "start": 1970.72,
  "end": 1977.44, "text": " of course there''s the old fashioned just, you know, things
  like smell tests like do these results", "tokens": [50364, 295, 1164, 456, 311,
  264, 1331, 40646, 445, 11, 291, 458, 11, 721, 411, 4316, 6921, 411, 360, 613, 3542,
  50700], "temperature": 0.0, "avg_logprob": -0.1246923848202354, "compression_ratio":
  1.6864406779661016, "no_speech_prob": 0.00042755273170769215}, {"id": 300, "seek":
  197072, "start": 1977.44, "end": 1985.28, "text": " look better to me as an expert,
  you obviously have to be careful there or to a small cohort of experts,", "tokens":
  [50700, 574, 1101, 281, 385, 382, 364, 5844, 11, 291, 2745, 362, 281, 312, 5026,
  456, 420, 281, 257, 1359, 28902, 295, 8572, 11, 51092], "temperature": 0.0, "avg_logprob":
  -0.1246923848202354, "compression_ratio": 1.6864406779661016, "no_speech_prob":
  0.00042755273170769215}, {"id": 301, "seek": 197072, "start": 1985.28, "end": 1990.56,
  "text": " you know, like maybe your colleagues, etc. might spend some time scoring.
  So all of these things,", "tokens": [51092, 291, 458, 11, 411, 1310, 428, 7734,
  11, 5183, 13, 1062, 3496, 512, 565, 22358, 13, 407, 439, 295, 613, 721, 11, 51356],
  "temperature": 0.0, "avg_logprob": -0.1246923848202354, "compression_ratio": 1.6864406779661016,
  "no_speech_prob": 0.00042755273170769215}, {"id": 302, "seek": 197072, "start":
  1990.56, "end": 1997.44, "text": " I think are techniques and measurements you can
  use to check to see whether results are, you know,", "tokens": [51356, 286, 519,
  366, 7512, 293, 15383, 291, 393, 764, 281, 1520, 281, 536, 1968, 3542, 366, 11,
  291, 458, 11, 51700], "temperature": 0.0, "avg_logprob": -0.1246923848202354, "compression_ratio":
  1.6864406779661016, "no_speech_prob": 0.00042755273170769215}, {"id": 303, "seek":
  199744, "start": 1997.52, "end": 2002.56, "text": " good enough for you them to
  go into production. I think there''s I think Ronnie, Ronnie,", "tokens": [50368,
  665, 1547, 337, 291, 552, 281, 352, 666, 4265, 13, 286, 519, 456, 311, 286, 519,
  46131, 11, 46131, 11, 50620], "temperature": 0.0, "avg_logprob": -0.1803128378731864,
  "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0014796133618801832},
  {"id": 304, "seek": 199744, "start": 2002.56, "end": 2007.6000000000001, "text":
  " co-hoved me, I forget the name of the book, but he has a really good book along
  with a co-author on", "tokens": [50620, 598, 12, 1289, 937, 385, 11, 286, 2870,
  264, 1315, 295, 264, 1446, 11, 457, 415, 575, 257, 534, 665, 1446, 2051, 365, 257,
  598, 12, 34224, 322, 50872], "temperature": 0.0, "avg_logprob": -0.1803128378731864,
  "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0014796133618801832},
  {"id": 305, "seek": 199744, "start": 2009.3600000000001, "end": 2014.64, "text":
  " online experimentation. It''s probably these days the Bible of online experimentation.
  So I would", "tokens": [50960, 2950, 37142, 13, 467, 311, 1391, 613, 1708, 264,
  6544, 295, 2950, 37142, 13, 407, 286, 576, 51224], "temperature": 0.0, "avg_logprob":
  -0.1803128378731864, "compression_ratio": 1.6842105263157894, "no_speech_prob":
  0.0014796133618801832}, {"id": 306, "seek": 199744, "start": 2014.64, "end": 2021.52,
  "text": " encourage users to check that out. And then, you know, there''s there''s
  lots of metrics that you can", "tokens": [51224, 5373, 5022, 281, 1520, 300, 484,
  13, 400, 550, 11, 291, 458, 11, 456, 311, 456, 311, 3195, 295, 16367, 300, 291,
  393, 51568], "temperature": 0.0, "avg_logprob": -0.1803128378731864, "compression_ratio":
  1.6842105263157894, "no_speech_prob": 0.0014796133618801832}, {"id": 307, "seek":
  202152, "start": 2021.52, "end": 2026.8, "text": " deploy, you know, that are pretty
  well standard and publicized. There''s some quick googling should", "tokens": [50364,
  7274, 11, 291, 458, 11, 300, 366, 1238, 731, 3832, 293, 1908, 1602, 13, 821, 311,
  512, 1702, 50061, 1688, 820, 50628], "temperature": 0.0, "avg_logprob": -0.19304724300608916,
  "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0011438294313848019},
  {"id": 308, "seek": 202152, "start": 2026.8, "end": 2033.68, "text": " find those
  for people. Yeah, for sure. Of course, I think you could measure some things like",
  "tokens": [50628, 915, 729, 337, 561, 13, 865, 11, 337, 988, 13, 2720, 1164, 11,
  286, 519, 291, 727, 3481, 512, 721, 411, 50972], "temperature": 0.0, "avg_logprob":
  -0.19304724300608916, "compression_ratio": 1.5294117647058822, "no_speech_prob":
  0.0011438294313848019}, {"id": 309, "seek": 202152, "start": 2033.68, "end": 2040.4,
  "text": " a DCG, which is offline, right? So like, but you do need like rated queries.
  And as a contributor to", "tokens": [50972, 257, 9114, 38, 11, 597, 307, 21857,
  11, 558, 30, 407, 411, 11, 457, 291, 360, 643, 411, 22103, 24109, 13, 400, 382,
  257, 42859, 281, 51308], "temperature": 0.0, "avg_logprob": -0.19304724300608916,
  "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0011438294313848019},
  {"id": 310, "seek": 202152, "start": 2040.4, "end": 2049.36, "text": " Qbit, which
  is a query rating system, open source system, I''m curious to to hear your opinion
  on,", "tokens": [51308, 1249, 5260, 11, 597, 307, 257, 14581, 10990, 1185, 11, 1269,
  4009, 1185, 11, 286, 478, 6369, 281, 281, 1568, 428, 4800, 322, 11, 51756], "temperature":
  0.0, "avg_logprob": -0.19304724300608916, "compression_ratio": 1.5294117647058822,
  "no_speech_prob": 0.0011438294313848019}, {"id": 311, "seek": 204936, "start": 2049.36,
  "end": 2055.04, "text": " you know, sort of on one hand, of course, you can always
  go and just check, sanity check,", "tokens": [50364, 291, 458, 11, 1333, 295, 322,
  472, 1011, 11, 295, 1164, 11, 291, 393, 1009, 352, 293, 445, 1520, 11, 47892, 1520,
  11, 50648], "temperature": 0.0, "avg_logprob": -0.21216041163394325, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.004200038034468889}, {"id": 312, "seek":
  204936, "start": 2056.0, "end": 2062.48, "text": " you know, smoke test, your, your,
  your runker. But that''s just maybe for engineers or product managers,", "tokens":
  [50696, 291, 458, 11, 8439, 1500, 11, 428, 11, 428, 11, 428, 367, 3197, 260, 13,
  583, 300, 311, 445, 1310, 337, 11955, 420, 1674, 14084, 11, 51020], "temperature":
  0.0, "avg_logprob": -0.21216041163394325, "compression_ratio": 1.6150627615062763,
  "no_speech_prob": 0.004200038034468889}, {"id": 313, "seek": 204936, "start": 2062.48,
  "end": 2068.8, "text": " like a smaller group versus when you go and try to understand
  the intent of queries at larger scale", "tokens": [51020, 411, 257, 4356, 1594,
  5717, 562, 291, 352, 293, 853, 281, 1223, 264, 8446, 295, 24109, 412, 4833, 4373,
  51336], "temperature": 0.0, "avg_logprob": -0.21216041163394325, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.004200038034468889}, {"id": 314, "seek":
  204936, "start": 2068.8, "end": 2073.52, "text": " with this manual effort. Have
  you seen, have you deployed such methods within organizations?", "tokens": [51336,
  365, 341, 9688, 4630, 13, 3560, 291, 1612, 11, 362, 291, 17826, 1270, 7150, 1951,
  6150, 30, 51572], "temperature": 0.0, "avg_logprob": -0.21216041163394325, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.004200038034468889}, {"id": 315, "seek":
  207352, "start": 2074.08, "end": 2080.8, "text": " What do you feel like doing this
  in the companies on more regular basis? And I also know, as a shout out", "tokens":
  [50392, 708, 360, 291, 841, 411, 884, 341, 294, 264, 3431, 322, 544, 3890, 5143,
  30, 400, 286, 611, 458, 11, 382, 257, 8043, 484, 50728], "temperature": 0.0, "avg_logprob":
  -0.16098369436061127, "compression_ratio": 1.632034632034632, "no_speech_prob":
  0.007157593499869108}, {"id": 316, "seek": 207352, "start": 2080.8, "end": 2085.6,
  "text": " to what you did in the course, search with the mail course, like you did
  ask us to", "tokens": [50728, 281, 437, 291, 630, 294, 264, 1164, 11, 3164, 365,
  264, 10071, 1164, 11, 411, 291, 630, 1029, 505, 281, 50968], "temperature": 0.0,
  "avg_logprob": -0.16098369436061127, "compression_ratio": 1.632034632034632, "no_speech_prob":
  0.007157593499869108}, {"id": 317, "seek": 207352, "start": 2086.56, "end": 2091.68,
  "text": " rate some queries and create a judgment, please, to get a feel of the
  process. And I think that", "tokens": [51016, 3314, 512, 24109, 293, 1884, 257,
  12216, 11, 1767, 11, 281, 483, 257, 841, 295, 264, 1399, 13, 400, 286, 519, 300,
  51272], "temperature": 0.0, "avg_logprob": -0.16098369436061127, "compression_ratio":
  1.632034632034632, "no_speech_prob": 0.007157593499869108}, {"id": 318, "seek":
  207352, "start": 2091.68, "end": 2099.36, "text": " by itself is a great idea because
  it pushes you towards, you know, further understanding what", "tokens": [51272,
  538, 2564, 307, 257, 869, 1558, 570, 309, 21020, 291, 3030, 11, 291, 458, 11, 3052,
  3701, 437, 51656], "temperature": 0.0, "avg_logprob": -0.16098369436061127, "compression_ratio":
  1.632034632034632, "no_speech_prob": 0.007157593499869108}, {"id": 319, "seek":
  209936, "start": 2099.44, "end": 2104.2400000000002, "text": " is it that you''re
  building for? So yeah. Yeah, I mean, I think, yeah, I mean, it makes,", "tokens":
  [50368, 307, 309, 300, 291, 434, 2390, 337, 30, 407, 1338, 13, 865, 11, 286, 914,
  11, 286, 519, 11, 1338, 11, 286, 914, 11, 309, 1669, 11, 50608], "temperature":
  0.0, "avg_logprob": -0.1421388021790155, "compression_ratio": 1.6832579185520362,
  "no_speech_prob": 0.000876779027748853}, {"id": 320, "seek": 209936, "start": 2104.2400000000002,
  "end": 2111.92, "text": " it makes a ton of sense to have, if you can afford to
  do offline evaluation using, you know,", "tokens": [50608, 309, 1669, 257, 2952,
  295, 2020, 281, 362, 11, 498, 291, 393, 6157, 281, 360, 21857, 13344, 1228, 11,
  291, 458, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1421388021790155, "compression_ratio":
  1.6832579185520362, "no_speech_prob": 0.000876779027748853}, {"id": 321, "seek":
  209936, "start": 2111.92, "end": 2119.28, "text": " professional annotators, you
  know, like, I don''t know how good mechanical Turk these days is,", "tokens": [50992,
  4843, 25339, 3391, 11, 291, 458, 11, 411, 11, 286, 500, 380, 458, 577, 665, 12070,
  15714, 613, 1708, 307, 11, 51360], "temperature": 0.0, "avg_logprob": -0.1421388021790155,
  "compression_ratio": 1.6832579185520362, "no_speech_prob": 0.000876779027748853},
  {"id": 322, "seek": 209936, "start": 2119.28, "end": 2125.52, "text": " but like,
  you know, something like a mechanical Turk or like, I forget what crowd flour is
  called", "tokens": [51360, 457, 411, 11, 291, 458, 11, 746, 411, 257, 12070, 15714,
  420, 411, 11, 286, 2870, 437, 6919, 7693, 307, 1219, 51672], "temperature": 0.0,
  "avg_logprob": -0.1421388021790155, "compression_ratio": 1.6832579185520362, "no_speech_prob":
  0.000876779027748853}, {"id": 323, "seek": 212552, "start": 2125.52, "end": 2131.04,
  "text": " now or I know we''ve worked with a company called Appen in the past, like,
  there are these companies", "tokens": [50364, 586, 420, 286, 458, 321, 600, 2732,
  365, 257, 2237, 1219, 3132, 268, 294, 264, 1791, 11, 411, 11, 456, 366, 613, 3431,
  50640], "temperature": 0.0, "avg_logprob": -0.1304206166948591, "compression_ratio":
  1.6554621848739495, "no_speech_prob": 0.002578917657956481}, {"id": 324, "seek":
  212552, "start": 2131.04, "end": 2137.52, "text": " out there that will provide
  you with a large number of annotators who will run your queries and", "tokens":
  [50640, 484, 456, 300, 486, 2893, 291, 365, 257, 2416, 1230, 295, 25339, 3391, 567,
  486, 1190, 428, 24109, 293, 50964], "temperature": 0.0, "avg_logprob": -0.1304206166948591,
  "compression_ratio": 1.6554621848739495, "no_speech_prob": 0.002578917657956481},
  {"id": 325, "seek": 212552, "start": 2137.52, "end": 2145.04, "text": " then rank
  them for you. And of course, you can use that as well. So again, like, you know,
  it often", "tokens": [50964, 550, 6181, 552, 337, 291, 13, 400, 295, 1164, 11, 291,
  393, 764, 300, 382, 731, 13, 407, 797, 11, 411, 11, 291, 458, 11, 309, 2049, 51340],
  "temperature": 0.0, "avg_logprob": -0.1304206166948591, "compression_ratio": 1.6554621848739495,
  "no_speech_prob": 0.002578917657956481}, {"id": 326, "seek": 212552, "start": 2145.04,
  "end": 2150.24, "text": " comes down to whether you''re monetizing your search results
  and folks who do monetize their search", "tokens": [51340, 1487, 760, 281, 1968,
  291, 434, 15556, 3319, 428, 3164, 3542, 293, 4024, 567, 360, 15556, 1125, 641, 3164,
  51600], "temperature": 0.0, "avg_logprob": -0.1304206166948591, "compression_ratio":
  1.6554621848739495, "no_speech_prob": 0.002578917657956481}, {"id": 327, "seek":
  215024, "start": 2150.3999999999996, "end": 2156.0, "text": " results will typically
  pay for those kinds of things, especially once they reach really large scales,",
  "tokens": [50372, 3542, 486, 5850, 1689, 337, 729, 3685, 295, 721, 11, 2318, 1564,
  436, 2524, 534, 2416, 17408, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10489437796852806,
  "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.004106334410607815},
  {"id": 328, "seek": 215024, "start": 2156.0, "end": 2164.8799999999997, "text":
  " you know, like your, your Amazon''s and the like. Where and how much you can do
  that often comes down", "tokens": [50652, 291, 458, 11, 411, 428, 11, 428, 6795,
  311, 293, 264, 411, 13, 2305, 293, 577, 709, 291, 393, 360, 300, 2049, 1487, 760,
  51096], "temperature": 0.0, "avg_logprob": -0.10489437796852806, "compression_ratio":
  1.6866952789699572, "no_speech_prob": 0.004106334410607815}, {"id": 329, "seek":
  215024, "start": 2164.8799999999997, "end": 2171.2, "text": " to budget and time,
  right? So, you know, if you have the budget, I''ve seen companies do that,", "tokens":
  [51096, 281, 4706, 293, 565, 11, 558, 30, 407, 11, 291, 458, 11, 498, 291, 362,
  264, 4706, 11, 286, 600, 1612, 3431, 360, 300, 11, 51412], "temperature": 0.0, "avg_logprob":
  -0.10489437796852806, "compression_ratio": 1.6866952789699572, "no_speech_prob":
  0.004106334410607815}, {"id": 330, "seek": 215024, "start": 2172.08, "end": 2176.24,
  "text": " you know, maybe I don''t know about weekly, there might be some that do
  that weekly at the really", "tokens": [51456, 291, 458, 11, 1310, 286, 500, 380,
  458, 466, 12460, 11, 456, 1062, 312, 512, 300, 360, 300, 12460, 412, 264, 534, 51664],
  "temperature": 0.0, "avg_logprob": -0.10489437796852806, "compression_ratio": 1.6866952789699572,
  "no_speech_prob": 0.004106334410607815}, {"id": 331, "seek": 217624, "start": 2176.24,
  "end": 2182.3999999999996, "text": " large scale, that gets really expensive quarterly
  or whenever there''s a major update to the system,", "tokens": [50364, 2416, 4373,
  11, 300, 2170, 534, 5124, 38633, 420, 5699, 456, 311, 257, 2563, 5623, 281, 264,
  1185, 11, 50672], "temperature": 0.0, "avg_logprob": -0.12486493145978009, "compression_ratio":
  1.6209677419354838, "no_speech_prob": 0.002458769828081131}, {"id": 332, "seek":
  217624, "start": 2182.3999999999996, "end": 2187.68, "text": " those kinds of things.
  So by all means, I mean, I think anything you can do to get, you know, I think",
  "tokens": [50672, 729, 3685, 295, 721, 13, 407, 538, 439, 1355, 11, 286, 914, 11,
  286, 519, 1340, 291, 393, 360, 281, 483, 11, 291, 458, 11, 286, 519, 50936], "temperature":
  0.0, "avg_logprob": -0.12486493145978009, "compression_ratio": 1.6209677419354838,
  "no_speech_prob": 0.002458769828081131}, {"id": 333, "seek": 217624, "start": 2187.68,
  "end": 2193.2, "text": " often in this space, we love to say, oh, well, this is
  the way you do it. And the reality is, is like,", "tokens": [50936, 2049, 294, 341,
  1901, 11, 321, 959, 281, 584, 11, 1954, 11, 731, 11, 341, 307, 264, 636, 291, 360,
  309, 13, 400, 264, 4103, 307, 11, 307, 411, 11, 51212], "temperature": 0.0, "avg_logprob":
  -0.12486493145978009, "compression_ratio": 1.6209677419354838, "no_speech_prob":
  0.002458769828081131}, {"id": 334, "seek": 217624, "start": 2194.72, "end": 2200.24,
  "text": " you want a hybrid approach to most of these things, right? Because there''s
  no one perfect way of,", "tokens": [51288, 291, 528, 257, 13051, 3109, 281, 881,
  295, 613, 721, 11, 558, 30, 1436, 456, 311, 572, 472, 2176, 636, 295, 11, 51564],
  "temperature": 0.0, "avg_logprob": -0.12486493145978009, "compression_ratio": 1.6209677419354838,
  "no_speech_prob": 0.002458769828081131}, {"id": 335, "seek": 220024, "start": 2200.8799999999997,
  "end": 2207.68, "text": " there''s no one perfect model and there''s no one perfect
  way of evaluating a model, right? And so", "tokens": [50396, 456, 311, 572, 472,
  2176, 2316, 293, 456, 311, 572, 472, 2176, 636, 295, 27479, 257, 2316, 11, 558,
  30, 400, 370, 50736], "temperature": 0.0, "avg_logprob": -0.14317961956592315, "compression_ratio":
  1.6830357142857142, "no_speech_prob": 0.004250739701092243}, {"id": 336, "seek":
  220024, "start": 2209.2799999999997, "end": 2214.8799999999997, "text": " you need
  to blend these and build up a broader sense of what actually works, right?", "tokens":
  [50816, 291, 643, 281, 10628, 613, 293, 1322, 493, 257, 13227, 2020, 295, 437, 767,
  1985, 11, 558, 30, 51096], "temperature": 0.0, "avg_logprob": -0.14317961956592315,
  "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.004250739701092243},
  {"id": 337, "seek": 220024, "start": 2215.9199999999996, "end": 2222.72, "text":
  " Yeah, absolutely. It''s just like, I guess, I guess, general awareness, like that
  these systems and", "tokens": [51148, 865, 11, 3122, 13, 467, 311, 445, 411, 11,
  286, 2041, 11, 286, 2041, 11, 2674, 8888, 11, 411, 300, 613, 3652, 293, 51488],
  "temperature": 0.0, "avg_logprob": -0.14317961956592315, "compression_ratio": 1.6830357142857142,
  "no_speech_prob": 0.004250739701092243}, {"id": 338, "seek": 220024, "start": 2222.72,
  "end": 2227.12, "text": " approaches exist and like when you feel stuck that you
  don''t know, okay, you don''t generate ideas", "tokens": [51488, 11587, 2514, 293,
  411, 562, 291, 841, 5541, 300, 291, 500, 380, 458, 11, 1392, 11, 291, 500, 380,
  8460, 3487, 51708], "temperature": 0.0, "avg_logprob": -0.14317961956592315, "compression_ratio":
  1.6830357142857142, "no_speech_prob": 0.004250739701092243}, {"id": 339, "seek":
  222712, "start": 2227.12, "end": 2231.6, "text": " where you can improve your search
  engine, you can go deeper and try to involve, you know,", "tokens": [50364, 689,
  291, 393, 3470, 428, 3164, 2848, 11, 291, 393, 352, 7731, 293, 853, 281, 9494, 11,
  291, 458, 11, 50588], "temperature": 0.0, "avg_logprob": -0.153562642215343, "compression_ratio":
  1.6772727272727272, "no_speech_prob": 0.0065754191018640995}, {"id": 340, "seek":
  222712, "start": 2231.6, "end": 2241.2, "text": " and the teachers, I believe, to
  help you understand. And before we move further to some of", "tokens": [50588, 293,
  264, 6023, 11, 286, 1697, 11, 281, 854, 291, 1223, 13, 400, 949, 321, 1286, 3052,
  281, 512, 295, 51068], "temperature": 0.0, "avg_logprob": -0.153562642215343, "compression_ratio":
  1.6772727272727272, "no_speech_prob": 0.0065754191018640995}, {"id": 341, "seek":
  222712, "start": 2241.2, "end": 2246.3199999999997, "text": " higher level questions,
  I still wanted to ask you a little bit more detailed question on if", "tokens":
  [51068, 2946, 1496, 1651, 11, 286, 920, 1415, 281, 1029, 291, 257, 707, 857, 544,
  9942, 1168, 322, 498, 51324], "temperature": 0.0, "avg_logprob": -0.153562642215343,
  "compression_ratio": 1.6772727272727272, "no_speech_prob": 0.0065754191018640995},
  {"id": 342, "seek": 222712, "start": 2246.3199999999997, "end": 2253.7599999999998,
  "text": " somebody in the audience or listeners wants to try to build the kind of
  end-to-end search engine", "tokens": [51324, 2618, 294, 264, 4034, 420, 23274, 2738,
  281, 853, 281, 1322, 264, 733, 295, 917, 12, 1353, 12, 521, 3164, 2848, 51696],
  "temperature": 0.0, "avg_logprob": -0.153562642215343, "compression_ratio": 1.6772727272727272,
  "no_speech_prob": 0.0065754191018640995}, {"id": 343, "seek": 225376, "start": 2253.84,
  "end": 2259.6800000000003, "text": " at home. So what are the available datasets,
  tools and algorithms exist today that will allow you", "tokens": [50368, 412, 1280,
  13, 407, 437, 366, 264, 2435, 42856, 11, 3873, 293, 14642, 2514, 965, 300, 486,
  2089, 291, 50660], "temperature": 0.0, "avg_logprob": -0.18385151158208432, "compression_ratio":
  1.5767634854771784, "no_speech_prob": 0.0061095645651221275}, {"id": 344, "seek":
  225376, "start": 2259.6800000000003, "end": 2266.32, "text": " to build this and
  train relevancy models and all these building blocks in the search engine?", "tokens":
  [50660, 281, 1322, 341, 293, 3847, 25916, 6717, 5245, 293, 439, 613, 2390, 8474,
  294, 264, 3164, 2848, 30, 50992], "temperature": 0.0, "avg_logprob": -0.18385151158208432,
  "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.0061095645651221275},
  {"id": 345, "seek": 225376, "start": 2267.5200000000004, "end": 2274.1600000000003,
  "text": " Yeah, I mean, it''s, you know, it''s interesting. I think in many ways
  we live in a golden age of", "tokens": [51052, 865, 11, 286, 914, 11, 309, 311,
  11, 291, 458, 11, 309, 311, 1880, 13, 286, 519, 294, 867, 2098, 321, 1621, 294,
  257, 9729, 3205, 295, 51384], "temperature": 0.0, "avg_logprob": -0.18385151158208432,
  "compression_ratio": 1.5767634854771784, "no_speech_prob": 0.0061095645651221275},
  {"id": 346, "seek": 225376, "start": 2274.1600000000003, "end": 2282.7200000000003,
  "text": " of search engines, right? Like, there are several just top notch open
  source freely available", "tokens": [51384, 295, 3164, 12982, 11, 558, 30, 1743,
  11, 456, 366, 2940, 445, 1192, 26109, 1269, 4009, 16433, 2435, 51812], "temperature":
  0.0, "avg_logprob": -0.18385151158208432, "compression_ratio": 1.5767634854771784,
  "no_speech_prob": 0.0061095645651221275}, {"id": 347, "seek": 228272, "start": 2282.72,
  "end": 2288.8799999999997, "text": " search engines on the market. There are a number
  of companies competing in this space,", "tokens": [50364, 3164, 12982, 322, 264,
  2142, 13, 821, 366, 257, 1230, 295, 3431, 15439, 294, 341, 1901, 11, 50672], "temperature":
  0.0, "avg_logprob": -0.12170405526763027, "compression_ratio": 1.6593886462882097,
  "no_speech_prob": 0.0015482836170122027}, {"id": 348, "seek": 228272, "start": 2289.52,
  "end": 2295.8399999999997, "text": " right? So, you know, picking an engine is almost
  like, hey, you know, it''s a plethora of riches.", "tokens": [50704, 558, 30, 407,
  11, 291, 458, 11, 8867, 364, 2848, 307, 1920, 411, 11, 4177, 11, 291, 458, 11, 309,
  311, 257, 499, 302, 7013, 295, 35777, 13, 51020], "temperature": 0.0, "avg_logprob":
  -0.12170405526763027, "compression_ratio": 1.6593886462882097, "no_speech_prob":
  0.0015482836170122027}, {"id": 349, "seek": 228272, "start": 2295.8399999999997,
  "end": 2302.48, "text": " It''s almost, it''s like, you''re, it''s a challenge to
  pick one because there''s so many good choices,", "tokens": [51020, 467, 311, 1920,
  11, 309, 311, 411, 11, 291, 434, 11, 309, 311, 257, 3430, 281, 1888, 472, 570, 456,
  311, 370, 867, 665, 7994, 11, 51352], "temperature": 0.0, "avg_logprob": -0.12170405526763027,
  "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.0015482836170122027},
  {"id": 350, "seek": 228272, "start": 2302.48, "end": 2307.68, "text": " right? And
  you''re often like, what specific features or domains am I going to participate
  in? So,", "tokens": [51352, 558, 30, 400, 291, 434, 2049, 411, 11, 437, 2685, 4122,
  420, 25514, 669, 286, 516, 281, 8197, 294, 30, 407, 11, 51612], "temperature": 0.0,
  "avg_logprob": -0.12170405526763027, "compression_ratio": 1.6593886462882097, "no_speech_prob":
  0.0015482836170122027}, {"id": 351, "seek": 230768, "start": 2307.68, "end": 2312.3199999999997,
  "text": " you know, it''s obviously one like, choose a good engine. And I think
  you really can''t go wrong", "tokens": [50364, 291, 458, 11, 309, 311, 2745, 472,
  411, 11, 2826, 257, 665, 2848, 13, 400, 286, 519, 291, 534, 393, 380, 352, 2085,
  50596], "temperature": 0.0, "avg_logprob": -0.1670391760139822, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.004894915968179703}, {"id": 352, "seek":
  230768, "start": 2312.3199999999997, "end": 2318.0, "text": " with any of the main
  ones. What, you know, it''s the Lucine-based ones, Solar Elastic Search Open Search.",
  "tokens": [50596, 365, 604, 295, 264, 2135, 2306, 13, 708, 11, 291, 458, 11, 309,
  311, 264, 9593, 533, 12, 6032, 2306, 11, 22385, 2699, 2750, 17180, 7238, 17180,
  13, 50880], "temperature": 0.0, "avg_logprob": -0.1670391760139822, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.004894915968179703}, {"id": 353, "seek":
  230768, "start": 2318.7999999999997, "end": 2324.72, "text": " I haven''t played
  with Vespa myself, but, you know, I think that one''s coming on strong as well.",
  "tokens": [50920, 286, 2378, 380, 3737, 365, 691, 279, 4306, 2059, 11, 457, 11,
  291, 458, 11, 286, 519, 300, 472, 311, 1348, 322, 2068, 382, 731, 13, 51216], "temperature":
  0.0, "avg_logprob": -0.1670391760139822, "compression_ratio": 1.6150627615062763,
  "no_speech_prob": 0.004894915968179703}, {"id": 354, "seek": 230768, "start": 2325.8399999999997,
  "end": 2330.8799999999997, "text": " You see a lot of interesting capabilities that
  are coming out of that. And then, you know,", "tokens": [51272, 509, 536, 257, 688,
  295, 1880, 10862, 300, 366, 1348, 484, 295, 300, 13, 400, 550, 11, 291, 458, 11,
  51524], "temperature": 0.0, "avg_logprob": -0.1670391760139822, "compression_ratio":
  1.6150627615062763, "no_speech_prob": 0.004894915968179703}, {"id": 355, "seek":
  233088, "start": 2331.44, "end": 2337.6800000000003, "text": " you have obviously
  the, the companies behind it. Of course, I''m co-founder of Lucidworks,", "tokens":
  [50392, 291, 362, 2745, 264, 11, 264, 3431, 2261, 309, 13, 2720, 1164, 11, 286,
  478, 598, 12, 33348, 295, 9593, 327, 18357, 11, 50704], "temperature": 0.0, "avg_logprob":
  -0.15004480679829915, "compression_ratio": 1.5912162162162162, "no_speech_prob":
  0.004984300117939711}, {"id": 356, "seek": 233088, "start": 2337.6800000000003,
  "end": 2341.76, "text": " and so still a big shout out and big fan there, because
  I think they''re doing a lot of interesting", "tokens": [50704, 293, 370, 920, 257,
  955, 8043, 484, 293, 955, 3429, 456, 11, 570, 286, 519, 436, 434, 884, 257, 688,
  295, 1880, 50908], "temperature": 0.0, "avg_logprob": -0.15004480679829915, "compression_ratio":
  1.5912162162162162, "no_speech_prob": 0.004984300117939711}, {"id": 357, "seek":
  233088, "start": 2341.76, "end": 2347.28, "text": " things. But you also see a number
  of other players in that space, both with deep learning or", "tokens": [50908, 721,
  13, 583, 291, 611, 536, 257, 1230, 295, 661, 4150, 294, 300, 1901, 11, 1293, 365,
  2452, 2539, 420, 51184], "temperature": 0.0, "avg_logprob": -0.15004480679829915,
  "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.004984300117939711},
  {"id": 358, "seek": 233088, "start": 2347.28, "end": 2353.04, "text": " neural-based
  approaches, as well as blended or hybrid or traditional approaches. So, one,", "tokens":
  [51184, 18161, 12, 6032, 11587, 11, 382, 731, 382, 27048, 420, 13051, 420, 5164,
  11587, 13, 407, 11, 472, 11, 51472], "temperature": 0.0, "avg_logprob": -0.15004480679829915,
  "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.004984300117939711},
  {"id": 359, "seek": 233088, "start": 2353.04, "end": 2358.56, "text": " start with
  your engine. See what it''s capable of. And then on the data set front, it really
  kind of", "tokens": [51472, 722, 365, 428, 2848, 13, 3008, 437, 309, 311, 8189,
  295, 13, 400, 550, 322, 264, 1412, 992, 1868, 11, 309, 534, 733, 295, 51748], "temperature":
  0.0, "avg_logprob": -0.15004480679829915, "compression_ratio": 1.5912162162162162,
  "no_speech_prob": 0.004984300117939711}, {"id": 360, "seek": 235856, "start": 2358.56,
  "end": 2364.56, "text": " depends on what your, what domain you''re in. But, you
  know, I''m a big fan. You know, I often start", "tokens": [50364, 5946, 322, 437,
  428, 11, 437, 9274, 291, 434, 294, 13, 583, 11, 291, 458, 11, 286, 478, 257, 955,
  3429, 13, 509, 458, 11, 286, 2049, 722, 50664], "temperature": 0.0, "avg_logprob":
  -0.16028053943927473, "compression_ratio": 1.6144067796610169, "no_speech_prob":
  0.0006086836219765246}, {"id": 361, "seek": 235856, "start": 2364.56, "end": 2371.6,
  "text": " with public data sets, Trek TREC is a great place to get data sets across
  a large number of", "tokens": [50664, 365, 1908, 1412, 6352, 11, 25845, 15176, 8140,
  307, 257, 869, 1081, 281, 483, 1412, 6352, 2108, 257, 2416, 1230, 295, 51016], "temperature":
  0.0, "avg_logprob": -0.16028053943927473, "compression_ratio": 1.6144067796610169,
  "no_speech_prob": 0.0006086836219765246}, {"id": 362, "seek": 235856, "start": 2371.6,
  "end": 2377.2, "text": " domains. You can also get queries. So, whether you want
  to do web search or e-commerce or legal or", "tokens": [51016, 25514, 13, 509, 393,
  611, 483, 24109, 13, 407, 11, 1968, 291, 528, 281, 360, 3670, 3164, 420, 308, 12,
  26926, 420, 5089, 420, 51296], "temperature": 0.0, "avg_logprob": -0.16028053943927473,
  "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.0006086836219765246},
  {"id": 363, "seek": 235856, "start": 2377.2, "end": 2385.6, "text": " enterprise
  or medical, like you can go to track and get a data set and start indexing that,",
  "tokens": [51296, 14132, 420, 4625, 11, 411, 291, 393, 352, 281, 2837, 293, 483,
  257, 1412, 992, 293, 722, 8186, 278, 300, 11, 51716], "temperature": 0.0, "avg_logprob":
  -0.16028053943927473, "compression_ratio": 1.6144067796610169, "no_speech_prob":
  0.0006086836219765246}, {"id": 364, "seek": 238560, "start": 2385.6, "end": 2392.16,
  "text": " playing around with it. These days also, it''s just super easy to go crawl.
  So, you know, get like", "tokens": [50364, 2433, 926, 365, 309, 13, 1981, 1708,
  611, 11, 309, 311, 445, 1687, 1858, 281, 352, 24767, 13, 407, 11, 291, 458, 11,
  483, 411, 50692], "temperature": 0.0, "avg_logprob": -0.15940564795385434, "compression_ratio":
  1.4630541871921183, "no_speech_prob": 0.005309475585818291}, {"id": 365, "seek":
  238560, "start": 2394.96, "end": 2403.04, "text": " scrappy or curl or WGET or whatever,
  or it''s one of these crawlers and go crawl websites. And then", "tokens": [50832,
  13943, 7966, 420, 22591, 420, 343, 38, 4850, 420, 2035, 11, 420, 309, 311, 472,
  295, 613, 13999, 11977, 293, 352, 24767, 12891, 13, 400, 550, 51236], "temperature":
  0.0, "avg_logprob": -0.15940564795385434, "compression_ratio": 1.4630541871921183,
  "no_speech_prob": 0.005309475585818291}, {"id": 366, "seek": 238560, "start": 2403.04,
  "end": 2410.08, "text": " you can start going from there. The query log side tends
  to be a little bit harder because companies", "tokens": [51236, 291, 393, 722, 516,
  490, 456, 13, 440, 14581, 3565, 1252, 12258, 281, 312, 257, 707, 857, 6081, 570,
  3431, 51588], "temperature": 0.0, "avg_logprob": -0.15940564795385434, "compression_ratio":
  1.4630541871921183, "no_speech_prob": 0.005309475585818291}, {"id": 367, "seek":
  241008, "start": 2410.08, "end": 2417.2799999999997, "text": " don''t like to release
  their queries. But there are several data sets that do have some form of", "tokens":
  [50364, 500, 380, 411, 281, 4374, 641, 24109, 13, 583, 456, 366, 2940, 1412, 6352,
  300, 360, 362, 512, 1254, 295, 50724], "temperature": 0.0, "avg_logprob": -0.10343896126260563,
  "compression_ratio": 1.6008583690987124, "no_speech_prob": 0.002642524428665638},
  {"id": 368, "seek": 241008, "start": 2417.2799999999997, "end": 2422.88, "text":
  " queries with them. They may not be enough for you to fully test all the features
  of an engine.", "tokens": [50724, 24109, 365, 552, 13, 814, 815, 406, 312, 1547,
  337, 291, 281, 4498, 1500, 439, 264, 4122, 295, 364, 2848, 13, 51004], "temperature":
  0.0, "avg_logprob": -0.10343896126260563, "compression_ratio": 1.6008583690987124,
  "no_speech_prob": 0.002642524428665638}, {"id": 369, "seek": 241008, "start": 2423.52,
  "end": 2430.24, "text": " So, in our class, we use a really old data set from Best
  Buy that has query logs. In it,", "tokens": [51036, 407, 11, 294, 527, 1508, 11,
  321, 764, 257, 534, 1331, 1412, 992, 490, 9752, 19146, 300, 575, 14581, 20820, 13,
  682, 309, 11, 51372], "temperature": 0.0, "avg_logprob": -0.10343896126260563, "compression_ratio":
  1.6008583690987124, "no_speech_prob": 0.002642524428665638}, {"id": 370, "seek":
  241008, "start": 2430.24, "end": 2435.7599999999998, "text": " well, query click
  logs. But for instance, it doesn''t tell you what was shown the user. It just",
  "tokens": [51372, 731, 11, 14581, 2052, 20820, 13, 583, 337, 5197, 11, 309, 1177,
  380, 980, 291, 437, 390, 4898, 264, 4195, 13, 467, 445, 51648], "temperature": 0.0,
  "avg_logprob": -0.10343896126260563, "compression_ratio": 1.6008583690987124, "no_speech_prob":
  0.002642524428665638}, {"id": 371, "seek": 243576, "start": 2435.76, "end": 2441.28,
  "text": " tells you what they clicked on. And so, you can''t actually build full
  models or effective models with", "tokens": [50364, 5112, 291, 437, 436, 23370,
  322, 13, 400, 370, 11, 291, 393, 380, 767, 1322, 1577, 5245, 420, 4942, 5245, 365,
  50640], "temperature": 0.0, "avg_logprob": -0.10002184647780199, "compression_ratio":
  1.8043478260869565, "no_speech_prob": 0.00430570263415575}, {"id": 372, "seek":
  243576, "start": 2441.28, "end": 2446.96, "text": " that. But it''s actually a really
  good e-commerce data set because it has all of the problems of a", "tokens": [50640,
  300, 13, 583, 309, 311, 767, 257, 534, 665, 308, 12, 26926, 1412, 992, 570, 309,
  575, 439, 295, 264, 2740, 295, 257, 50924], "temperature": 0.0, "avg_logprob": -0.10002184647780199,
  "compression_ratio": 1.8043478260869565, "no_speech_prob": 0.00430570263415575},
  {"id": 373, "seek": 243576, "start": 2446.96, "end": 2452.96, "text": " data set
  that comes from a company. Namely, there''s a lot of missing data in there. There''s
  a lot", "tokens": [50924, 1412, 992, 300, 1487, 490, 257, 2237, 13, 10684, 736,
  11, 456, 311, 257, 688, 295, 5361, 1412, 294, 456, 13, 821, 311, 257, 688, 51224],
  "temperature": 0.0, "avg_logprob": -0.10002184647780199, "compression_ratio": 1.8043478260869565,
  "no_speech_prob": 0.00430570263415575}, {"id": 374, "seek": 243576, "start": 2452.96,
  "end": 2457.2000000000003, "text": " of bad data. But there''s also a lot of really
  good data. And so, starting with those, and then I think,", "tokens": [51224, 295,
  1578, 1412, 13, 583, 456, 311, 611, 257, 688, 295, 534, 665, 1412, 13, 400, 370,
  11, 2891, 365, 729, 11, 293, 550, 286, 519, 11, 51436], "temperature": 0.0, "avg_logprob":
  -0.10002184647780199, "compression_ratio": 1.8043478260869565, "no_speech_prob":
  0.00430570263415575}, {"id": 375, "seek": 243576, "start": 2457.2000000000003, "end":
  2463.2000000000003, "text": " you know, you kind of just start to push the engine
  through its paces. Start with the tutorials,", "tokens": [51436, 291, 458, 11, 291,
  733, 295, 445, 722, 281, 2944, 264, 2848, 807, 1080, 280, 2116, 13, 6481, 365, 264,
  17616, 11, 51736], "temperature": 0.0, "avg_logprob": -0.10002184647780199, "compression_ratio":
  1.8043478260869565, "no_speech_prob": 0.00430570263415575}, {"id": 376, "seek":
  246320, "start": 2463.2, "end": 2469.4399999999996, "text": " the basic features,
  and then see where you can go deeper. Can you actually get Best In Class", "tokens":
  [50364, 264, 3875, 4122, 11, 293, 550, 536, 689, 291, 393, 352, 7731, 13, 1664,
  291, 767, 483, 9752, 682, 9471, 50676], "temperature": 0.0, "avg_logprob": -0.13748544057210285,
  "compression_ratio": 1.662162162162162, "no_speech_prob": 0.007721584755927324},
  {"id": 377, "seek": 246320, "start": 2471.2799999999997, "end": 2478.24, "text":
  " relevance measurement out of it? Can you get Best In Class speed performance out
  of it?", "tokens": [50768, 32684, 13160, 484, 295, 309, 30, 1664, 291, 483, 9752,
  682, 9471, 3073, 3389, 484, 295, 309, 30, 51116], "temperature": 0.0, "avg_logprob":
  -0.13748544057210285, "compression_ratio": 1.662162162162162, "no_speech_prob":
  0.007721584755927324}, {"id": 378, "seek": 246320, "start": 2478.24, "end": 2483.2,
  "text": " And then just work your way through the engine. And these days, you can
  typically do that in,", "tokens": [51116, 400, 550, 445, 589, 428, 636, 807, 264,
  2848, 13, 400, 613, 1708, 11, 291, 393, 5850, 360, 300, 294, 11, 51364], "temperature":
  0.0, "avg_logprob": -0.13748544057210285, "compression_ratio": 1.662162162162162,
  "no_speech_prob": 0.007721584755927324}, {"id": 379, "seek": 246320, "start": 2484.0,
  "end": 2490.7999999999997, "text": " say, less than a week. And that''s really amazing,
  right? Especially when you combine that with", "tokens": [51404, 584, 11, 1570,
  813, 257, 1243, 13, 400, 300, 311, 534, 2243, 11, 558, 30, 8545, 562, 291, 10432,
  300, 365, 51744], "temperature": 0.0, "avg_logprob": -0.13748544057210285, "compression_ratio":
  1.662162162162162, "no_speech_prob": 0.007721584755927324}, {"id": 380, "seek":
  249080, "start": 2490.8, "end": 2494.7200000000003, "text": " all the great information
  out on the web, right? Like, you know, I think when I was getting started,", "tokens":
  [50364, 439, 264, 869, 1589, 484, 322, 264, 3670, 11, 558, 30, 1743, 11, 291, 458,
  11, 286, 519, 562, 286, 390, 1242, 1409, 11, 50560], "temperature": 0.0, "avg_logprob":
  -0.10030921936035156, "compression_ratio": 1.6920415224913494, "no_speech_prob":
  0.009588594548404217}, {"id": 381, "seek": 249080, "start": 2494.7200000000003,
  "end": 2501.6800000000003, "text": " it was, you know, you had to go and really
  dig in underneath the hood and kind of figure out a lot", "tokens": [50560, 309,
  390, 11, 291, 458, 11, 291, 632, 281, 352, 293, 534, 2528, 294, 7223, 264, 13376,
  293, 733, 295, 2573, 484, 257, 688, 50908], "temperature": 0.0, "avg_logprob": -0.10030921936035156,
  "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.009588594548404217},
  {"id": 382, "seek": 249080, "start": 2501.6800000000003, "end": 2507.84, "text":
  " of those pieces these days. It would take several weeks, if not months, you know,
  month or more to", "tokens": [50908, 295, 729, 3755, 613, 1708, 13, 467, 576, 747,
  2940, 3259, 11, 498, 406, 2493, 11, 291, 458, 11, 1618, 420, 544, 281, 51216], "temperature":
  0.0, "avg_logprob": -0.10030921936035156, "compression_ratio": 1.6920415224913494,
  "no_speech_prob": 0.009588594548404217}, {"id": 383, "seek": 249080, "start": 2507.84,
  "end": 2513.2000000000003, "text": " really feel like you understood an engine and
  where it went. And I think these days, it''s just so", "tokens": [51216, 534, 841,
  411, 291, 7320, 364, 2848, 293, 689, 309, 1437, 13, 400, 286, 519, 613, 1708, 11,
  309, 311, 445, 370, 51484], "temperature": 0.0, "avg_logprob": -0.10030921936035156,
  "compression_ratio": 1.6920415224913494, "no_speech_prob": 0.009588594548404217},
  {"id": 384, "seek": 249080, "start": 2513.2000000000003, "end": 2518.2400000000002,
  "text": " much easier to do that, which is awesome. Yeah, absolutely. And I remember
  during the course", "tokens": [51484, 709, 3571, 281, 360, 300, 11, 597, 307, 3476,
  13, 865, 11, 3122, 13, 400, 286, 1604, 1830, 264, 1164, 51736], "temperature": 0.0,
  "avg_logprob": -0.10030921936035156, "compression_ratio": 1.6920415224913494, "no_speech_prob":
  0.009588594548404217}, {"id": 385, "seek": 251824, "start": 2518.24, "end": 2527.2799999999997,
  "text": " we had to do it within a week. So per project. So that was super exciting.
  And I think this would", "tokens": [50364, 321, 632, 281, 360, 309, 1951, 257, 1243,
  13, 407, 680, 1716, 13, 407, 300, 390, 1687, 4670, 13, 400, 286, 519, 341, 576,
  50816], "temperature": 0.0, "avg_logprob": -0.15960913735467033, "compression_ratio":
  1.5368421052631578, "no_speech_prob": 0.0042487503960728645}, {"id": 386, "seek":
  251824, "start": 2527.2799999999997, "end": 2533.12, "text": " not be a vector podcast
  if I wouldn''t ask you also on your opinion in vector search. Like what''s your",
  "tokens": [50816, 406, 312, 257, 8062, 7367, 498, 286, 2759, 380, 1029, 291, 611,
  322, 428, 4800, 294, 8062, 3164, 13, 1743, 437, 311, 428, 51108], "temperature":
  0.0, "avg_logprob": -0.15960913735467033, "compression_ratio": 1.5368421052631578,
  "no_speech_prob": 0.0042487503960728645}, {"id": 387, "seek": 251824, "start": 2533.9199999999996,
  "end": 2542.24, "text": " feel for how it will augment the search engine experience
  on the user side as well as on the", "tokens": [51148, 841, 337, 577, 309, 486,
  29919, 264, 3164, 2848, 1752, 322, 264, 4195, 1252, 382, 731, 382, 322, 264, 51564],
  "temperature": 0.0, "avg_logprob": -0.15960913735467033, "compression_ratio": 1.5368421052631578,
  "no_speech_prob": 0.0042487503960728645}, {"id": 388, "seek": 254224, "start": 2542.3199999999997,
  "end": 2549.52, "text": " development side and connected to that. What do you think
  the search engine engineer profession is", "tokens": [50368, 3250, 1252, 293, 4582,
  281, 300, 13, 708, 360, 291, 519, 264, 3164, 2848, 11403, 7032, 307, 50728], "temperature":
  0.0, "avg_logprob": -0.1299250176612367, "compression_ratio": 1.6965811965811965,
  "no_speech_prob": 0.003778139129281044}, {"id": 389, "seek": 254224, "start": 2549.52,
  "end": 2554.8799999999997, "text": " going to be like soon? And I think it''s already
  shaping up in many ways. Like the boundary between", "tokens": [50728, 516, 281,
  312, 411, 2321, 30, 400, 286, 519, 309, 311, 1217, 25945, 493, 294, 867, 2098, 13,
  1743, 264, 12866, 1296, 50996], "temperature": 0.0, "avg_logprob": -0.1299250176612367,
  "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.003778139129281044},
  {"id": 390, "seek": 254224, "start": 2554.8799999999997, "end": 2562.08, "text":
  " data scientists and the search engineer blend. Do you feel yourself like that?
  Do you think this is", "tokens": [50996, 1412, 7708, 293, 264, 3164, 11403, 10628,
  13, 1144, 291, 841, 1803, 411, 300, 30, 1144, 291, 519, 341, 307, 51356], "temperature":
  0.0, "avg_logprob": -0.1299250176612367, "compression_ratio": 1.6965811965811965,
  "no_speech_prob": 0.003778139129281044}, {"id": 391, "seek": 254224, "start": 2562.08,
  "end": 2567.7599999999998, "text": " the direction we are going? Or do you think
  it''s going to be like a form that will wear off? That''s", "tokens": [51356, 264,
  3513, 321, 366, 516, 30, 1610, 360, 291, 519, 309, 311, 516, 281, 312, 411, 257,
  1254, 300, 486, 3728, 766, 30, 663, 311, 51640], "temperature": 0.0, "avg_logprob":
  -0.1299250176612367, "compression_ratio": 1.6965811965811965, "no_speech_prob":
  0.003778139129281044}, {"id": 392, "seek": 256776, "start": 2567.84, "end": 2573.5200000000004,
  "text": " at some point. Yeah, I mean, it''s, well, it''s not going to wear off.
  I mean, there''s too much money", "tokens": [50368, 412, 512, 935, 13, 865, 11,
  286, 914, 11, 309, 311, 11, 731, 11, 309, 311, 406, 516, 281, 3728, 766, 13, 286,
  914, 11, 456, 311, 886, 709, 1460, 50652], "temperature": 0.0, "avg_logprob": -0.18364983070187452,
  "compression_ratio": 1.5591397849462365, "no_speech_prob": 0.0016648039454594254},
  {"id": 393, "seek": 256776, "start": 2573.5200000000004, "end": 2581.1200000000003,
  "text": " and too much investment and too much better results. I will state upfront,
  I''m not an expert on", "tokens": [50652, 293, 886, 709, 6078, 293, 886, 709, 1101,
  3542, 13, 286, 486, 1785, 30264, 11, 286, 478, 406, 364, 5844, 322, 51032], "temperature":
  0.0, "avg_logprob": -0.18364983070187452, "compression_ratio": 1.5591397849462365,
  "no_speech_prob": 0.0016648039454594254}, {"id": 394, "seek": 256776, "start": 2581.1200000000003,
  "end": 2589.84, "text": " these vector engines, right? Like I, it''s kind of interesting.
  Like they, I went back and look", "tokens": [51032, 613, 8062, 12982, 11, 558, 30,
  1743, 286, 11, 309, 311, 733, 295, 1880, 13, 1743, 436, 11, 286, 1437, 646, 293,
  574, 51468], "temperature": 0.0, "avg_logprob": -0.18364983070187452, "compression_ratio":
  1.5591397849462365, "no_speech_prob": 0.0016648039454594254}, {"id": 395, "seek":
  258984, "start": 2589.84, "end": 2598.48, "text": " through some of my talks and
  I think I gave a talk in 2013 on what the Lucine and Solar community", "tokens":
  [50364, 807, 512, 295, 452, 6686, 293, 286, 519, 286, 2729, 257, 751, 294, 9012,
  322, 437, 264, 9593, 533, 293, 22385, 1768, 50796], "temperature": 0.0, "avg_logprob":
  -0.16044195810953776, "compression_ratio": 1.5051020408163265, "no_speech_prob":
  0.006680662278085947}, {"id": 396, "seek": 258984, "start": 2598.48, "end": 2606.0,
  "text": " needed to do next. And one of the things was we need to add support for
  dense vectors. That was 2013.", "tokens": [50796, 2978, 281, 360, 958, 13, 400,
  472, 295, 264, 721, 390, 321, 643, 281, 909, 1406, 337, 18011, 18875, 13, 663, 390,
  9012, 13, 51172], "temperature": 0.0, "avg_logprob": -0.16044195810953776, "compression_ratio":
  1.5051020408163265, "no_speech_prob": 0.006680662278085947}, {"id": 397, "seek":
  258984, "start": 2606.7200000000003, "end": 2615.04, "text": " I think we just got
  dense vector support in solar. Elastic maybe was there a little bit sooner,", "tokens":
  [51208, 286, 519, 321, 445, 658, 18011, 8062, 1406, 294, 7936, 13, 2699, 2750, 1310,
  390, 456, 257, 707, 857, 15324, 11, 51624], "temperature": 0.0, "avg_logprob": -0.16044195810953776,
  "compression_ratio": 1.5051020408163265, "no_speech_prob": 0.006680662278085947},
  {"id": 398, "seek": 261504, "start": 2615.04, "end": 2622.32, "text": " but roughly
  same time frame. There are plugins, of course, that have been around like the K&N",
  "tokens": [50364, 457, 9810, 912, 565, 3920, 13, 821, 366, 33759, 11, 295, 1164,
  11, 300, 362, 668, 926, 411, 264, 591, 5, 45, 50728], "temperature": 0.0, "avg_logprob":
  -0.18812683837054528, "compression_ratio": 1.5, "no_speech_prob": 0.004257539752870798},
  {"id": 399, "seek": 261504, "start": 2622.32, "end": 2628.08, "text": " plugins,
  things like that. Hey folks, like this stuff is here to stay. I mean, the really
  interesting", "tokens": [50728, 33759, 11, 721, 411, 300, 13, 1911, 4024, 11, 411,
  341, 1507, 307, 510, 281, 1754, 13, 286, 914, 11, 264, 534, 1880, 51016], "temperature":
  0.0, "avg_logprob": -0.18812683837054528, "compression_ratio": 1.5, "no_speech_prob":
  0.004257539752870798}, {"id": 400, "seek": 261504, "start": 2628.08, "end": 2637.36,
  "text": " questions, you''re starting to see these hybrid models where, like BM25
  is still really good and", "tokens": [51016, 1651, 11, 291, 434, 2891, 281, 536,
  613, 13051, 5245, 689, 11, 411, 15901, 6074, 307, 920, 534, 665, 293, 51480], "temperature":
  0.0, "avg_logprob": -0.18812683837054528, "compression_ratio": 1.5, "no_speech_prob":
  0.004257539752870798}, {"id": 401, "seek": 263736, "start": 2637.36, "end": 2644.56,
  "text": " really fast at that first pass retrieval. It''s kind of hard to beat in
  terms of the scale at which", "tokens": [50364, 534, 2370, 412, 300, 700, 1320,
  19817, 3337, 13, 467, 311, 733, 295, 1152, 281, 4224, 294, 2115, 295, 264, 4373,
  412, 597, 50724], "temperature": 0.0, "avg_logprob": -0.11015045249855125, "compression_ratio":
  1.6322314049586777, "no_speech_prob": 0.0017154898960143328}, {"id": 402, "seek":
  263736, "start": 2644.56, "end": 2654.6400000000003, "text": " you can get a first
  pass rank, right? And then feeding it, those results into much deeper or more",
  "tokens": [50724, 291, 393, 483, 257, 700, 1320, 6181, 11, 558, 30, 400, 550, 12919,
  309, 11, 729, 3542, 666, 709, 7731, 420, 544, 51228], "temperature": 0.0, "avg_logprob":
  -0.11015045249855125, "compression_ratio": 1.6322314049586777, "no_speech_prob":
  0.0017154898960143328}, {"id": 403, "seek": 263736, "start": 2654.6400000000003,
  "end": 2661.28, "text": " capable engines. I think that''s been around for a while
  and academia has proven that out. Clearly,", "tokens": [51228, 8189, 12982, 13,
  286, 519, 300, 311, 668, 926, 337, 257, 1339, 293, 28937, 575, 12785, 300, 484,
  13, 24120, 11, 51560], "temperature": 0.0, "avg_logprob": -0.11015045249855125,
  "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0017154898960143328},
  {"id": 404, "seek": 263736, "start": 2661.28, "end": 2666.88, "text": " like using
  embeddings and vectors for things like query understanding and content understanding
  and", "tokens": [51560, 411, 1228, 12240, 29432, 293, 18875, 337, 721, 411, 14581,
  3701, 293, 2701, 3701, 293, 51840], "temperature": 0.0, "avg_logprob": -0.11015045249855125,
  "compression_ratio": 1.6322314049586777, "no_speech_prob": 0.0017154898960143328},
  {"id": 405, "seek": 266688, "start": 2666.88, "end": 2673.52, "text": " using tools
  like Burnt, etc. for enriching your understanding, your content, and then", "tokens":
  [50364, 1228, 3873, 411, 7031, 580, 11, 5183, 13, 337, 18849, 278, 428, 3701, 11,
  428, 2701, 11, 293, 550, 50696], "temperature": 0.0, "avg_logprob": -0.17834359949285333,
  "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.0001374840212520212},
  {"id": 406, "seek": 266688, "start": 2675.6800000000003, "end": 2683.12, "text":
  " making those searchable. That''s all, I think, well and good. I think the really
  interesting", "tokens": [50804, 1455, 729, 3164, 712, 13, 663, 311, 439, 11, 286,
  519, 11, 731, 293, 665, 13, 286, 519, 264, 534, 1880, 51176], "temperature": 0.0,
  "avg_logprob": -0.17834359949285333, "compression_ratio": 1.5113636363636365, "no_speech_prob":
  0.0001374840212520212}, {"id": 407, "seek": 266688, "start": 2683.12, "end": 2692.2400000000002,
  "text": " question will be is whether the vector engines can add all of the layers
  that the sparse", "tokens": [51176, 1168, 486, 312, 307, 1968, 264, 8062, 12982,
  393, 909, 439, 295, 264, 7914, 300, 264, 637, 11668, 51632], "temperature": 0.0,
  "avg_logprob": -0.17834359949285333, "compression_ratio": 1.5113636363636365, "no_speech_prob":
  0.0001374840212520212}, {"id": 408, "seek": 269224, "start": 2692.24, "end": 2697.3599999999997,
  "text": " approaches have, I don''t know about perfected, but added over the years,
  you know, the", "tokens": [50364, 11587, 362, 11, 286, 500, 380, 458, 466, 2176,
  292, 11, 457, 3869, 670, 264, 924, 11, 291, 458, 11, 264, 50620], "temperature":
  0.0, "avg_logprob": -0.13036582175265538, "compression_ratio": 1.6592920353982301,
  "no_speech_prob": 0.0015548147493973374}, {"id": 409, "seek": 269224, "start": 2697.3599999999997,
  "end": 2702.56, "text": " fascinating, the aggregations, the spell checkings, the
  highlighting, all of those things that", "tokens": [50620, 10343, 11, 264, 16743,
  763, 11, 264, 9827, 1520, 1109, 11, 264, 26551, 11, 439, 295, 729, 721, 300, 50880],
  "temperature": 0.0, "avg_logprob": -0.13036582175265538, "compression_ratio": 1.6592920353982301,
  "no_speech_prob": 0.0015548147493973374}, {"id": 410, "seek": 269224, "start": 2702.56,
  "end": 2709.4399999999996, "text": " actually go into building a search application.
  If the vector engines deliver all of those things", "tokens": [50880, 767, 352,
  666, 2390, 257, 3164, 3861, 13, 759, 264, 8062, 12982, 4239, 439, 295, 729, 721,
  51224], "temperature": 0.0, "avg_logprob": -0.13036582175265538, "compression_ratio":
  1.6592920353982301, "no_speech_prob": 0.0015548147493973374}, {"id": 411, "seek":
  269224, "start": 2709.4399999999996, "end": 2717.3599999999997, "text": " and deliver
  better results, that''s probably a no brainer, right? In the meantime, we have these",
  "tokens": [51224, 293, 4239, 1101, 3542, 11, 300, 311, 1391, 257, 572, 3567, 260,
  11, 558, 30, 682, 264, 14991, 11, 321, 362, 613, 51620], "temperature": 0.0, "avg_logprob":
  -0.13036582175265538, "compression_ratio": 1.6592920353982301, "no_speech_prob":
  0.0015548147493973374}, {"id": 412, "seek": 271736, "start": 2717.36, "end": 2722.0,
  "text": " hybrids because I think there nobody is delivering all of the capabilities.
  The other things that''s", "tokens": [50364, 2477, 1443, 3742, 570, 286, 519, 456,
  5079, 307, 14666, 439, 295, 264, 10862, 13, 440, 661, 721, 300, 311, 50596], "temperature":
  0.0, "avg_logprob": -0.17125726782757303, "compression_ratio": 1.6544117647058822,
  "no_speech_prob": 0.0016281824791803956}, {"id": 413, "seek": 271736, "start": 2722.0,
  "end": 2728.2400000000002, "text": " interesting with the dense vectors, right,
  is that you can start to map multimodal data types", "tokens": [50596, 1880, 365,
  264, 18011, 18875, 11, 558, 11, 307, 300, 291, 393, 722, 281, 4471, 32972, 378,
  304, 1412, 3467, 50908], "temperature": 0.0, "avg_logprob": -0.17125726782757303,
  "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.0016281824791803956},
  {"id": 414, "seek": 271736, "start": 2728.2400000000002, "end": 2734.48, "text":
  " all into the same engine. So images and text and audio, etc. Right? And again,
  like I''m not an", "tokens": [50908, 439, 666, 264, 912, 2848, 13, 407, 5267, 293,
  2487, 293, 6278, 11, 5183, 13, 1779, 30, 400, 797, 11, 411, 286, 478, 406, 364,
  51220], "temperature": 0.0, "avg_logprob": -0.17125726782757303, "compression_ratio":
  1.6544117647058822, "no_speech_prob": 0.0016281824791803956}, {"id": 415, "seek":
  271736, "start": 2734.48, "end": 2738.32, "text": " expert on this, but that''s
  my understanding. So then, so then you can query across", "tokens": [51220, 5844,
  322, 341, 11, 457, 300, 311, 452, 3701, 13, 407, 550, 11, 370, 550, 291, 393, 14581,
  2108, 51412], "temperature": 0.0, "avg_logprob": -0.17125726782757303, "compression_ratio":
  1.6544117647058822, "no_speech_prob": 0.0016281824791803956}, {"id": 416, "seek":
  271736, "start": 2740.0, "end": 2743.44, "text": " spaces, if you will. Again, like
  I''m not using the right terminology here, but", "tokens": [51496, 7673, 11, 498,
  291, 486, 13, 3764, 11, 411, 286, 478, 406, 1228, 264, 558, 27575, 510, 11, 457,
  51668], "temperature": 0.0, "avg_logprob": -0.17125726782757303, "compression_ratio":
  1.6544117647058822, "no_speech_prob": 0.0016281824791803956}, {"id": 417, "seek":
  274344, "start": 2743.84, "end": 2751.04, "text": " and that to me is often the,
  at least people talk about that like it''s a holy grail. I''m not fully", "tokens":
  [50384, 293, 300, 281, 385, 307, 2049, 264, 11, 412, 1935, 561, 751, 466, 300, 411,
  309, 311, 257, 10622, 1295, 388, 13, 286, 478, 406, 4498, 50744], "temperature":
  0.0, "avg_logprob": -0.12974972085854442, "compression_ratio": 1.672340425531915,
  "no_speech_prob": 0.008284059353172779}, {"id": 418, "seek": 274344, "start": 2751.04,
  "end": 2757.76, "text": " convinced people will actually search that way. I still
  think that remains to seem because there''s", "tokens": [50744, 12561, 561, 486,
  767, 3164, 300, 636, 13, 286, 920, 519, 300, 7023, 281, 1643, 570, 456, 311, 51080],
  "temperature": 0.0, "avg_logprob": -0.12974972085854442, "compression_ratio": 1.672340425531915,
  "no_speech_prob": 0.008284059353172779}, {"id": 419, "seek": 274344, "start": 2757.76,
  "end": 2764.16, "text": " a lot of implications for the the user interface and the
  user experience is how you interact with", "tokens": [51080, 257, 688, 295, 16602,
  337, 264, 264, 4195, 9226, 293, 264, 4195, 1752, 307, 577, 291, 4648, 365, 51400],
  "temperature": 0.0, "avg_logprob": -0.12974972085854442, "compression_ratio": 1.672340425531915,
  "no_speech_prob": 0.008284059353172779}, {"id": 420, "seek": 274344, "start": 2764.16,
  "end": 2768.96, "text": " that. You know, like people have long talked about, oh,
  hey, I''m going to take a picture and then", "tokens": [51400, 300, 13, 509, 458,
  11, 411, 561, 362, 938, 2825, 466, 11, 1954, 11, 4177, 11, 286, 478, 516, 281, 747,
  257, 3036, 293, 550, 51640], "temperature": 0.0, "avg_logprob": -0.12974972085854442,
  "compression_ratio": 1.672340425531915, "no_speech_prob": 0.008284059353172779},
  {"id": 421, "seek": 276896, "start": 2768.96, "end": 2773.92, "text": " get my back,
  my search results, but like I don''t every time I use those tools, I''m like, okay,",
  "tokens": [50364, 483, 452, 646, 11, 452, 3164, 3542, 11, 457, 411, 286, 500, 380,
  633, 565, 286, 764, 729, 3873, 11, 286, 478, 411, 11, 1392, 11, 50612], "temperature":
  0.0, "avg_logprob": -0.10362108321416946, "compression_ratio": 1.606694560669456,
  "no_speech_prob": 0.0011397736379876733}, {"id": 422, "seek": 276896, "start": 2773.92,
  "end": 2780.32, "text": " that''s nice, but it''s still clunky from a user experience
  standpoint, right? So, so like there''s", "tokens": [50612, 300, 311, 1481, 11,
  457, 309, 311, 920, 596, 25837, 490, 257, 4195, 1752, 15827, 11, 558, 30, 407, 11,
  370, 411, 456, 311, 50932], "temperature": 0.0, "avg_logprob": -0.10362108321416946,
  "compression_ratio": 1.606694560669456, "no_speech_prob": 0.0011397736379876733},
  {"id": 423, "seek": 276896, "start": 2780.32, "end": 2788.64, "text": " a lot of
  that work above and beyond just the core engine that has to be solved. But clearly,",
  "tokens": [50932, 257, 688, 295, 300, 589, 3673, 293, 4399, 445, 264, 4965, 2848,
  300, 575, 281, 312, 13041, 13, 583, 4448, 11, 51348], "temperature": 0.0, "avg_logprob":
  -0.10362108321416946, "compression_ratio": 1.606694560669456, "no_speech_prob":
  0.0011397736379876733}, {"id": 424, "seek": 276896, "start": 2788.64, "end": 2793.6,
  "text": " there''s a lot of money and effort going into it. And so like as a search
  engineer, you can''t ignore", "tokens": [51348, 456, 311, 257, 688, 295, 1460, 293,
  4630, 516, 666, 309, 13, 400, 370, 411, 382, 257, 3164, 11403, 11, 291, 393, 380,
  11200, 51596], "temperature": 0.0, "avg_logprob": -0.10362108321416946, "compression_ratio":
  1.606694560669456, "no_speech_prob": 0.0011397736379876733}, {"id": 425, "seek":
  279360, "start": 2794.0, "end": 2799.2799999999997, "text": " as a data scientist,
  you can''t ignore it. And so you''ve got to get up on how these are built.", "tokens":
  [50384, 382, 257, 1412, 12662, 11, 291, 393, 380, 11200, 309, 13, 400, 370, 291,
  600, 658, 281, 483, 493, 322, 577, 613, 366, 3094, 13, 50648], "temperature": 0.0,
  "avg_logprob": -0.0827808478443893, "compression_ratio": 1.6163793103448276, "no_speech_prob":
  0.008597486652433872}, {"id": 426, "seek": 279360, "start": 2800.0, "end": 2806.3199999999997,
  "text": " I think all the major engines open source and private have some form of
  it at this point of", "tokens": [50684, 286, 519, 439, 264, 2563, 12982, 1269, 4009,
  293, 4551, 362, 512, 1254, 295, 309, 412, 341, 935, 295, 51000], "temperature":
  0.0, "avg_logprob": -0.0827808478443893, "compression_ratio": 1.6163793103448276,
  "no_speech_prob": 0.008597486652433872}, {"id": 427, "seek": 279360, "start": 2806.3199999999997,
  "end": 2812.4, "text": " blended models. Again, like, you know, if you''re in a
  domain that you don''t have enough data for", "tokens": [51000, 27048, 5245, 13,
  3764, 11, 411, 11, 291, 458, 11, 498, 291, 434, 294, 257, 9274, 300, 291, 500, 380,
  362, 1547, 1412, 337, 51304], "temperature": 0.0, "avg_logprob": -0.0827808478443893,
  "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.008597486652433872},
  {"id": 428, "seek": 279360, "start": 2812.4, "end": 2818.72, "text": " these and
  may or may not work, although again, like one of the interesting things with these",
  "tokens": [51304, 613, 293, 815, 420, 815, 406, 589, 11, 4878, 797, 11, 411, 472,
  295, 264, 1880, 721, 365, 613, 51620], "temperature": 0.0, "avg_logprob": -0.0827808478443893,
  "compression_ratio": 1.6163793103448276, "no_speech_prob": 0.008597486652433872},
  {"id": 429, "seek": 281872, "start": 2819.52, "end": 2825.3599999999997, "text":
  " neural models, right, is you can often train on a general model and then just
  use a few examples", "tokens": [50404, 18161, 5245, 11, 558, 11, 307, 291, 393,
  2049, 3847, 322, 257, 2674, 2316, 293, 550, 445, 764, 257, 1326, 5110, 50696], "temperature":
  0.0, "avg_logprob": -0.08858027403382049, "compression_ratio": 1.6375545851528384,
  "no_speech_prob": 0.0026207230985164642}, {"id": 430, "seek": 281872, "start": 2825.3599999999997,
  "end": 2832.0, "text": " from your domain to essentially tailor that general model
  to your environment, right? Like I''m", "tokens": [50696, 490, 428, 9274, 281, 4476,
  33068, 300, 2674, 2316, 281, 428, 2823, 11, 558, 30, 1743, 286, 478, 51028], "temperature":
  0.0, "avg_logprob": -0.08858027403382049, "compression_ratio": 1.6375545851528384,
  "no_speech_prob": 0.0026207230985164642}, {"id": 431, "seek": 281872, "start": 2832.0,
  "end": 2838.3199999999997, "text": " working on one of my clients is doing this
  in the NLP space right now. We''re using a general", "tokens": [51028, 1364, 322,
  472, 295, 452, 6982, 307, 884, 341, 294, 264, 426, 45196, 1901, 558, 586, 13, 492,
  434, 1228, 257, 2674, 51344], "temperature": 0.0, "avg_logprob": -0.08858027403382049,
  "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0026207230985164642},
  {"id": 432, "seek": 281872, "start": 2838.3199999999997, "end": 2845.4399999999996,
  "text": " model around analyzing contracts and then we''re applying domain specific
  things to it. And", "tokens": [51344, 2316, 926, 23663, 13952, 293, 550, 321, 434,
  9275, 9274, 2685, 721, 281, 309, 13, 400, 51700], "temperature": 0.0, "avg_logprob":
  -0.08858027403382049, "compression_ratio": 1.6375545851528384, "no_speech_prob":
  0.0026207230985164642}, {"id": 433, "seek": 284544, "start": 2845.44, "end": 2851.92,
  "text": " it''s really interesting how effective it is with very few examples, right?
  That''s an NLP problem,", "tokens": [50364, 309, 311, 534, 1880, 577, 4942, 309,
  307, 365, 588, 1326, 5110, 11, 558, 30, 663, 311, 364, 426, 45196, 1154, 11, 50688],
  "temperature": 0.0, "avg_logprob": -0.0966919198328135, "compression_ratio": 1.5219123505976095,
  "no_speech_prob": 0.0020108248572796583}, {"id": 434, "seek": 284544, "start": 2851.92,
  "end": 2857.04, "text": " not a search problem, but you know, so I think you''re
  going to just continue to see that trend", "tokens": [50688, 406, 257, 3164, 1154,
  11, 457, 291, 458, 11, 370, 286, 519, 291, 434, 516, 281, 445, 2354, 281, 536, 300,
  6028, 50944], "temperature": 0.0, "avg_logprob": -0.0966919198328135, "compression_ratio":
  1.5219123505976095, "no_speech_prob": 0.0020108248572796583}, {"id": 435, "seek":
  284544, "start": 2857.04, "end": 2863.76, "text": " and grow and expand, right?
  So you''ve got to be on board with it. Yeah, absolutely. And you can", "tokens":
  [50944, 293, 1852, 293, 5268, 11, 558, 30, 407, 291, 600, 658, 281, 312, 322, 3150,
  365, 309, 13, 865, 11, 3122, 13, 400, 291, 393, 51280], "temperature": 0.0, "avg_logprob":
  -0.0966919198328135, "compression_ratio": 1.5219123505976095, "no_speech_prob":
  0.0020108248572796583}, {"id": 436, "seek": 284544, "start": 2863.76, "end": 2869.84,
  "text": " find of course more conversation on the podcast about this. But I think
  I agree with you that", "tokens": [51280, 915, 295, 1164, 544, 3761, 322, 264, 7367,
  466, 341, 13, 583, 286, 519, 286, 3986, 365, 291, 300, 51584], "temperature": 0.0,
  "avg_logprob": -0.0966919198328135, "compression_ratio": 1.5219123505976095, "no_speech_prob":
  0.0020108248572796583}, {"id": 437, "seek": 286984, "start": 2870.48, "end": 2876.48,
  "text": " the multimodality aspect of vector search is quite exciting. And where
  the data sits in images,", "tokens": [50396, 264, 32972, 378, 1860, 4171, 295, 8062,
  3164, 307, 1596, 4670, 13, 400, 689, 264, 1412, 12696, 294, 5267, 11, 50696], "temperature":
  0.0, "avg_logprob": -0.14240115880966187, "compression_ratio": 1.5766129032258065,
  "no_speech_prob": 0.003340014023706317}, {"id": 438, "seek": 286984, "start": 2876.48,
  "end": 2881.44, "text": " for instance, that haven''t been annotated yet, right?
  And so many images uploaded every single day", "tokens": [50696, 337, 5197, 11,
  300, 2378, 380, 668, 25339, 770, 1939, 11, 558, 30, 400, 370, 867, 5267, 17135,
  633, 2167, 786, 50944], "temperature": 0.0, "avg_logprob": -0.14240115880966187,
  "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.003340014023706317},
  {"id": 439, "seek": 286984, "start": 2881.44, "end": 2889.1200000000003, "text":
  " in videos, you know, if the model is able to transcend the the domains so easily
  like clip model,", "tokens": [50944, 294, 2145, 11, 291, 458, 11, 498, 264, 2316,
  307, 1075, 281, 28535, 264, 264, 25514, 370, 3612, 411, 7353, 2316, 11, 51328],
  "temperature": 0.0, "avg_logprob": -0.14240115880966187, "compression_ratio": 1.5766129032258065,
  "no_speech_prob": 0.003340014023706317}, {"id": 440, "seek": 286984, "start": 2889.1200000000003,
  "end": 2895.6800000000003, "text": " for instance, built by OpenAI, it''s not a
  perfect model. Sometimes it fails, but sometimes it also", "tokens": [51328, 337,
  5197, 11, 3094, 538, 7238, 48698, 11, 309, 311, 406, 257, 2176, 2316, 13, 4803,
  309, 18199, 11, 457, 2171, 309, 611, 51656], "temperature": 0.0, "avg_logprob":
  -0.14240115880966187, "compression_ratio": 1.5766129032258065, "no_speech_prob":
  0.003340014023706317}, {"id": 441, "seek": 289568, "start": 2895.68, "end": 2903.2799999999997,
  "text": " uses you like, how could it figure out, you know, to work so reliably
  on my data that it hasn''t", "tokens": [50364, 4960, 291, 411, 11, 577, 727, 309,
  2573, 484, 11, 291, 458, 11, 281, 589, 370, 49927, 322, 452, 1412, 300, 309, 6132,
  380, 50744], "temperature": 0.0, "avg_logprob": -0.13609039783477783, "compression_ratio":
  1.6805555555555556, "no_speech_prob": 0.004394518677145243}, {"id": 442, "seek":
  289568, "start": 2903.2799999999997, "end": 2909.2, "text": " seen before? That''s
  amazing. Well, and it goes back to your earlier question, which is like,", "tokens":
  [50744, 1612, 949, 30, 663, 311, 2243, 13, 1042, 11, 293, 309, 1709, 646, 281, 428,
  3071, 1168, 11, 597, 307, 411, 11, 51040], "temperature": 0.0, "avg_logprob": -0.13609039783477783,
  "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.004394518677145243},
  {"id": 443, "seek": 289568, "start": 2909.2, "end": 2914.24, "text": " you know,
  at the end of the day, folks like go evaluate it and see whether it works better
  for you.", "tokens": [51040, 291, 458, 11, 412, 264, 917, 295, 264, 786, 11, 4024,
  411, 352, 13059, 309, 293, 536, 1968, 309, 1985, 1101, 337, 291, 13, 51292], "temperature":
  0.0, "avg_logprob": -0.13609039783477783, "compression_ratio": 1.6805555555555556,
  "no_speech_prob": 0.004394518677145243}, {"id": 444, "seek": 289568, "start": 2914.96,
  "end": 2918.7999999999997, "text": " And then like I said, even earlier, I mean,
  they''re all just vectors and we''re all just trying to", "tokens": [51328, 400,
  550, 411, 286, 848, 11, 754, 3071, 11, 286, 914, 11, 436, 434, 439, 445, 18875,
  293, 321, 434, 439, 445, 1382, 281, 51520], "temperature": 0.0, "avg_logprob": -0.13609039783477783,
  "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.004394518677145243},
  {"id": 445, "seek": 289568, "start": 2918.7999999999997, "end": 2925.04, "text":
  " calculate cosines between the user''s query and and the vector. And so in some
  regards, like we''re", "tokens": [51520, 8873, 3792, 1652, 1296, 264, 4195, 311,
  14581, 293, 293, 264, 8062, 13, 400, 370, 294, 512, 14258, 11, 411, 321, 434, 51832],
  "temperature": 0.0, "avg_logprob": -0.13609039783477783, "compression_ratio": 1.6805555555555556,
  "no_speech_prob": 0.004394518677145243}, {"id": 446, "seek": 292504, "start": 2925.12,
  "end": 2932.4, "text": " just building a better vector, right? It''s just a better
  vector. It has more information encoded in it.", "tokens": [50368, 445, 2390, 257,
  1101, 8062, 11, 558, 30, 467, 311, 445, 257, 1101, 8062, 13, 467, 575, 544, 1589,
  2058, 12340, 294, 309, 13, 50732], "temperature": 0.0, "avg_logprob": -0.16321213731487977,
  "compression_ratio": 1.528, "no_speech_prob": 0.000682392914313823}, {"id": 447,
  "seek": 292504, "start": 2932.4, "end": 2937.44, "text": " And so if I can query
  that more effectively, then why wouldn''t you use it?", "tokens": [50732, 400, 370,
  498, 286, 393, 14581, 300, 544, 8659, 11, 550, 983, 2759, 380, 291, 764, 309, 30,
  50984], "temperature": 0.0, "avg_logprob": -0.16321213731487977, "compression_ratio":
  1.528, "no_speech_prob": 0.000682392914313823}, {"id": 448, "seek": 292504, "start":
  2938.8, "end": 2945.36, "text": " Yeah, yeah, exactly. And of course, there are
  other subtopics there how to make it faster and so on,", "tokens": [51052, 865,
  11, 1338, 11, 2293, 13, 400, 295, 1164, 11, 456, 366, 661, 7257, 404, 1167, 456,
  577, 281, 652, 309, 4663, 293, 370, 322, 11, 51380], "temperature": 0.0, "avg_logprob":
  -0.16321213731487977, "compression_ratio": 1.528, "no_speech_prob": 0.000682392914313823},
  {"id": 449, "seek": 292504, "start": 2945.36, "end": 2951.2, "text": " but I think
  eventually we will, hey, Google figured it out for 10% of the queries. So I guess
  the rest", "tokens": [51380, 457, 286, 519, 4728, 321, 486, 11, 4177, 11, 3329,
  8932, 309, 484, 337, 1266, 4, 295, 264, 24109, 13, 407, 286, 2041, 264, 1472, 51672],
  "temperature": 0.0, "avg_logprob": -0.16321213731487977, "compression_ratio": 1.528,
  "no_speech_prob": 0.000682392914313823}, {"id": 450, "seek": 295120, "start": 2951.2,
  "end": 2957.8399999999997, "text": " of the world will catch up. Before we continue
  to the questions from the audience of which we have", "tokens": [50364, 295, 264,
  1002, 486, 3745, 493, 13, 4546, 321, 2354, 281, 264, 1651, 490, 264, 4034, 295,
  597, 321, 362, 50696], "temperature": 0.0, "avg_logprob": -0.11356743176778157,
  "compression_ratio": 1.565040650406504, "no_speech_prob": 0.006591591518372297},
  {"id": 451, "seek": 295120, "start": 2957.8399999999997, "end": 2963.2799999999997,
  "text": " at you, I do love asking, and if you can keep it a little bit short, because
  we are short on time,", "tokens": [50696, 412, 291, 11, 286, 360, 959, 3365, 11,
  293, 498, 291, 393, 1066, 309, 257, 707, 857, 2099, 11, 570, 321, 366, 2099, 322,
  565, 11, 50968], "temperature": 0.0, "avg_logprob": -0.11356743176778157, "compression_ratio":
  1.565040650406504, "no_speech_prob": 0.006591591518372297}, {"id": 452, "seek":
  295120, "start": 2963.2799999999997, "end": 2970.08, "text": " but I''m still super,
  super interested to hear your motivation to stay in this space. You have", "tokens":
  [50968, 457, 286, 478, 920, 1687, 11, 1687, 3102, 281, 1568, 428, 12335, 281, 1754,
  294, 341, 1901, 13, 509, 362, 51308], "temperature": 0.0, "avg_logprob": -0.11356743176778157,
  "compression_ratio": 1.565040650406504, "no_speech_prob": 0.006591591518372297},
  {"id": 453, "seek": 295120, "start": 2970.08, "end": 2975.3599999999997, "text":
  " tried so many things in your career, right? Looking at your LinkedIn profiles,
  just on and on", "tokens": [51308, 3031, 370, 867, 721, 294, 428, 3988, 11, 558,
  30, 11053, 412, 428, 20657, 23693, 11, 445, 322, 293, 322, 51572], "temperature":
  0.0, "avg_logprob": -0.11356743176778157, "compression_ratio": 1.565040650406504,
  "no_speech_prob": 0.006591591518372297}, {"id": 454, "seek": 297536, "start": 2975.36,
  "end": 2981.52, "text": " experiences and fractional CTO and full-time CTO and an
  engineer and so on and book author.", "tokens": [50364, 5235, 293, 17948, 1966,
  383, 15427, 293, 1577, 12, 3766, 383, 15427, 293, 364, 11403, 293, 370, 322, 293,
  1446, 3793, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14109558860460916,
  "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0034909816458821297},
  {"id": 455, "seek": 297536, "start": 2983.6, "end": 2988.2400000000002, "text":
  " What motivates you to stay in this space today and also go into education teaching?",
  "tokens": [50776, 708, 42569, 291, 281, 1754, 294, 341, 1901, 965, 293, 611, 352,
  666, 3309, 4571, 30, 51008], "temperature": 0.0, "avg_logprob": -0.14109558860460916,
  "compression_ratio": 1.5574468085106383, "no_speech_prob": 0.0034909816458821297},
  {"id": 456, "seek": 297536, "start": 2988.8, "end": 2994.0, "text": " Yeah, I mean,
  it''s funny. I think, well, even when I was at Wikimedia and I quote, unquote,",
  "tokens": [51036, 865, 11, 286, 914, 11, 309, 311, 4074, 13, 286, 519, 11, 731,
  11, 754, 562, 286, 390, 412, 23377, 332, 14212, 293, 286, 6513, 11, 37557, 11, 51296],
  "temperature": 0.0, "avg_logprob": -0.14109558860460916, "compression_ratio": 1.5574468085106383,
  "no_speech_prob": 0.0034909816458821297}, {"id": 457, "seek": 297536, "start": 2994.0,
  "end": 3001.04, "text": " left search, I mean, we still ran a very large search
  engine and I always enjoyed my conversations", "tokens": [51296, 1411, 3164, 11,
  286, 914, 11, 321, 920, 5872, 257, 588, 2416, 3164, 2848, 293, 286, 1009, 4626,
  452, 7315, 51648], "temperature": 0.0, "avg_logprob": -0.14109558860460916, "compression_ratio":
  1.5574468085106383, "no_speech_prob": 0.0034909816458821297}, {"id": 458, "seek":
  300104, "start": 3001.04, "end": 3007.04, "text": " with a search team at Wikimedia
  just because they were, you know, it''s such a high traffic website", "tokens":
  [50364, 365, 257, 3164, 1469, 412, 23377, 332, 14212, 445, 570, 436, 645, 11, 291,
  458, 11, 309, 311, 1270, 257, 1090, 6419, 3144, 50664], "temperature": 0.0, "avg_logprob":
  -0.14159415318415716, "compression_ratio": 1.544041450777202, "no_speech_prob":
  0.0011936401715502143}, {"id": 459, "seek": 300104, "start": 3007.04, "end": 3012.48,
  "text": " and search there, I think does something like 6,000 queries per second
  or something like that. So", "tokens": [50664, 293, 3164, 456, 11, 286, 519, 775,
  746, 411, 1386, 11, 1360, 24109, 680, 1150, 420, 746, 411, 300, 13, 407, 50936],
  "temperature": 0.0, "avg_logprob": -0.14159415318415716, "compression_ratio": 1.544041450777202,
  "no_speech_prob": 0.0011936401715502143}, {"id": 460, "seek": 300104, "start": 3014.24,
  "end": 3021.04, "text": " you know, in some ways, and this is reflecting back on
  my career, I mean, I think I fell in love with", "tokens": [51024, 291, 458, 11,
  294, 512, 2098, 11, 293, 341, 307, 23543, 646, 322, 452, 3988, 11, 286, 914, 11,
  286, 519, 286, 5696, 294, 959, 365, 51364], "temperature": 0.0, "avg_logprob": -0.14159415318415716,
  "compression_ratio": 1.544041450777202, "no_speech_prob": 0.0011936401715502143},
  {"id": 461, "seek": 302104, "start": 3021.84, "end": 3035.04, "text": " language
  and the way humans use language and find information back circa 1999 or so when
  I started", "tokens": [50404, 2856, 293, 264, 636, 6255, 764, 2856, 293, 915, 1589,
  646, 45972, 19952, 420, 370, 562, 286, 1409, 51064], "temperature": 0.0, "avg_logprob":
  -0.19641939331503475, "compression_ratio": 1.505050505050505, "no_speech_prob":
  0.004666642285883427}, {"id": 462, "seek": 302104, "start": 3035.04, "end": 3042.24,
  "text": " at a small company called TextWise run by Liz Litty who is one of the
  pioneers in the natural language", "tokens": [51064, 412, 257, 1359, 2237, 1219,
  18643, 54, 908, 1190, 538, 16480, 441, 10016, 567, 307, 472, 295, 264, 47381, 294,
  264, 3303, 2856, 51424], "temperature": 0.0, "avg_logprob": -0.19641939331503475,
  "compression_ratio": 1.505050505050505, "no_speech_prob": 0.004666642285883427},
  {"id": 463, "seek": 302104, "start": 3042.24, "end": 3048.4, "text": " processing
  field and it just happened to have a search project that I started working on, right?",
  "tokens": [51424, 9007, 2519, 293, 309, 445, 2011, 281, 362, 257, 3164, 1716, 300,
  286, 1409, 1364, 322, 11, 558, 30, 51732], "temperature": 0.0, "avg_logprob": -0.19641939331503475,
  "compression_ratio": 1.505050505050505, "no_speech_prob": 0.004666642285883427},
  {"id": 464, "seek": 304840, "start": 3048.48, "end": 3053.6800000000003, "text":
  " But to me, you know, at the end of the day, like, this space and this is why I
  went to Wikimedia.", "tokens": [50368, 583, 281, 385, 11, 291, 458, 11, 412, 264,
  917, 295, 264, 786, 11, 411, 11, 341, 1901, 293, 341, 307, 983, 286, 1437, 281,
  23377, 332, 14212, 13, 50628], "temperature": 0.0, "avg_logprob": -0.13151549319831693,
  "compression_ratio": 1.663677130044843, "no_speech_prob": 0.0019197214860469103},
  {"id": 465, "seek": 304840, "start": 3053.6800000000003, "end": 3058.0, "text":
  " So I say, searches that necessarily the through line, even though it''s often
  the main,", "tokens": [50628, 407, 286, 584, 11, 26701, 300, 4725, 264, 807, 1622,
  11, 754, 1673, 309, 311, 2049, 264, 2135, 11, 50844], "temperature": 0.0, "avg_logprob":
  -0.13151549319831693, "compression_ratio": 1.663677130044843, "no_speech_prob":
  0.0019197214860469103}, {"id": 466, "seek": 304840, "start": 3059.12, "end": 3064.56,
  "text": " it appears to be the through line in my career, the deeper through line,
  I think, is that", "tokens": [50900, 309, 7038, 281, 312, 264, 807, 1622, 294, 452,
  3988, 11, 264, 7731, 807, 1622, 11, 286, 519, 11, 307, 300, 51172], "temperature":
  0.0, "avg_logprob": -0.13151549319831693, "compression_ratio": 1.663677130044843,
  "no_speech_prob": 0.0019197214860469103}, {"id": 467, "seek": 304840, "start": 3064.56,
  "end": 3071.36, "text": " I am fascinated by how we can leverage computers to help
  users make more informed, more capable,", "tokens": [51172, 286, 669, 24597, 538,
  577, 321, 393, 13982, 10807, 281, 854, 5022, 652, 544, 11740, 11, 544, 8189, 11,
  51512], "temperature": 0.0, "avg_logprob": -0.13151549319831693, "compression_ratio":
  1.663677130044843, "no_speech_prob": 0.0019197214860469103}, {"id": 468, "seek":
  307136, "start": 3071.36, "end": 3081.1200000000003, "text": " more aware decisions
  in their lives, whether that''s purchasing online or political or governmental",
  "tokens": [50364, 544, 3650, 5327, 294, 641, 2909, 11, 1968, 300, 311, 20906, 2950,
  420, 3905, 420, 43391, 50852], "temperature": 0.0, "avg_logprob": -0.1194511340214656,
  "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.006363488733768463},
  {"id": 469, "seek": 307136, "start": 3081.1200000000003, "end": 3086.96, "text":
  " or whatever it is, like, I am fascinated by how we can help people make more informed
  decisions", "tokens": [50852, 420, 2035, 309, 307, 11, 411, 11, 286, 669, 24597,
  538, 577, 321, 393, 854, 561, 652, 544, 11740, 5327, 51144], "temperature": 0.0,
  "avg_logprob": -0.1194511340214656, "compression_ratio": 1.5159574468085106, "no_speech_prob":
  0.006363488733768463}, {"id": 470, "seek": 307136, "start": 3086.96, "end": 3098.96,
  "text": " because I think that''s the thing that lifts us out, right? And so education
  then is a easy", "tokens": [51144, 570, 286, 519, 300, 311, 264, 551, 300, 30501,
  505, 484, 11, 558, 30, 400, 370, 3309, 550, 307, 257, 1858, 51744], "temperature":
  0.0, "avg_logprob": -0.1194511340214656, "compression_ratio": 1.5159574468085106,
  "no_speech_prob": 0.006363488733768463}, {"id": 471, "seek": 309896, "start": 3098.96,
  "end": 3105.44, "text": " follow-on from that through line, right? Like, the more
  people I can help use these tools and", "tokens": [50364, 1524, 12, 266, 490, 300,
  807, 1622, 11, 558, 30, 1743, 11, 264, 544, 561, 286, 393, 854, 764, 613, 3873,
  293, 50688], "temperature": 0.0, "avg_logprob": -0.13307300123196203, "compression_ratio":
  1.7123287671232876, "no_speech_prob": 0.002534678904339671}, {"id": 472, "seek":
  309896, "start": 3105.44, "end": 3112.0, "text": " also learn myself, the better
  off will I''ll be, right? Like, we have to use these tools to,", "tokens": [50688,
  611, 1466, 2059, 11, 264, 1101, 766, 486, 286, 603, 312, 11, 558, 30, 1743, 11,
  321, 362, 281, 764, 613, 3873, 281, 11, 51016], "temperature": 0.0, "avg_logprob":
  -0.13307300123196203, "compression_ratio": 1.7123287671232876, "no_speech_prob":
  0.002534678904339671}, {"id": 473, "seek": 309896, "start": 3113.12, "end": 3119.12,
  "text": " you know, to help us as humans get along better, etc. be more informed,
  so on, so forth, right?", "tokens": [51072, 291, 458, 11, 281, 854, 505, 382, 6255,
  483, 2051, 1101, 11, 5183, 13, 312, 544, 11740, 11, 370, 322, 11, 370, 5220, 11,
  558, 30, 51372], "temperature": 0.0, "avg_logprob": -0.13307300123196203, "compression_ratio":
  1.7123287671232876, "no_speech_prob": 0.002534678904339671}, {"id": 474, "seek":
  309896, "start": 3119.12, "end": 3124.64, "text": " So that''s probably the through
  line of the career, right? Is this how do you help people find", "tokens": [51372,
  407, 300, 311, 1391, 264, 807, 1622, 295, 264, 3988, 11, 558, 30, 1119, 341, 577,
  360, 291, 854, 561, 915, 51648], "temperature": 0.0, "avg_logprob": -0.13307300123196203,
  "compression_ratio": 1.7123287671232876, "no_speech_prob": 0.002534678904339671},
  {"id": 475, "seek": 312464, "start": 3124.72, "end": 3131.04, "text": " information
  and take action that makes us all better? Absolutely, this is very deep. Thanks
  so much.", "tokens": [50368, 1589, 293, 747, 3069, 300, 1669, 505, 439, 1101, 30,
  7021, 11, 341, 307, 588, 2452, 13, 2561, 370, 709, 13, 50684], "temperature": 0.0,
  "avg_logprob": -0.12683395866875177, "compression_ratio": 1.6795774647887325, "no_speech_prob":
  0.03353035822510719}, {"id": 476, "seek": 312464, "start": 3131.68, "end": 3137.2799999999997,
  "text": " I love asking this question because I''m super motivated to stay in the
  space, but I also love to", "tokens": [50716, 286, 959, 3365, 341, 1168, 570, 286,
  478, 1687, 14515, 281, 1754, 294, 264, 1901, 11, 457, 286, 611, 959, 281, 50996],
  "temperature": 0.0, "avg_logprob": -0.12683395866875177, "compression_ratio": 1.6795774647887325,
  "no_speech_prob": 0.03353035822510719}, {"id": 477, "seek": 312464, "start": 3137.2799999999997,
  "end": 3142.7999999999997, "text": " see the facets and the motivation of other
  professionals like yourself that I''m looking up to.", "tokens": [50996, 536, 264,
  49752, 293, 264, 12335, 295, 661, 11954, 411, 1803, 300, 286, 478, 1237, 493, 281,
  13, 51272], "temperature": 0.0, "avg_logprob": -0.12683395866875177, "compression_ratio":
  1.6795774647887325, "no_speech_prob": 0.03353035822510719}, {"id": 478, "seek":
  312464, "start": 3143.52, "end": 3147.6, "text": " I really enjoyed this conversation.
  Is there an announcement that you want to make in terms of", "tokens": [51308, 286,
  534, 4626, 341, 3761, 13, 1119, 456, 364, 12847, 300, 291, 528, 281, 652, 294, 2115,
  295, 51512], "temperature": 0.0, "avg_logprob": -0.12683395866875177, "compression_ratio":
  1.6795774647887325, "no_speech_prob": 0.03353035822510719}, {"id": 479, "seek":
  312464, "start": 3147.6, "end": 3151.92, "text": " the courses that you''re going
  to be teaching soon? Yeah, that''s great. I appreciate that,", "tokens": [51512,
  264, 7712, 300, 291, 434, 516, 281, 312, 4571, 2321, 30, 865, 11, 300, 311, 869,
  13, 286, 4449, 300, 11, 51728], "temperature": 0.0, "avg_logprob": -0.12683395866875177,
  "compression_ratio": 1.6795774647887325, "no_speech_prob": 0.03353035822510719},
  {"id": 480, "seek": 315192, "start": 3151.92, "end": 3156.0, "text": " the Metri,
  and I know we''d have some user questions, and I''m happy to stay on a little bit
  longer as", "tokens": [50364, 264, 6377, 470, 11, 293, 286, 458, 321, 1116, 362,
  512, 4195, 1651, 11, 293, 286, 478, 2055, 281, 1754, 322, 257, 707, 857, 2854, 382,
  50568], "temperature": 0.0, "avg_logprob": -0.15142239026786866, "compression_ratio":
  1.7304964539007093, "no_speech_prob": 0.005808789748698473}, {"id": 481, "seek":
  315192, "start": 3156.0, "end": 3162.7200000000003, "text": " well, get those. Yes,
  we actually, we have two classes coming up. So one of the things we learned", "tokens":
  [50568, 731, 11, 483, 729, 13, 1079, 11, 321, 767, 11, 321, 362, 732, 5359, 1348,
  493, 13, 407, 472, 295, 264, 721, 321, 3264, 50904], "temperature": 0.0, "avg_logprob":
  -0.15142239026786866, "compression_ratio": 1.7304964539007093, "no_speech_prob":
  0.005808789748698473}, {"id": 482, "seek": 315192, "start": 3162.7200000000003,
  "end": 3169.04, "text": " in the first run of search with machine learning is, you
  know, effectively we had one week of", "tokens": [50904, 294, 264, 700, 1190, 295,
  3164, 365, 3479, 2539, 307, 11, 291, 458, 11, 8659, 321, 632, 472, 1243, 295, 51220],
  "temperature": 0.0, "avg_logprob": -0.15142239026786866, "compression_ratio": 1.7304964539007093,
  "no_speech_prob": 0.005808789748698473}, {"id": 483, "seek": 315192, "start": 3169.04,
  "end": 3174.8, "text": " trying to get everybody on to the same page of how does
  open search work and what are the basics", "tokens": [51220, 1382, 281, 483, 2201,
  322, 281, 264, 912, 3028, 295, 577, 775, 1269, 3164, 589, 293, 437, 366, 264, 14688,
  51508], "temperature": 0.0, "avg_logprob": -0.15142239026786866, "compression_ratio":
  1.7304964539007093, "no_speech_prob": 0.005808789748698473}, {"id": 484, "seek":
  315192, "start": 3174.8, "end": 3181.52, "text": " of search? And then we had three
  weeks of fairly intense machine learning in a search environment,", "tokens": [51508,
  295, 3164, 30, 400, 550, 321, 632, 1045, 3259, 295, 6457, 9447, 3479, 2539, 294,
  257, 3164, 2823, 11, 51844], "temperature": 0.0, "avg_logprob": -0.15142239026786866,
  "compression_ratio": 1.7304964539007093, "no_speech_prob": 0.005808789748698473},
  {"id": 485, "seek": 318152, "start": 3181.52, "end": 3187.2, "text": " and one of
  the things that happened in the class because we didn''t have a lot of prerequisites",
  "tokens": [50364, 293, 472, 295, 264, 721, 300, 2011, 294, 264, 1508, 570, 321,
  994, 380, 362, 257, 688, 295, 38333, 15398, 3324, 50648], "temperature": 0.0, "avg_logprob":
  -0.10143456999788579, "compression_ratio": 1.6860986547085202, "no_speech_prob":
  0.00024349303566850722}, {"id": 486, "seek": 318152, "start": 3187.2, "end": 3193.28,
  "text": " is we had a really wide array of students of folks who were deep experts
  like yourself,", "tokens": [50648, 307, 321, 632, 257, 534, 4874, 10225, 295, 1731,
  295, 4024, 567, 645, 2452, 8572, 411, 1803, 11, 50952], "temperature": 0.0, "avg_logprob":
  -0.10143456999788579, "compression_ratio": 1.6860986547085202, "no_speech_prob":
  0.00024349303566850722}, {"id": 487, "seek": 318152, "start": 3194.88, "end": 3200.8,
  "text": " as well as like totally new to this arena. And what happened, I think,
  is that first week for", "tokens": [51032, 382, 731, 382, 411, 3879, 777, 281, 341,
  18451, 13, 400, 437, 2011, 11, 286, 519, 11, 307, 300, 700, 1243, 337, 51328], "temperature":
  0.0, "avg_logprob": -0.10143456999788579, "compression_ratio": 1.6860986547085202,
  "no_speech_prob": 0.00024349303566850722}, {"id": 488, "seek": 318152, "start":
  3200.8, "end": 3206.96, "text": " the new people was like, hey, this is too much
  for me to get up to speed. And for the folks who had", "tokens": [51328, 264, 777,
  561, 390, 411, 11, 4177, 11, 341, 307, 886, 709, 337, 385, 281, 483, 493, 281, 3073,
  13, 400, 337, 264, 4024, 567, 632, 51636], "temperature": 0.0, "avg_logprob": -0.10143456999788579,
  "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.00024349303566850722},
  {"id": 489, "seek": 320696, "start": 3206.96, "end": 3212.0, "text": " already done
  search, it was like, hey, I already know how to do all of this. And so trying to,",
  "tokens": [50364, 1217, 1096, 3164, 11, 309, 390, 411, 11, 4177, 11, 286, 1217,
  458, 577, 281, 360, 439, 295, 341, 13, 400, 370, 1382, 281, 11, 50616], "temperature":
  0.0, "avg_logprob": -0.12190031236217867, "compression_ratio": 1.5973451327433628,
  "no_speech_prob": 0.0003050028462894261}, {"id": 490, "seek": 320696, "start": 3213.04,
  "end": 3216.48, "text": " trying to go across that gap, I think we kind of ended
  up in this", "tokens": [50668, 1382, 281, 352, 2108, 300, 7417, 11, 286, 519, 321,
  733, 295, 4590, 493, 294, 341, 50840], "temperature": 0.0, "avg_logprob": -0.12190031236217867,
  "compression_ratio": 1.5973451327433628, "no_speech_prob": 0.0003050028462894261},
  {"id": 491, "seek": 320696, "start": 3217.6, "end": 3223.52, "text": " lukewarm
  area where nobody was quite satisfied. So one of the things we did was we split
  out the new", "tokens": [50896, 10438, 330, 49240, 1859, 689, 5079, 390, 1596, 11239,
  13, 407, 472, 295, 264, 721, 321, 630, 390, 321, 7472, 484, 264, 777, 51192], "temperature":
  0.0, "avg_logprob": -0.12190031236217867, "compression_ratio": 1.5973451327433628,
  "no_speech_prob": 0.0003050028462894261}, {"id": 492, "seek": 320696, "start": 3223.52,
  "end": 3230.48, "text": " stuff into a two week class called search fundamentals,
  which covers all of the basic intuitions of", "tokens": [51192, 1507, 666, 257,
  732, 1243, 1508, 1219, 3164, 29505, 11, 597, 10538, 439, 295, 264, 3875, 16224,
  626, 295, 51540], "temperature": 0.0, "avg_logprob": -0.12190031236217867, "compression_ratio":
  1.5973451327433628, "no_speech_prob": 0.0003050028462894261}, {"id": 493, "seek":
  323048, "start": 3230.48, "end": 3238.16, "text": " search, whether it''s deep learning
  based or a sparse learning based or sparse vector based,", "tokens": [50364, 3164,
  11, 1968, 309, 311, 2452, 2539, 2361, 420, 257, 637, 11668, 2539, 2361, 420, 637,
  11668, 8062, 2361, 11, 50748], "temperature": 0.0, "avg_logprob": -0.1676989514777001,
  "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0009253321914002299},
  {"id": 494, "seek": 323048, "start": 3238.16, "end": 3244.72, "text": " sorry. And
  so we cover, you know, indexing querying, facetying, spell checking, auto-complete,",
  "tokens": [50748, 2597, 13, 400, 370, 321, 2060, 11, 291, 458, 11, 8186, 278, 7083,
  1840, 11, 1915, 302, 1840, 11, 9827, 8568, 11, 8399, 12, 1112, 17220, 11, 51076],
  "temperature": 0.0, "avg_logprob": -0.1676989514777001, "compression_ratio": 1.654867256637168,
  "no_speech_prob": 0.0009253321914002299}, {"id": 495, "seek": 323048, "start": 3244.72,
  "end": 3249.12, "text": " kind of all the building blocks of a search application.
  And then with the machine learning", "tokens": [51076, 733, 295, 439, 264, 2390,
  8474, 295, 257, 3164, 3861, 13, 400, 550, 365, 264, 3479, 2539, 51296], "temperature":
  0.0, "avg_logprob": -0.1676989514777001, "compression_ratio": 1.654867256637168,
  "no_speech_prob": 0.0009253321914002299}, {"id": 496, "seek": 323048, "start": 3249.12,
  "end": 3255.84, "text": " class, because we''re dropping that beginner class week,
  we now have added in a neural retrieval", "tokens": [51296, 1508, 11, 570, 321,
  434, 13601, 300, 22080, 1508, 1243, 11, 321, 586, 362, 3869, 294, 257, 18161, 19817,
  3337, 51632], "temperature": 0.0, "avg_logprob": -0.1676989514777001, "compression_ratio":
  1.654867256637168, "no_speech_prob": 0.0009253321914002299}, {"id": 497, "seek":
  325584, "start": 3256.0, "end": 3262.32, "text": " dance retrieval into that as
  well. And so the search with fundamentals class starts next Monday,", "tokens":
  [50372, 4489, 19817, 3337, 666, 300, 382, 731, 13, 400, 370, 264, 3164, 365, 29505,
  1508, 3719, 958, 8138, 11, 50688], "temperature": 0.0, "avg_logprob": -0.14036091108967488,
  "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.004025400150567293},
  {"id": 498, "seek": 325584, "start": 3262.32, "end": 3269.76, "text": " June 6th.
  You can still sign up. It''s $200. There''s a code, DGSearch 10. And then search
  with machine", "tokens": [50688, 6928, 1386, 392, 13, 509, 393, 920, 1465, 493,
  13, 467, 311, 1848, 7629, 13, 821, 311, 257, 3089, 11, 413, 38, 10637, 1178, 1266,
  13, 400, 550, 3164, 365, 3479, 51060], "temperature": 0.0, "avg_logprob": -0.14036091108967488,
  "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.004025400150567293},
  {"id": 499, "seek": 325584, "start": 3269.76, "end": 3275.36, "text": " learning
  is two weeks after that. And that''s a four week class. Both are project intensive.
  Every", "tokens": [51060, 2539, 307, 732, 3259, 934, 300, 13, 400, 300, 311, 257,
  1451, 1243, 1508, 13, 6767, 366, 1716, 18957, 13, 2048, 51340], "temperature": 0.0,
  "avg_logprob": -0.14036091108967488, "compression_ratio": 1.7074829931972788, "no_speech_prob":
  0.004025400150567293}, {"id": 500, "seek": 325584, "start": 3275.36, "end": 3279.92,
  "text": " week, you''re going to do a project, you''re going to write code, you''re
  going to interact with students,", "tokens": [51340, 1243, 11, 291, 434, 516, 281,
  360, 257, 1716, 11, 291, 434, 516, 281, 2464, 3089, 11, 291, 434, 516, 281, 4648,
  365, 1731, 11, 51568], "temperature": 0.0, "avg_logprob": -0.14036091108967488,
  "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.004025400150567293},
  {"id": 501, "seek": 325584, "start": 3279.92, "end": 3285.76, "text": " you''re
  going to hear lectures, so on, so forth. In many ways, I think it''s modeled after
  a university", "tokens": [51568, 291, 434, 516, 281, 1568, 16564, 11, 370, 322,
  11, 370, 5220, 13, 682, 867, 2098, 11, 286, 519, 309, 311, 37140, 934, 257, 5454,
  51860], "temperature": 0.0, "avg_logprob": -0.14036091108967488, "compression_ratio":
  1.7074829931972788, "no_speech_prob": 0.004025400150567293}, {"id": 502, "seek":
  328576, "start": 3285.76, "end": 3289.92, "text": " style class where you, you know,
  every week you have homework, every week you have lectures,", "tokens": [50364,
  3758, 1508, 689, 291, 11, 291, 458, 11, 633, 1243, 291, 362, 14578, 11, 633, 1243,
  291, 362, 16564, 11, 50572], "temperature": 0.0, "avg_logprob": -0.19481226031699878,
  "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.000557929917704314},
  {"id": 503, "seek": 328576, "start": 3290.96, "end": 3298.4, "text": " and so on,
  so forth. So yeah, please sign up. Yeah, that''s awesome. What I''ve personally
  enjoyed", "tokens": [50624, 293, 370, 322, 11, 370, 5220, 13, 407, 1338, 11, 1767,
  1465, 493, 13, 865, 11, 300, 311, 3476, 13, 708, 286, 600, 5665, 4626, 50996], "temperature":
  0.0, "avg_logprob": -0.19481226031699878, "compression_ratio": 1.6309012875536482,
  "no_speech_prob": 0.000557929917704314}, {"id": 504, "seek": 328576, "start": 3298.4,
  "end": 3305.44, "text": " during the course, the search with the Mel four weeks
  course was the atmosphere. The atmosphere", "tokens": [50996, 1830, 264, 1164, 11,
  264, 3164, 365, 264, 7375, 1451, 3259, 1164, 390, 264, 8018, 13, 440, 8018, 51348],
  "temperature": 0.0, "avg_logprob": -0.19481226031699878, "compression_ratio": 1.6309012875536482,
  "no_speech_prob": 0.000557929917704314}, {"id": 505, "seek": 328576, "start": 3305.44,
  "end": 3311.6800000000003, "text": " that was basically creating itself amongst
  the students and was over 100 people there on Slack", "tokens": [51348, 300, 390,
  1936, 4084, 2564, 12918, 264, 1731, 293, 390, 670, 2319, 561, 456, 322, 37211, 51660],
  "temperature": 0.0, "avg_logprob": -0.19481226031699878, "compression_ratio": 1.6309012875536482,
  "no_speech_prob": 0.000557929917704314}, {"id": 506, "seek": 331168, "start": 3311.68,
  "end": 3317.04, "text": " helping each other. That was just amazing. Somebody saved
  me like a ton of time by just sharing,", "tokens": [50364, 4315, 1184, 661, 13,
  663, 390, 445, 2243, 13, 13463, 6624, 385, 411, 257, 2952, 295, 565, 538, 445, 5414,
  11, 50632], "temperature": 0.0, "avg_logprob": -0.1370131326100183, "compression_ratio":
  1.6655405405405406, "no_speech_prob": 0.020679641515016556}, {"id": 507, "seek":
  331168, "start": 3317.04, "end": 3322.3199999999997, "text": " you know, a recipe
  that I followed and quickly went through to some hurdle. And I learned, and I,",
  "tokens": [50632, 291, 458, 11, 257, 6782, 300, 286, 6263, 293, 2661, 1437, 807,
  281, 512, 47423, 13, 400, 286, 3264, 11, 293, 286, 11, 50896], "temperature": 0.0,
  "avg_logprob": -0.1370131326100183, "compression_ratio": 1.6655405405405406, "no_speech_prob":
  0.020679641515016556}, {"id": 508, "seek": 331168, "start": 3322.3199999999997,
  "end": 3327.52, "text": " of course, I knew some stuff. Yes, I''m an expert in this
  field, but also you can put your expertise,", "tokens": [50896, 295, 1164, 11, 286,
  2586, 512, 1507, 13, 1079, 11, 286, 478, 364, 5844, 294, 341, 2519, 11, 457, 611,
  291, 393, 829, 428, 11769, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1370131326100183,
  "compression_ratio": 1.6655405405405406, "no_speech_prob": 0.020679641515016556},
  {"id": 509, "seek": 331168, "start": 3327.52, "end": 3332.56, "text": " you know,
  to a test when you, when you run so fast during the course and the support that
  you guys", "tokens": [51156, 291, 458, 11, 281, 257, 1500, 562, 291, 11, 562, 291,
  1190, 370, 2370, 1830, 264, 1164, 293, 264, 1406, 300, 291, 1074, 51408], "temperature":
  0.0, "avg_logprob": -0.1370131326100183, "compression_ratio": 1.6655405405405406,
  "no_speech_prob": 0.020679641515016556}, {"id": 510, "seek": 331168, "start": 3332.56,
  "end": 3338.7999999999997, "text": " provided was amazing. So that''s amazing. I''ve
  enjoyed this conversation so much. Now we are moving", "tokens": [51408, 5649, 390,
  2243, 13, 407, 300, 311, 2243, 13, 286, 600, 4626, 341, 3761, 370, 709, 13, 823,
  321, 366, 2684, 51720], "temperature": 0.0, "avg_logprob": -0.1370131326100183,
  "compression_ratio": 1.6655405405405406, "no_speech_prob": 0.020679641515016556},
  {"id": 511, "seek": 333880, "start": 3338.8, "end": 3345.6800000000003, "text":
  " to the questions from the audience. And I''ll pick the, and feel free to ask questions,
  please.", "tokens": [50364, 281, 264, 1651, 490, 264, 4034, 13, 400, 286, 603, 1888,
  264, 11, 293, 841, 1737, 281, 1029, 1651, 11, 1767, 13, 50708], "temperature": 0.0,
  "avg_logprob": -0.19465838307919708, "compression_ratio": 1.7793427230046948, "no_speech_prob":
  0.01837104558944702}, {"id": 512, "seek": 333880, "start": 3346.88, "end": 3353.04,
  "text": " We still have a few minutes. The first question comes from Avynash, who
  is currently testing the", "tokens": [50768, 492, 920, 362, 257, 1326, 2077, 13,
  440, 700, 1168, 1487, 490, 11667, 2534, 1299, 11, 567, 307, 4362, 4997, 264, 51076],
  "temperature": 0.0, "avg_logprob": -0.19465838307919708, "compression_ratio": 1.7793427230046948,
  "no_speech_prob": 0.01837104558944702}, {"id": 513, "seek": 333880, "start": 3353.04,
  "end": 3360.0, "text": " approach of buying coder to find the similar sentence,
  top 10. And later passing the top 10", "tokens": [51076, 3109, 295, 6382, 17656,
  260, 281, 915, 264, 2531, 8174, 11, 1192, 1266, 13, 400, 1780, 8437, 264, 1192,
  1266, 51424], "temperature": 0.0, "avg_logprob": -0.19465838307919708, "compression_ratio":
  1.7793427230046948, "no_speech_prob": 0.01837104558944702}, {"id": 514, "seek":
  333880, "start": 3360.0, "end": 3365.52, "text": " sentence to a crossing coder
  model to find the most similar sentence in the top 10 using cosine", "tokens": [51424,
  8174, 281, 257, 14712, 17656, 260, 2316, 281, 915, 264, 881, 2531, 8174, 294, 264,
  1192, 1266, 1228, 23565, 51700], "temperature": 0.0, "avg_logprob": -0.19465838307919708,
  "compression_ratio": 1.7793427230046948, "no_speech_prob": 0.01837104558944702},
  {"id": 515, "seek": 336552, "start": 3365.52, "end": 3372.0, "text": " similarity.
  Yeah, I guess he''s asking for advice is this an appropriate method.", "tokens":
  [50364, 32194, 13, 865, 11, 286, 2041, 415, 311, 3365, 337, 5192, 307, 341, 364,
  6854, 3170, 13, 50688], "temperature": 0.0, "avg_logprob": -0.14145879487733584,
  "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00481015257537365},
  {"id": 516, "seek": 336552, "start": 3374.64, "end": 3380.72, "text": " This is
  where my expertise just is not. So Avynash, I will apologize. I do not know enough
  here to", "tokens": [50820, 639, 307, 689, 452, 11769, 445, 307, 406, 13, 407, 11667,
  2534, 1299, 11, 286, 486, 12328, 13, 286, 360, 406, 458, 1547, 510, 281, 51124],
  "temperature": 0.0, "avg_logprob": -0.14145879487733584, "compression_ratio": 1.4242424242424243,
  "no_speech_prob": 0.00481015257537365}, {"id": 517, "seek": 336552, "start": 3380.72,
  "end": 3386.88, "text": " give you advice. I would probably ask first, like, what
  is the actual problem? Are you trying to solve?", "tokens": [51124, 976, 291, 5192,
  13, 286, 576, 1391, 1029, 700, 11, 411, 11, 437, 307, 264, 3539, 1154, 30, 2014,
  291, 1382, 281, 5039, 30, 51432], "temperature": 0.0, "avg_logprob": -0.14145879487733584,
  "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.00481015257537365},
  {"id": 518, "seek": 338688, "start": 3387.12, "end": 3396.2400000000002, "text":
  " You know, so if you''re trying to find similar sentences, then from my understanding
  of it, that my", "tokens": [50376, 509, 458, 11, 370, 498, 291, 434, 1382, 281,
  915, 2531, 16579, 11, 550, 490, 452, 3701, 295, 309, 11, 300, 452, 50832], "temperature":
  0.0, "avg_logprob": -0.22691947855847946, "compression_ratio": 1.636734693877551,
  "no_speech_prob": 0.03233594074845314}, {"id": 519, "seek": 338688, "start": 3396.2400000000002,
  "end": 3401.6, "text": " basic level understanding of what you''re describing, it
  sounds like a reasonable, a reasonable approach.", "tokens": [50832, 3875, 1496,
  3701, 295, 437, 291, 434, 16141, 11, 309, 3263, 411, 257, 10585, 11, 257, 10585,
  3109, 13, 51100], "temperature": 0.0, "avg_logprob": -0.22691947855847946, "compression_ratio":
  1.636734693877551, "no_speech_prob": 0.03233594074845314}, {"id": 520, "seek": 338688,
  "start": 3401.6, "end": 3406.48, "text": " But there are people who are much in
  probably Dmitry, you probably could answer this one better than I,", "tokens": [51100,
  583, 456, 366, 561, 567, 366, 709, 294, 1391, 413, 3508, 627, 11, 291, 1391, 727,
  1867, 341, 472, 1101, 813, 286, 11, 51344], "temperature": 0.0, "avg_logprob": -0.22691947855847946,
  "compression_ratio": 1.636734693877551, "no_speech_prob": 0.03233594074845314},
  {"id": 521, "seek": 338688, "start": 3406.48, "end": 3412.0, "text": " but I have
  not played with or tried out those specific types of capabilities. So I don''t have",
  "tokens": [51344, 457, 286, 362, 406, 3737, 365, 420, 3031, 484, 729, 2685, 3467,
  295, 10862, 13, 407, 286, 500, 380, 362, 51620], "temperature": 0.0, "avg_logprob":
  -0.22691947855847946, "compression_ratio": 1.636734693877551, "no_speech_prob":
  0.03233594074845314}, {"id": 522, "seek": 341200, "start": 3412.72, "end": 3419.2,
  "text": " good advice there. I have worked in general on sentence similarity type
  problems. It is always", "tokens": [50400, 665, 5192, 456, 13, 286, 362, 2732, 294,
  2674, 322, 8174, 32194, 2010, 2740, 13, 467, 307, 1009, 50724], "temperature": 0.0,
  "avg_logprob": -0.11199063195122613, "compression_ratio": 1.6929824561403508, "no_speech_prob":
  0.005875768139958382}, {"id": 523, "seek": 341200, "start": 3419.2, "end": 3426.16,
  "text": " challenging. In fact, I have a my current company that I''m one of my
  fractional clients. We are", "tokens": [50724, 7595, 13, 682, 1186, 11, 286, 362,
  257, 452, 2190, 2237, 300, 286, 478, 472, 295, 452, 17948, 1966, 6982, 13, 492,
  366, 51072], "temperature": 0.0, "avg_logprob": -0.11199063195122613, "compression_ratio":
  1.6929824561403508, "no_speech_prob": 0.005875768139958382}, {"id": 524, "seek":
  341200, "start": 3426.16, "end": 3432.88, "text": " doing sentence similarity or
  clause similarity types problems. And I think they are we are using", "tokens":
  [51072, 884, 8174, 32194, 420, 25925, 32194, 3467, 2740, 13, 400, 286, 519, 436,
  366, 321, 366, 1228, 51408], "temperature": 0.0, "avg_logprob": -0.11199063195122613,
  "compression_ratio": 1.6929824561403508, "no_speech_prob": 0.005875768139958382},
  {"id": 525, "seek": 341200, "start": 3432.88, "end": 3438.16, "text": " similar
  modeling techniques, but I''m not doing the day to day modeling on that. So I''m
  really just", "tokens": [51408, 2531, 15983, 7512, 11, 457, 286, 478, 406, 884,
  264, 786, 281, 786, 15983, 322, 300, 13, 407, 286, 478, 534, 445, 51672], "temperature":
  0.0, "avg_logprob": -0.11199063195122613, "compression_ratio": 1.6929824561403508,
  "no_speech_prob": 0.005875768139958382}, {"id": 526, "seek": 343816, "start": 3438.16,
  "end": 3447.2, "text": " trusting the data scientists on that. Yeah, I can add to
  this that I happen to have given a", "tokens": [50364, 28235, 264, 1412, 7708, 322,
  300, 13, 865, 11, 286, 393, 909, 281, 341, 300, 286, 1051, 281, 362, 2212, 257,
  50816], "temperature": 0.0, "avg_logprob": -0.22103147004780016, "compression_ratio":
  1.644736842105263, "no_speech_prob": 0.005836143624037504}, {"id": 527, "seek":
  343816, "start": 3447.8399999999997, "end": 3453.7599999999998, "text": " community
  talk during the search with the mail course. And there I actually go explicitly
  into", "tokens": [50848, 1768, 751, 1830, 264, 3164, 365, 264, 10071, 1164, 13,
  400, 456, 286, 767, 352, 20803, 666, 51144], "temperature": 0.0, "avg_logprob":
  -0.22103147004780016, "compression_ratio": 1.644736842105263, "no_speech_prob":
  0.005836143624037504}, {"id": 528, "seek": 343816, "start": 3453.7599999999998,
  "end": 3458.8799999999997, "text": " this by encoder and cross-ent coder. So only
  one thing is that cross-ent coder is much more", "tokens": [51144, 341, 538, 2058,
  19866, 293, 3278, 12, 317, 17656, 260, 13, 407, 787, 472, 551, 307, 300, 3278, 12,
  317, 17656, 260, 307, 709, 544, 51400], "temperature": 0.0, "avg_logprob": -0.22103147004780016,
  "compression_ratio": 1.644736842105263, "no_speech_prob": 0.005836143624037504},
  {"id": 529, "seek": 343816, "start": 3460.48, "end": 3465.2799999999997, "text":
  " computationally intensive. And so you don''t want to run it on a huge amount of
  sentences. And it", "tokens": [51480, 24903, 379, 18957, 13, 400, 370, 291, 500,
  380, 528, 281, 1190, 309, 322, 257, 2603, 2372, 295, 16579, 13, 400, 309, 51720],
  "temperature": 0.0, "avg_logprob": -0.22103147004780016, "compression_ratio": 1.644736842105263,
  "no_speech_prob": 0.005836143624037504}, {"id": 530, "seek": 346528, "start": 3465.28,
  "end": 3471.0400000000004, "text": " looks like that''s what you''re doing. So that
  sounds sensible to me. I think I would pay more", "tokens": [50364, 1542, 411, 300,
  311, 437, 291, 434, 884, 13, 407, 300, 3263, 25380, 281, 385, 13, 286, 519, 286,
  576, 1689, 544, 50652], "temperature": 0.0, "avg_logprob": -0.15293543266527582,
  "compression_ratio": 1.5591836734693878, "no_speech_prob": 0.005863623693585396},
  {"id": 531, "seek": 346528, "start": 3471.0400000000004, "end": 3477.44, "text":
  " attention to testing your approach. So make sure to reserve some part of your
  data set to test it.", "tokens": [50652, 3202, 281, 4997, 428, 3109, 13, 407, 652,
  988, 281, 17824, 512, 644, 295, 428, 1412, 992, 281, 1500, 309, 13, 50972], "temperature":
  0.0, "avg_logprob": -0.15293543266527582, "compression_ratio": 1.5591836734693878,
  "no_speech_prob": 0.005863623693585396}, {"id": 532, "seek": 346528, "start": 3478.32,
  "end": 3483.44, "text": " Careful. Yeah, this is the cool thing for me coming back
  in from Wikiland is I''m learning so", "tokens": [51016, 32932, 13, 865, 11, 341,
  307, 264, 1627, 551, 337, 385, 1348, 646, 294, 490, 35892, 1661, 307, 286, 478,
  2539, 370, 51272], "temperature": 0.0, "avg_logprob": -0.15293543266527582, "compression_ratio":
  1.5591836734693878, "no_speech_prob": 0.005863623693585396}, {"id": 533, "seek":
  346528, "start": 3483.44, "end": 3488.6400000000003, "text": " much now too. Like
  this is I''ve been digging my way through a lot of these things, but as you can",
  "tokens": [51272, 709, 586, 886, 13, 1743, 341, 307, 286, 600, 668, 17343, 452,
  636, 807, 257, 688, 295, 613, 721, 11, 457, 382, 291, 393, 51532], "temperature":
  0.0, "avg_logprob": -0.15293543266527582, "compression_ratio": 1.5591836734693878,
  "no_speech_prob": 0.005863623693585396}, {"id": 534, "seek": 348864, "start": 3488.64,
  "end": 3496.08, "text": " see, this is why it''s the gold age because there''s so
  many approaches and they''re often", "tokens": [50364, 536, 11, 341, 307, 983, 309,
  311, 264, 3821, 3205, 570, 456, 311, 370, 867, 11587, 293, 436, 434, 2049, 50736],
  "temperature": 0.0, "avg_logprob": -0.26228651087334814, "compression_ratio": 1.5296610169491525,
  "no_speech_prob": 0.00551283173263073}, {"id": 535, "seek": 348864, "start": 3496.08,
  "end": 3502.4, "text": " improving state of the art every week, right? Yeah, exactly.
  A lot of things is happening.", "tokens": [50736, 11470, 1785, 295, 264, 1523, 633,
  1243, 11, 558, 30, 865, 11, 2293, 13, 316, 688, 295, 721, 307, 2737, 13, 51052],
  "temperature": 0.0, "avg_logprob": -0.26228651087334814, "compression_ratio": 1.5296610169491525,
  "no_speech_prob": 0.00551283173263073}, {"id": 536, "seek": 348864, "start": 3503.04,
  "end": 3507.2, "text": " Another question I''m taking now from the chat, Carlos
  is asking, I''d like to know", "tokens": [51084, 3996, 1168, 286, 478, 1940, 586,
  490, 264, 5081, 11, 19646, 307, 3365, 11, 286, 1116, 411, 281, 458, 51292], "temperature":
  0.0, "avg_logprob": -0.26228651087334814, "compression_ratio": 1.5296610169491525,
  "no_speech_prob": 0.00551283173263073}, {"id": 537, "seek": 348864, "start": 3508.48,
  "end": 3515.68, "text": " Grandsepinion inside about learning to boost. He gives
  also a link to a presentation at a high-stack", "tokens": [51356, 6757, 405, 17836,
  313, 1854, 466, 2539, 281, 9194, 13, 634, 2709, 611, 257, 2113, 281, 257, 5860,
  412, 257, 1090, 12, 372, 501, 51716], "temperature": 0.0, "avg_logprob": -0.26228651087334814,
  "compression_ratio": 1.5296610169491525, "no_speech_prob": 0.00551283173263073},
  {"id": 538, "seek": 351568, "start": 3516.16, "end": 3521.3599999999997, "text":
  " high-stack conference. I don''t know if you''re familiar with this approach, Grant.
  Can you say anything?", "tokens": [50388, 1090, 12, 372, 501, 7586, 13, 286, 500,
  380, 458, 498, 291, 434, 4963, 365, 341, 3109, 11, 17529, 13, 1664, 291, 584, 1340,
  30, 50648], "temperature": 0.0, "avg_logprob": -0.1990061556355337, "compression_ratio":
  1.5817307692307692, "no_speech_prob": 0.014524207450449467}, {"id": 539, "seek":
  351568, "start": 3523.12, "end": 3529.6, "text": " I am not. I''d like to know learning
  to boost interesting. Another thing to go learn.", "tokens": [50736, 286, 669, 406,
  13, 286, 1116, 411, 281, 458, 2539, 281, 9194, 1880, 13, 3996, 551, 281, 352, 1466,
  13, 51060], "temperature": 0.0, "avg_logprob": -0.1990061556355337, "compression_ratio":
  1.5817307692307692, "no_speech_prob": 0.014524207450449467}, {"id": 540, "seek":
  351568, "start": 3530.64, "end": 3533.2799999999997, "text": " Yeah, I think it
  was all kind of learning to rank.", "tokens": [51112, 865, 11, 286, 519, 309, 390,
  439, 733, 295, 2539, 281, 6181, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1990061556355337,
  "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.014524207450449467},
  {"id": 541, "seek": 351568, "start": 3535.2, "end": 3540.8799999999997, "text":
  " I think it''s related, but I actually don''t know myself like that much in detail,
  but that", "tokens": [51340, 286, 519, 309, 311, 4077, 11, 457, 286, 767, 500, 380,
  458, 2059, 411, 300, 709, 294, 2607, 11, 457, 300, 51624], "temperature": 0.0, "avg_logprob":
  -0.1990061556355337, "compression_ratio": 1.5817307692307692, "no_speech_prob":
  0.014524207450449467}, {"id": 542, "seek": 354088, "start": 3541.36, "end": 3547.28,
  "text": " that presentation was great. It looked like new thing, but at the same
  time kind of familiar.", "tokens": [50388, 300, 5860, 390, 869, 13, 467, 2956, 411,
  777, 551, 11, 457, 412, 264, 912, 565, 733, 295, 4963, 13, 50684], "temperature":
  0.0, "avg_logprob": -0.20237319365791653, "compression_ratio": 1.553648068669528,
  "no_speech_prob": 0.009666667319834232}, {"id": 543, "seek": 354088, "start": 3550.08,
  "end": 3554.7200000000003, "text": " Basically, instead of learning to rank, you
  learn the boost values as far as I remember.", "tokens": [50824, 8537, 11, 2602,
  295, 2539, 281, 6181, 11, 291, 1466, 264, 9194, 4190, 382, 1400, 382, 286, 1604,
  13, 51056], "temperature": 0.0, "avg_logprob": -0.20237319365791653, "compression_ratio":
  1.553648068669528, "no_speech_prob": 0.009666667319834232}, {"id": 544, "seek":
  354088, "start": 3558.0, "end": 3563.84, "text": " It sounds interesting and reasonable.
  Again, at the end of the day, how do we shape these vectors?", "tokens": [51220,
  467, 3263, 1880, 293, 10585, 13, 3764, 11, 412, 264, 917, 295, 264, 786, 11, 577,
  360, 321, 3909, 613, 18875, 30, 51512], "temperature": 0.0, "avg_logprob": -0.20237319365791653,
  "compression_ratio": 1.553648068669528, "no_speech_prob": 0.009666667319834232},
  {"id": 545, "seek": 354088, "start": 3563.84, "end": 3568.6400000000003, "text":
  " I know that''s a generic wave in your hands, but I would take this and go try
  it.", "tokens": [51512, 286, 458, 300, 311, 257, 19577, 5772, 294, 428, 2377, 11,
  457, 286, 576, 747, 341, 293, 352, 853, 309, 13, 51752], "temperature": 0.0, "avg_logprob":
  -0.20237319365791653, "compression_ratio": 1.553648068669528, "no_speech_prob":
  0.009666667319834232}, {"id": 546, "seek": 356864, "start": 3568.96, "end": 3577.04,
  "text": " I think most of these machine learning systems you''re trying to learn
  weights that then shape the way", "tokens": [50380, 286, 519, 881, 295, 613, 3479,
  2539, 3652, 291, 434, 1382, 281, 1466, 17443, 300, 550, 3909, 264, 636, 50784],
  "temperature": 0.0, "avg_logprob": -0.20973227680593298, "compression_ratio": 1.5449735449735449,
  "no_speech_prob": 0.00460564810782671}, {"id": 547, "seek": 356864, "start": 3577.04,
  "end": 3586.16, "text": " that vector gets called. If it works on your domain and
  it''s fast enough and you can maintain it,", "tokens": [50784, 300, 8062, 2170,
  1219, 13, 759, 309, 1985, 322, 428, 9274, 293, 309, 311, 2370, 1547, 293, 291, 393,
  6909, 309, 11, 51240], "temperature": 0.0, "avg_logprob": -0.20973227680593298,
  "compression_ratio": 1.5449735449735449, "no_speech_prob": 0.00460564810782671},
  {"id": 548, "seek": 356864, "start": 3587.3599999999997, "end": 3596.08, "text":
  " then go for it. You don''t need some experts blessing on it. It certainly sounds
  interesting.", "tokens": [51300, 550, 352, 337, 309, 13, 509, 500, 380, 643, 512,
  8572, 13869, 322, 309, 13, 467, 3297, 3263, 1880, 13, 51736], "temperature": 0.0,
  "avg_logprob": -0.20973227680593298, "compression_ratio": 1.5449735449735449, "no_speech_prob":
  0.00460564810782671}, {"id": 549, "seek": 359608, "start": 3596.08, "end": 3602.24,
  "text": " LTR certainly has its own challenges in terms of tweaking and tuning.
  I know I''ve struggled with that", "tokens": [50364, 441, 25936, 3297, 575, 1080,
  1065, 4759, 294, 2115, 295, 6986, 2456, 293, 15164, 13, 286, 458, 286, 600, 19023,
  365, 300, 50672], "temperature": 0.0, "avg_logprob": -0.17012994939630682, "compression_ratio":
  1.6125, "no_speech_prob": 0.017327241599559784}, {"id": 550, "seek": 359608, "start":
  3602.24, "end": 3611.6, "text": " with LTR a lot. I know I''ve struggled with hand-tuned
  boost a lot as well, so anything that helps", "tokens": [50672, 365, 441, 25936,
  257, 688, 13, 286, 458, 286, 600, 19023, 365, 1011, 12, 83, 43703, 9194, 257, 688,
  382, 731, 11, 370, 1340, 300, 3665, 51140], "temperature": 0.0, "avg_logprob": -0.17012994939630682,
  "compression_ratio": 1.6125, "no_speech_prob": 0.017327241599559784}, {"id": 551,
  "seek": 359608, "start": 3611.6, "end": 3619.36, "text": " do that I think would
  be good. Yeah, awesome. The next question comes from Nico, Hey, Nico,", "tokens":
  [51140, 360, 300, 286, 519, 576, 312, 665, 13, 865, 11, 3476, 13, 440, 958, 1168,
  1487, 490, 15115, 11, 1911, 11, 15115, 11, 51528], "temperature": 0.0, "avg_logprob":
  -0.17012994939630682, "compression_ratio": 1.6125, "no_speech_prob": 0.017327241599559784},
  {"id": 552, "seek": 359608, "start": 3619.36, "end": 3625.04, "text": " a former
  colleague from AlphaSense. If you''re hosting an information search engine which
  should", "tokens": [51528, 257, 5819, 13532, 490, 20588, 50, 1288, 13, 759, 291,
  434, 16058, 364, 1589, 3164, 2848, 597, 820, 51812], "temperature": 0.0, "avg_logprob":
  -0.17012994939630682, "compression_ratio": 1.6125, "no_speech_prob": 0.017327241599559784},
  {"id": 553, "seek": 362504, "start": 3625.04, "end": 3632.72, "text": " catch new
  topics like COVID when it hit, how do you notice that your boosting model of vector",
  "tokens": [50364, 3745, 777, 8378, 411, 4566, 562, 309, 2045, 11, 577, 360, 291,
  3449, 300, 428, 43117, 2316, 295, 8062, 50748], "temperature": 0.0, "avg_logprob":
  -0.137120177946895, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.006344288121908903},
  {"id": 554, "seek": 362504, "start": 3632.72, "end": 3638.08, "text": " embedding
  model does not recognize queries related to these new topics proactively?", "tokens":
  [50748, 12240, 3584, 2316, 775, 406, 5521, 24109, 4077, 281, 613, 777, 8378, 447,
  45679, 30, 51016], "temperature": 0.0, "avg_logprob": -0.137120177946895, "compression_ratio":
  1.6273584905660377, "no_speech_prob": 0.006344288121908903}, {"id": 555, "seek":
  362504, "start": 3639.44, "end": 3643.04, "text": " Yeah, that''s where I think
  the instrumentation of your system comes in, right?", "tokens": [51084, 865, 11,
  300, 311, 689, 286, 519, 264, 7198, 399, 295, 428, 1185, 1487, 294, 11, 558, 30,
  51264], "temperature": 0.0, "avg_logprob": -0.137120177946895, "compression_ratio":
  1.6273584905660377, "no_speech_prob": 0.006344288121908903}, {"id": 556, "seek":
  362504, "start": 3644.24, "end": 3649.44, "text": " And the human and the loop on
  that instrumentation in the system, right? I mean, I think", "tokens": [51324, 400,
  264, 1952, 293, 264, 6367, 322, 300, 7198, 399, 294, 264, 1185, 11, 558, 30, 286,
  914, 11, 286, 519, 51584], "temperature": 0.0, "avg_logprob": -0.137120177946895,
  "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.006344288121908903},
  {"id": 557, "seek": 364944, "start": 3649.84, "end": 3656.64, "text": " nobody talks
  about it, but even at the really large successful search engines, there''s still
  people", "tokens": [50384, 5079, 6686, 466, 309, 11, 457, 754, 412, 264, 534, 2416,
  4406, 3164, 12982, 11, 456, 311, 920, 561, 50724], "temperature": 0.0, "avg_logprob":
  -0.14560079038812873, "compression_ratio": 1.7236842105263157, "no_speech_prob":
  0.005095826927572489}, {"id": 558, "seek": 364944, "start": 3656.64, "end": 3663.84,
  "text": " who are reviewing where things are working and not working, right? And
  generally they''re doing it", "tokens": [50724, 567, 366, 19576, 689, 721, 366,
  1364, 293, 406, 1364, 11, 558, 30, 400, 5101, 436, 434, 884, 309, 51084], "temperature":
  0.0, "avg_logprob": -0.14560079038812873, "compression_ratio": 1.7236842105263157,
  "no_speech_prob": 0.005095826927572489}, {"id": 559, "seek": 364944, "start": 3663.84,
  "end": 3671.04, "text": " at the experimentation level, but people still dig into
  queries. What queries are underperforming?", "tokens": [51084, 412, 264, 37142,
  1496, 11, 457, 561, 920, 2528, 666, 24109, 13, 708, 24109, 366, 833, 26765, 278,
  30, 51444], "temperature": 0.0, "avg_logprob": -0.14560079038812873, "compression_ratio":
  1.7236842105263157, "no_speech_prob": 0.005095826927572489}, {"id": 560, "seek":
  364944, "start": 3671.52, "end": 3676.2400000000002, "text": " What documents are
  underperforming? I think there''s tools, there''s a lot of good tools out there",
  "tokens": [51468, 708, 8512, 366, 833, 26765, 278, 30, 286, 519, 456, 311, 3873,
  11, 456, 311, 257, 688, 295, 665, 3873, 484, 456, 51704], "temperature": 0.0, "avg_logprob":
  -0.14560079038812873, "compression_ratio": 1.7236842105263157, "no_speech_prob":
  0.005095826927572489}, {"id": 561, "seek": 367624, "start": 3676.24, "end": 3682.16,
  "text": " for anomaly detection as well. So recognizing when new queries are coming
  in is something like", "tokens": [50364, 337, 42737, 17784, 382, 731, 13, 407, 18538,
  562, 777, 24109, 366, 1348, 294, 307, 746, 411, 50660], "temperature": 0.0, "avg_logprob":
  -0.10794573944884461, "compression_ratio": 1.6805555555555556, "no_speech_prob":
  0.0014734393917024136}, {"id": 562, "seek": 367624, "start": 3682.16, "end": 3692.24,
  "text": " anomaly detection algorithms will work with, right? You know, looking
  at your top queries,", "tokens": [50660, 42737, 17784, 14642, 486, 589, 365, 11,
  558, 30, 509, 458, 11, 1237, 412, 428, 1192, 24109, 11, 51164], "temperature": 0.0,
  "avg_logprob": -0.10794573944884461, "compression_ratio": 1.6805555555555556, "no_speech_prob":
  0.0014734393917024136}, {"id": 563, "seek": 367624, "start": 3692.24, "end": 3697.68,
  "text": " your trending queries, and then again, looking at those results, there
  are machine learning", "tokens": [51164, 428, 28692, 24109, 11, 293, 550, 797, 11,
  1237, 412, 729, 3542, 11, 456, 366, 3479, 2539, 51436], "temperature": 0.0, "avg_logprob":
  -0.10794573944884461, "compression_ratio": 1.6805555555555556, "no_speech_prob":
  0.0014734393917024136}, {"id": 564, "seek": 367624, "start": 3697.68, "end": 3702.64,
  "text": " approaches to automatically identifying and alerting on those kinds of
  things, again,", "tokens": [51436, 11587, 281, 6772, 16696, 293, 419, 27187, 322,
  729, 3685, 295, 721, 11, 797, 11, 51684], "temperature": 0.0, "avg_logprob": -0.10794573944884461,
  "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.0014734393917024136},
  {"id": 565, "seek": 370264, "start": 3702.64, "end": 3708.3199999999997, "text":
  " along the anomaly detection line. But at the end of the day, you can always do
  that with people", "tokens": [50364, 2051, 264, 42737, 17784, 1622, 13, 583, 412,
  264, 917, 295, 264, 786, 11, 291, 393, 1009, 360, 300, 365, 561, 50648], "temperature":
  0.0, "avg_logprob": -0.15962262587113815, "compression_ratio": 1.5748987854251013,
  "no_speech_prob": 0.013607312925159931}, {"id": 566, "seek": 370264, "start": 3708.3199999999997,
  "end": 3714.72, "text": " as well, right? And that''s where humans maybe are better
  at still at recognizing some of those things.", "tokens": [50648, 382, 731, 11,
  558, 30, 400, 300, 311, 689, 6255, 1310, 366, 1101, 412, 920, 412, 18538, 512, 295,
  729, 721, 13, 50968], "temperature": 0.0, "avg_logprob": -0.15962262587113815, "compression_ratio":
  1.5748987854251013, "no_speech_prob": 0.013607312925159931}, {"id": 567, "seek":
  370264, "start": 3716.16, "end": 3721.2799999999997, "text": " Yeah, and I think
  you also alluded to this somewhat. I mean, this question is to me, it''s like",
  "tokens": [51040, 865, 11, 293, 286, 519, 291, 611, 33919, 281, 341, 8344, 13, 286,
  914, 11, 341, 1168, 307, 281, 385, 11, 309, 311, 411, 51296], "temperature": 0.0,
  "avg_logprob": -0.15962262587113815, "compression_ratio": 1.5748987854251013, "no_speech_prob":
  0.013607312925159931}, {"id": 568, "seek": 370264, "start": 3721.2799999999997,
  "end": 3727.44, "text": " chicken-eyed problem, right? So if a new topic arises
  in the queries and also in the documents,", "tokens": [51296, 4662, 12, 37860, 1154,
  11, 558, 30, 407, 498, 257, 777, 4829, 27388, 294, 264, 24109, 293, 611, 294, 264,
  8512, 11, 51604], "temperature": 0.0, "avg_logprob": -0.15962262587113815, "compression_ratio":
  1.5748987854251013, "no_speech_prob": 0.013607312925159931}, {"id": 569, "seek":
  372744, "start": 3727.44, "end": 3736.08, "text": " but I haven''t handled it yet
  before prior to this, then what can I do live? So I think you said", "tokens": [50364,
  457, 286, 2378, 380, 18033, 309, 1939, 949, 4059, 281, 341, 11, 550, 437, 393, 286,
  360, 1621, 30, 407, 286, 519, 291, 848, 50796], "temperature": 0.0, "avg_logprob":
  -0.17442673444747925, "compression_ratio": 1.6270491803278688, "no_speech_prob":
  0.006583199370652437}, {"id": 570, "seek": 372744, "start": 3736.08, "end": 3741.76,
  "text": " that try to measure things like if some top ranking documents are not
  clicked, then that''s probably", "tokens": [50796, 300, 853, 281, 3481, 721, 411,
  498, 512, 1192, 17833, 8512, 366, 406, 23370, 11, 550, 300, 311, 1391, 51080], "temperature":
  0.0, "avg_logprob": -0.17442673444747925, "compression_ratio": 1.6270491803278688,
  "no_speech_prob": 0.006583199370652437}, {"id": 571, "seek": 372744, "start": 3741.76,
  "end": 3747.2000000000003, "text": " a signal of something is smoky there. Go check
  it out. Another thing that I think I could recommend,", "tokens": [51080, 257, 6358,
  295, 746, 307, 32073, 88, 456, 13, 1037, 1520, 309, 484, 13, 3996, 551, 300, 286,
  519, 286, 727, 2748, 11, 51352], "temperature": 0.0, "avg_logprob": -0.17442673444747925,
  "compression_ratio": 1.6270491803278688, "no_speech_prob": 0.006583199370652437},
  {"id": 572, "seek": 372744, "start": 3747.2000000000003, "end": 3753.36, "text":
  " maybe from my side, is you could try to cluster your queries actually. And sometimes
  the funny thing", "tokens": [51352, 1310, 490, 452, 1252, 11, 307, 291, 727, 853,
  281, 13630, 428, 24109, 767, 13, 400, 2171, 264, 4074, 551, 51660], "temperature":
  0.0, "avg_logprob": -0.17442673444747925, "compression_ratio": 1.6270491803278688,
  "no_speech_prob": 0.006583199370652437}, {"id": 573, "seek": 375336, "start": 3753.36,
  "end": 3759.44, "text": " is that queries are related in some way, right? So like
  if it''s a completely new cluster and usually", "tokens": [50364, 307, 300, 24109,
  366, 4077, 294, 512, 636, 11, 558, 30, 407, 411, 498, 309, 311, 257, 2584, 777,
  13630, 293, 2673, 50668], "temperature": 0.0, "avg_logprob": -0.1298132946616725,
  "compression_ratio": 1.5942622950819672, "no_speech_prob": 0.007006326224654913},
  {"id": 574, "seek": 375336, "start": 3759.44, "end": 3767.84, "text": " dense retrieval
  helps a lot there, pre-trained models on your domain or maybe on some generic domain",
  "tokens": [50668, 18011, 19817, 3337, 3665, 257, 688, 456, 11, 659, 12, 17227, 2001,
  5245, 322, 428, 9274, 420, 1310, 322, 512, 19577, 9274, 51088], "temperature": 0.0,
  "avg_logprob": -0.1298132946616725, "compression_ratio": 1.5942622950819672, "no_speech_prob":
  0.007006326224654913}, {"id": 575, "seek": 375336, "start": 3767.84, "end": 3773.28,
  "text": " like news, they might still pick these things up and put them in the same
  basket, then ask some", "tokens": [51088, 411, 2583, 11, 436, 1062, 920, 1888, 613,
  721, 493, 293, 829, 552, 294, 264, 912, 8390, 11, 550, 1029, 512, 51360], "temperature":
  0.0, "avg_logprob": -0.1298132946616725, "compression_ratio": 1.5942622950819672,
  "no_speech_prob": 0.007006326224654913}, {"id": 576, "seek": 375336, "start": 3773.28,
  "end": 3778.6400000000003, "text": " human annotators to go and check. Instead of
  checking the whole multimillion log, you know,", "tokens": [51360, 1952, 25339,
  3391, 281, 352, 293, 1520, 13, 7156, 295, 8568, 264, 1379, 32972, 11836, 3565, 11,
  291, 458, 11, 51628], "temperature": 0.0, "avg_logprob": -0.1298132946616725, "compression_ratio":
  1.5942622950819672, "no_speech_prob": 0.007006326224654913}, {"id": 577, "seek":
  377864, "start": 3778.72, "end": 3782.08, "text": " which would be super, super
  complicated. And you know, I agree.", "tokens": [50368, 597, 576, 312, 1687, 11,
  1687, 6179, 13, 400, 291, 458, 11, 286, 3986, 13, 50536], "temperature": 0.0, "avg_logprob":
  -0.16583304935031468, "compression_ratio": 1.640625, "no_speech_prob": 0.021722517907619476},
  {"id": 578, "seek": 377864, "start": 3782.64, "end": 3786.16, "text": " And the
  nice thing about like, you know, especially, you know, these engines,", "tokens":
  [50564, 400, 264, 1481, 551, 466, 411, 11, 291, 458, 11, 2318, 11, 291, 458, 11,
  613, 12982, 11, 50740], "temperature": 0.0, "avg_logprob": -0.16583304935031468,
  "compression_ratio": 1.640625, "no_speech_prob": 0.021722517907619476}, {"id": 579,
  "seek": 377864, "start": 3788.08, "end": 3795.3599999999997, "text": " you know,
  there is still the good old BM25 case where like at least the basic level keywords",
  "tokens": [50836, 291, 458, 11, 456, 307, 920, 264, 665, 1331, 15901, 6074, 1389,
  689, 411, 412, 1935, 264, 3875, 1496, 21009, 51200], "temperature": 0.0, "avg_logprob":
  -0.16583304935031468, "compression_ratio": 1.640625, "no_speech_prob": 0.021722517907619476},
  {"id": 580, "seek": 377864, "start": 3795.3599999999997, "end": 3801.3599999999997,
  "text": " are going to match. And so if a new term comes in for COVID and like it''s
  in the documents,", "tokens": [51200, 366, 516, 281, 2995, 13, 400, 370, 498, 257,
  777, 1433, 1487, 294, 337, 4566, 293, 411, 309, 311, 294, 264, 8512, 11, 51500],
  "temperature": 0.0, "avg_logprob": -0.16583304935031468, "compression_ratio": 1.640625,
  "no_speech_prob": 0.021722517907619476}, {"id": 581, "seek": 377864, "start": 3801.3599999999997,
  "end": 3806.72, "text": " you''ll at least probably get an exact match. You may
  not deal with the fuzzy matches all that", "tokens": [51500, 291, 603, 412, 1935,
  1391, 483, 364, 1900, 2995, 13, 509, 815, 406, 2028, 365, 264, 34710, 10676, 439,
  300, 51768], "temperature": 0.0, "avg_logprob": -0.16583304935031468, "compression_ratio":
  1.640625, "no_speech_prob": 0.021722517907619476}, {"id": 582, "seek": 380672, "start":
  3806.72, "end": 3811.7599999999998, "text": " well, but you know, like something''s
  better than nothing. And then that allows you to start to", "tokens": [50364, 731,
  11, 457, 291, 458, 11, 411, 746, 311, 1101, 813, 1825, 13, 400, 550, 300, 4045,
  291, 281, 722, 281, 50616], "temperature": 0.0, "avg_logprob": -0.16903831647789996,
  "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.00046816596295684576},
  {"id": 583, "seek": 380672, "start": 3811.7599999999998, "end": 3819.52, "text":
  " iterate on it. Yeah, exactly. So the next question from Q&A panel is from Chris
  for the search with", "tokens": [50616, 44497, 322, 309, 13, 865, 11, 2293, 13,
  407, 264, 958, 1168, 490, 1249, 5, 32, 4831, 307, 490, 6688, 337, 264, 3164, 365,
  51004], "temperature": 0.0, "avg_logprob": -0.16903831647789996, "compression_ratio":
  1.5238095238095237, "no_speech_prob": 0.00046816596295684576}, {"id": 584, "seek":
  380672, "start": 3819.52, "end": 3823.9199999999996, "text": " ML course, which
  front-end framework are most students using for their projects?", "tokens": [51004,
  21601, 1164, 11, 597, 1868, 12, 521, 8388, 366, 881, 1731, 1228, 337, 641, 4455,
  30, 51224], "temperature": 0.0, "avg_logprob": -0.16903831647789996, "compression_ratio":
  1.5238095238095237, "no_speech_prob": 0.00046816596295684576}, {"id": 585, "seek":
  380672, "start": 3826.24, "end": 3830.72, "text": " Front-end framework feels a
  little open-ended to me, but I mean, I can tell.", "tokens": [51340, 17348, 12,
  521, 8388, 3417, 257, 707, 1269, 12, 3502, 281, 385, 11, 457, 286, 914, 11, 286,
  393, 980, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16903831647789996, "compression_ratio":
  1.5238095238095237, "no_speech_prob": 0.00046816596295684576}, {"id": 586, "seek":
  383072, "start": 3831.2799999999997, "end": 3836.9599999999996, "text": " So one
  of the things we''re doing in both classes is we try to work with a real data set-end
  with", "tokens": [50392, 407, 472, 295, 264, 721, 321, 434, 884, 294, 1293, 5359,
  307, 321, 853, 281, 589, 365, 257, 957, 1412, 992, 12, 521, 365, 50676], "temperature":
  0.0, "avg_logprob": -0.11868405085737987, "compression_ratio": 1.662162162162162,
  "no_speech_prob": 0.0034896009601652622}, {"id": 587, "seek": 383072, "start": 3836.9599999999996,
  "end": 3842.8799999999997, "text": " a real search application. For better or for
  worse, we chose not to use notebooks.", "tokens": [50676, 257, 957, 3164, 3861,
  13, 1171, 1101, 420, 337, 5324, 11, 321, 5111, 406, 281, 764, 43782, 13, 50972],
  "temperature": 0.0, "avg_logprob": -0.11868405085737987, "compression_ratio": 1.662162162162162,
  "no_speech_prob": 0.0034896009601652622}, {"id": 588, "seek": 383072, "start": 3844.3199999999997,
  "end": 3848.56, "text": " Notebooks are great for a lot of things, but I don''t
  know that they always show you how actual", "tokens": [51044, 11633, 15170, 366,
  869, 337, 257, 688, 295, 721, 11, 457, 286, 500, 380, 458, 300, 436, 1009, 855,
  291, 577, 3539, 51256], "temperature": 0.0, "avg_logprob": -0.11868405085737987,
  "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0034896009601652622},
  {"id": 589, "seek": 383072, "start": 3848.56, "end": 3854.08, "text": " applications
  work. So we actually build out a really simple application. The front-end is like",
  "tokens": [51256, 5821, 589, 13, 407, 321, 767, 1322, 484, 257, 534, 2199, 3861,
  13, 440, 1868, 12, 521, 307, 411, 51532], "temperature": 0.0, "avg_logprob": -0.11868405085737987,
  "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0034896009601652622},
  {"id": 590, "seek": 385408, "start": 3854.16, "end": 3862.4, "text": " tailwinds,
  CSS, and really simple flask serving layer for the APIs. And then we use", "tokens":
  [50368, 6838, 12199, 82, 11, 24387, 11, 293, 534, 2199, 932, 3863, 8148, 4583, 337,
  264, 21445, 13, 400, 550, 321, 764, 50780], "temperature": 0.0, "avg_logprob": -0.17761532689484072,
  "compression_ratio": 1.5138121546961325, "no_speech_prob": 0.0010503839002922177},
  {"id": 591, "seek": 385408, "start": 3863.52, "end": 3871.52, "text": " open search
  for the search engine and things like fast text and a few other things for ML side
  of it.", "tokens": [50836, 1269, 3164, 337, 264, 3164, 2848, 293, 721, 411, 2370,
  2487, 293, 257, 1326, 661, 721, 337, 21601, 1252, 295, 309, 13, 51236], "temperature":
  0.0, "avg_logprob": -0.17761532689484072, "compression_ratio": 1.5138121546961325,
  "no_speech_prob": 0.0010503839002922177}, {"id": 592, "seek": 385408, "start": 3871.52,
  "end": 3877.84, "text": " You know, we use the learning to rank plugin for open
  search, trying to think if there''s", "tokens": [51236, 509, 458, 11, 321, 764,
  264, 2539, 281, 6181, 23407, 337, 1269, 3164, 11, 1382, 281, 519, 498, 456, 311,
  51552], "temperature": 0.0, "avg_logprob": -0.17761532689484072, "compression_ratio":
  1.5138121546961325, "no_speech_prob": 0.0010503839002922177}, {"id": 593, "seek":
  387784, "start": 3877.84, "end": 3884.96, "text": " anything else in our stack.
  It''s primarily Python, but I think if you were a Java user or any of the", "tokens":
  [50364, 1340, 1646, 294, 527, 8630, 13, 467, 311, 10029, 15329, 11, 457, 286, 519,
  498, 291, 645, 257, 10745, 4195, 420, 604, 295, 264, 50720], "temperature": 0.0,
  "avg_logprob": -0.09764885646040722, "compression_ratio": 1.590717299578059, "no_speech_prob":
  0.0013443896314129233}, {"id": 594, "seek": 387784, "start": 3884.96, "end": 3891.1200000000003,
  "text": " other languages where there''s clients for open search, you would do just
  fine in the class.", "tokens": [50720, 661, 8650, 689, 456, 311, 6982, 337, 1269,
  3164, 11, 291, 576, 360, 445, 2489, 294, 264, 1508, 13, 51028], "temperature": 0.0,
  "avg_logprob": -0.09764885646040722, "compression_ratio": 1.590717299578059, "no_speech_prob":
  0.0013443896314129233}, {"id": 595, "seek": 387784, "start": 3891.1200000000003,
  "end": 3897.76, "text": " You maybe just won''t be able to use all of the Python
  capabilities that we have in the class.", "tokens": [51028, 509, 1310, 445, 1582,
  380, 312, 1075, 281, 764, 439, 295, 264, 15329, 10862, 300, 321, 362, 294, 264,
  1508, 13, 51360], "temperature": 0.0, "avg_logprob": -0.09764885646040722, "compression_ratio":
  1.590717299578059, "no_speech_prob": 0.0013443896314129233}, {"id": 596, "seek":
  387784, "start": 3897.76, "end": 3904.2400000000002, "text": " I hope that answers
  your question, Chris. The repositories are all at least the base level", "tokens":
  [51360, 286, 1454, 300, 6338, 428, 1168, 11, 6688, 13, 440, 22283, 2083, 366, 439,
  412, 1935, 264, 3096, 1496, 51684], "temperature": 0.0, "avg_logprob": -0.09764885646040722,
  "compression_ratio": 1.590717299578059, "no_speech_prob": 0.0013443896314129233},
  {"id": 597, "seek": 390424, "start": 3904.24, "end": 3910.8799999999997, "text":
  " repositories are all available under my GitHub. So you can just go to my GitHub,
  which excuse me is", "tokens": [50364, 22283, 2083, 366, 439, 2435, 833, 452, 23331,
  13, 407, 291, 393, 445, 352, 281, 452, 23331, 11, 597, 8960, 385, 307, 50696], "temperature":
  0.0, "avg_logprob": -0.19815291298760307, "compression_ratio": 1.6351931330472103,
  "no_speech_prob": 0.013288472779095173}, {"id": 598, "seek": 390424, "start": 3910.8799999999997,
  "end": 3919.12, "text": " GSING, ERS, and put that in the chat. And then you can
  see the frameworks we use.", "tokens": [50696, 460, 20262, 30237, 11, 462, 43580,
  11, 293, 829, 300, 294, 264, 5081, 13, 400, 550, 291, 393, 536, 264, 29834, 321,
  764, 13, 51108], "temperature": 0.0, "avg_logprob": -0.19815291298760307, "compression_ratio":
  1.6351931330472103, "no_speech_prob": 0.013288472779095173}, {"id": 599, "seek":
  390424, "start": 3920.08, "end": 3926.08, "text": " Yeah, awesome. And I can just,
  you know, you can pick these things up or you can, if you know Python,", "tokens":
  [51156, 865, 11, 3476, 13, 400, 286, 393, 445, 11, 291, 458, 11, 291, 393, 1888,
  613, 721, 493, 420, 291, 393, 11, 498, 291, 458, 15329, 11, 51456], "temperature":
  0.0, "avg_logprob": -0.19815291298760307, "compression_ratio": 1.6351931330472103,
  "no_speech_prob": 0.013288472779095173}, {"id": 600, "seek": 390424, "start": 3926.08,
  "end": 3932.9599999999996, "text": " it''s probably easy for you, but if you don''t,
  you can pick this up. And the next question is from", "tokens": [51456, 309, 311,
  1391, 1858, 337, 291, 11, 457, 498, 291, 500, 380, 11, 291, 393, 1888, 341, 493,
  13, 400, 264, 958, 1168, 307, 490, 51800], "temperature": 0.0, "avg_logprob": -0.19815291298760307,
  "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.013288472779095173},
  {"id": 601, "seek": 393296, "start": 3932.96, "end": 3939.76, "text": " the chat
  from quasi, I hope you pronounce your name correctly. As these days, most of these",
  "tokens": [50364, 264, 5081, 490, 20954, 11, 286, 1454, 291, 19567, 428, 1315, 8944,
  13, 1018, 613, 1708, 11, 881, 295, 613, 50704], "temperature": 0.0, "avg_logprob":
  -0.20160731402310458, "compression_ratio": 1.6092436974789917, "no_speech_prob":
  0.00234916596673429}, {"id": 602, "seek": 393296, "start": 3939.76, "end": 3945.52,
  "text": " sort of approaches are based on transformers for anyone who wants to try
  out IR approach using", "tokens": [50704, 1333, 295, 11587, 366, 2361, 322, 4088,
  433, 337, 2878, 567, 2738, 281, 853, 484, 16486, 3109, 1228, 50992], "temperature":
  0.0, "avg_logprob": -0.20160731402310458, "compression_ratio": 1.6092436974789917,
  "no_speech_prob": 0.00234916596673429}, {"id": 603, "seek": 393296, "start": 3945.52,
  "end": 3951.84, "text": " transformers as a pet project. Does grant have any recommendations
  in terms of cloud services tools?", "tokens": [50992, 4088, 433, 382, 257, 3817,
  1716, 13, 4402, 6386, 362, 604, 10434, 294, 2115, 295, 4588, 3328, 3873, 30, 51308],
  "temperature": 0.0, "avg_logprob": -0.20160731402310458, "compression_ratio": 1.6092436974789917,
  "no_speech_prob": 0.00234916596673429}, {"id": 604, "seek": 393296, "start": 3954.08,
  "end": 3960.0, "text": " I don''t have any specific recommendations. I know I''ve
  looked at there''s several players. I was", "tokens": [51420, 286, 500, 380, 362,
  604, 2685, 10434, 13, 286, 458, 286, 600, 2956, 412, 456, 311, 2940, 4150, 13, 286,
  390, 51716], "temperature": 0.0, "avg_logprob": -0.20160731402310458, "compression_ratio":
  1.6092436974789917, "no_speech_prob": 0.00234916596673429}, {"id": 605, "seek":
  396000, "start": 3960.0, "end": 3966.72, "text": " so for instance, I saw somebody
  in one of the IR communities that I was in with posted around,", "tokens": [50364,
  370, 337, 5197, 11, 286, 1866, 2618, 294, 472, 295, 264, 16486, 4456, 300, 286,
  390, 294, 365, 9437, 926, 11, 50700], "temperature": 0.0, "avg_logprob": -0.24450718626684073,
  "compression_ratio": 1.5578512396694215, "no_speech_prob": 0.001972643891349435},
  {"id": 606, "seek": 396000, "start": 3966.72, "end": 3971.28, "text": " I think
  I don''t know how he''s pronounced about quadrant, I think QDR and T. I know there''s",
  "tokens": [50700, 286, 519, 286, 500, 380, 458, 577, 415, 311, 23155, 466, 46856,
  11, 286, 519, 1249, 35, 49, 293, 314, 13, 286, 458, 456, 311, 50928], "temperature":
  0.0, "avg_logprob": -0.24450718626684073, "compression_ratio": 1.5578512396694215,
  "no_speech_prob": 0.001972643891349435}, {"id": 607, "seek": 396000, "start": 3971.28,
  "end": 3979.04, "text": " UVA, I know there''s pine cone, elastic, solar, and open
  search all have dense vector retrieval", "tokens": [50928, 17887, 32, 11, 286, 458,
  456, 311, 15113, 19749, 11, 17115, 11, 7936, 11, 293, 1269, 3164, 439, 362, 18011,
  8062, 19817, 3337, 51316], "temperature": 0.0, "avg_logprob": -0.24450718626684073,
  "compression_ratio": 1.5578512396694215, "no_speech_prob": 0.001972643891349435},
  {"id": 608, "seek": 396000, "start": 3979.04, "end": 3986.0, "text": " capabilities.
  I''ve been playing around just getting started with hugging face. I''m a little
  late", "tokens": [51316, 10862, 13, 286, 600, 668, 2433, 926, 445, 1242, 1409, 365,
  41706, 1851, 13, 286, 478, 257, 707, 3469, 51664], "temperature": 0.0, "avg_logprob":
  -0.24450718626684073, "compression_ratio": 1.5578512396694215, "no_speech_prob":
  0.001972643891349435}, {"id": 609, "seek": 398600, "start": 3986.0, "end": 3991.04,
  "text": " to the hugging face game when it comes to these things. I know a lot of
  people I talk to use", "tokens": [50364, 281, 264, 41706, 1851, 1216, 562, 309,
  1487, 281, 613, 721, 13, 286, 458, 257, 688, 295, 561, 286, 751, 281, 764, 50616],
  "temperature": 0.0, "avg_logprob": -0.13947213556348664, "compression_ratio": 1.5938864628820961,
  "no_speech_prob": 0.005920142401009798}, {"id": 610, "seek": 398600, "start": 3991.04,
  "end": 3998.96, "text": " colab to build and run these systems. And so I think you
  can probably get started. Again,", "tokens": [50616, 1173, 455, 281, 1322, 293,
  1190, 613, 3652, 13, 400, 370, 286, 519, 291, 393, 1391, 483, 1409, 13, 3764, 11,
  51012], "temperature": 0.0, "avg_logprob": -0.13947213556348664, "compression_ratio":
  1.5938864628820961, "no_speech_prob": 0.005920142401009798}, {"id": 611, "seek":
  398600, "start": 3998.96, "end": 4004.08, "text": " like Demetri, you may have better
  tutorials. I know you''ve posted a bunch of stuff on medium", "tokens": [51012,
  411, 4686, 302, 470, 11, 291, 815, 362, 1101, 17616, 13, 286, 458, 291, 600, 9437,
  257, 3840, 295, 1507, 322, 6399, 51268], "temperature": 0.0, "avg_logprob": -0.13947213556348664,
  "compression_ratio": 1.5938864628820961, "no_speech_prob": 0.005920142401009798},
  {"id": 612, "seek": 398600, "start": 4004.08, "end": 4009.52, "text": " amount,
  how to get started in this as have other people. So I would start there, I guess,",
  "tokens": [51268, 2372, 11, 577, 281, 483, 1409, 294, 341, 382, 362, 661, 561, 13,
  407, 286, 576, 722, 456, 11, 286, 2041, 11, 51540], "temperature": 0.0, "avg_logprob":
  -0.13947213556348664, "compression_ratio": 1.5938864628820961, "no_speech_prob":
  0.005920142401009798}, {"id": 613, "seek": 400952, "start": 4010.16, "end": 4016.32,
  "text": " any one of those you probably won''t do wrong with. And then for me, I
  always go back to,", "tokens": [50396, 604, 472, 295, 729, 291, 1391, 1582, 380,
  360, 2085, 365, 13, 400, 550, 337, 385, 11, 286, 1009, 352, 646, 281, 11, 50704],
  "temperature": 0.0, "avg_logprob": -0.13880052773848825, "compression_ratio": 1.819905213270142,
  "no_speech_prob": 0.004882677458226681}, {"id": 614, "seek": 400952, "start": 4016.32,
  "end": 4022.64, "text": " like I like to take a data set that I''m familiar with
  first rather than a technology that I''m", "tokens": [50704, 411, 286, 411, 281,
  747, 257, 1412, 992, 300, 286, 478, 4963, 365, 700, 2831, 813, 257, 2899, 300, 286,
  478, 51020], "temperature": 0.0, "avg_logprob": -0.13880052773848825, "compression_ratio":
  1.819905213270142, "no_speech_prob": 0.004882677458226681}, {"id": 615, "seek":
  400952, "start": 4022.64, "end": 4029.12, "text": " unfamiliar with. Whenever I''m
  learning something new, I start with something I''m familiar with and", "tokens":
  [51020, 29415, 365, 13, 14159, 286, 478, 2539, 746, 777, 11, 286, 722, 365, 746,
  286, 478, 4963, 365, 293, 51344], "temperature": 0.0, "avg_logprob": -0.13880052773848825,
  "compression_ratio": 1.819905213270142, "no_speech_prob": 0.004882677458226681},
  {"id": 616, "seek": 400952, "start": 4029.12, "end": 4037.92, "text": " then try
  to apply that thing to the new technology as opposed to picking the technology first
  and then", "tokens": [51344, 550, 853, 281, 3079, 300, 551, 281, 264, 777, 2899,
  382, 8851, 281, 8867, 264, 2899, 700, 293, 550, 51784], "temperature": 0.0, "avg_logprob":
  -0.13880052773848825, "compression_ratio": 1.819905213270142, "no_speech_prob":
  0.004882677458226681}, {"id": 617, "seek": 403792, "start": 4038.56, "end": 4043.84,
  "text": " trying to, you know, kind of go back and forth between the tutorials that
  they provide. But", "tokens": [50396, 1382, 281, 11, 291, 458, 11, 733, 295, 352,
  646, 293, 5220, 1296, 264, 17616, 300, 436, 2893, 13, 583, 50660], "temperature":
  0.0, "avg_logprob": -0.09673656116832387, "compression_ratio": 1.7318840579710144,
  "no_speech_prob": 0.0014678132720291615}, {"id": 618, "seek": 403792, "start": 4043.84,
  "end": 4049.2000000000003, "text": " I always like to go back to a domain I''m familiar
  with because then I don''t have to rebuild my", "tokens": [50660, 286, 1009, 411,
  281, 352, 646, 281, 257, 9274, 286, 478, 4963, 365, 570, 550, 286, 500, 380, 362,
  281, 16877, 452, 50928], "temperature": 0.0, "avg_logprob": -0.09673656116832387,
  "compression_ratio": 1.7318840579710144, "no_speech_prob": 0.0014678132720291615},
  {"id": 619, "seek": 403792, "start": 4049.2000000000003, "end": 4053.92, "text":
  " intuition. Right. So for instance, I''ve never really done image search, but I''ve
  done e-commerce", "tokens": [50928, 24002, 13, 1779, 13, 407, 337, 5197, 11, 286,
  600, 1128, 534, 1096, 3256, 3164, 11, 457, 286, 600, 1096, 308, 12, 26926, 51164],
  "temperature": 0.0, "avg_logprob": -0.09673656116832387, "compression_ratio": 1.7318840579710144,
  "no_speech_prob": 0.0014678132720291615}, {"id": 620, "seek": 403792, "start": 4053.92,
  "end": 4060.8, "text": " search all the time. So it makes way more sense for me
  to try out transformers with e-commerce", "tokens": [51164, 3164, 439, 264, 565,
  13, 407, 309, 1669, 636, 544, 2020, 337, 385, 281, 853, 484, 4088, 433, 365, 308,
  12, 26926, 51508], "temperature": 0.0, "avg_logprob": -0.09673656116832387, "compression_ratio":
  1.7318840579710144, "no_speech_prob": 0.0014678132720291615}, {"id": 621, "seek":
  403792, "start": 4060.8, "end": 4067.28, "text": " than it does with images just
  because I don''t know the core intuition as much on the images as I do", "tokens":
  [51508, 813, 309, 775, 365, 5267, 445, 570, 286, 500, 380, 458, 264, 4965, 24002,
  382, 709, 322, 264, 5267, 382, 286, 360, 51832], "temperature": 0.0, "avg_logprob":
  -0.09673656116832387, "compression_ratio": 1.7318840579710144, "no_speech_prob":
  0.0014678132720291615}, {"id": 622, "seek": 406728, "start": 4067.28, "end": 4074.4,
  "text": " for e-commerce. So I would probably start that way first. Yeah, I agree.
  And another thing,", "tokens": [50364, 337, 308, 12, 26926, 13, 407, 286, 576, 1391,
  722, 300, 636, 700, 13, 865, 11, 286, 3986, 13, 400, 1071, 551, 11, 50720], "temperature":
  0.0, "avg_logprob": -0.1799041863643762, "compression_ratio": 1.5346938775510204,
  "no_speech_prob": 0.006555759813636541}, {"id": 623, "seek": 406728, "start": 4074.4,
  "end": 4079.2000000000003, "text": " yeah, of course, Grant, you thank you, you
  mentioned, you know, my medium blog post, there are", "tokens": [50720, 1338, 11,
  295, 1164, 11, 17529, 11, 291, 1309, 291, 11, 291, 2835, 11, 291, 458, 11, 452,
  6399, 6968, 2183, 11, 456, 366, 50960], "temperature": 0.0, "avg_logprob": -0.1799041863643762,
  "compression_ratio": 1.5346938775510204, "no_speech_prob": 0.006555759813636541},
  {"id": 624, "seek": 406728, "start": 4079.2000000000003, "end": 4084.32, "text":
  " a lot more people blogging on this, but I have a specific collection on medium,
  37 minutes", "tokens": [50960, 257, 688, 544, 561, 6968, 3249, 322, 341, 11, 457,
  286, 362, 257, 2685, 5765, 322, 6399, 11, 13435, 2077, 51216], "temperature": 0.0,
  "avg_logprob": -0.1799041863643762, "compression_ratio": 1.5346938775510204, "no_speech_prob":
  0.006555759813636541}, {"id": 625, "seek": 406728, "start": 4084.96, "end": 4091.92,
  "text": " by sheer reading time. You can go through like basics like exact can and
  search all the way up to,", "tokens": [51248, 538, 23061, 3760, 565, 13, 509, 393,
  352, 807, 411, 14688, 411, 1900, 393, 293, 3164, 439, 264, 636, 493, 281, 11, 51596],
  "temperature": 0.0, "avg_logprob": -0.1799041863643762, "compression_ratio": 1.5346938775510204,
  "no_speech_prob": 0.006555759813636541}, {"id": 626, "seek": 409192, "start": 4092.8,
  "end": 4097.76, "text": " you know, neural retrieval, which is approximate nearest
  neighbor search because you cannot do", "tokens": [50408, 291, 458, 11, 18161, 19817,
  3337, 11, 597, 307, 30874, 23831, 5987, 3164, 570, 291, 2644, 360, 50656], "temperature":
  0.0, "avg_logprob": -0.1365921762254503, "compression_ratio": 1.6125, "no_speech_prob":
  0.003066863864660263}, {"id": 627, "seek": 409192, "start": 4097.76, "end": 4104.8,
  "text": " exact can and search at scale. It will just not not scale. So you have
  to kind of go and cut some", "tokens": [50656, 1900, 393, 293, 3164, 412, 4373,
  13, 467, 486, 445, 406, 406, 4373, 13, 407, 291, 362, 281, 733, 295, 352, 293, 1723,
  512, 51008], "temperature": 0.0, "avg_logprob": -0.1365921762254503, "compression_ratio":
  1.6125, "no_speech_prob": 0.003066863864660263}, {"id": 628, "seek": 409192, "start":
  4104.8, "end": 4109.68, "text": " corners, so to say, but actually in a more mathematical
  sense, you create this algorithms that", "tokens": [51008, 12413, 11, 370, 281,
  584, 11, 457, 767, 294, 257, 544, 18894, 2020, 11, 291, 1884, 341, 14642, 300, 51252],
  "temperature": 0.0, "avg_logprob": -0.1365921762254503, "compression_ratio": 1.6125,
  "no_speech_prob": 0.003066863864660263}, {"id": 629, "seek": 409192, "start": 4109.68,
  "end": 4117.2, "text": " are beautifully handling this complexity for you. So go
  check it out. I think the next and probably", "tokens": [51252, 366, 16525, 13175,
  341, 14024, 337, 291, 13, 407, 352, 1520, 309, 484, 13, 286, 519, 264, 958, 293,
  1391, 51628], "temperature": 0.0, "avg_logprob": -0.1365921762254503, "compression_ratio":
  1.6125, "no_speech_prob": 0.003066863864660263}, {"id": 630, "seek": 411720, "start":
  4117.28, "end": 4124.72, "text": " last question, but not least, is from a shish,
  is the search with a mail course right to step into", "tokens": [50368, 1036, 1168,
  11, 457, 406, 1935, 11, 307, 490, 257, 402, 742, 11, 307, 264, 3164, 365, 257, 10071,
  1164, 558, 281, 1823, 666, 50740], "temperature": 0.0, "avg_logprob": -0.1991677235082253,
  "compression_ratio": 1.5875, "no_speech_prob": 0.006210990250110626}, {"id": 631,
  "seek": 411720, "start": 4124.72, "end": 4131.28, "text": " if I''m looking to learn
  about semantic search and add the functionality to SQL or no SQL databases?", "tokens":
  [50740, 498, 286, 478, 1237, 281, 1466, 466, 47982, 3164, 293, 909, 264, 14980,
  281, 19200, 420, 572, 19200, 22380, 30, 51068], "temperature": 0.0, "avg_logprob":
  -0.1991677235082253, "compression_ratio": 1.5875, "no_speech_prob": 0.006210990250110626},
  {"id": 632, "seek": 411720, "start": 4133.28, "end": 4136.88, "text": " That''s
  an interesting question. I guess I haven''t thought about it in that sense. I mean,
  I think,", "tokens": [51168, 663, 311, 364, 1880, 1168, 13, 286, 2041, 286, 2378,
  380, 1194, 466, 309, 294, 300, 2020, 13, 286, 914, 11, 286, 519, 11, 51348], "temperature":
  0.0, "avg_logprob": -0.1991677235082253, "compression_ratio": 1.5875, "no_speech_prob":
  0.006210990250110626}, {"id": 633, "seek": 411720, "start": 4139.36, "end": 4146.96,
  "text": " you know, I think a lot of the techniques we use in the ML class relate
  to semantic", "tokens": [51472, 291, 458, 11, 286, 519, 257, 688, 295, 264, 7512,
  321, 764, 294, 264, 21601, 1508, 10961, 281, 47982, 51852], "temperature": 0.0,
  "avg_logprob": -0.1991677235082253, "compression_ratio": 1.5875, "no_speech_prob":
  0.006210990250110626}, {"id": 634, "seek": 414696, "start": 4146.96, "end": 4154.24,
  "text": " search and relate to like how can we get better relevance out of the engine?
  So semantic search being", "tokens": [50364, 3164, 293, 10961, 281, 411, 577, 393,
  321, 483, 1101, 32684, 484, 295, 264, 2848, 30, 407, 47982, 3164, 885, 50728], "temperature":
  0.0, "avg_logprob": -0.1753984769185384, "compression_ratio": 1.6470588235294117,
  "no_speech_prob": 0.0009869193891063333}, {"id": 635, "seek": 414696, "start": 4154.24,
  "end": 4159.76, "text": " one of those types of capabilities, a kind of semantic
  search often is a pretty loaded phrase. So", "tokens": [50728, 472, 295, 729, 3467,
  295, 10862, 11, 257, 733, 295, 47982, 3164, 2049, 307, 257, 1238, 13210, 9535, 13,
  407, 51004], "temperature": 0.0, "avg_logprob": -0.1753984769185384, "compression_ratio":
  1.6470588235294117, "no_speech_prob": 0.0009869193891063333}, {"id": 636, "seek":
  414696, "start": 4159.76, "end": 4167.12, "text": " depending on what you''re trying
  to do there, as you should your mileage may vary. But we certainly", "tokens": [51004,
  5413, 322, 437, 291, 434, 1382, 281, 360, 456, 11, 382, 291, 820, 428, 43121, 815,
  10559, 13, 583, 321, 3297, 51372], "temperature": 0.0, "avg_logprob": -0.1753984769185384,
  "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0009869193891063333},
  {"id": 637, "seek": 414696, "start": 4167.12, "end": 4173.76, "text": " cover things
  like classifying your content, classifying your queries. We do learning to rank.",
  "tokens": [51372, 2060, 721, 411, 1508, 5489, 428, 2701, 11, 1508, 5489, 428, 24109,
  13, 492, 360, 2539, 281, 6181, 13, 51704], "temperature": 0.0, "avg_logprob": -0.1753984769185384,
  "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0009869193891063333},
  {"id": 638, "seek": 417376, "start": 4174.72, "end": 4182.64, "text": " We talk
  about synonym expansion query, you know, smarter queries, better filters, all of
  those kinds of", "tokens": [50412, 492, 751, 466, 5451, 12732, 11260, 14581, 11,
  291, 458, 11, 20294, 24109, 11, 1101, 15995, 11, 439, 295, 729, 3685, 295, 50808],
  "temperature": 0.0, "avg_logprob": -0.16645102751882454, "compression_ratio": 1.574468085106383,
  "no_speech_prob": 0.001611536368727684}, {"id": 639, "seek": 417376, "start": 4182.64,
  "end": 4188.72, "text": " things, I think fall can be loosely coupled into semantic
  search. If you''re talking more like you", "tokens": [50808, 721, 11, 286, 519,
  2100, 393, 312, 37966, 29482, 666, 47982, 3164, 13, 759, 291, 434, 1417, 544, 411,
  291, 51112], "temperature": 0.0, "avg_logprob": -0.16645102751882454, "compression_ratio":
  1.574468085106383, "no_speech_prob": 0.001611536368727684}, {"id": 640, "seek":
  417376, "start": 4188.72, "end": 4198.56, "text": " want to do like graph-based
  inferences or, you know, using things like wiki data or dbpedia or", "tokens": [51112,
  528, 281, 360, 411, 4295, 12, 6032, 13596, 2667, 420, 11, 291, 458, 11, 1228, 721,
  411, 261, 9850, 1412, 420, 274, 65, 3452, 654, 420, 51604], "temperature": 0.0,
  "avg_logprob": -0.16645102751882454, "compression_ratio": 1.574468085106383, "no_speech_prob":
  0.001611536368727684}, {"id": 641, "seek": 419856, "start": 4198.56, "end": 4204.64,
  "text": " those kinds of things to infer relationships and do semantic search that
  way. We don''t really", "tokens": [50364, 729, 3685, 295, 721, 281, 13596, 6159,
  293, 360, 47982, 3164, 300, 636, 13, 492, 500, 380, 534, 50668], "temperature":
  0.0, "avg_logprob": -0.1109636671402875, "compression_ratio": 1.5274725274725274,
  "no_speech_prob": 0.000490899255964905}, {"id": 642, "seek": 419856, "start": 4204.64,
  "end": 4212.4800000000005, "text": " cover those as much. We do base off of open
  search, but I think the concepts apply in general.", "tokens": [50668, 2060, 729,
  382, 709, 13, 492, 360, 3096, 766, 295, 1269, 3164, 11, 457, 286, 519, 264, 10392,
  3079, 294, 2674, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1109636671402875,
  "compression_ratio": 1.5274725274725274, "no_speech_prob": 0.000490899255964905},
  {"id": 643, "seek": 419856, "start": 4213.68, "end": 4221.6, "text": " With the
  SQL and no SQL databases, like I know a lot of them have kind of baseline search",
  "tokens": [51120, 2022, 264, 19200, 293, 572, 19200, 22380, 11, 411, 286, 458, 257,
  688, 295, 552, 362, 733, 295, 20518, 3164, 51516], "temperature": 0.0, "avg_logprob":
  -0.1109636671402875, "compression_ratio": 1.5274725274725274, "no_speech_prob":
  0.000490899255964905}, {"id": 644, "seek": 422160, "start": 4221.6, "end": 4229.360000000001,
  "text": " functionality in them. And so you would be able to apply some of the principles
  because a lot of", "tokens": [50364, 14980, 294, 552, 13, 400, 370, 291, 576, 312,
  1075, 281, 3079, 512, 295, 264, 9156, 570, 257, 688, 295, 50752], "temperature":
  0.0, "avg_logprob": -0.10869394322877289, "compression_ratio": 1.7130044843049328,
  "no_speech_prob": 0.007272040005773306}, {"id": 645, "seek": 422160, "start": 4229.360000000001,
  "end": 4238.240000000001, "text": " the principles we do, you actually do either
  before indexing or before querying. So those would", "tokens": [50752, 264, 9156,
  321, 360, 11, 291, 767, 360, 2139, 949, 8186, 278, 420, 949, 7083, 1840, 13, 407,
  729, 576, 51196], "temperature": 0.0, "avg_logprob": -0.10869394322877289, "compression_ratio":
  1.7130044843049328, "no_speech_prob": 0.007272040005773306}, {"id": 646, "seek":
  422160, "start": 4238.240000000001, "end": 4243.280000000001, "text": " certainly
  apply, you know, because at the end of the day, you''re just using those things
  to then", "tokens": [51196, 3297, 3079, 11, 291, 458, 11, 570, 412, 264, 917, 295,
  264, 786, 11, 291, 434, 445, 1228, 729, 721, 281, 550, 51448], "temperature": 0.0,
  "avg_logprob": -0.10869394322877289, "compression_ratio": 1.7130044843049328, "no_speech_prob":
  0.007272040005773306}, {"id": 647, "seek": 422160, "start": 4243.280000000001, "end":
  4249.52, "text": " generate a better query or a better document to be stored in
  your engine. And so I don''t see", "tokens": [51448, 8460, 257, 1101, 14581, 420,
  257, 1101, 4166, 281, 312, 12187, 294, 428, 2848, 13, 400, 370, 286, 500, 380, 536,
  51760], "temperature": 0.0, "avg_logprob": -0.10869394322877289, "compression_ratio":
  1.7130044843049328, "no_speech_prob": 0.007272040005773306}, {"id": 648, "seek":
  424952, "start": 4249.68, "end": 4254.96, "text": " your reason why they went work
  in a no-SQL store or a SQL store. It''s just then how do you", "tokens": [50372,
  428, 1778, 983, 436, 1437, 589, 294, 257, 572, 12, 39934, 3531, 420, 257, 19200,
  3531, 13, 467, 311, 445, 550, 577, 360, 291, 50636], "temperature": 0.0, "avg_logprob":
  -0.16992644893312916, "compression_ratio": 1.5809128630705394, "no_speech_prob":
  0.005734000355005264}, {"id": 649, "seek": 424952, "start": 4254.96, "end": 4262.160000000001,
  "text": " translate that into your query language, right? But we do use open search.
  All the examples are", "tokens": [50636, 13799, 300, 666, 428, 14581, 2856, 11,
  558, 30, 583, 321, 360, 764, 1269, 3164, 13, 1057, 264, 5110, 366, 50996], "temperature":
  0.0, "avg_logprob": -0.16992644893312916, "compression_ratio": 1.5809128630705394,
  "no_speech_prob": 0.005734000355005264}, {"id": 650, "seek": 424952, "start": 4262.160000000001,
  "end": 4268.4800000000005, "text": " open search. You would have to do the work
  to leap to that, whatever it is your engine is doing.", "tokens": [50996, 1269,
  3164, 13, 509, 576, 362, 281, 360, 264, 589, 281, 19438, 281, 300, 11, 2035, 309,
  307, 428, 2848, 307, 884, 13, 51312], "temperature": 0.0, "avg_logprob": -0.16992644893312916,
  "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.005734000355005264},
  {"id": 651, "seek": 424952, "start": 4269.6, "end": 4275.360000000001, "text": "
  Yeah, absolutely. And the good thing is that open search does have a K&N plugin.
  They call it K&N", "tokens": [51368, 865, 11, 3122, 13, 400, 264, 665, 551, 307,
  300, 1269, 3164, 775, 362, 257, 591, 5, 45, 23407, 13, 814, 818, 309, 591, 5, 45,
  51656], "temperature": 0.0, "avg_logprob": -0.16992644893312916, "compression_ratio":
  1.5809128630705394, "no_speech_prob": 0.005734000355005264}, {"id": 652, "seek":
  427536, "start": 4275.36, "end": 4280.719999999999, "text": " plugin, but it''s
  actually approximate nearest neighbor search. And so it''s off heap for those",
  "tokens": [50364, 23407, 11, 457, 309, 311, 767, 30874, 23831, 5987, 3164, 13, 400,
  370, 309, 311, 766, 33591, 337, 729, 50632], "temperature": 0.0, "avg_logprob":
  -0.11169684926668803, "compression_ratio": 1.531496062992126, "no_speech_prob":
  0.019404180347919464}, {"id": 653, "seek": 427536, "start": 4280.719999999999, "end":
  4288.32, "text": " who care. So it''s not inside Java, but it still allows you to
  get a feel of how neural search will", "tokens": [50632, 567, 1127, 13, 407, 309,
  311, 406, 1854, 10745, 11, 457, 309, 920, 4045, 291, 281, 483, 257, 841, 295, 577,
  18161, 3164, 486, 51012], "temperature": 0.0, "avg_logprob": -0.11169684926668803,
  "compression_ratio": 1.531496062992126, "no_speech_prob": 0.019404180347919464},
  {"id": 654, "seek": 427536, "start": 4288.32, "end": 4294.08, "text": " influence
  your results at. And you can also, you know, mix and match, sort of using more traditional",
  "tokens": [51012, 6503, 428, 3542, 412, 13, 400, 291, 393, 611, 11, 291, 458, 11,
  2890, 293, 2995, 11, 1333, 295, 1228, 544, 5164, 51300], "temperature": 0.0, "avg_logprob":
  -0.11169684926668803, "compression_ratio": 1.531496062992126, "no_speech_prob":
  0.019404180347919464}, {"id": 655, "seek": 427536, "start": 4294.08, "end": 4300.48,
  "text": " VM25 with this. Awesome. This was the last question. Thanks so much to
  everyone who asked their", "tokens": [51300, 18038, 6074, 365, 341, 13, 10391, 13,
  639, 390, 264, 1036, 1168, 13, 2561, 370, 709, 281, 1518, 567, 2351, 641, 51620],
  "temperature": 0.0, "avg_logprob": -0.11169684926668803, "compression_ratio": 1.531496062992126,
  "no_speech_prob": 0.019404180347919464}, {"id": 656, "seek": 430048, "start": 4300.48,
  "end": 4307.12, "text": " questions live. And, you know, consider joining the course
  if you haven''t yet. And, Grant,", "tokens": [50364, 1651, 1621, 13, 400, 11, 291,
  458, 11, 1949, 5549, 264, 1164, 498, 291, 2378, 380, 1939, 13, 400, 11, 17529, 11,
  50696], "temperature": 0.0, "avg_logprob": -0.17734616994857788, "compression_ratio":
  1.6394849785407726, "no_speech_prob": 0.003930052742362022}, {"id": 657, "seek":
  430048, "start": 4307.12, "end": 4312.879999999999, "text": " thanks so much for
  this session and for answering the question and sharing your wisdom. I''ve enjoyed",
  "tokens": [50696, 3231, 370, 709, 337, 341, 5481, 293, 337, 13430, 264, 1168, 293,
  5414, 428, 10712, 13, 286, 600, 4626, 50984], "temperature": 0.0, "avg_logprob":
  -0.17734616994857788, "compression_ratio": 1.6394849785407726, "no_speech_prob":
  0.003930052742362022}, {"id": 658, "seek": 430048, "start": 4312.879999999999, "end":
  4319.2, "text": " this conversation very much. Thank you. Thanks so much for having
  me, Dmitry, and keep up the great", "tokens": [50984, 341, 3761, 588, 709, 13, 1044,
  291, 13, 2561, 370, 709, 337, 1419, 385, 11, 413, 3508, 627, 11, 293, 1066, 493,
  264, 869, 51300], "temperature": 0.0, "avg_logprob": -0.17734616994857788, "compression_ratio":
  1.6394849785407726, "no_speech_prob": 0.003930052742362022}, {"id": 659, "seek":
  430048, "start": 4319.2, "end": 4325.759999999999, "text": " work. I love the podcast.
  And it''s awesome to see a search dedicated podcast out there. So", "tokens": [51300,
  589, 13, 286, 959, 264, 7367, 13, 400, 309, 311, 3476, 281, 536, 257, 3164, 8374,
  7367, 484, 456, 13, 407, 51628], "temperature": 0.0, "avg_logprob": -0.17734616994857788,
  "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.003930052742362022},
  {"id": 660, "seek": 432576, "start": 4326.400000000001, "end": 4331.76, "text":
  " congrats and good luck with that. Thank you so much. All right. Bye-bye. Bye,
  folks.", "tokens": [50396, 8882, 1720, 293, 665, 3668, 365, 300, 13, 1044, 291,
  370, 709, 13, 1057, 558, 13, 4621, 12, 6650, 13, 4621, 11, 4024, 13, 50664], "temperature":
  0.0, "avg_logprob": -0.25870293837327224, "compression_ratio": 1.1734693877551021,
  "no_speech_prob": 0.0075907097198069096}, {"id": 661, "seek": 432576, "start": 4332.72,
  "end": 4341.68, "text": " Thanks, Dmitry. Thanks, Grant.", "tokens": [50712, 2561,
  11, 413, 3508, 627, 13, 2561, 11, 17529, 13, 51160], "temperature": 0.0, "avg_logprob":
  -0.25870293837327224, "compression_ratio": 1.1734693877551021, "no_speech_prob":
  0.0075907097198069096}, {"id": 662, "seek": 435576, "start": 4355.76, "end": 4357.72,
  "text": " All right.", "tokens": [50364, 1057, 558, 13, 50462], "temperature": 1.0,
  "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393, "no_speech_prob":
  0.5679353475570679}, {"id": 663, "seek": 435576, "start": 4357.84, "end": 4358.76,
  "text": " Dmitry.", "tokens": [50468, 413, 3508, 627, 13, 50514], "temperature":
  1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 664, "seek": 435576, "start": 4361.08,
  "end": 4361.56, "text": " Right, now.", "tokens": [50630, 1779, 11, 586, 13, 50654],
  "temperature": 1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 665, "seek": 435576, "start": 4361.8,
  "end": 4362.16, "text": " Two.", "tokens": [50666, 4453, 13, 50684], "temperature":
  1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 666, "seek": 435576, "start": 4362.54,
  "end": 4362.8, "text": " Two.", "tokens": [50703, 4453, 13, 50716], "temperature":
  1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 667, "seek": 435576, "start": 4362.8,
  "end": 4364.34, "text": " Until eight.", "tokens": [50716, 9088, 3180, 13, 50793],
  "temperature": 1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 668, "seek": 435576, "start": 4364.5,
  "end": 4365.8, "text": " One.", "tokens": [50801, 1485, 13, 50866], "temperature":
  1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 669, "seek": 435576, "start": 4365.8,
  "end": 4366.64, "text": " Four.", "tokens": [50866, 7451, 13, 50908], "temperature":
  1.0, "avg_logprob": -3.493879631384095, "compression_ratio": 1.2868852459016393,
  "no_speech_prob": 0.5679353475570679}, {"id": 670, "seek": 435576, "start": 4366.9400000000005,
  "end": 4374.860000000001, "text": " Okay, let''s get back to what you want and now
  we can direct that back to the millions.", "tokens": [50923, 1033, 11, 718, 311,
  483, 646, 281, 437, 291, 528, 293, 586, 321, 393, 2047, 300, 646, 281, 264, 6803,
  13, 51319], "temperature": 1.0, "avg_logprob": -3.493879631384095, "compression_ratio":
  1.2868852459016393, "no_speech_prob": 0.5679353475570679}, {"id": 671, "seek": 435576,
  "start": 4375.64, "end": 4376.24, "text": " Okay.", "tokens": [51358, 1033, 13,
  51388], "temperature": 1.0, "avg_logprob": -3.493879631384095, "compression_ratio":
  1.2868852459016393, "no_speech_prob": 0.5679353475570679}]'
---

Hello there, vector podcast is here. I'm Dimitri Khan and I'll be hosting this session. And just a few words on the logistics. Everyone in the audience feel free to submit your questions either through a Q&A panel or directly in the chat and we will try to handle as many questions as we can.
I'll save you words about core eyes. What's core eyes is a new education platform that transforms the way professionals build technical high demand skills through top industry instructors and collective peer learning.
And the format of their courses is innovative mixing live instructor sessions with real world projects and fireside chats like this one technique with operators who experts in their fields.
I will say a few words about myself as well, untraditionally on the podcast, but I think it becomes a tradition now second time. I said and Dimitri Khan I have a PhD in natural language processing. I've worked at company Alphasense helped to build the search stack.
I spent like a decade, you know, there. I've been a principal AI scientist at silo AI. It's a AI consulting gig focusing on a number of ML verticals. And recently I joined company Tom Tom as a senior product manager working on search. I've also been a contributor and user of Cupid.
It's a query rating tool go check it out. It's an open source tool. So overall I spent like 16 years in developing search engines for startups and multinational technology giants. I also happen to be hosting this podcast vector podcast go check it out. I'll share the link in a second.
And I'm also blogging on medium on on my findings in vector search. So you might hear me talking about vector search here and there. And today I'm super super excited to have Grant in your soul with me. I've known Grant since about 2011.
Not personally, but I've seen I've seen him on stage on you know Berlin buzzwords conference and Lucinda revolution. And he has been a long contributor and open source as well. Solary, Lucinda Mahoot and others. And very very effective presenter.
I just watched a few presentations as a homework for this session. There will be some questions from there. But hey Grant, let's start with an introduction from Indio own words.
Hey Dmitri and thank you so much for having me on the vector podcast and obviously props to co rise here as well for helping sponsor this. Both Daniel Tungaling and I are on the co rise platform and really enjoying our time there. So real quick about myself. As you said, my name is Grant Ingersoll.
I guess these days a long standing user and contributor and committer. And generally somebody who participates in the search space, if you will, I think I wrote my first Lucine code back in 2004 or so. I guess that maybe makes me old.
As far as my background is I was one of the co founders of Lucidworks, which is one of the leading companies in the search space. I then left them in 2019 to become the chief technology officer at the Wikimedia Foundation.
You probably know them better as the nonprofit behind Wikipedia and Wikidata. So I was the CTO there for two years. And then in August or so of 2021, I took some time off. And then in January of 2022, I went on my own as a consultant and an instructor for co rise. So here we are now.
I am commonly doing work in what I would call fractional CTO land, which means I primarily help companies kind of get their technology stack in order, make decisions about technology, higher teams, upgrade teams, do all the things that a CTO would do. Often for small businesses and or startups.
And so that's really my background. Really happy to be here and looking forward to the podcast. Awesome. Great to have you, really grand. And also, you know, finally, I have a chance to ask some questions and chat to you in this cozy atmosphere as well. And I wanted to start with a question.
So I was watching a kind of short interview you gave. During Berlin buzzwords 2016, where you said how you split your time as then CTO of I believe, Lucid works. You said that you split your time between three C's, which is writing code, going to conferences and talking to customers.
Now that you're independent, is this how you spend your time or did you did you get some new letters of the alphabet? Yeah.
Yeah, and there's often in there as well, colleagues and co-workers, you know, especially in, you know, the CTO role is kind of a funny one, right? Depending on the company, it can mean a lot of different things. At some companies, CTOs are entirely outward facing.
It's effectively a sales role or a marketing role, right? You're out evangelizing the product, you're talking to customers, etc. In a startup, the CTO is often the primary engineer.
If you're a two person startup and you're just getting off the ground, you probably have the CTO title if you're the technical one in that startup, and you're probably writing all the code, right?
In other places, you're running your engineering team, and you may not be writing as much code, but you're responsible for the team.
I guess over my years, I've worn all of those hats. I've been out doing conferences and evangelizing. I've done a lot of sales work, especially later on at Lucidworks, I did a lot of sales work as the company evolved and grew.
When I was at Wikimedia, it was all pretty much internal running the technology team, making, you know, helping making technology decisions, all of those kinds of things. So I wouldn't necessarily say it's changed much.
I still do write some code, but not as much as as I used to, I guess, when I was a full-time engineer. But yeah, it still roughly falls into those categories.
Yeah, and I mean, like having been a student on your course, I've really enjoyed so much code that you've written to support this infrastructure of building the search engine. And I mean, you are still highly technical person, so I wouldn't discount that.
And I mean, this is something that is dear to my heart as well for me being an engineer, to talk to like-minded person. And in this segment year, in the same conference, 2017, you gave an excellent talk title, BM25, is so yesterday, modern techniques for better search.
And what's funny, and I'm going to share the link as well. But what's funny is that I don't know if you noticed it yourself, but you again have three C's in there. I wonder if you did it on purpose. What you have there as building blocks of this kind of journey of building a search engine.
So the first one is content. And you piggyback on solar capabilities, but in general, it could be any search engine out there with rules for content, like with boosting, manual boosting, you know, lending pages, and so on. The second C is collaboration.
So that's like the way you put it, it's collective intelligence to predict user behavior based on like historical aggregated data. And this is where I think recommenders come in, popularity, signals, and so on.
And last but not least, you have context, which is when you ask questions, who are you, where are you, you know, what I have you done previously.
And this is when you start doing market and user segmentation and venture into personalization and so on, would you say that you view the search engine journey and development the same way today, or have you have you changed your perspective? I really need to check and get a little more creative.
I think I'm using the letter C there too many times in a row, but I mean, I think a lot of that still stands pretty true.
Regardless of the engine you're using or whether you're using deep learning techniques or not, like, you know, at the end of the day, you're trying to match users to information that will help them make better decisions or be more informed, right.
And you know, these days, I would probably add in one more. I'm trying to think of how I could be witty and make it into another C, but you know, in working with Daniel Tongueleg on this class, one of the things that is just absolutely wowed me is the query understanding aspects of it.
And so maybe you could put that into the context category if you wanted. But, you know, realistically speaking that that work you can do, especially in large scale environments where you have a lot of queries, to really understand what users are asking or intending to ask when they put in a query.
So I would probably throw that in there if I didn't include that back then. And so like I said, maybe that's part of your content or your or your context is the actual query, a user or this, the set of queries that a user is asking.
You know, but I still think a lot of that stands at the conceptual level, right, is you have to have some, you know, if you think about it, this is the vector podcast, right.
All of this stuff we're building vectors and then essentially calculating this fancy version of a cosine similarity between them.
And at the end of the day, all of these techniques we're doing are effectively how can we shape those vectors so that things that are meant to be closer together, show up closer together and things that are not as related the cosine is further apart, right.
Like at the end of the day, like that math doesn't change, yet all these techniques, whether it's deep learning, et cetera, are all about creating those vectors and doing that calculation, right.
And so by understanding your content, you're shifting those vectors, you're transforming them in the space, you're adding synonyms, you're adding embeddings, all of those kinds of things, you're adding proper nouns, you're you're doing noun phrases, et cetera.
By understanding your context, you're able to ask better queries, right, which is shifting the query vector, right. And by using popularity, et cetera, you're also then shifting those vectors by essentially adding more weight to things that are more popular, right.
You know, so at the end of the day, like, yeah, I would say I'd still stand by that with the caveat is really bringing forward the query understanding aspect of it.
Yeah, I think query understanding, you put it brilliantly, it's like really an exciting space and we actually recorded a podcast as well with Daniel Tankilank, where he explained a lot of it, he also blogged a lot about it. So go check it out.
And like in that same presentation, like when you demoed the capabilities of Lucidworks platform, where you played a lot with different like ranking strategies, basically like you pre-trained some of them and you you were able to switch live, I felt like you you are a tinkerer as well.
You enjoy really going deep down into the what search engine can do and what you you can extract from the data.
And my question is, where do you see the balance between kind of like doing this in a more manual fashion, where you actually educate yourself, right, versus like throwing it to a machine learning model? Yeah, it's a great question.
 I mean, I think you know, obviously, and I see my former colleague and co-founder Eric Hatcher is on, I mean, he used to always say it depends and I'd say it depends here of course as well, which is, you know, I mean, there's there's some situations where you just you don't have enough data for machine learning, right?
So by default, you are going to be manually tuning the situation, right? You you see that a lot in enterprise systems, especially smaller enterprise systems or in niche applications where, you know, effectively search just needs to be good enough.
Maybe you're not monetizing search. And so you don't, you know, you just kind of need it to be reasonably good, right? It's a feature in a much broader set of features that users are going to engage with.
And so, you know, where and how you would use machine learning in those situations, you know, you may or may not.
 In the situations where you have lots and lots of data, lots of users, you're probably monetizing search, whether that's via e-commerce or or web search or ads or whatever, like, you know, I think machine learning makes a lot more sense there and and it's a lot easier to run these types of experiments that allow you to tinker not just with the hand-ranked models, which I think hand-ranking still has its place, right?
Because they help you form intuition about what is in your data, right?
And that intuition is really important even in a machine learning world because, you know, at the end of the day, even with machine learning, while you can still try out, you can try out a lot more features and approaches, you still have limited time, right?
 And so, you still have to have some intuition about what's going to work and I think there's no substitute for that intuition helping guide you into what matters, like, so for instance, in a learning to rank scenario where you're actually learning a ranking model, you still are often building up those systems using the features of your data.
 So you have to know what those features are and one of the nice things is like with tools like Lucine-based engines like OpenSearch or Solar or Elastic, I'm sure Vespa has the same kind of thing, you can go and play around with those, you can create your own function queries that allow you to roughly try out different formulas for ranking and then you can go and turn those things into machine learning models, right?
That learn a much more effective function than what you could come up with, right? So, I think even in this world of large data sets and machine learning, you're still going to have to build intuition, right? Yeah, absolutely.
And like in your own experience and in the experience of the teams that you supported, how do you nurture this intuition? Like, do you read books? Do you constantly experiment?
 And also like when it comes in, you know, to understanding fundamentals of search, let's say knowing how TFIDF formula composed or you have 25, what are the trade-offs versus sort of like going and actually experimenting and trying out things, you know, where do you see that balance as well for yourself maybe and also for the teams around you?
Yeah, I mean, I think everybody will have their own, you know, kind of depending on where you come from, right? Like if, you know, like if you have, if you've done deep academic work, you're probably going to have a lot more understanding of the math and the theoretical side of it.
And then you're going to have to develop the intuition of real world data, right? How messy it is, how clunky it is, how full of junk and spam, et cetera, right? Because a lot of times when you're dealing with academic data sets, they're pretty clean, right?
Relatively speaking, they still of course have their own set of garbage and nuances in them.
Whereas if you're an engineer and you're coming at it from like, hey, you know, often what I see with engineers is they come at it from a quantitative standpoint of like, I want to make sure this is scalable and reliable. So they're solving for the hardening of the system problem first.
And then they often will develop the the relevant side of it or the the understanding of the data second. Now again, broad generalizations there because, you know, folks have all kinds of different backgrounds.
But you know, so like as a leader in somebody who does, you know, manages people in this space, like I would often just work with you depending on what your background and understanding and intuition is.
And then, you know, try to help you complement whatever it is you're missing there, right? Like I think you have to have an understanding of how these engines work.
I've often seen folks who don't have an understanding of all the capabilities of these modern search engines recreate the wheel, right? Like they're reinventing the wheel because they they're coming from this first principles of the math that they learn at the academic level.
And then, but they don't necessarily know how that applies to real data in the real world. Whereas a lot of these, you know, modern search engines, because they are, they grew up in large scale, you know, publicly traded high volume spaces.
They've really been hardened on the engineering side and they really know how to deal with all the nuances of real world data, right?
And so, you by learning those kinds of things, you will be much more effective at the at bringing to bear your intuitions and understandings from whichever background that is.
I don't know if that makes sense or not. Yeah, no, absolutely. Yeah, actually in the same presentation, you also said like, you've seen cases where you you come into helper company and they they point you to sort of like a data, the ace almost of 10,000 rules.
And so you you said they have that in principle, you could just remove solar or whatever search engine you have and just use those rules to retrieve documents, right? But when you go and ask specific questions, what what what this rule does?
The answer that you you illustrated was well, it was created by Joy, you know, and he quit five years ago.
So he then said it makes sense. So we keep it. So how do you go about convincing the organization or teams to change their perception and sort of like become more flexible and move into this flywheel of experiments? Yeah, it's hard.
And again, I think, you know, I mean, you have to look at incentives and first principles there, right? Like, again, if you're in this boat of like searches, just a feature, there may or may not be any incentive.
But if you're in this boat of like, hey, search is a really critical aspect of what we do. Our users use it all the time. It's key to revenue. It's key to timeliness or it's, you know, people's lives are on the line, et cetera. You're going to invest in making sure searches as capable as possible.
Those folks usually don't take much convincing once you can show them a better way, right? They're often already frustrated by the sheer number of rules that they have.
And so one of the things that can often work in those situations, I think is, you know, you can start to just learn the, you know, a lot of these machine learning systems will actually learn the set of rules, right?
And so if you want, you can just start to learn the rules by the fact that you're gathering your queries and your click logs and you're looking at the engagements users are having with the system, with the rules in place.
And then over time, you know, that will learn it.
That the harder part often is getting that last part, which is true experimentation whereby they actually have a system in place for running multi-variant experiments or AB tests, right?
And they can actually try out different approaches and see which one wins and see which one's most effective and then go with that from, you know, until the next one beats it, right? That's a fair amount of engineering work to get in place.
It's also a fair amount of math to do in order to make sure it's appropriate. These days, there are systems and tools that allow you to do it, but if you want to homegrown it, you know, that can take a lot of work.
So getting people to be in that mindset, especially in environments or company cultures where like there's pride in being right, you know, you sometimes see that in a lot of companies where it's like whoever's the boss has to be right kind of situation.
Those types of companies are always going to struggle with experiment mindsets because, you know, they reward, quote unquote, being right as opposed to, quote unquote, you know, rewarding longer term growth and incremental improvements with the occasional failures, right?
So you really have to look at company culture first and potentially reset that and then build and bake in the the necessary engineering work to make experiments work.
Yeah, absolutely. I agree to that same thought that, you know, without failures, you cannot really breed the culture of creating cool new stuff because you basically cannot unleash yourself to go and mess with your code base, right? And do things and create new stuff.
So like, you need to be brave for sure. Well, as I think the front of mind Ted Dunning said, the cool thing about experimentation frameworks is you get to be wrong and that's okay, right? Like you're actually right by the fact that you're wrong.
You're because you're right in the long run, right? Yes. Even if any given experiment is flat or bad, right? But overall, you know, in the long run, you're going to win out because you're going to just, it's easier and easier for you to add in a new approach. Yeah, absolutely.
I think that Turnbull also said, like, you know, how you basically accumulate this bruises, right? So you're like, Oscar tissue as some other people say. So I think without doing things, you can't without failing as well, you can't learn. So I totally agree to that.
But still for those who are still learning, you know, and we are discussing, to some extent, the courses that you couldn't be teaching, you know, where do you start? Like, let's say you have some data, right? You have some click logs within your organization or maybe you found some data set.
Where do you start? How do you go about dissecting that data set? What do you do with it as next steps and what to avoid maybe and what good things to know to keep in mind?
Yeah, I mean, I think, you know, first off, I mean, a lot of companies aren't even at all that great at actually collecting and managing their query logs, right?
So if you're, if you've got a search engine up and running and you want to improve it, I mean, I think the first thing you have to start to do is again, it kind of goes back to this first principles.
Like, if I'm not measuring things that help me understand what users are doing, and that's the first step, right? Like, make sure you're able to process your query logs and capture things like session history and what users clicked on, what they saw.
A lot of companies will only measure what was clicked on, but they actually don't measure what was seen by the user or at least inferred to be seen by the user.
And that can be a big loss because a lot of these machine learning systems, you need to know what wasn't chosen just as much as you need to know what was chosen, right? So really make sure you've got the instrumentation of your system in place.
And guess what? A search engine is a great place to store all of that data as well, right? As elastic as proven out with their using search for logs and spawn as well, right? And so make sure you're captioning all that stuff. And then again, I think this is where your intuition starts to come in.
So whenever I get a new data set, a new set of click logs, I start to look at, well, what are my most popular queries? What are users asking today? What are they asking overall? What led to zero results?
How often are they rewriting their queries like they typed in a query and then they didn't like the results.
So they rewrote it. You know, all of these things are pretty easily discoverable in query logs, right? So just start digging in and building some intuition for those things.
 So for instance, one of the things when I was back at Lucidworks that we would do is what we call like head tail analysis or long tail analysis is another thing you see in the literature, you know, especially in the e-commerce world where you have this power law distribution where most people ask the same things over and over, but you often have a really long tail.
When you analyze the long tail in a lot of e-commerce situations, what you often find, for instance, is the long tail is actually pretty highly correlated to the head queries, right? And so developing that intuition of like, you know, why are these long tail queries working or not working?
That can then help you do much better at all of your queries, right? And so, you know, from those click logs, then you start to focus on, well, how do I improve my head or my torso queries, like the ones that are most common?
And then as you go on, then you can look at how do I handle long tail queries depending on how important they are to you? You know, and from from that click log, then you can start to build either, you know, in some cases, you still might make sense for you to have rules.
And then, and then you can also look at, you know, like again, like I would try to look at it the problem holistically, what's going to get me the most bang for my buck in terms of where I should spend my time, right?
So in the short run, rules are probably easier, but they're harder to maintain in the long run.
And of course, you can only manage so many rules on your own and, you know, even with several people, whereas machine learning may take more work up front, but in the long run is probably easier to maintain.
Although I do still wonder, you know, if we're going to run into the same kind of problems we have with rules with machine learning models where we have so many different models that are being applied and they're built by different teams and they're applied in different scenarios.
And, and next thing you know, you have a complexity problem on that front as well.
 But, you know, luckily, like with things like machine learning operations becoming more of a focus and people getting much more rigorous about how they deploy and manage models, I think most of those problems will be mitigated in one run, but it still goes back to the same core principles, which you need to have good housekeeping in order to be successful both with rules and with machine learning models.
I don't know if that that was kind of long wind. I don't know if that answered the question or not. It does, it does.
I mean, it gives the intuition, especially where you said the connection between, you know, that that was an insight actually to me, like the connection between head and tail that 50% of tail may correlate with your head. And that's amazing.
Like 50% of this super hard queries could be kind of, you know, removed from that complexity space, right? Which is, again, you know, your mileage may vary, right? Like it depends on your data set in Europe, but you know, like in e-commerce, right?
If if I phone 13 or whatever is the head query, there's probably a tail query that's, you know, silver 64 gigabyte iPhone 13 with case, right? Like that's probably a tail query or at least a torso query.
And once you have those types of realizations, you can start to link these up. And then the cool thing really is that then the things you know about the head can apply to those types of tail queries as well.
And so you're actually, you might be able to more effectively manage those tail queries, even without machine learning models. Yeah, absolutely. And just a quick reminder to our respected audience, feel free to send your questions.
Otherwise, I will ask all the questions myself, which, which of course I have, but, you know, I'm sure you guys have guys and girls. I'm sure you have some interesting cases. We do get a few questions already, but we will we'll answer them in the end of this session.
And couple coupling, you know, that process of sort of, you know, crafting the signals and training your model and deploying it and ML ops that you mentioned.
How do you when it comes to measurement, how do you measure? How do you make sure that, you know, what happens right now in production still makes sense that they don't need to do any hectic action about, you know, okay, pulling the model back or something like that.
What's your sense on on on that front? And like, maybe some measurements that you have deployed yourself and have been observing every single day and relying on it. And again, it depends on your what, you know, kind of what domain you work in.
But, you know, I mean, there's there's lots of literature on how to score and and, you know, test your model.
 So things like precision and recall where you're looking at what users are clicking on and whether they're finding the results, things like zero results or often one of the things that I find helpful is like what what you would call surprising results where documents are occurring fairly high up in the results, but they're not actually garnering the clicks that you would expect given that position.
So for instance, you know, I mean, many people in search understand that there's a position bias that's just built into all of us as humans. We we trust the machine. And so we click on the first one.
 Well, if you if you consistently see that a document is appearing at say number one or number two in the results, but it's getting way less clicks than say the six or seventh document, that might be an indication to you that that document isn't particularly relevant or for whatever reasons users aren't liking it.
So those kinds of more subtle metrics can also be informative.
 I think, you know, if you have a AB experiment, testing framework in place, obviously you can do all of your metrics around AB testing, you know, start with just giving a certain amount of traffic to your new approach and then ramping up as it meets your metrics, whatever that is or what, you know, your targets are if that's things like add the cards, etc.
You can ramp up those those types of tests as you as it proves out. There's obviously there's things you can do offline as well, like especially if you have enough query logs.
And if your index hasn't changed that much, but maybe just the approach you're taking has, then you can you can replay your logs, you can test out and you know, effectively simulate what users might click on in those scenarios.
And then of course there's the old fashioned just, you know, things like smell tests like do these results look better to me as an expert, you obviously have to be careful there or to a small cohort of experts, you know, like maybe your colleagues, etc. might spend some time scoring.
So all of these things, I think are techniques and measurements you can use to check to see whether results are, you know, good enough for you them to go into production.
I think there's I think Ronnie, Ronnie, co-hoved me, I forget the name of the book, but he has a really good book along with a co-author on online experimentation. It's probably these days the Bible of online experimentation. So I would encourage users to check that out.
And then, you know, there's there's lots of metrics that you can deploy, you know, that are pretty well standard and publicized. There's some quick googling should find those for people. Yeah, for sure.
Of course, I think you could measure some things like a DCG, which is offline, right? So like, but you do need like rated queries.
And as a contributor to Qbit, which is a query rating system, open source system, I'm curious to to hear your opinion on, you know, sort of on one hand, of course, you can always go and just check, sanity check, you know, smoke test, your, your, your runker.
But that's just maybe for engineers or product managers, like a smaller group versus when you go and try to understand the intent of queries at larger scale with this manual effort.
Have you seen, have you deployed such methods within organizations? What do you feel like doing this in the companies on more regular basis?
And I also know, as a shout out to what you did in the course, search with the mail course, like you did ask us to rate some queries and create a judgment, please, to get a feel of the process.
And I think that by itself is a great idea because it pushes you towards, you know, further understanding what is it that you're building for? So yeah.
 Yeah, I mean, I think, yeah, I mean, it makes, it makes a ton of sense to have, if you can afford to do offline evaluation using, you know, professional annotators, you know, like, I don't know how good mechanical Turk these days is, but like, you know, something like a mechanical Turk or like, I forget what crowd flour is called now or I know we've worked with a company called Appen in the past, like, there are these companies out there that will provide you with a large number of annotators who will run your queries and then rank them for you.
And of course, you can use that as well.
So again, like, you know, it often comes down to whether you're monetizing your search results and folks who do monetize their search results will typically pay for those kinds of things, especially once they reach really large scales, you know, like your, your Amazon's and the like.
Where and how much you can do that often comes down to budget and time, right?
So, you know, if you have the budget, I've seen companies do that, you know, maybe I don't know about weekly, there might be some that do that weekly at the really large scale, that gets really expensive quarterly or whenever there's a major update to the system, those kinds of things.
So by all means, I mean, I think anything you can do to get, you know, I think often in this space, we love to say, oh, well, this is the way you do it.
And the reality is, is like, you want a hybrid approach to most of these things, right? Because there's no one perfect way of, there's no one perfect model and there's no one perfect way of evaluating a model, right?
And so you need to blend these and build up a broader sense of what actually works, right? Yeah, absolutely.
 It's just like, I guess, I guess, general awareness, like that these systems and approaches exist and like when you feel stuck that you don't know, okay, you don't generate ideas where you can improve your search engine, you can go deeper and try to involve, you know, and the teachers, I believe, to help you understand.
And before we move further to some of higher level questions, I still wanted to ask you a little bit more detailed question on if somebody in the audience or listeners wants to try to build the kind of end-to-end search engine at home.
So what are the available datasets, tools and algorithms exist today that will allow you to build this and train relevancy models and all these building blocks in the search engine? Yeah, I mean, it's, you know, it's interesting.
I think in many ways we live in a golden age of of search engines, right? Like, there are several just top notch open source freely available search engines on the market.
There are a number of companies competing in this space, right? So, you know, picking an engine is almost like, hey, you know, it's a plethora of riches.
It's almost, it's like, you're, it's a challenge to pick one because there's so many good choices, right? And you're often like, what specific features or domains am I going to participate in? So, you know, it's obviously one like, choose a good engine.
And I think you really can't go wrong with any of the main ones. What, you know, it's the Lucine-based ones, Solar Elastic Search Open Search. I haven't played with Vespa myself, but, you know, I think that one's coming on strong as well.
You see a lot of interesting capabilities that are coming out of that. And then, you know, you have obviously the, the companies behind it. Of course, I'm co-founder of Lucidworks, and so still a big shout out and big fan there, because I think they're doing a lot of interesting things.
But you also see a number of other players in that space, both with deep learning or neural-based approaches, as well as blended or hybrid or traditional approaches. So, one, start with your engine. See what it's capable of.
And then on the data set front, it really kind of depends on what your, what domain you're in. But, you know, I'm a big fan. You know, I often start with public data sets, Trek TREC is a great place to get data sets across a large number of domains. You can also get queries.
So, whether you want to do web search or e-commerce or legal or enterprise or medical, like you can go to track and get a data set and start indexing that, playing around with it. These days also, it's just super easy to go crawl.
So, you know, get like scrappy or curl or WGET or whatever, or it's one of these crawlers and go crawl websites. And then you can start going from there. The query log side tends to be a little bit harder because companies don't like to release their queries.
But there are several data sets that do have some form of queries with them. They may not be enough for you to fully test all the features of an engine. So, in our class, we use a really old data set from Best Buy that has query logs. In it, well, query click logs.
But for instance, it doesn't tell you what was shown the user. It just tells you what they clicked on. And so, you can't actually build full models or effective models with that.
But it's actually a really good e-commerce data set because it has all of the problems of a data set that comes from a company. Namely, there's a lot of missing data in there. There's a lot of bad data. But there's also a lot of really good data.
And so, starting with those, and then I think, you know, you kind of just start to push the engine through its paces. Start with the tutorials, the basic features, and then see where you can go deeper.
Can you actually get Best In Class relevance measurement out of it? Can you get Best In Class speed performance out of it? And then just work your way through the engine. And these days, you can typically do that in, say, less than a week.
And that's really amazing, right? Especially when you combine that with all the great information out on the web, right? Like, you know, I think when I was getting started, it was, you know, you had to go and really dig in underneath the hood and kind of figure out a lot of those pieces these days.
It would take several weeks, if not months, you know, month or more to really feel like you understood an engine and where it went. And I think these days, it's just so much easier to do that, which is awesome. Yeah, absolutely. And I remember during the course we had to do it within a week.
So per project. So that was super exciting. And I think this would not be a vector podcast if I wouldn't ask you also on your opinion in vector search.
Like what's your feel for how it will augment the search engine experience on the user side as well as on the development side and connected to that. What do you think the search engine engineer profession is going to be like soon? And I think it's already shaping up in many ways.
Like the boundary between data scientists and the search engineer blend. Do you feel yourself like that? Do you think this is the direction we are going? Or do you think it's going to be like a form that will wear off? That's at some point. Yeah, I mean, it's, well, it's not going to wear off.
I mean, there's too much money and too much investment and too much better results. I will state upfront, I'm not an expert on these vector engines, right? Like I, it's kind of interesting.
Like they, I went back and look through some of my talks and I think I gave a talk in 2013 on what the Lucine and Solar community needed to do next. And one of the things was we need to add support for dense vectors. That was 2013. I think we just got dense vector support in solar.
Elastic maybe was there a little bit sooner, but roughly same time frame. There are plugins, of course, that have been around like the K&N plugins, things like that. Hey folks, like this stuff is here to stay.
I mean, the really interesting questions, you're starting to see these hybrid models where, like BM25 is still really good and really fast at that first pass retrieval.
It's kind of hard to beat in terms of the scale at which you can get a first pass rank, right? And then feeding it, those results into much deeper or more capable engines. I think that's been around for a while and academia has proven that out.
Clearly, like using embeddings and vectors for things like query understanding and content understanding and using tools like Burnt, etc. for enriching your understanding, your content, and then making those searchable. That's all, I think, well and good.
 I think the really interesting question will be is whether the vector engines can add all of the layers that the sparse approaches have, I don't know about perfected, but added over the years, you know, the fascinating, the aggregations, the spell checkings, the highlighting, all of those things that actually go into building a search application.
If the vector engines deliver all of those things and deliver better results, that's probably a no brainer, right? In the meantime, we have these hybrids because I think there nobody is delivering all of the capabilities.
The other things that's interesting with the dense vectors, right, is that you can start to map multimodal data types all into the same engine. So images and text and audio, etc. Right? And again, like I'm not an expert on this, but that's my understanding.
So then, so then you can query across spaces, if you will. Again, like I'm not using the right terminology here, but and that to me is often the, at least people talk about that like it's a holy grail. I'm not fully convinced people will actually search that way.
I still think that remains to seem because there's a lot of implications for the the user interface and the user experience is how you interact with that.
You know, like people have long talked about, oh, hey, I'm going to take a picture and then get my back, my search results, but like I don't every time I use those tools, I'm like, okay, that's nice, but it's still clunky from a user experience standpoint, right?
So, so like there's a lot of that work above and beyond just the core engine that has to be solved.
But clearly, there's a lot of money and effort going into it. And so like as a search engineer, you can't ignore as a data scientist, you can't ignore it. And so you've got to get up on how these are built.
I think all the major engines open source and private have some form of it at this point of blended models.
 Again, like, you know, if you're in a domain that you don't have enough data for these and may or may not work, although again, like one of the interesting things with these neural models, right, is you can often train on a general model and then just use a few examples from your domain to essentially tailor that general model to your environment, right?
Like I'm working on one of my clients is doing this in the NLP space right now.
We're using a general model around analyzing contracts and then we're applying domain specific things to it.
And it's really interesting how effective it is with very few examples, right? That's an NLP problem, not a search problem, but you know, so I think you're going to just continue to see that trend and grow and expand, right? So you've got to be on board with it. Yeah, absolutely.
And you can find of course more conversation on the podcast about this. But I think I agree with you that the multimodality aspect of vector search is quite exciting.
And where the data sits in images, for instance, that haven't been annotated yet, right? And so many images uploaded every single day in videos, you know, if the model is able to transcend the the domains so easily like clip model, for instance, built by OpenAI, it's not a perfect model.
Sometimes it fails, but sometimes it also uses you like, how could it figure out, you know, to work so reliably on my data that it hasn't seen before? That's amazing.
Well, and it goes back to your earlier question, which is like, you know, at the end of the day, folks like go evaluate it and see whether it works better for you.
And then like I said, even earlier, I mean, they're all just vectors and we're all just trying to calculate cosines between the user's query and and the vector. And so in some regards, like we're just building a better vector, right? It's just a better vector. It has more information encoded in it.
And so if I can query that more effectively, then why wouldn't you use it? Yeah, yeah, exactly. And of course, there are other subtopics there how to make it faster and so on, but I think eventually we will, hey, Google figured it out for 10% of the queries.
So I guess the rest of the world will catch up.
Before we continue to the questions from the audience of which we have at you, I do love asking, and if you can keep it a little bit short, because we are short on time, but I'm still super, super interested to hear your motivation to stay in this space.
You have tried so many things in your career, right? Looking at your LinkedIn profiles, just on and on experiences and fractional CTO and full-time CTO and an engineer and so on and book author.
What motivates you to stay in this space today and also go into education teaching? Yeah, I mean, it's funny.
 I think, well, even when I was at Wikimedia and I quote, unquote, left search, I mean, we still ran a very large search engine and I always enjoyed my conversations with a search team at Wikimedia just because they were, you know, it's such a high traffic website and search there, I think does something like 6,000 queries per second or something like that.
 So you know, in some ways, and this is reflecting back on my career, I mean, I think I fell in love with language and the way humans use language and find information back circa 1999 or so when I started at a small company called TextWise run by Liz Litty who is one of the pioneers in the natural language processing field and it just happened to have a search project that I started working on, right?
But to me, you know, at the end of the day, like, this space and this is why I went to Wikimedia.
 So I say, searches that necessarily the through line, even though it's often the main, it appears to be the through line in my career, the deeper through line, I think, is that I am fascinated by how we can leverage computers to help users make more informed, more capable, more aware decisions in their lives, whether that's purchasing online or political or governmental or whatever it is, like, I am fascinated by how we can help people make more informed decisions because I think that's the thing that lifts us out, right?
And so education then is a easy follow-on from that through line, right? Like, the more people I can help use these tools and also learn myself, the better off will I'll be, right? Like, we have to use these tools to, you know, to help us as humans get along better, etc.
be more informed, so on, so forth, right? So that's probably the through line of the career, right? Is this how do you help people find information and take action that makes us all better? Absolutely, this is very deep. Thanks so much.
I love asking this question because I'm super motivated to stay in the space, but I also love to see the facets and the motivation of other professionals like yourself that I'm looking up to. I really enjoyed this conversation.
Is there an announcement that you want to make in terms of the courses that you're going to be teaching soon? Yeah, that's great. I appreciate that, the Metri, and I know we'd have some user questions, and I'm happy to stay on a little bit longer as well, get those.
Yes, we actually, we have two classes coming up.
So one of the things we learned in the first run of search with machine learning is, you know, effectively we had one week of trying to get everybody on to the same page of how does open search work and what are the basics of search?
 And then we had three weeks of fairly intense machine learning in a search environment, and one of the things that happened in the class because we didn't have a lot of prerequisites is we had a really wide array of students of folks who were deep experts like yourself, as well as like totally new to this arena.
And what happened, I think, is that first week for the new people was like, hey, this is too much for me to get up to speed. And for the folks who had already done search, it was like, hey, I already know how to do all of this.
And so trying to, trying to go across that gap, I think we kind of ended up in this lukewarm area where nobody was quite satisfied.
So one of the things we did was we split out the new stuff into a two week class called search fundamentals, which covers all of the basic intuitions of search, whether it's deep learning based or a sparse learning based or sparse vector based, sorry.
And so we cover, you know, indexing querying, facetying, spell checking, auto-complete, kind of all the building blocks of a search application.
And then with the machine learning class, because we're dropping that beginner class week, we now have added in a neural retrieval dance retrieval into that as well. And so the search with fundamentals class starts next Monday, June 6th. You can still sign up. It's $200.
There's a code, DGSearch 10. And then search with machine learning is two weeks after that. And that's a four week class. Both are project intensive.
Every week, you're going to do a project, you're going to write code, you're going to interact with students, you're going to hear lectures, so on, so forth.
In many ways, I think it's modeled after a university style class where you, you know, every week you have homework, every week you have lectures, and so on, so forth. So yeah, please sign up. Yeah, that's awesome.
What I've personally enjoyed during the course, the search with the Mel four weeks course was the atmosphere. The atmosphere that was basically creating itself amongst the students and was over 100 people there on Slack helping each other. That was just amazing.
Somebody saved me like a ton of time by just sharing, you know, a recipe that I followed and quickly went through to some hurdle. And I learned, and I, of course, I knew some stuff.
Yes, I'm an expert in this field, but also you can put your expertise, you know, to a test when you, when you run so fast during the course and the support that you guys provided was amazing. So that's amazing. I've enjoyed this conversation so much.
Now we are moving to the questions from the audience. And I'll pick the, and feel free to ask questions, please. We still have a few minutes. The first question comes from Avynash, who is currently testing the approach of buying coder to find the similar sentence, top 10.
And later passing the top 10 sentence to a crossing coder model to find the most similar sentence in the top 10 using cosine similarity. Yeah, I guess he's asking for advice is this an appropriate method. This is where my expertise just is not. So Avynash, I will apologize.
I do not know enough here to give you advice.
I would probably ask first, like, what is the actual problem? Are you trying to solve? You know, so if you're trying to find similar sentences, then from my understanding of it, that my basic level understanding of what you're describing, it sounds like a reasonable, a reasonable approach.
But there are people who are much in probably Dmitry, you probably could answer this one better than I, but I have not played with or tried out those specific types of capabilities. So I don't have good advice there. I have worked in general on sentence similarity type problems.
It is always challenging. In fact, I have a my current company that I'm one of my fractional clients. We are doing sentence similarity or clause similarity types problems. And I think they are we are using similar modeling techniques, but I'm not doing the day to day modeling on that.
So I'm really just trusting the data scientists on that. Yeah, I can add to this that I happen to have given a community talk during the search with the mail course. And there I actually go explicitly into this by encoder and cross-ent coder.
So only one thing is that cross-ent coder is much more computationally intensive. And so you don't want to run it on a huge amount of sentences. And it looks like that's what you're doing. So that sounds sensible to me. I think I would pay more attention to testing your approach.
So make sure to reserve some part of your data set to test it. Careful. Yeah, this is the cool thing for me coming back in from Wikiland is I'm learning so much now too.
Like this is I've been digging my way through a lot of these things, but as you can see, this is why it's the gold age because there's so many approaches and they're often improving state of the art every week, right? Yeah, exactly. A lot of things is happening.
Another question I'm taking now from the chat, Carlos is asking, I'd like to know Grandsepinion inside about learning to boost. He gives also a link to a presentation at a high-stack high-stack conference. I don't know if you're familiar with this approach, Grant. Can you say anything? I am not.
I'd like to know learning to boost interesting. Another thing to go learn. Yeah, I think it was all kind of learning to rank. I think it's related, but I actually don't know myself like that much in detail, but that that presentation was great.
It looked like new thing, but at the same time kind of familiar. Basically, instead of learning to rank, you learn the boost values as far as I remember. It sounds interesting and reasonable.
Again, at the end of the day, how do we shape these vectors? I know that's a generic wave in your hands, but I would take this and go try it. I think most of these machine learning systems you're trying to learn weights that then shape the way that vector gets called.
If it works on your domain and it's fast enough and you can maintain it, then go for it. You don't need some experts blessing on it. It certainly sounds interesting. LTR certainly has its own challenges in terms of tweaking and tuning. I know I've struggled with that with LTR a lot.
I know I've struggled with hand-tuned boost a lot as well, so anything that helps do that I think would be good. Yeah, awesome. The next question comes from Nico, Hey, Nico, a former colleague from AlphaSense.
If you're hosting an information search engine which should catch new topics like COVID when it hit, how do you notice that your boosting model of vector embedding model does not recognize queries related to these new topics proactively?
Yeah, that's where I think the instrumentation of your system comes in, right? And the human and the loop on that instrumentation in the system, right?
I mean, I think nobody talks about it, but even at the really large successful search engines, there's still people who are reviewing where things are working and not working, right? And generally they're doing it at the experimentation level, but people still dig into queries.
What queries are underperforming? What documents are underperforming? I think there's tools, there's a lot of good tools out there for anomaly detection as well.
So recognizing when new queries are coming in is something like anomaly detection algorithms will work with, right?
You know, looking at your top queries, your trending queries, and then again, looking at those results, there are machine learning approaches to automatically identifying and alerting on those kinds of things, again, along the anomaly detection line.
But at the end of the day, you can always do that with people as well, right? And that's where humans maybe are better at still at recognizing some of those things. Yeah, and I think you also alluded to this somewhat.
I mean, this question is to me, it's like chicken-eyed problem, right? So if a new topic arises in the queries and also in the documents, but I haven't handled it yet before prior to this, then what can I do live?
So I think you said that try to measure things like if some top ranking documents are not clicked, then that's probably a signal of something is smoky there.
Go check it out. Another thing that I think I could recommend, maybe from my side, is you could try to cluster your queries actually.
And sometimes the funny thing is that queries are related in some way, right?
So like if it's a completely new cluster and usually dense retrieval helps a lot there, pre-trained models on your domain or maybe on some generic domain like news, they might still pick these things up and put them in the same basket, then ask some human annotators to go and check.
Instead of checking the whole multimillion log, you know, which would be super, super complicated. And you know, I agree.
And the nice thing about like, you know, especially, you know, these engines, you know, there is still the good old BM25 case where like at least the basic level keywords are going to match.
And so if a new term comes in for COVID and like it's in the documents, you'll at least probably get an exact match. You may not deal with the fuzzy matches all that well, but you know, like something's better than nothing. And then that allows you to start to iterate on it. Yeah, exactly.
So the next question from Q&A panel is from Chris for the search with ML course, which front-end framework are most students using for their projects? Front-end framework feels a little open-ended to me, but I mean, I can tell.
So one of the things we're doing in both classes is we try to work with a real data set-end with a real search application. For better or for worse, we chose not to use notebooks. Notebooks are great for a lot of things, but I don't know that they always show you how actual applications work.
So we actually build out a really simple application. The front-end is like tailwinds, CSS, and really simple flask serving layer for the APIs. And then we use open search for the search engine and things like fast text and a few other things for ML side of it.
You know, we use the learning to rank plugin for open search, trying to think if there's anything else in our stack. It's primarily Python, but I think if you were a Java user or any of the other languages where there's clients for open search, you would do just fine in the class.
You maybe just won't be able to use all of the Python capabilities that we have in the class. I hope that answers your question, Chris. The repositories are all at least the base level repositories are all available under my GitHub.
So you can just go to my GitHub, which excuse me is GSING, ERS, and put that in the chat. And then you can see the frameworks we use. Yeah, awesome.
And I can just, you know, you can pick these things up or you can, if you know Python, it's probably easy for you, but if you don't, you can pick this up. And the next question is from the chat from quasi, I hope you pronounce your name correctly.
As these days, most of these sort of approaches are based on transformers for anyone who wants to try out IR approach using transformers as a pet project. Does grant have any recommendations in terms of cloud services tools? I don't have any specific recommendations.
I know I've looked at there's several players. I was so for instance, I saw somebody in one of the IR communities that I was in with posted around, I think I don't know how he's pronounced about quadrant, I think QDR and T.
I know there's UVA, I know there's pine cone, elastic, solar, and open search all have dense vector retrieval capabilities. I've been playing around just getting started with hugging face. I'm a little late to the hugging face game when it comes to these things.
I know a lot of people I talk to use colab to build and run these systems. And so I think you can probably get started. Again, like Demetri, you may have better tutorials. I know you've posted a bunch of stuff on medium amount, how to get started in this as have other people.
So I would start there, I guess, any one of those you probably won't do wrong with. And then for me, I always go back to, like I like to take a data set that I'm familiar with first rather than a technology that I'm unfamiliar with.
Whenever I'm learning something new, I start with something I'm familiar with and then try to apply that thing to the new technology as opposed to picking the technology first and then trying to, you know, kind of go back and forth between the tutorials that they provide.
But I always like to go back to a domain I'm familiar with because then I don't have to rebuild my intuition. Right. So for instance, I've never really done image search, but I've done e-commerce search all the time.
So it makes way more sense for me to try out transformers with e-commerce than it does with images just because I don't know the core intuition as much on the images as I do for e-commerce. So I would probably start that way first. Yeah, I agree.
And another thing, yeah, of course, Grant, you thank you, you mentioned, you know, my medium blog post, there are a lot more people blogging on this, but I have a specific collection on medium, 37 minutes by sheer reading time.
You can go through like basics like exact can and search all the way up to, you know, neural retrieval, which is approximate nearest neighbor search because you cannot do exact can and search at scale. It will just not not scale.
So you have to kind of go and cut some corners, so to say, but actually in a more mathematical sense, you create this algorithms that are beautifully handling this complexity for you. So go check it out.
I think the next and probably last question, but not least, is from a shish, is the search with a mail course right to step into if I'm looking to learn about semantic search and add the functionality to SQL or no SQL databases? That's an interesting question.
I guess I haven't thought about it in that sense.
I mean, I think, you know, I think a lot of the techniques we use in the ML class relate to semantic search and relate to like how can we get better relevance out of the engine? So semantic search being one of those types of capabilities, a kind of semantic search often is a pretty loaded phrase.
So depending on what you're trying to do there, as you should your mileage may vary. But we certainly cover things like classifying your content, classifying your queries. We do learning to rank.
We talk about synonym expansion query, you know, smarter queries, better filters, all of those kinds of things, I think fall can be loosely coupled into semantic search.
If you're talking more like you want to do like graph-based inferences or, you know, using things like wiki data or dbpedia or those kinds of things to infer relationships and do semantic search that way. We don't really cover those as much.
We do base off of open search, but I think the concepts apply in general. With the SQL and no SQL databases, like I know a lot of them have kind of baseline search functionality in them.
And so you would be able to apply some of the principles because a lot of the principles we do, you actually do either before indexing or before querying.
So those would certainly apply, you know, because at the end of the day, you're just using those things to then generate a better query or a better document to be stored in your engine. And so I don't see your reason why they went work in a no-SQL store or a SQL store.
It's just then how do you translate that into your query language, right? But we do use open search. All the examples are open search. You would have to do the work to leap to that, whatever it is your engine is doing. Yeah, absolutely. And the good thing is that open search does have a K&N plugin.
They call it K&N plugin, but it's actually approximate nearest neighbor search. And so it's off heap for those who care. So it's not inside Java, but it still allows you to get a feel of how neural search will influence your results at.
And you can also, you know, mix and match, sort of using more traditional VM25 with this. Awesome. This was the last question. Thanks so much to everyone who asked their questions live. And, you know, consider joining the course if you haven't yet.
And, Grant, thanks so much for this session and for answering the question and sharing your wisdom. I've enjoyed this conversation very much. Thank you. Thanks so much for having me, Dmitry, and keep up the great work. I love the podcast.
And it's awesome to see a search dedicated podcast out there. So congrats and good luck with that. Thank you so much. All right. Bye-bye. Bye, folks. Thanks, Dmitry. Thanks, Grant. All right. Dmitry. Right, now. Two. Two. Until eight. One. Four.
Okay, let's get back to what you want and now we can direct that back to the millions. Okay.