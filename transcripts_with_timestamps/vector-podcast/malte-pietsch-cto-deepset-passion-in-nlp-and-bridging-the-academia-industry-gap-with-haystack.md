---
description: '<p>YouTube: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=N5Brb7Rzc2c">https://www.youtube.com/watch?v=N5Brb7Rzc2c</a></p><p>Topics:</p><p>00:00
  Introduction</p><p>01:12 Malte’s background</p><p>07:58 NLP crossing paths with
  Search</p><p>11:20 Product discovery: early stage repetitive use cases pre-dating
  Haystack</p><p>16:25 Acyclic directed graph for modeling a complex search pipeline</p><p>18:22
  Early integrations with Vector Databases</p><p>20:09 Aha!-use case in Haystack</p><p>23:23
  Capabilities of Haystack today</p><p>30:11 Deepset Cloud: end-to-end deployment,
  experiment tracking, observability, evaluation, debugging and communicating with
  stakeholders</p><p>39:00 Examples of value for the end-users of Deepset Cloud</p><p>46:00
  Success metrics</p><p>50:35 Where Haystack is taking us beyond MLOps for search
  experimentation</p><p>57:13 Haystack as a smart assistant to guide experiments</p><p>1:02:49
  Multimodality</p><p>1:05:53 Future of the Vector Search / NLP field: large language
  models</p><p>1:15:13 Incorporating knowledge into Language Models &amp; an Open
  NLP Meetup on this topic</p><p>1:16:25 The magical question of WHY</p><p>1:23:47
  Announcements from Malte</p><p>Show notes:</p><p>- Haystack: <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://github.com/deepset-ai/haystack/">https://github.com/deepset-ai/haystack/</a></p><p>-
  Deepset Cloud: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.deepset.ai/deepset-cloud">https://www.deepset.ai/deepset-cloud</a></p><p>-
  Tutorial: Build Your First QA System: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://haystack.deepset.ai/tutorials/v0.5.0/first-qa-system">https://haystack.deepset.ai/tutorials/v0.5.0/first-qa-system</a></p><p>-
  Open NLP Meetup on Sep 29th (Nils Reimers talking about “Incorporating New Knowledge
  Into LMs”): <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.meetup.com/open-nlp-meetup/events/287159377/">https://www.meetup.com/open-nlp-meetup/events/287159377/</a></p><p>-
  Atlas Paper (Few shot learning with retrieval augmented large language models):
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://arxiv.org/abs/2208.03299">https://arxiv.org/abs/2208.03299</a></p><p>-
  Zero click search: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.searchmetrics.com/glossary/zero-click-searches/">https://www.searchmetrics.com/glossary/zero-click-searches/</a></p><p>Very
  large LMs:</p><p>- 540B PaLM by Google: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://lnkd.in/eajsjCMr">https://lnkd.in/eajsjCMr</a></p><p>- 11B
  Atlas by Meta: <a target="_blank" rel="noopener noreferrer nofollow" href="https://lnkd.in/eENzNkrG">https://lnkd.in/eENzNkrG</a></p><p>-
  20B AlexaTM by Amazon: <a target="_blank" rel="noopener noreferrer nofollow" href="https://lnkd.in/eyBaZDTy">https://lnkd.in/eyBaZDTy</a></p><p>-
  Players in Vector Search: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=8IOpgmXf5r8">https://www.youtube.com/watch?v=8IOpgmXf5r8</a>
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://dmitry-kan.medium.com/players-in-vector-search-video-2fd390d00d6">https://dmitry-kan.medium.com/players-in-vector-search-video-2fd390d00d6</a></p><p>-
  Click Residual: A Query Success Metric: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://observer.wunderwood.org/2022/08/08/click-residual-a-query-success-metric/">https://observer.wunderwood.org/2022/08/08/click-residual-a-query-success-metric/</a></p><p>-
  Tutorials and papers around incorporating Knowledge into Language Models: <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://cs.stanford.edu/people/cgzhu/">https://cs.stanford.edu/people/cgzhu/</a></p>'
image_url: https://media.rss.com/vector-podcast/20220830_070827_46ba9c40226c9b5c8e39886c99b0aea3.jpg
pub_date: Tue, 30 Aug 2022 07:27:26 GMT
title: Malte Pietsch - CTO, Deepset - Passion in NLP and bridging the academia-industry
  gap with Haystack
url: https://rss.com/podcasts/vector-podcast/599924
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 29.0, "text": " Hello
  there, Vector Podcast. Season 2, we are relaunching after summer and it was a little",
  "tokens": [50364, 2425, 456, 11, 691, 20814, 29972, 13, 16465, 568, 11, 321, 366,
  5195, 46079, 934, 4266, 293, 309, 390, 257, 707, 51814], "temperature": 0.0, "avg_logprob":
  -0.43664016723632815, "compression_ratio": 1.0588235294117647, "no_speech_prob":
  0.09692271053791046}, {"id": 1, "seek": 2900, "start": 29.0, "end": 33.08, "text":
  " bit of break last episode was from Berlin buzzwords and today,", "tokens": [50364,
  857, 295, 1821, 1036, 3500, 390, 490, 13848, 13036, 13832, 293, 965, 11, 50568],
  "temperature": 0.0, "avg_logprob": -0.31167654389316596, "compression_ratio": 1.5369649805447472,
  "no_speech_prob": 0.5647331476211548}, {"id": 2, "seek": 2900, "start": 33.08, "end":
  40.96, "text": " coincidentally, we have a guest from Berlin, multi-peach, a studio
  of deep set, the company", "tokens": [50568, 13001, 36578, 11, 321, 362, 257, 8341,
  490, 13848, 11, 4825, 12, 494, 608, 11, 257, 6811, 295, 2452, 992, 11, 264, 2237,
  50962], "temperature": 0.0, "avg_logprob": -0.31167654389316596, "compression_ratio":
  1.5369649805447472, "no_speech_prob": 0.5647331476211548}, {"id": 3, "seek": 2900,
  "start": 40.96, "end": 46.400000000000006, "text": " behind Haystack. So we''re
  going to be diving into what I call a neural framework, but I wonder", "tokens":
  [50962, 2261, 8721, 372, 501, 13, 407, 321, 434, 516, 281, 312, 20241, 666, 437,
  286, 818, 257, 18161, 8388, 11, 457, 286, 2441, 51234], "temperature": 0.0, "avg_logprob":
  -0.31167654389316596, "compression_ratio": 1.5369649805447472, "no_speech_prob":
  0.5647331476211548}, {"id": 4, "seek": 2900, "start": 46.400000000000006, "end":
  54.0, "text": " if Malta would give a different picture there, but still very interested
  to learn and dive", "tokens": [51234, 498, 5746, 1328, 576, 976, 257, 819, 3036,
  456, 11, 457, 920, 588, 3102, 281, 1466, 293, 9192, 51614], "temperature": 0.0,
  "avg_logprob": -0.31167654389316596, "compression_ratio": 1.5369649805447472, "no_speech_prob":
  0.5647331476211548}, {"id": 5, "seek": 2900, "start": 54.0, "end": 56.8, "text":
  " into multiple topics there. Hey, Malta, how you doing?", "tokens": [51614, 666,
  3866, 8378, 456, 13, 1911, 11, 5746, 1328, 11, 577, 291, 884, 30, 51754], "temperature":
  0.0, "avg_logprob": -0.31167654389316596, "compression_ratio": 1.5369649805447472,
  "no_speech_prob": 0.5647331476211548}, {"id": 6, "seek": 5680, "start": 57.8, "end":
  60.8, "text": " I''m good doing great. Thanks for having me today. How are you doing?",
  "tokens": [50414, 286, 478, 665, 884, 869, 13, 2561, 337, 1419, 385, 965, 13, 1012,
  366, 291, 884, 30, 50564], "temperature": 0.0, "avg_logprob": -0.20092537379500888,
  "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.041570864617824554},
  {"id": 7, "seek": 5680, "start": 60.8, "end": 66.8, "text": " I''m good. I''m great.
  It''s still summer. It''s super hot as we were exchanging before the recording.",
  "tokens": [50564, 286, 478, 665, 13, 286, 478, 869, 13, 467, 311, 920, 4266, 13,
  467, 311, 1687, 2368, 382, 321, 645, 6210, 9741, 949, 264, 6613, 13, 50864], "temperature":
  0.0, "avg_logprob": -0.20092537379500888, "compression_ratio": 1.6444444444444444,
  "no_speech_prob": 0.041570864617824554}, {"id": 8, "seek": 5680, "start": 66.8,
  "end": 74.8, "text": " It''s super, super hot, but I like it. So yeah, I think before
  we dive into what is Haystack,", "tokens": [50864, 467, 311, 1687, 11, 1687, 2368,
  11, 457, 286, 411, 309, 13, 407, 1338, 11, 286, 519, 949, 321, 9192, 666, 437, 307,
  8721, 372, 501, 11, 51264], "temperature": 0.0, "avg_logprob": -0.20092537379500888,
  "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.041570864617824554},
  {"id": 9, "seek": 5680, "start": 74.8, "end": 83.8, "text": " I really like to learn
  about yourself and what is your background and how did you find yourself in this
  space", "tokens": [51264, 286, 534, 411, 281, 1466, 466, 1803, 293, 437, 307, 428,
  3678, 293, 577, 630, 291, 915, 1803, 294, 341, 1901, 51714], "temperature": 0.0,
  "avg_logprob": -0.20092537379500888, "compression_ratio": 1.6444444444444444, "no_speech_prob":
  0.041570864617824554}, {"id": 10, "seek": 8380, "start": 83.8, "end": 89.8, "text":
  " of what we call Vector Search? I wonder if you describe it differently, but I
  call it Vector Search,", "tokens": [50364, 295, 437, 321, 818, 691, 20814, 17180,
  30, 286, 2441, 498, 291, 6786, 309, 7614, 11, 457, 286, 818, 309, 691, 20814, 17180,
  11, 50664], "temperature": 0.0, "avg_logprob": -0.18857783856599228, "compression_ratio":
  1.5622119815668203, "no_speech_prob": 0.14885388314723969}, {"id": 11, "seek": 8380,
  "start": 89.8, "end": 93.8, "text": " Vector Search players. So can you tell a bit
  about that?", "tokens": [50664, 691, 20814, 17180, 4150, 13, 407, 393, 291, 980,
  257, 857, 466, 300, 30, 50864], "temperature": 0.0, "avg_logprob": -0.18857783856599228,
  "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.14885388314723969},
  {"id": 12, "seek": 8380, "start": 93.8, "end": 100.8, "text": " Yeah, I''m sure
  I''m happy. So I would say my background is mostly in NLP engineering,", "tokens":
  [50864, 865, 11, 286, 478, 988, 286, 478, 2055, 13, 407, 286, 576, 584, 452, 3678,
  307, 5240, 294, 426, 45196, 7043, 11, 51214], "temperature": 0.0, "avg_logprob":
  -0.18857783856599228, "compression_ratio": 1.5622119815668203, "no_speech_prob":
  0.14885388314723969}, {"id": 13, "seek": 8380, "start": 100.8, "end": 107.8, "text":
  " what I would call probably these days. And during my studies, I basically had
  no clue about NLP.", "tokens": [51214, 437, 286, 576, 818, 1391, 613, 1708, 13,
  400, 1830, 452, 5313, 11, 286, 1936, 632, 572, 13602, 466, 426, 45196, 13, 51564],
  "temperature": 0.0, "avg_logprob": -0.18857783856599228, "compression_ratio": 1.5622119815668203,
  "no_speech_prob": 0.14885388314723969}, {"id": 14, "seek": 10780, "start": 108.8,
  "end": 115.8, "text": " I think it wasn''t really any part of our coursework or
  something really a thing.", "tokens": [50414, 286, 519, 309, 2067, 380, 534, 604,
  644, 295, 527, 1164, 1902, 420, 746, 534, 257, 551, 13, 50764], "temperature": 0.0,
  "avg_logprob": -0.19991210569818335, "compression_ratio": 1.6055045871559632, "no_speech_prob":
  0.04129105433821678}, {"id": 15, "seek": 10780, "start": 115.8, "end": 122.8, "text":
  " And for me, all then started basically after my studies, went to the research
  project in the US,", "tokens": [50764, 400, 337, 385, 11, 439, 550, 1409, 1936,
  934, 452, 5313, 11, 1437, 281, 264, 2132, 1716, 294, 264, 2546, 11, 51114], "temperature":
  0.0, "avg_logprob": -0.19991210569818335, "compression_ratio": 1.6055045871559632,
  "no_speech_prob": 0.04129105433821678}, {"id": 16, "seek": 10780, "start": 122.8,
  "end": 126.8, "text": " which was at the intersection of machine learning and healthcare.",
  "tokens": [51114, 597, 390, 412, 264, 15236, 295, 3479, 2539, 293, 8884, 13, 51314],
  "temperature": 0.0, "avg_logprob": -0.19991210569818335, "compression_ratio": 1.6055045871559632,
  "no_speech_prob": 0.04129105433821678}, {"id": 17, "seek": 10780, "start": 126.8,
  "end": 134.8, "text": " And the big, big focus there was on numerical data. So we
  were basically trying to find signals, patterns,", "tokens": [51314, 400, 264, 955,
  11, 955, 1879, 456, 390, 322, 29054, 1412, 13, 407, 321, 645, 1936, 1382, 281, 915,
  12354, 11, 8294, 11, 51714], "temperature": 0.0, "avg_logprob": -0.19991210569818335,
  "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.04129105433821678},
  {"id": 18, "seek": 13480, "start": 135.8, "end": 141.8, "text": " and laboratory
  measurements for kidney disease patients to predict some kind of risks.", "tokens":
  [50414, 293, 16523, 15383, 337, 19000, 4752, 4209, 281, 6069, 512, 733, 295, 10888,
  13, 50714], "temperature": 0.0, "avg_logprob": -0.16551578367078626, "compression_ratio":
  1.6122448979591837, "no_speech_prob": 0.007430925965309143}, {"id": 19, "seek":
  13480, "start": 141.8, "end": 149.8, "text": " And there was all the kind of numerical
  data. And NLP wasn''t really really scope of that project,", "tokens": [50714, 400,
  456, 390, 439, 264, 733, 295, 29054, 1412, 13, 400, 426, 45196, 2067, 380, 534,
  534, 11923, 295, 300, 1716, 11, 51114], "temperature": 0.0, "avg_logprob": -0.16551578367078626,
  "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.007430925965309143},
  {"id": 20, "seek": 13480, "start": 149.8, "end": 160.8, "text": " but there was
  for me, that basically one kind of event that made me then get in touch with NLP
  and eventually fell at fall in love.", "tokens": [51114, 457, 456, 390, 337, 385,
  11, 300, 1936, 472, 733, 295, 2280, 300, 1027, 385, 550, 483, 294, 2557, 365, 426,
  45196, 293, 4728, 5696, 412, 2100, 294, 959, 13, 51664], "temperature": 0.0, "avg_logprob":
  -0.16551578367078626, "compression_ratio": 1.6122448979591837, "no_speech_prob":
  0.007430925965309143}, {"id": 21, "seek": 16080, "start": 161.8, "end": 167.8, "text":
  " And it was really in this project, we tried to predict a lot of these risk factors
  through a lot of,", "tokens": [50414, 400, 309, 390, 534, 294, 341, 1716, 11, 321,
  3031, 281, 6069, 257, 688, 295, 613, 3148, 6771, 807, 257, 688, 295, 11, 50714],
  "temperature": 0.0, "avg_logprob": -0.18421941333346897, "compression_ratio": 1.6470588235294117,
  "no_speech_prob": 0.0091433459892869}, {"id": 22, "seek": 16080, "start": 167.8,
  "end": 172.8, "text": " I would say, quite fancy modeling to get some good signals.",
  "tokens": [50714, 286, 576, 584, 11, 1596, 10247, 15983, 281, 483, 512, 665, 12354,
  13, 50964], "temperature": 0.0, "avg_logprob": -0.18421941333346897, "compression_ratio":
  1.6470588235294117, "no_speech_prob": 0.0091433459892869}, {"id": 23, "seek": 16080,
  "start": 172.8, "end": 183.8, "text": " And at the end, it kind of worked. We were
  able to predict some risks, but when we then talked to doctors and showed them these
  results or asked for their feedback,", "tokens": [50964, 400, 412, 264, 917, 11,
  309, 733, 295, 2732, 13, 492, 645, 1075, 281, 6069, 512, 10888, 11, 457, 562, 321,
  550, 2825, 281, 8778, 293, 4712, 552, 613, 3542, 420, 2351, 337, 641, 5824, 11,
  51514], "temperature": 0.0, "avg_logprob": -0.18421941333346897, "compression_ratio":
  1.6470588235294117, "no_speech_prob": 0.0091433459892869}, {"id": 24, "seek": 16080,
  "start": 183.8, "end": 189.8, "text": " they said, yeah, yeah, that''s all correct.
  Yeah, but it''s not really new. We knew that before.", "tokens": [51514, 436, 848,
  11, 1338, 11, 1338, 11, 300, 311, 439, 3006, 13, 865, 11, 457, 309, 311, 406, 534,
  777, 13, 492, 2586, 300, 949, 13, 51814], "temperature": 0.0, "avg_logprob": -0.18421941333346897,
  "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0091433459892869},
  {"id": 25, "seek": 18980, "start": 190.8, "end": 199.8, "text": " But this part
  here, this is like, this is an interesting one. And this is what we do there. And
  that was basically the only small part where we,", "tokens": [50414, 583, 341, 644,
  510, 11, 341, 307, 411, 11, 341, 307, 364, 1880, 472, 13, 400, 341, 307, 437, 321,
  360, 456, 13, 400, 300, 390, 1936, 264, 787, 1359, 644, 689, 321, 11, 50864], "temperature":
  0.0, "avg_logprob": -0.20698158187095564, "compression_ratio": 1.8256880733944953,
  "no_speech_prob": 0.0024313770700246096}, {"id": 26, "seek": 18980, "start": 199.8,
  "end": 210.8, "text": " where we looked at written notes of of doctors during treatments.
  And from a modeling perspective, that was really, I would say, nothing fancy, nothing
  advanced,", "tokens": [50864, 689, 321, 2956, 412, 3720, 5570, 295, 295, 8778, 1830,
  15795, 13, 400, 490, 257, 15983, 4585, 11, 300, 390, 534, 11, 286, 576, 584, 11,
  1825, 10247, 11, 1825, 7339, 11, 51414], "temperature": 0.0, "avg_logprob": -0.20698158187095564,
  "compression_ratio": 1.8256880733944953, "no_speech_prob": 0.0024313770700246096},
  {"id": 27, "seek": 18980, "start": 210.8, "end": 215.8, "text": " nothing where
  we spend a lot of time. But at the end, it was the point, I think, where the,",
  "tokens": [51414, 1825, 689, 321, 3496, 257, 688, 295, 565, 13, 583, 412, 264, 917,
  11, 309, 390, 264, 935, 11, 286, 519, 11, 689, 264, 11, 51664], "temperature": 0.0,
  "avg_logprob": -0.20698158187095564, "compression_ratio": 1.8256880733944953, "no_speech_prob":
  0.0024313770700246096}, {"id": 28, "seek": 21580, "start": 216.8, "end": 223.8,
  "text": " the doctor''s physician saw the biggest value. And that kind of got me
  to think again, thought, okay, well, like,", "tokens": [50414, 264, 4631, 311, 16456,
  1866, 264, 3880, 2158, 13, 400, 300, 733, 295, 658, 385, 281, 519, 797, 11, 1194,
  11, 1392, 11, 731, 11, 411, 11, 50764], "temperature": 0.0, "avg_logprob": -0.2544753991284417,
  "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.016170455142855644},
  {"id": 29, "seek": 21580, "start": 224.8, "end": 231.8, "text": " just this kind
  of data source, it was something they couldn''t really access before. And now with
  this, like, very simple,", "tokens": [50814, 445, 341, 733, 295, 1412, 4009, 11,
  309, 390, 746, 436, 2809, 380, 534, 2105, 949, 13, 400, 586, 365, 341, 11, 411,
  11, 588, 2199, 11, 51164], "temperature": 0.0, "avg_logprob": -0.2544753991284417,
  "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.016170455142855644},
  {"id": 30, "seek": 21580, "start": 231.8, "end": 241.8, "text": " native methods,
  they somehow saw a value, a new thing. And that''s basically where I thought, oh,
  what, it''s cool. What can you actually then do with more advanced methods of,",
  "tokens": [51164, 8470, 7150, 11, 436, 6063, 1866, 257, 2158, 11, 257, 777, 551,
  13, 400, 300, 311, 1936, 689, 286, 1194, 11, 1954, 11, 437, 11, 309, 311, 1627,
  13, 708, 393, 291, 767, 550, 360, 365, 544, 7339, 7150, 295, 11, 51664], "temperature":
  0.0, "avg_logprob": -0.2544753991284417, "compression_ratio": 1.6491935483870968,
  "no_speech_prob": 0.016170455142855644}, {"id": 31, "seek": 24180, "start": 242.8,
  "end": 253.8, "text": " if you have more fancy models, how can you make this kind
  of unused data source than accessible. And yeah, basically, realizing this, the
  power of it.", "tokens": [50414, 498, 291, 362, 544, 10247, 5245, 11, 577, 393,
  291, 652, 341, 733, 295, 44383, 1412, 4009, 813, 9515, 13, 400, 1338, 11, 1936,
  11, 16734, 341, 11, 264, 1347, 295, 309, 13, 50964], "temperature": 0.0, "avg_logprob":
  -0.28173087193415713, "compression_ratio": 1.6431924882629108, "no_speech_prob":
  0.002493217820301652}, {"id": 32, "seek": 24180, "start": 254.8, "end": 268.8, "text":
  " And that''s basically when it then started digging deeper, working more on energy,
  at some point, then set left research, because I was really interested in seeing
  these models working the real world.", "tokens": [51014, 400, 300, 311, 1936, 562,
  309, 550, 1409, 17343, 7731, 11, 1364, 544, 322, 2281, 11, 412, 512, 935, 11, 550,
  992, 1411, 2132, 11, 570, 286, 390, 534, 3102, 294, 2577, 613, 5245, 1364, 264,
  957, 1002, 13, 51714], "temperature": 0.0, "avg_logprob": -0.28173087193415713,
  "compression_ratio": 1.6431924882629108, "no_speech_prob": 0.002493217820301652},
  {"id": 33, "seek": 26880, "start": 269.8, "end": 288.8, "text": " How do they work
  at scale? How can they really then solve problems every day? And basically, and
  came back to Germany, worked in a couple of startups, always just say, NAP at scale,
  kind of intersection, a lot in online advertisement, recommend our systems.", "tokens":
  [50414, 1012, 360, 436, 589, 412, 4373, 30, 1012, 393, 436, 534, 550, 5039, 2740,
  633, 786, 30, 400, 1936, 11, 293, 1361, 646, 281, 7244, 11, 2732, 294, 257, 1916,
  295, 28041, 11, 1009, 445, 584, 11, 426, 4715, 412, 4373, 11, 733, 295, 15236, 11,
  257, 688, 294, 2950, 31370, 11, 2748, 527, 3652, 13, 51364], "temperature": 0.0,
  "avg_logprob": -0.27561473846435547, "compression_ratio": 1.4277777777777778, "no_speech_prob":
  0.007864031009376049}, {"id": 34, "seek": 28880, "start": 289.8, "end": 308.8, "text":
  " And then eventually four years ago, then we started sort of deep set. And together
  with two colleagues, we found the deep set basically because we saw this big motion
  appeared was kind of piling up.", "tokens": [50414, 400, 550, 4728, 1451, 924, 2057,
  11, 550, 321, 1409, 1333, 295, 2452, 992, 13, 400, 1214, 365, 732, 7734, 11, 321,
  1352, 264, 2452, 992, 1936, 570, 321, 1866, 341, 955, 5394, 8516, 390, 733, 295,
  280, 4883, 493, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2754840638902452,
  "compression_ratio": 1.434782608695652, "no_speech_prob": 0.09630188345909119},
  {"id": 35, "seek": 30880, "start": 308.8, "end": 321.8, "text": " There was a whole
  like still pre transformers, but there were early science, I think, on research
  that, that things are becoming more feasible and super interesting things became
  possible.", "tokens": [50364, 821, 390, 257, 1379, 411, 920, 659, 4088, 433, 11,
  457, 456, 645, 2440, 3497, 11, 286, 519, 11, 322, 2132, 300, 11, 300, 721, 366,
  5617, 544, 26648, 293, 1687, 1880, 721, 3062, 1944, 13, 51014], "temperature": 0.0,
  "avg_logprob": -0.19103850920995077, "compression_ratio": 1.752, "no_speech_prob":
  0.061876144260168076}, {"id": 36, "seek": 30880, "start": 322.8, "end": 337.8, "text":
  " At the same time, we also saw that there''s this big gap, you know, like things
  becoming possible on research side, didn''t really mean people were using it in
  production in the industry. And I think we were at this, this interesting bubble
  back then.", "tokens": [51064, 1711, 264, 912, 565, 11, 321, 611, 1866, 300, 456,
  311, 341, 955, 7417, 11, 291, 458, 11, 411, 721, 5617, 1944, 322, 2132, 1252, 11,
  994, 380, 534, 914, 561, 645, 1228, 309, 294, 4265, 294, 264, 3518, 13, 400, 286,
  519, 321, 645, 412, 341, 11, 341, 1880, 12212, 646, 550, 13, 51814], "temperature":
  0.0, "avg_logprob": -0.19103850920995077, "compression_ratio": 1.752, "no_speech_prob":
  0.061876144260168076}, {"id": 37, "seek": 33780, "start": 337.8, "end": 348.8, "text":
  " We did it, we applied deep learning models at scale, saw how that worked, but
  also saw how much of work it actually is of manual work to get it done.", "tokens":
  [50364, 492, 630, 309, 11, 321, 6456, 2452, 2539, 5245, 412, 4373, 11, 1866, 577,
  300, 2732, 11, 457, 611, 1866, 577, 709, 295, 589, 309, 767, 307, 295, 9688, 589,
  281, 483, 309, 1096, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20719686760959855,
  "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.009239474311470985},
  {"id": 38, "seek": 33780, "start": 348.8, "end": 366.8, "text": " And basically
  up the early days of deep set were mainly around, how can we bridge that gap, how
  can we get latest models from research into production in the industry, what kind
  of product tooling can we do.", "tokens": [50914, 400, 1936, 493, 264, 2440, 1708,
  295, 2452, 992, 645, 8704, 926, 11, 577, 393, 321, 7283, 300, 7417, 11, 577, 393,
  321, 483, 6792, 5245, 490, 2132, 666, 4265, 294, 264, 3518, 11, 437, 733, 295, 1674,
  46593, 393, 321, 360, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20719686760959855,
  "compression_ratio": 1.6682242990654206, "no_speech_prob": 0.009239474311470985},
  {"id": 39, "seek": 36780, "start": 367.8, "end": 372.8, "text": " And can we build
  to make that transition easier.", "tokens": [50364, 400, 393, 321, 1322, 281, 652,
  300, 6034, 3571, 13, 50614], "temperature": 0.0, "avg_logprob": -0.28563130912134205,
  "compression_ratio": 1.477124183006536, "no_speech_prob": 0.0020190493669360876},
  {"id": 40, "seek": 36780, "start": 372.8, "end": 380.8, "text": " Yeah, and that''s
  basically how we, we ended up in the, in the startup world building building out
  deep set.", "tokens": [50614, 865, 11, 293, 300, 311, 1936, 577, 321, 11, 321, 4590,
  493, 294, 264, 11, 294, 264, 18578, 1002, 2390, 2390, 484, 2452, 992, 13, 51014],
  "temperature": 0.0, "avg_logprob": -0.28563130912134205, "compression_ratio": 1.477124183006536,
  "no_speech_prob": 0.0020190493669360876}, {"id": 41, "seek": 36780, "start": 380.8,
  "end": 388.8, "text": " And, yeah, initially, that was really more about we saw
  this problem.", "tokens": [51014, 400, 11, 1338, 11, 9105, 11, 300, 390, 534, 544,
  466, 321, 1866, 341, 1154, 13, 51414], "temperature": 0.0, "avg_logprob": -0.28563130912134205,
  "compression_ratio": 1.477124183006536, "no_speech_prob": 0.0020190493669360876},
  {"id": 42, "seek": 38880, "start": 388.8, "end": 397.8, "text": " We had a couple
  of product hypothesis, but we didn''t, didn''t like say place a bet on directly
  on one of them.", "tokens": [50364, 492, 632, 257, 1916, 295, 1674, 17291, 11, 457,
  321, 994, 380, 11, 994, 380, 411, 584, 1081, 257, 778, 322, 3838, 322, 472, 295,
  552, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18604428840406012, "compression_ratio":
  1.5337423312883436, "no_speech_prob": 0.2281481772661209}, {"id": 43, "seek": 38880,
  "start": 397.8, "end": 407.8, "text": " We rather said, okay, let''s, let''s go
  out there. Let''s really try to understand for one year what are really repetitive
  use cases out there.", "tokens": [50814, 492, 2831, 848, 11, 1392, 11, 718, 311,
  11, 718, 311, 352, 484, 456, 13, 961, 311, 534, 853, 281, 1223, 337, 472, 1064,
  437, 366, 534, 29404, 764, 3331, 484, 456, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.18604428840406012, "compression_ratio": 1.5337423312883436, "no_speech_prob":
  0.2281481772661209}, {"id": 44, "seek": 40780, "start": 407.8, "end": 420.8, "text":
  " What are really the pain points of other enterprise teams that are working in
  that field and then kind of settling on a product and then building it out.", "tokens":
  [50364, 708, 366, 534, 264, 1822, 2793, 295, 661, 14132, 5491, 300, 366, 1364, 294,
  300, 2519, 293, 550, 733, 295, 33841, 322, 257, 1674, 293, 550, 2390, 309, 484,
  13, 51014], "temperature": 0.0, "avg_logprob": -0.17773635570819563, "compression_ratio":
  1.4797297297297298, "no_speech_prob": 0.03821157291531563}, {"id": 45, "seek": 40780,
  "start": 420.8, "end": 426.8, "text": " Yeah, that''s basically after one year,
  how we ended up in search.", "tokens": [51014, 865, 11, 300, 311, 1936, 934, 472,
  1064, 11, 577, 321, 4590, 493, 294, 3164, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.17773635570819563, "compression_ratio": 1.4797297297297298, "no_speech_prob":
  0.03821157291531563}, {"id": 46, "seek": 42680, "start": 426.8, "end": 452.8, "text":
  " And of course, I would say really the one use case, the dominant use case, there
  was present in every company that we worked with and that was really a big say,
  valuable use case, where the push not only came from the developers who wanted to
  do something better, but also actually from the, from the business side where people
  saw big value inside Eric.", "tokens": [50364, 400, 295, 1164, 11, 286, 576, 584,
  534, 264, 472, 764, 1389, 11, 264, 15657, 764, 1389, 11, 456, 390, 1974, 294, 633,
  2237, 300, 321, 2732, 365, 293, 300, 390, 534, 257, 955, 584, 11, 8263, 764, 1389,
  11, 689, 264, 2944, 406, 787, 1361, 490, 264, 8849, 567, 1415, 281, 360, 746, 1101,
  11, 457, 611, 767, 490, 264, 11, 490, 264, 1606, 1252, 689, 561, 1866, 955, 2158,
  1854, 9336, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2226718511336889,
  "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.5127701163291931},
  {"id": 47, "seek": 45280, "start": 452.8, "end": 463.8, "text": " I use Google every
  day, where can''t we have something similar in our product or our internal data
  sets and and that thing was something that got us done really interested.", "tokens":
  [50364, 286, 764, 3329, 633, 786, 11, 689, 393, 380, 321, 362, 746, 2531, 294, 527,
  1674, 420, 527, 6920, 1412, 6352, 293, 293, 300, 551, 390, 746, 300, 658, 505, 1096,
  534, 3102, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21586583734868647,
  "compression_ratio": 1.704, "no_speech_prob": 0.01011658739298582}, {"id": 48, "seek":
  45280, "start": 463.8, "end": 477.8, "text": " And on the same time that on the
  the tech side, basically learning more and more about the pain points, why is it
  actually so difficult for for people in these and these enterprises to build modern
  search systems, what could you actually do to help them.", "tokens": [50914, 400,
  322, 264, 912, 565, 300, 322, 264, 264, 7553, 1252, 11, 1936, 2539, 544, 293, 544,
  466, 264, 1822, 2793, 11, 983, 307, 309, 767, 370, 2252, 337, 337, 561, 294, 613,
  293, 613, 29034, 281, 1322, 4363, 3164, 3652, 11, 437, 727, 291, 767, 360, 281,
  854, 552, 13, 51614], "temperature": 0.0, "avg_logprob": -0.21586583734868647, "compression_ratio":
  1.704, "no_speech_prob": 0.01011658739298582}, {"id": 49, "seek": 47780, "start":
  477.8, "end": 504.8, "text": " Yeah, that''s fascinating. Actually four or five
  years ago, could you have imagined that an L.P. would cross paths with search because
  like in many ways, this bar search, which existed for many, many years before was
  in some sense, I sense it that way in mailing list, let''s say a patch is solar
  mailing list, people were dreaming about applying an L.P. in some way, compared
  to what is happening right now.", "tokens": [50364, 865, 11, 300, 311, 10343, 13,
  5135, 1451, 420, 1732, 924, 2057, 11, 727, 291, 362, 16590, 300, 364, 441, 13, 47,
  13, 576, 3278, 14518, 365, 3164, 570, 411, 294, 867, 2098, 11, 341, 2159, 3164,
  11, 597, 13135, 337, 867, 11, 867, 924, 949, 390, 294, 512, 2020, 11, 286, 2020,
  309, 300, 636, 294, 41612, 1329, 11, 718, 311, 584, 257, 9972, 307, 7936, 41612,
  1329, 11, 561, 645, 21475, 466, 9275, 364, 441, 13, 47, 13, 294, 512, 636, 11, 5347,
  281, 437, 307, 2737, 558, 586, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22746465603510538,
  "compression_ratio": 1.6370967741935485, "no_speech_prob": 0.10398313403129578},
  {"id": 50, "seek": 50480, "start": 504.8, "end": 532.8, "text": " I don''t want
  to downplay those efforts, but I''m saying things like you could embed a post tag,
  part of speech tag on on term level, and then use that during search again, you
  need to run some kind of parser on the query, and then use that payload information
  to filter through let''s say adjectives and verbs or something bad, you know, I
  don''t know if there was any practical application in place, probably there was.",
  "tokens": [50364, 286, 500, 380, 528, 281, 760, 2858, 729, 6484, 11, 457, 286, 478,
  1566, 721, 411, 291, 727, 12240, 257, 2183, 6162, 11, 644, 295, 6218, 6162, 322,
  322, 1433, 1496, 11, 293, 550, 764, 300, 1830, 3164, 797, 11, 291, 643, 281, 1190,
  512, 733, 295, 21156, 260, 322, 264, 14581, 11, 293, 550, 764, 300, 30918, 1589,
  281, 6608, 807, 718, 311, 584, 29378, 1539, 293, 30051, 420, 746, 1578, 11, 291,
  458, 11, 286, 500, 380, 458, 498, 456, 390, 604, 8496, 3861, 294, 1081, 11, 1391,
  456, 390, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10818461781924534, "compression_ratio":
  1.6482213438735178, "no_speech_prob": 0.043999310582876205}, {"id": 51, "seek":
  53280, "start": 532.8, "end": 551.8, "text": " But again, if you compare that to
  what is happening today, you basically have a vast array of models right in deep
  learning models that can be applied directly to search using vector search approach,
  could you have imagined this happening when you when you were about to start the
  company.", "tokens": [50364, 583, 797, 11, 498, 291, 6794, 300, 281, 437, 307, 2737,
  965, 11, 291, 1936, 362, 257, 8369, 10225, 295, 5245, 558, 294, 2452, 2539, 5245,
  300, 393, 312, 6456, 3838, 281, 3164, 1228, 8062, 3164, 3109, 11, 727, 291, 362,
  16590, 341, 2737, 562, 291, 562, 291, 645, 466, 281, 722, 264, 2237, 13, 51314],
  "temperature": 0.0, "avg_logprob": -0.09641309511863579, "compression_ratio": 1.6235955056179776,
  "no_speech_prob": 0.015384525991976261}, {"id": 52, "seek": 55180, "start": 551.8,
  "end": 579.8, "text": " No, I would say I was I think what we we had big big say
  dreams about N.A.P. and we we were true believers that that things become easier
  and say more feasible in production, but that was more actually under I would say
  transfer learning side and making models to say more easily adoptable to certain
  domains for search, I think that was for us.", "tokens": [50364, 883, 11, 286, 576,
  584, 286, 390, 286, 519, 437, 321, 321, 632, 955, 955, 584, 7505, 466, 426, 13,
  32, 13, 47, 13, 293, 321, 321, 645, 2074, 23125, 300, 300, 721, 1813, 3571, 293,
  584, 544, 26648, 294, 4265, 11, 457, 300, 390, 544, 767, 833, 286, 576, 584, 5003,
  2539, 1252, 293, 1455, 5245, 281, 584, 544, 3612, 6878, 712, 281, 1629, 25514, 337,
  3164, 11, 286, 519, 300, 390, 337, 505, 13, 51764], "temperature": 0.0, "avg_logprob":
  -0.24728968143463134, "compression_ratio": 1.6507177033492824, "no_speech_prob":
  0.1343073695898056}, {"id": 53, "seek": 57980, "start": 579.8, "end": 599.8, "text":
  " And only then on our journey where we kind of realized, oh, like that''s actually
  two interesting different fields kind of connecting over time right and also I felt
  from at least from my perspective, from a community side from the people who worked
  on information retrieval.", "tokens": [50364, 400, 787, 550, 322, 527, 4671, 689,
  321, 733, 295, 5334, 11, 1954, 11, 411, 300, 311, 767, 732, 1880, 819, 7909, 733,
  295, 11015, 670, 565, 558, 293, 611, 286, 2762, 490, 412, 1935, 490, 452, 4585,
  11, 490, 257, 1768, 1252, 490, 264, 561, 567, 2732, 322, 1589, 19817, 3337, 13,
  51364], "temperature": 0.0, "avg_logprob": -0.2910017047012061, "compression_ratio":
  1.4972677595628416, "no_speech_prob": 0.04161360487341881}, {"id": 54, "seek": 59980,
  "start": 599.8, "end": 626.8, "text": " I think for a long time, a big, like a lot
  of skeptic people, I wouldn''t be talking about any key or dance dance retrieval
  for good reason right because I think there was also like a lot of hype around deep
  learning and still what''s a lot of promises that were made like that it will just
  outperform space retrieval out of the box.", "tokens": [50364, 286, 519, 337, 257,
  938, 565, 11, 257, 955, 11, 411, 257, 688, 295, 19128, 299, 561, 11, 286, 2759,
  380, 312, 1417, 466, 604, 2141, 420, 4489, 4489, 19817, 3337, 337, 665, 1778, 558,
  570, 286, 519, 456, 390, 611, 411, 257, 688, 295, 24144, 926, 2452, 2539, 293, 920,
  437, 311, 257, 688, 295, 16403, 300, 645, 1027, 411, 300, 309, 486, 445, 484, 26765,
  1901, 19817, 3337, 484, 295, 264, 2424, 13, 51714], "temperature": 0.0, "avg_logprob":
  -0.2797529969034316, "compression_ratio": 1.6305418719211822, "no_speech_prob":
  0.15161512792110443}, {"id": 55, "seek": 62680, "start": 626.8, "end": 632.8, "text":
  " And then I think many of these promises were not hold for a long time.", "tokens":
  [50364, 400, 550, 286, 519, 867, 295, 613, 16403, 645, 406, 1797, 337, 257, 938,
  565, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1850408471148947, "compression_ratio":
  1.5876288659793814, "no_speech_prob": 0.029741784557700157}, {"id": 56, "seek":
  62680, "start": 632.8, "end": 647.8, "text": " But I think then basically there
  was another phase where I think people realized, oh, actually now it''s kind of
  starting to work and not only just in research and these ivory towers and lab settings
  but actually also in reality at scale.", "tokens": [50664, 583, 286, 519, 550, 1936,
  456, 390, 1071, 5574, 689, 286, 519, 561, 5334, 11, 1954, 11, 767, 586, 309, 311,
  733, 295, 2891, 281, 589, 293, 406, 787, 445, 294, 2132, 293, 613, 49218, 25045,
  293, 2715, 6257, 457, 767, 611, 294, 4103, 412, 4373, 13, 51414], "temperature":
  0.0, "avg_logprob": -0.1850408471148947, "compression_ratio": 1.5876288659793814,
  "no_speech_prob": 0.029741784557700157}, {"id": 57, "seek": 64780, "start": 647.8,
  "end": 669.8, "text": " And I think that was then fast also here, the moment where
  I''ve got really interesting and I think since then just crazy to see how things
  are progressing when thinking about a multi model search or now just was like more
  I say going away from document retrieval to maybe something like question answering
  which we do a lot.", "tokens": [50364, 400, 286, 519, 300, 390, 550, 2370, 611,
  510, 11, 264, 1623, 689, 286, 600, 658, 534, 1880, 293, 286, 519, 1670, 550, 445,
  3219, 281, 536, 577, 721, 366, 36305, 562, 1953, 466, 257, 4825, 2316, 3164, 420,
  586, 445, 390, 411, 544, 286, 584, 516, 1314, 490, 4166, 19817, 3337, 281, 1310,
  746, 411, 1168, 13430, 597, 321, 360, 257, 688, 13, 51464], "temperature": 0.0,
  "avg_logprob": -0.26162394355325136, "compression_ratio": 1.603960396039604, "no_speech_prob":
  0.20492742955684662}, {"id": 58, "seek": 66980, "start": 669.8, "end": 679.8, "text":
  " And really really crazy to see what''s possible these days and I couldn''t have
  imagined that it''s going so fast.", "tokens": [50364, 400, 534, 534, 3219, 281,
  536, 437, 311, 1944, 613, 1708, 293, 286, 2809, 380, 362, 16590, 300, 309, 311,
  516, 370, 2370, 13, 50864], "temperature": 0.0, "avg_logprob": -0.218989186472707,
  "compression_ratio": 1.4805825242718447, "no_speech_prob": 0.12468904256820679},
  {"id": 59, "seek": 66980, "start": 679.8, "end": 683.8, "text": " Yeah, and there
  are a lot of contributors as well, of course.", "tokens": [50864, 865, 11, 293,
  456, 366, 257, 688, 295, 45627, 382, 731, 11, 295, 1164, 13, 51064], "temperature":
  0.0, "avg_logprob": -0.218989186472707, "compression_ratio": 1.4805825242718447,
  "no_speech_prob": 0.12468904256820679}, {"id": 60, "seek": 66980, "start": 683.8,
  "end": 692.8, "text": " I just happened to give a talk about players in vector search.
  I will link it in the show notes, which was just published with C''s.", "tokens":
  [51064, 286, 445, 2011, 281, 976, 257, 751, 466, 4150, 294, 8062, 3164, 13, 286,
  486, 2113, 309, 294, 264, 855, 5570, 11, 597, 390, 445, 6572, 365, 383, 311, 13,
  51514], "temperature": 0.0, "avg_logprob": -0.218989186472707, "compression_ratio":
  1.4805825242718447, "no_speech_prob": 0.12468904256820679}, {"id": 61, "seek": 69280,
  "start": 692.8, "end": 704.8, "text": " London IR meet up, but even that during
  that presentation, I felt like I''m scratching the the tip of the iceberg in some
  sense, I know there is so much happening.", "tokens": [50364, 7042, 16486, 1677,
  493, 11, 457, 754, 300, 1830, 300, 5860, 11, 286, 2762, 411, 286, 478, 29699, 264,
  264, 4125, 295, 264, 38880, 294, 512, 2020, 11, 286, 458, 456, 307, 370, 709, 2737,
  13, 50964], "temperature": 0.0, "avg_logprob": -0.2050486711355356, "compression_ratio":
  1.6798418972332017, "no_speech_prob": 0.31858739256858826}, {"id": 62, "seek": 69280,
  "start": 704.8, "end": 719.8, "text": " And in Heystack, like did you have a vision
  for the product, like you said, you didn''t know what the product will be, but you
  knew sort of the repetitive use cases in a way, right, and also challenges, can
  you share some of the early day challenges that you saw.", "tokens": [50964, 400,
  294, 1911, 372, 501, 11, 411, 630, 291, 362, 257, 5201, 337, 264, 1674, 11, 411,
  291, 848, 11, 291, 994, 380, 458, 437, 264, 1674, 486, 312, 11, 457, 291, 2586,
  1333, 295, 264, 29404, 764, 3331, 294, 257, 636, 11, 558, 11, 293, 611, 4759, 11,
  393, 291, 2073, 512, 295, 264, 2440, 786, 4759, 300, 291, 1866, 13, 51714], "temperature":
  0.0, "avg_logprob": -0.2050486711355356, "compression_ratio": 1.6798418972332017,
  "no_speech_prob": 0.31858739256858826}, {"id": 63, "seek": 71980, "start": 719.8,
  "end": 729.8, "text": " And do you think that they are solved today or are they
  still kind of like in the mix of we need to fix something''s there.", "tokens":
  [50364, 400, 360, 291, 519, 300, 436, 366, 13041, 965, 420, 366, 436, 920, 733,
  295, 411, 294, 264, 2890, 295, 321, 643, 281, 3191, 746, 311, 456, 13, 50864], "temperature":
  0.0, "avg_logprob": -0.16654385113325276, "compression_ratio": 1.5408805031446542,
  "no_speech_prob": 0.09999927133321762}, {"id": 64, "seek": 71980, "start": 729.8,
  "end": 738.8, "text": " So I think that was basically all about this first year
  of deep set, where we did these learnings where wasn''t that clear.", "tokens":
  [50864, 407, 286, 519, 300, 390, 1936, 439, 466, 341, 700, 1064, 295, 2452, 992,
  11, 689, 321, 630, 613, 2539, 82, 689, 2067, 380, 300, 1850, 13, 51314], "temperature":
  0.0, "avg_logprob": -0.16654385113325276, "compression_ratio": 1.5408805031446542,
  "no_speech_prob": 0.09999927133321762}, {"id": 65, "seek": 73880, "start": 738.8,
  "end": 748.8, "text": " But after that year, I think we had a lot of clear insights
  and at least for us, a clear vision also for Heystack, what we want to want to solve
  there.", "tokens": [50364, 583, 934, 300, 1064, 11, 286, 519, 321, 632, 257, 688,
  295, 1850, 14310, 293, 412, 1935, 337, 505, 11, 257, 1850, 5201, 611, 337, 1911,
  372, 501, 11, 437, 321, 528, 281, 528, 281, 5039, 456, 13, 50864], "temperature":
  0.0, "avg_logprob": -0.17051504770914713, "compression_ratio": 1.5625, "no_speech_prob":
  0.021068288013339043}, {"id": 66, "seek": 73880, "start": 748.8, "end": 761.8, "text":
  " And I would say the big challenge, the big problem that we focused on that we
  saw in the industry was having just all these get up technologies and.", "tokens":
  [50864, 400, 286, 576, 584, 264, 955, 3430, 11, 264, 955, 1154, 300, 321, 5178,
  322, 300, 321, 1866, 294, 264, 3518, 390, 1419, 445, 439, 613, 483, 493, 7943, 293,
  13, 51514], "temperature": 0.0, "avg_logprob": -0.17051504770914713, "compression_ratio":
  1.5625, "no_speech_prob": 0.021068288013339043}, {"id": 67, "seek": 76180, "start":
  761.8, "end": 774.8, "text": " And basically Heystack is trying and always as I
  would say as a design philosophy design principle has two things in place that try
  to bring these data technologies together in a meaningful way.", "tokens": [50364,
  400, 1936, 1911, 372, 501, 307, 1382, 293, 1009, 382, 286, 576, 584, 382, 257, 1715,
  10675, 1715, 8665, 575, 732, 721, 294, 1081, 300, 853, 281, 1565, 613, 1412, 7943,
  1214, 294, 257, 10995, 636, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24527852963178587,
  "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.029410675168037415},
  {"id": 68, "seek": 76180, "start": 774.8, "end": 789.8, "text": " And what I mean
  with that is basically if you think about search it''s what say really it''s a lot
  more than then model right and it typically you have factor databases.", "tokens":
  [51014, 400, 437, 286, 914, 365, 300, 307, 1936, 498, 291, 519, 466, 3164, 309,
  311, 437, 584, 534, 309, 311, 257, 688, 544, 813, 550, 2316, 558, 293, 309, 5850,
  291, 362, 5952, 22380, 13, 51764], "temperature": 0.0, "avg_logprob": -0.24527852963178587,
  "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.029410675168037415},
  {"id": 69, "seek": 78980, "start": 789.8, "end": 799.8, "text": " And you may be
  chained together multiple models, you have something you want to do at indexing
  time, you have other things you want to do a query time.", "tokens": [50364, 400,
  291, 815, 312, 417, 3563, 1214, 3866, 5245, 11, 291, 362, 746, 291, 528, 281, 360,
  412, 8186, 278, 565, 11, 291, 362, 661, 721, 291, 528, 281, 360, 257, 14581, 565,
  13, 50864], "temperature": 0.0, "avg_logprob": -0.15708896590442192, "compression_ratio":
  1.685, "no_speech_prob": 0.018163319677114487}, {"id": 70, "seek": 78980, "start":
  799.8, "end": 812.8, "text": " And for each of these say kind of components that
  you need at the end, there are so many different options that you''re that you can
  plug in and often it''s hard to say in the early days.", "tokens": [50864, 400,
  337, 1184, 295, 613, 584, 733, 295, 6677, 300, 291, 643, 412, 264, 917, 11, 456,
  366, 370, 867, 819, 3956, 300, 291, 434, 300, 291, 393, 5452, 294, 293, 2049, 309,
  311, 1152, 281, 584, 294, 264, 2440, 1708, 13, 51514], "temperature": 0.0, "avg_logprob":
  -0.15708896590442192, "compression_ratio": 1.685, "no_speech_prob": 0.018163319677114487},
  {"id": 71, "seek": 81280, "start": 812.8, "end": 829.8, "text": " And then you know,
  do I go for elastic search or something like pine cone electrical database, do I
  go for this model or that model, do I need a, I don''t know, just the retriever
  in my pipeline or do I actually also need to add a re rank or something else.",
  "tokens": [50364, 400, 550, 291, 458, 11, 360, 286, 352, 337, 17115, 3164, 420,
  746, 411, 15113, 19749, 12147, 8149, 11, 360, 286, 352, 337, 341, 2316, 420, 300,
  2316, 11, 360, 286, 643, 257, 11, 286, 500, 380, 458, 11, 445, 264, 19817, 331,
  294, 452, 15517, 420, 360, 286, 767, 611, 643, 281, 909, 257, 319, 6181, 420, 746,
  1646, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3190241300142728, "compression_ratio":
  1.6, "no_speech_prob": 0.08912377059459686}, {"id": 72, "seek": 82980, "start":
  829.8, "end": 835.8, "text": " And we just saw that teams are aware of actually
  spending a lot of time on.", "tokens": [50364, 400, 321, 445, 1866, 300, 5491, 366,
  3650, 295, 767, 6434, 257, 688, 295, 565, 322, 13, 50664], "temperature": 0.0, "avg_logprob":
  -0.3001950127737863, "compression_ratio": 1.0273972602739727, "no_speech_prob":
  0.05606675148010254}, {"id": 73, "seek": 83580, "start": 835.8, "end": 849.8, "text":
  " And then we''re doing these things together manually. And even when they had it
  once there was and constant or maintenance work or iterations where they have to
  exchange one component of the system.", "tokens": [50364, 400, 550, 321, 434, 884,
  613, 721, 1214, 16945, 13, 400, 754, 562, 436, 632, 309, 1564, 456, 390, 293, 5754,
  420, 11258, 589, 420, 36540, 689, 436, 362, 281, 7742, 472, 6542, 295, 264, 1185,
  13, 51064], "temperature": 0.0, "avg_logprob": -0.30508428906637525, "compression_ratio":
  1.6195652173913044, "no_speech_prob": 0.47956162691116333}, {"id": 74, "seek": 83580,
  "start": 849.8, "end": 857.8, "text": " And that was really just slowing them down
  a lot and sometimes even then causing that a project got.", "tokens": [51064, 400,
  300, 390, 534, 445, 26958, 552, 760, 257, 688, 293, 2171, 754, 550, 9853, 300, 257,
  1716, 658, 13, 51464], "temperature": 0.0, "avg_logprob": -0.30508428906637525,
  "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.47956162691116333},
  {"id": 75, "seek": 85780, "start": 857.8, "end": 869.8, "text": " So over time,
  not really ending up in production, but kind of dying at the prototyping stage,
  because it just took so long and and things got kind of sidetracked.", "tokens":
  [50364, 407, 670, 565, 11, 406, 534, 8121, 493, 294, 4265, 11, 457, 733, 295, 8639,
  412, 264, 46219, 3381, 3233, 11, 570, 309, 445, 1890, 370, 938, 293, 293, 721, 658,
  733, 295, 20822, 27965, 25949, 13, 50964], "temperature": 0.0, "avg_logprob": -0.28619562784830727,
  "compression_ratio": 1.5572139303482586, "no_speech_prob": 0.017590856179594994},
  {"id": 76, "seek": 85780, "start": 869.8, "end": 878.8, "text": " And with hastag,
  we basically tried to solve that and having very clear building blocks like, for
  example, the retriever, which very clean their face.", "tokens": [50964, 400, 365,
  6581, 559, 11, 321, 1936, 3031, 281, 5039, 300, 293, 1419, 588, 1850, 2390, 8474,
  411, 11, 337, 1365, 11, 264, 19817, 331, 11, 597, 588, 2541, 641, 1851, 13, 51414],
  "temperature": 0.0, "avg_logprob": -0.28619562784830727, "compression_ratio": 1.5572139303482586,
  "no_speech_prob": 0.017590856179594994}, {"id": 77, "seek": 87880, "start": 878.8,
  "end": 896.8, "text": " And within that you can swap a lot of different technology
  models and the same for a slew vector database document stores where you can very
  easily change between something like elastic search, pine cone, we veate and whatnot.",
  "tokens": [50364, 400, 1951, 300, 291, 393, 18135, 257, 688, 295, 819, 2899, 5245,
  293, 264, 912, 337, 257, 2426, 86, 8062, 8149, 4166, 9512, 689, 291, 393, 588, 3612,
  1319, 1296, 746, 411, 17115, 3164, 11, 15113, 19749, 11, 321, 1241, 473, 293, 25882,
  13, 51264], "temperature": 0.0, "avg_logprob": -0.43700941403706867, "compression_ratio":
  1.4394904458598725, "no_speech_prob": 0.0024109873920679092}, {"id": 78, "seek":
  89680, "start": 896.8, "end": 916.8, "text": " So I would say that''s the was the
  one thing this building blocks and trying to get the focus of developers back on
  making these creative decisions what they actually want to have in their pipeline,
  trying it out with with anti users, rather than just spending time on doing things
  together.", "tokens": [50364, 407, 286, 576, 584, 300, 311, 264, 390, 264, 472,
  551, 341, 2390, 8474, 293, 1382, 281, 483, 264, 1879, 295, 8849, 646, 322, 1455,
  613, 5880, 5327, 437, 436, 767, 528, 281, 362, 294, 641, 15517, 11, 1382, 309, 484,
  365, 365, 6061, 5022, 11, 2831, 813, 445, 6434, 565, 322, 884, 721, 1214, 13, 51364],
  "temperature": 0.0, "avg_logprob": -0.18553767204284669, "compression_ratio": 1.5508021390374331,
  "no_speech_prob": 0.017965903505682945}, {"id": 79, "seek": 91680, "start": 916.8,
  "end": 933.8, "text": " And the second thing is I would say very deep concept also
  in hastag up pipelines. So really what we saw is it''s not just one model. It''s
  typically a couple of steps that you want to have there.", "tokens": [50364, 400,
  264, 1150, 551, 307, 286, 576, 584, 588, 2452, 3410, 611, 294, 6581, 559, 493, 40168,
  13, 407, 534, 437, 321, 1866, 307, 309, 311, 406, 445, 472, 2316, 13, 467, 311,
  5850, 257, 1916, 295, 4439, 300, 291, 528, 281, 362, 456, 13, 51214], "temperature":
  0.0, "avg_logprob": -0.18035824444829202, "compression_ratio": 1.3661971830985915,
  "no_speech_prob": 0.04663967341184616}, {"id": 80, "seek": 93380, "start": 934.8,
  "end": 950.8, "text": " So in hastag we started early on having direct as to click
  graphs where you can have different notes and basically when you have a query or
  indexing time file that kind of hits the pipeline, you can root it for this graph.",
  "tokens": [50414, 407, 294, 6581, 559, 321, 1409, 2440, 322, 1419, 2047, 382, 281,
  2052, 24877, 689, 291, 393, 362, 819, 5570, 293, 1936, 562, 291, 362, 257, 14581,
  420, 8186, 278, 565, 3991, 300, 733, 295, 8664, 264, 15517, 11, 291, 393, 5593,
  309, 337, 341, 4295, 13, 51214], "temperature": 0.0, "avg_logprob": -0.24802230386173024,
  "compression_ratio": 1.4701986754966887, "no_speech_prob": 0.16086244583129883},
  {"id": 81, "seek": 95080, "start": 950.8, "end": 963.8, "text": " That can be very
  easy. There is a set of a query. I do put it to a retriever and I get back my documents
  or can go basically quite complex where you say all like depending on the query
  type.", "tokens": [50364, 663, 393, 312, 588, 1858, 13, 821, 307, 257, 992, 295,
  257, 14581, 13, 286, 360, 829, 309, 281, 257, 19817, 331, 293, 286, 483, 646, 452,
  8512, 420, 393, 352, 1936, 1596, 3997, 689, 291, 584, 439, 411, 5413, 322, 264,
  14581, 2010, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2815162502989477,
  "compression_ratio": 1.3768115942028984, "no_speech_prob": 0.09382446855306625},
  {"id": 82, "seek": 96380, "start": 963.8, "end": 980.8, "text": " If it''s a keyword
  query, I rooted a certain path in my graph, my pipeline, or if it''s a question,
  maybe I go a different way and I have different models, I''m basically involved
  in my in my search request.", "tokens": [50364, 759, 309, 311, 257, 20428, 14581,
  11, 286, 25277, 257, 1629, 3100, 294, 452, 4295, 11, 452, 15517, 11, 420, 498, 309,
  311, 257, 1168, 11, 1310, 286, 352, 257, 819, 636, 293, 286, 362, 819, 5245, 11,
  286, 478, 1936, 3288, 294, 452, 294, 452, 3164, 5308, 13, 51214], "temperature":
  0.0, "avg_logprob": -0.2510858751692862, "compression_ratio": 1.4265734265734267,
  "no_speech_prob": 0.03975219652056694}, {"id": 83, "seek": 98080, "start": 980.8,
  "end": 1005.8, "text": " And these two, I was here, the core principles in hastag
  up. That''s very interesting. So that second thing they are cyclic graph with a
  love for very complex scenarios, right. Like as you explained, we couldn''t principle
  support question answering use case side by side with the kind of like normal search
  with theory, rankers and stuff, right. Is that correct.", "tokens": [50364, 400,
  613, 732, 11, 286, 390, 510, 11, 264, 4965, 9156, 294, 6581, 559, 493, 13, 663,
  311, 588, 1880, 13, 407, 300, 1150, 551, 436, 366, 38154, 1050, 4295, 365, 257,
  959, 337, 588, 3997, 15077, 11, 558, 13, 1743, 382, 291, 8825, 11, 321, 2809, 380,
  8665, 1406, 1168, 13430, 764, 1389, 1252, 538, 1252, 365, 264, 733, 295, 411, 2710,
  3164, 365, 5261, 11, 6181, 433, 293, 1507, 11, 558, 13, 1119, 300, 3006, 13, 51614],
  "temperature": 0.0, "avg_logprob": -0.318989939805938, "compression_ratio": 1.5427350427350428,
  "no_speech_prob": 0.09990677982568741}, {"id": 84, "seek": 100580, "start": 1005.8,
  "end": 1020.8, "text": " Exactly. So that''s what we basically learned from customers
  like when we saw there was a big interest in something like question answering and
  people say, wow, that''s amazing. Can we use that for our website or for our product
  here.", "tokens": [50364, 7587, 13, 407, 300, 311, 437, 321, 1936, 3264, 490, 4581,
  411, 562, 321, 1866, 456, 390, 257, 955, 1179, 294, 746, 411, 1168, 13430, 293,
  561, 584, 11, 6076, 11, 300, 311, 2243, 13, 1664, 321, 764, 300, 337, 527, 3144,
  420, 337, 527, 1674, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18177322240976188,
  "compression_ratio": 1.45, "no_speech_prob": 0.023152818903326988}, {"id": 85, "seek":
  102080, "start": 1021.8, "end": 1033.8, "text": " But doing that switch in a production
  case is quite tough, right. Like if people are used to do keyword queries and they
  know I know I have to enter your keywords to get basically my results.", "tokens":
  [50414, 583, 884, 300, 3679, 294, 257, 4265, 1389, 307, 1596, 4930, 11, 558, 13,
  1743, 498, 561, 366, 1143, 281, 360, 20428, 24109, 293, 436, 458, 286, 458, 286,
  362, 281, 3242, 428, 21009, 281, 483, 1936, 452, 3542, 13, 51014], "temperature":
  0.0, "avg_logprob": -0.2019599776670157, "compression_ratio": 1.6266666666666667,
  "no_speech_prob": 0.2565976679325104}, {"id": 86, "seek": 102080, "start": 1033.8,
  "end": 1045.8, "text": " And then from one day to the other, you switch to more
  semantic queries, maybe more questions or also I think dance retrieval, if you really
  have more sentences that you use.", "tokens": [51014, 400, 550, 490, 472, 786, 281,
  264, 661, 11, 291, 3679, 281, 544, 47982, 24109, 11, 1310, 544, 1651, 420, 611,
  286, 519, 4489, 19817, 3337, 11, 498, 291, 534, 362, 544, 16579, 300, 291, 764,
  13, 51614], "temperature": 0.0, "avg_logprob": -0.2019599776670157, "compression_ratio":
  1.6266666666666667, "no_speech_prob": 0.2565976679325104}, {"id": 87, "seek": 104580,
  "start": 1045.8, "end": 1056.8, "text": " It takes some time for people to adjust
  and we saw that in a couple of scenarios that basically the traffic kind of requests
  that come in.", "tokens": [50364, 467, 2516, 512, 565, 337, 561, 281, 4369, 293,
  321, 1866, 300, 294, 257, 1916, 295, 15077, 300, 1936, 264, 6419, 733, 295, 12475,
  300, 808, 294, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19762579600016275,
  "compression_ratio": 1.5748792270531402, "no_speech_prob": 0.002506805118173361},
  {"id": 88, "seek": 104580, "start": 1056.8, "end": 1069.8, "text": " Start a lot
  with keyword queries and then over time slowly shift towards more semantic queries.
  When people realize, oh, I can actually also ask a question and all this like, like
  Google.", "tokens": [50914, 6481, 257, 688, 365, 20428, 24109, 293, 550, 670, 565,
  5692, 5513, 3030, 544, 47982, 24109, 13, 1133, 561, 4325, 11, 1954, 11, 286, 393,
  767, 611, 1029, 257, 1168, 293, 439, 341, 411, 11, 411, 3329, 13, 51564], "temperature":
  0.0, "avg_logprob": -0.19762579600016275, "compression_ratio": 1.5748792270531402,
  "no_speech_prob": 0.002506805118173361}, {"id": 89, "seek": 106980, "start": 1069.8,
  "end": 1098.8, "text": " And then there''s a trend, but you need everything to have
  an option your system to allow both for certain time and and hasty basically with
  the query classifier where you can initially basically classify is that a question
  or a keyword query or you could go with also semantically like what a topic level
  saying all like this is a query for certain type of category in my my document set.",
  "tokens": [50364, 400, 550, 456, 311, 257, 6028, 11, 457, 291, 643, 1203, 281, 362,
  364, 3614, 428, 1185, 281, 2089, 1293, 337, 1629, 565, 293, 293, 6581, 88, 1936,
  365, 264, 14581, 1508, 9902, 689, 291, 393, 9105, 1936, 33872, 307, 300, 257, 1168,
  420, 257, 20428, 14581, 420, 291, 727, 352, 365, 611, 4361, 49505, 411, 437, 257,
  4829, 1496, 1566, 439, 411, 341, 307, 257, 14581, 337, 1629, 2010, 295, 7719, 294,
  452, 452, 4166, 992, 13, 51814], "temperature": 0.0, "avg_logprob": -0.286276514937238,
  "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.05763981118798256},
  {"id": 90, "seek": 109880, "start": 1098.8, "end": 1101.8, "text": " And then maybe
  do something different.", "tokens": [50364, 400, 550, 1310, 360, 746, 819, 13, 50514],
  "temperature": 0.0, "avg_logprob": -0.2573685091595317, "compression_ratio": 1.7053571428571428,
  "no_speech_prob": 0.013653156347572803}, {"id": 91, "seek": 109880, "start": 1101.8,
  "end": 1110.8, "text": " And like early on Hey stack did it integrate with any database
  per se was it like the last search back then.", "tokens": [50514, 400, 411, 2440,
  322, 1911, 8630, 630, 309, 13365, 365, 604, 8149, 680, 369, 390, 309, 411, 264,
  1036, 3164, 646, 550, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2573685091595317,
  "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.013653156347572803},
  {"id": 92, "seek": 109880, "start": 1110.8, "end": 1117.8, "text": " Yeah, like
  the basically starting point was the last search was the very first document store
  we had.", "tokens": [50964, 865, 11, 411, 264, 1936, 2891, 935, 390, 264, 1036,
  3164, 390, 264, 588, 700, 4166, 3531, 321, 632, 13, 51314], "temperature": 0.0,
  "avg_logprob": -0.2573685091595317, "compression_ratio": 1.7053571428571428, "no_speech_prob":
  0.013653156347572803}, {"id": 93, "seek": 109880, "start": 1117.8, "end": 1125.8,
  "text": " But the last search back then didn''t I believe didn''t support neural
  search right so how did you actually gel these things together.", "tokens": [51314,
  583, 264, 1036, 3164, 646, 550, 994, 380, 286, 1697, 994, 380, 1406, 18161, 3164,
  558, 370, 577, 630, 291, 767, 4087, 613, 721, 1214, 13, 51714], "temperature": 0.0,
  "avg_logprob": -0.2573685091595317, "compression_ratio": 1.7053571428571428, "no_speech_prob":
  0.013653156347572803}, {"id": 94, "seek": 112580, "start": 1125.8, "end": 1129.8,
  "text": " Yeah, that was just that kind of coming in over time right so it was.",
  "tokens": [50364, 865, 11, 300, 390, 445, 300, 733, 295, 1348, 294, 670, 565, 558,
  370, 309, 390, 13, 50564], "temperature": 0.0, "avg_logprob": -0.24707120259602863,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.012001951225101948},
  {"id": 95, "seek": 112580, "start": 1129.8, "end": 1134.8, "text": " Think the the
  era where elastic search was for us was really.", "tokens": [50564, 6557, 264, 264,
  4249, 689, 17115, 3164, 390, 337, 505, 390, 534, 13, 50814], "temperature": 0.0,
  "avg_logprob": -0.24707120259602863, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.012001951225101948}, {"id": 96, "seek": 112580, "start": 1134.8, "end": 1141.8,
  "text": " We came from a question answering use cases a lot and there was really
  like how do we scale that how can we now.", "tokens": [50814, 492, 1361, 490, 257,
  1168, 13430, 764, 3331, 257, 688, 293, 456, 390, 534, 411, 577, 360, 321, 4373,
  300, 577, 393, 321, 586, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24707120259602863,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.012001951225101948},
  {"id": 97, "seek": 112580, "start": 1141.8, "end": 1151.8, "text": " Ask questions
  not on a single document and single small passage, but how can we do it actually
  on millions of files and.", "tokens": [51164, 12320, 1651, 406, 322, 257, 2167,
  4166, 293, 2167, 1359, 11497, 11, 457, 577, 393, 321, 360, 309, 767, 322, 6803,
  295, 7098, 293, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24707120259602863,
  "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.012001951225101948},
  {"id": 98, "seek": 115180, "start": 1151.8, "end": 1165.8, "text": " And the 25
  work as a retriever step before that was was okay was not not too bad and that''s
  kind of how it started and then very fast evolved into into a say back to search
  direction.", "tokens": [50364, 400, 264, 3552, 589, 382, 257, 19817, 331, 1823,
  949, 300, 390, 390, 1392, 390, 406, 406, 886, 1578, 293, 300, 311, 733, 295, 577,
  309, 1409, 293, 550, 588, 2370, 14178, 666, 666, 257, 584, 646, 281, 3164, 3513,
  13, 51064], "temperature": 0.0, "avg_logprob": -0.2835579367244945, "compression_ratio":
  1.6136363636363635, "no_speech_prob": 0.0006131429108791053}, {"id": 99, "seek":
  115180, "start": 1165.8, "end": 1170.8, "text": " Where we had them a files basically
  as a as a next document store.", "tokens": [51064, 2305, 321, 632, 552, 257, 7098,
  1936, 382, 257, 382, 257, 958, 4166, 3531, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.2835579367244945, "compression_ratio": 1.6136363636363635, "no_speech_prob":
  0.0006131429108791053}, {"id": 100, "seek": 115180, "start": 1170.8, "end": 1178.8,
  "text": " In combination with some some SQL database for for the metadata and so
  on and then it basically kind of.", "tokens": [51314, 682, 6562, 365, 512, 512,
  19200, 8149, 337, 337, 264, 26603, 293, 370, 322, 293, 550, 309, 1936, 733, 295,
  13, 51714], "temperature": 0.0, "avg_logprob": -0.2835579367244945, "compression_ratio":
  1.6136363636363635, "no_speech_prob": 0.0006131429108791053}, {"id": 101, "seek":
  117880, "start": 1178.8, "end": 1190.8, "text": " I think took off on the lecture
  database side with the nervous we via a pine cone and so on and so forth open search
  today is also part of the face deck.", "tokens": [50364, 286, 519, 1890, 766, 322,
  264, 7991, 8149, 1252, 365, 264, 6296, 321, 5766, 257, 15113, 19749, 293, 370, 322,
  293, 370, 5220, 1269, 3164, 965, 307, 611, 644, 295, 264, 1851, 9341, 13, 50964],
  "temperature": 0.0, "avg_logprob": -0.42298108477925145, "compression_ratio": 1.5299539170506913,
  "no_speech_prob": 0.012067960575222969}, {"id": 102, "seek": 117880, "start": 1190.8,
  "end": 1193.8, "text": " But that was I think then just.", "tokens": [50964, 583,
  300, 390, 286, 519, 550, 445, 13, 51114], "temperature": 0.0, "avg_logprob": -0.42298108477925145,
  "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.012067960575222969},
  {"id": 103, "seek": 117880, "start": 1193.8, "end": 1197.8, "text": " Half half
  here after we launched a stick.", "tokens": [51114, 15917, 1922, 510, 934, 321,
  8730, 257, 2897, 13, 51314], "temperature": 0.0, "avg_logprob": -0.42298108477925145,
  "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.012067960575222969},
  {"id": 104, "seek": 117880, "start": 1197.8, "end": 1204.8, "text": " Oh yeah, that''s
  awesome. That sounds quite quick. I know that BBA was also emerging about the same
  time.", "tokens": [51314, 876, 1338, 11, 300, 311, 3476, 13, 663, 3263, 1596, 1702,
  13, 286, 458, 300, 363, 9295, 390, 611, 14989, 466, 264, 912, 565, 13, 51664], "temperature":
  0.0, "avg_logprob": -0.42298108477925145, "compression_ratio": 1.5299539170506913,
  "no_speech_prob": 0.012067960575222969}, {"id": 105, "seek": 120480, "start": 1204.8,
  "end": 1209.8, "text": " And then and then neighbors I guess as well. Yeah, that''s
  that''s that sounds super cool.", "tokens": [50364, 400, 550, 293, 550, 12512, 286,
  2041, 382, 731, 13, 865, 11, 300, 311, 300, 311, 300, 3263, 1687, 1627, 13, 50614],
  "temperature": 0.0, "avg_logprob": -0.18779731750488282, "compression_ratio": 1.6634615384615385,
  "no_speech_prob": 0.055835068225860596}, {"id": 106, "seek": 120480, "start": 1209.8,
  "end": 1228.8, "text": " And was there any as you were approaching your clients
  or like prospects was there any specific use case that you would be demoing with
  because you knew this would trigger the aha moment like question answering or maybe
  a specific domain where you did that.", "tokens": [50614, 400, 390, 456, 604, 382,
  291, 645, 14908, 428, 6982, 420, 411, 32933, 390, 456, 604, 2685, 764, 1389, 300,
  291, 576, 312, 10723, 278, 365, 570, 291, 2586, 341, 576, 7875, 264, 47340, 1623,
  411, 1168, 13430, 420, 1310, 257, 2685, 9274, 689, 291, 630, 300, 13, 51564], "temperature":
  0.0, "avg_logprob": -0.18779731750488282, "compression_ratio": 1.6634615384615385,
  "no_speech_prob": 0.055835068225860596}, {"id": 107, "seek": 122880, "start": 1228.8,
  "end": 1238.8, "text": " Yeah, I would say we were for us it was a lot around question
  answering back then that was really very great that I think many of these aha moments.",
  "tokens": [50364, 865, 11, 286, 576, 584, 321, 645, 337, 505, 309, 390, 257, 688,
  926, 1168, 13430, 646, 550, 300, 390, 534, 588, 869, 300, 286, 519, 867, 295, 613,
  47340, 6065, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2535906303219679,
  "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0030102732125669718},
  {"id": 108, "seek": 122880, "start": 1238.8, "end": 1246.8, "text": " As to remember
  we were at one client and when this meeting and it was like on the in the financial
  domain.", "tokens": [50864, 1018, 281, 1604, 321, 645, 412, 472, 6423, 293, 562,
  341, 3440, 293, 309, 390, 411, 322, 264, 294, 264, 4669, 9274, 13, 51264], "temperature":
  0.0, "avg_logprob": -0.2535906303219679, "compression_ratio": 1.645021645021645,
  "no_speech_prob": 0.0030102732125669718}, {"id": 109, "seek": 122880, "start": 1246.8,
  "end": 1257.8, "text": " So we''re interested in asking questions on financial reports
  of certain companies and basically accelerating their analysis.", "tokens": [51264,
  407, 321, 434, 3102, 294, 3365, 1651, 322, 4669, 7122, 295, 1629, 3431, 293, 1936,
  34391, 641, 5215, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2535906303219679,
  "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0030102732125669718},
  {"id": 110, "seek": 125780, "start": 1257.8, "end": 1275.8, "text": " And at one
  point in this meeting we showed what you can do with question answering ask these
  questions and they also like suggested own questions that we should ask and they
  work so they were that point and convinced oh like that''s not fake.", "tokens":
  [50364, 400, 412, 472, 935, 294, 341, 3440, 321, 4712, 437, 291, 393, 360, 365,
  1168, 13430, 1029, 613, 1651, 293, 436, 611, 411, 10945, 1065, 1651, 300, 321, 820,
  1029, 293, 436, 589, 370, 436, 645, 300, 935, 293, 12561, 1954, 411, 300, 311, 406,
  7592, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2992697697059781, "compression_ratio":
  1.6394557823129252, "no_speech_prob": 0.0038970394525676966}, {"id": 111, "seek":
  127580, "start": 1275.8, "end": 1304.8, "text": " And like smoke and mirror here.
  And the basically the boss of the department was standing up and shouting like wow
  that''s that''s amazing and went out of the office and at the office next door and
  and carried over colleagues and said like you have to see that and that was actually
  even before we started building hastag but was these kind of moments were very important
  to see like this is something.", "tokens": [50364, 400, 411, 8439, 293, 8013, 510,
  13, 400, 264, 1936, 264, 5741, 295, 264, 5882, 390, 4877, 493, 293, 20382, 411,
  6076, 300, 311, 300, 311, 2243, 293, 1437, 484, 295, 264, 3398, 293, 412, 264, 3398,
  958, 2853, 293, 293, 9094, 670, 7734, 293, 848, 411, 291, 362, 281, 536, 300, 293,
  300, 390, 767, 754, 949, 321, 1409, 2390, 6581, 559, 457, 390, 613, 733, 295, 6065,
  645, 588, 1021, 281, 536, 411, 341, 307, 746, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.3284063913735999, "compression_ratio": 1.7467248908296944, "no_speech_prob":
  0.33951041102409363}, {"id": 112, "seek": 130480, "start": 1304.8, "end": 1319.8,
  "text": " That is not just fascinating for for techies like we were but also say
  business people and users see that value and see value and their work for it.",
  "tokens": [50364, 663, 307, 406, 445, 10343, 337, 337, 7553, 530, 411, 321, 645,
  457, 611, 584, 1606, 561, 293, 5022, 536, 300, 2158, 293, 536, 2158, 293, 641, 589,
  337, 309, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15099378313337053, "compression_ratio":
  1.3703703703703705, "no_speech_prob": 0.00979765597730875}, {"id": 113, "seek":
  131980, "start": 1319.8, "end": 1346.8, "text": " I can imagine that and it''s like
  a class of what we call knowledge workers right it''s something that you spend so
  much time on crafting this queries and I have spent some time in the full text finance
  I would say at alpha sense and remember some of the clients they had accumulated
  Boolean queries over a period of 20 years right and they were like so long it''s
  like several pages.", "tokens": [50364, 286, 393, 3811, 300, 293, 309, 311, 411,
  257, 1508, 295, 437, 321, 818, 3601, 5600, 558, 309, 311, 746, 300, 291, 3496, 370,
  709, 565, 322, 29048, 341, 24109, 293, 286, 362, 4418, 512, 565, 294, 264, 1577,
  2487, 10719, 286, 576, 584, 412, 8961, 2020, 293, 1604, 512, 295, 264, 6982, 436,
  632, 31346, 23351, 28499, 24109, 670, 257, 2896, 295, 945, 924, 558, 293, 436, 645,
  411, 370, 938, 309, 311, 411, 2940, 7183, 13, 51714], "temperature": 0.0, "avg_logprob":
  -0.11544191546556426, "compression_ratio": 1.6493506493506493, "no_speech_prob":
  0.5693442225456238}, {"id": 114, "seek": 134680, "start": 1346.8, "end": 1375.8,
  "text": " When you when you when you slap that into solar it runs for three minutes
  because our index layout was not what it is today and was not very optimal and it''s
  crazy to see what what people kind of start doing as work around right so we are
  at a similar case with a with an airplane manufacturer was not financial domain
  but really on some more maintenance level analyzing", "tokens": [50364, 1133, 291,
  562, 291, 562, 291, 21075, 300, 666, 7936, 309, 6676, 337, 1045, 2077, 570, 527,
  8186, 13333, 390, 406, 437, 309, 307, 965, 293, 390, 406, 588, 16252, 293, 309,
  311, 3219, 281, 536, 437, 437, 561, 733, 295, 722, 884, 382, 589, 926, 558, 370,
  321, 366, 412, 257, 2531, 1389, 365, 257, 365, 364, 17130, 18022, 390, 406, 4669,
  9274, 457, 534, 322, 512, 544, 11258, 1496, 23663, 51814], "temperature": 0.0, "avg_logprob":
  -0.1869453505465859, "compression_ratio": 1.6742081447963801, "no_speech_prob":
  0.03269074112176895}, {"id": 115, "seek": 137580, "start": 1375.8, "end": 1399.8,
  "text": " basically issues that come up maybe in certain technical areas and they
  also have like this crazy Boolean search queries and people just became experts
  and crafting that but it took them really long like asking for sending one query
  creating this query I was taking easily like minutes.", "tokens": [50364, 1936,
  2663, 300, 808, 493, 1310, 294, 1629, 6191, 3179, 293, 436, 611, 362, 411, 341,
  3219, 23351, 28499, 3164, 24109, 293, 561, 445, 3062, 8572, 293, 29048, 300, 457,
  309, 1890, 552, 534, 938, 411, 3365, 337, 7750, 472, 14581, 4084, 341, 14581, 286,
  390, 1940, 3612, 411, 2077, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2198630766435103,
  "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.01872316189110279},
  {"id": 116, "seek": 139980, "start": 1399.8, "end": 1428.8, "text": " Yeah exactly
  and so what hey stack is today can you can you elaborate a bit on the architecture
  and maybe if it''s possible if you find it easy if you put if you pick what say
  use case actually I recently I was talking to one stakeholder who wanted to build
  a chatbot but it was a very specific domain so that chatbot would actually ask you
  some kind of philosophical question.", "tokens": [50364, 865, 2293, 293, 370, 437,
  4177, 8630, 307, 965, 393, 291, 393, 291, 20945, 257, 857, 322, 264, 9482, 293,
  1310, 498, 309, 311, 1944, 498, 291, 915, 309, 1858, 498, 291, 829, 498, 291, 1888,
  437, 584, 764, 1389, 767, 286, 3938, 286, 390, 1417, 281, 472, 43406, 567, 1415,
  281, 1322, 257, 5081, 18870, 457, 309, 390, 257, 588, 2685, 9274, 370, 300, 5081,
  18870, 576, 767, 1029, 291, 512, 733, 295, 25066, 1168, 13, 51814], "temperature":
  0.0, "avg_logprob": -0.1847158670425415, "compression_ratio": 1.6563876651982379,
  "no_speech_prob": 0.024833954870700836}, {"id": 117, "seek": 142980, "start": 1429.8,
  "end": 1453.8, "text": " So I think it''s a very difficult then like questions sort
  of a little bit like distracting you from from what''s going on let''s say you are
  on a conference and in a lot of things go through your mind but you don''t register
  maybe what''s going on you don''t get see the value and and that Zenbot might kind
  of ask you and well essentially allow you to pause and reflect right.", "tokens":
  [50364, 407, 286, 519, 309, 311, 257, 588, 2252, 550, 411, 1651, 1333, 295, 257,
  707, 857, 411, 36689, 291, 490, 490, 437, 311, 516, 322, 718, 311, 584, 291, 366,
  322, 257, 7586, 293, 294, 257, 688, 295, 721, 352, 807, 428, 1575, 457, 291, 500,
  380, 7280, 1310, 437, 311, 516, 322, 291, 500, 380, 483, 536, 264, 2158, 293, 293,
  300, 22387, 18870, 1062, 733, 295, 1029, 291, 293, 731, 4476, 2089, 291, 281, 10465,
  293, 5031, 558, 13, 51564], "temperature": 0.0, "avg_logprob": -0.25512452967026655,
  "compression_ratio": 1.6986301369863013, "no_speech_prob": 0.08729945868253708},
  {"id": 118, "seek": 145380, "start": 1453.8, "end": 1472.8, "text": " What I realized
  is that yeah I could pick another shelf model let''s say question answering bird
  or something but it probably wouldn''t work on what I want right my domain is different
  and I had an electronic book with this Zen type of statements.", "tokens": [50364,
  708, 286, 5334, 307, 300, 1338, 286, 727, 1888, 1071, 15222, 2316, 718, 311, 584,
  1168, 13430, 5255, 420, 746, 457, 309, 1391, 2759, 380, 589, 322, 437, 286, 528,
  558, 452, 9274, 307, 819, 293, 286, 632, 364, 10092, 1446, 365, 341, 22387, 2010,
  295, 12363, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13467521850879377,
  "compression_ratio": 1.4080459770114941, "no_speech_prob": 0.03453877195715904},
  {"id": 119, "seek": 147280, "start": 1472.8, "end": 1485.8, "text": " So this one
  question I''m hinting to is kind of fine tuning or maybe even right retraining right
  but where would I start with hey stack and can you walk me through the architecture.",
  "tokens": [50364, 407, 341, 472, 1168, 286, 478, 12075, 278, 281, 307, 733, 295,
  2489, 15164, 420, 1310, 754, 558, 49356, 1760, 558, 457, 689, 576, 286, 722, 365,
  4177, 8630, 293, 393, 291, 1792, 385, 807, 264, 9482, 13, 51014], "temperature":
  0.0, "avg_logprob": -0.21200343540736608, "compression_ratio": 1.3636363636363635,
  "no_speech_prob": 0.16099584102630615}, {"id": 120, "seek": 148580, "start": 1485.8,
  "end": 1514.8, "text": " So as mentioned earlier into core principles are these
  building blocks and using this building blocks to assemble pipelines and I would
  say the core we come from is question answering and search but by now I would say
  the framework has evolved a lot in that direction if you have a lot of different
  notes and can support a lot of different use cases going to translation zero short
  classification.", "tokens": [50414, 407, 382, 2835, 3071, 666, 4965, 9156, 366,
  613, 2390, 8474, 293, 1228, 341, 2390, 8474, 281, 22364, 40168, 293, 286, 576, 584,
  264, 4965, 321, 808, 490, 307, 1168, 13430, 293, 3164, 457, 538, 586, 286, 576,
  584, 264, 8388, 575, 14178, 257, 688, 294, 300, 3513, 498, 291, 362, 257, 688, 295,
  819, 5570, 293, 393, 1406, 257, 688, 295, 819, 764, 3331, 516, 281, 12853, 4018,
  2099, 21538, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20851414998372395,
  "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.0936017706990242},
  {"id": 121, "seek": 151580, "start": 1515.8, "end": 1524.8, "text": " And you could
  produce these notes in isolation or you can kind of assemble them and use them within
  your search pipeline.", "tokens": [50364, 400, 291, 727, 5258, 613, 5570, 294, 16001,
  420, 291, 393, 733, 295, 22364, 552, 293, 764, 552, 1951, 428, 3164, 15517, 13,
  50814], "temperature": 0.0, "avg_logprob": -0.19384542004815464, "compression_ratio":
  1.6775510204081632, "no_speech_prob": 0.002056156052276492}, {"id": 122, "seek":
  151580, "start": 1524.8, "end": 1544.8, "text": " So usually I think what what our
  users through and how they start is now they often come with a kind of search use
  case pick one of the standard pipelines that we have so we can very easily the few
  lines of Python created pipeline for no it''s a question answering or maybe dance
  retrieval.", "tokens": [50814, 407, 2673, 286, 519, 437, 437, 527, 5022, 807, 293,
  577, 436, 722, 307, 586, 436, 2049, 808, 365, 257, 733, 295, 3164, 764, 1389, 1888,
  472, 295, 264, 3832, 40168, 300, 321, 362, 370, 321, 393, 588, 3612, 264, 1326,
  3876, 295, 15329, 2942, 15517, 337, 572, 309, 311, 257, 1168, 13430, 420, 1310,
  4489, 19817, 3337, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19384542004815464,
  "compression_ratio": 1.6775510204081632, "no_speech_prob": 0.002056156052276492},
  {"id": 123, "seek": 154580, "start": 1545.8, "end": 1566.8, "text": " Pick a document
  store you pick one model from for example the hackenface model hub and and we give
  some recommendations on which models might be my people starting point and then
  it''s very easy actually to just put your files into a into a pipeline can be PDF
  files we do the conversion basically for you there''s a note for it.", "tokens":
  [50364, 14129, 257, 4166, 3531, 291, 1888, 472, 2316, 490, 337, 1365, 264, 10339,
  268, 2868, 2316, 11838, 293, 293, 321, 976, 512, 10434, 322, 597, 5245, 1062, 312,
  452, 561, 2891, 935, 293, 550, 309, 311, 588, 1858, 767, 281, 445, 829, 428, 7098,
  666, 257, 666, 257, 15517, 393, 312, 17752, 7098, 321, 360, 264, 14298, 1936, 337,
  291, 456, 311, 257, 3637, 337, 309, 13, 51414], "temperature": 0.0, "avg_logprob":
  -0.2539628794495489, "compression_ratio": 1.5497630331753554, "no_speech_prob":
  0.006160913500934839}, {"id": 124, "seek": 156680, "start": 1567.8, "end": 1584.8,
  "text": " And just have a basic say demo system up and running in a few minutes
  and that''s often already I think a good good starting point if you are maybe also
  new to that field if you just want to try it quickly out on this kind of ebooks
  that you mentioned.", "tokens": [50414, 400, 445, 362, 257, 3875, 584, 10723, 1185,
  493, 293, 2614, 294, 257, 1326, 2077, 293, 300, 311, 2049, 1217, 286, 519, 257,
  665, 665, 2891, 935, 498, 291, 366, 1310, 611, 777, 281, 300, 2519, 498, 291, 445,
  528, 281, 853, 309, 2661, 484, 322, 341, 733, 295, 308, 15170, 300, 291, 2835, 13,
  51264], "temperature": 0.0, "avg_logprob": -0.15944649001299324, "compression_ratio":
  1.494047619047619, "no_speech_prob": 0.08734586834907532}, {"id": 125, "seek": 158480,
  "start": 1585.8, "end": 1610.8, "text": " And get a get a first let''s say quality
  of understanding how good piece of the shelf pipelines for my use case get this
  first data point and then basically enter the I would say next next steps typically
  in your project if you see all like this is promising but not enough for really
  going to production.", "tokens": [50414, 400, 483, 257, 483, 257, 700, 718, 311,
  584, 3125, 295, 3701, 577, 665, 2522, 295, 264, 15222, 40168, 337, 452, 764, 1389,
  483, 341, 700, 1412, 935, 293, 550, 1936, 3242, 264, 286, 576, 584, 958, 958, 4439,
  5850, 294, 428, 1716, 498, 291, 536, 439, 411, 341, 307, 20257, 457, 406, 1547,
  337, 534, 516, 281, 4265, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17646285891532898,
  "compression_ratio": 1.5751295336787565, "no_speech_prob": 0.10711150616407394},
  {"id": 126, "seek": 161080, "start": 1610.8, "end": 1629.8, "text": " And then typically
  go more in this experimentation mode they say all it''s now maybe evaluate compare
  a couple of different models let''s maybe adjust this pipeline a bit or add a re-ranker
  maybe or go maybe to the to a hybrid retriever pipeline where we come.", "tokens":
  [50364, 400, 550, 5850, 352, 544, 294, 341, 37142, 4391, 436, 584, 439, 309, 311,
  586, 1310, 13059, 6794, 257, 1916, 295, 819, 5245, 718, 311, 1310, 4369, 341, 15517,
  257, 857, 420, 909, 257, 319, 12, 20479, 260, 1310, 420, 352, 1310, 281, 264, 281,
  257, 13051, 19817, 331, 15517, 689, 321, 808, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.29111651716561154, "compression_ratio": 1.5266272189349113, "no_speech_prob":
  0.02739662677049637}, {"id": 127, "seek": 162980, "start": 1629.8, "end": 1650.8,
  "text": " Basically have a 25 retriever in parallel to a dense retriever and we
  join these documents and hastic has a lot of functionality that makes that easy
  to to basically change a pipeline as you wonder very quickly and then evaluate if
  that gives you any any benefit.", "tokens": [50364, 8537, 362, 257, 3552, 19817,
  331, 294, 8952, 281, 257, 18011, 19817, 331, 293, 321, 3917, 613, 8512, 293, 6581,
  299, 575, 257, 688, 295, 14980, 300, 1669, 300, 1858, 281, 281, 1936, 1319, 257,
  15517, 382, 291, 2441, 588, 2661, 293, 550, 13059, 498, 300, 2709, 291, 604, 604,
  5121, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24349680968693324, "compression_ratio":
  1.5380116959064327, "no_speech_prob": 0.013260602951049805}, {"id": 128, "seek":
  165080, "start": 1651.8, "end": 1678.8, "text": " If these say of the shelf options
  and combinations are not enough for use case then yeah you can go down the fine
  tuning route I would say we have also have a source the notation tool labeling tool
  where you can create training data and basically fine tune parts of your pipeline
  retriever or reader for question answering.", "tokens": [50414, 759, 613, 584, 295,
  264, 15222, 3956, 293, 21267, 366, 406, 1547, 337, 764, 1389, 550, 1338, 291, 393,
  352, 760, 264, 2489, 15164, 7955, 286, 576, 584, 321, 362, 611, 362, 257, 4009,
  264, 24657, 2290, 40244, 2290, 689, 291, 393, 1884, 3097, 1412, 293, 1936, 2489,
  10864, 3166, 295, 428, 15517, 19817, 331, 420, 15149, 337, 1168, 13430, 13, 51764],
  "temperature": 0.0, "avg_logprob": -0.2928010793832632, "compression_ratio": 1.6649484536082475,
  "no_speech_prob": 0.015037382952868938}, {"id": 129, "seek": 167880, "start": 1678.8,
  "end": 1683.8, "text": " So basically I would say everything from a quick prototype
  tool.", "tokens": [50364, 407, 1936, 286, 576, 584, 1203, 490, 257, 1702, 19475,
  2290, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3243922551472982, "compression_ratio":
  1.393103448275862, "no_speech_prob": 0.001257824245840311}, {"id": 130, "seek":
  167880, "start": 1683.8, "end": 1693.8, "text": " Let''s do some some experiments
  here and there to then going and production and deploying it with a with a basic
  rest API until basically.", "tokens": [50614, 961, 311, 360, 512, 512, 12050, 510,
  293, 456, 281, 550, 516, 293, 4265, 293, 34198, 309, 365, 257, 365, 257, 3875, 1472,
  9362, 1826, 1936, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3243922551472982,
  "compression_ratio": 1.393103448275862, "no_speech_prob": 0.001257824245840311},
  {"id": 131, "seek": 169380, "start": 1694.8, "end": 1720.8, "text": " Sounds cool
  and so in that experimentation mode I guess one one one aspect is like fine tuning
  you mentioned right the other is kind of like what building blocks I could plug
  in right and I know you guys have really good documentation is there something like
  a tutorial or or some kind of walk through that would even help me discover is a
  user what are the options.", "tokens": [50414, 14576, 1627, 293, 370, 294, 300,
  37142, 4391, 286, 2041, 472, 472, 472, 4171, 307, 411, 2489, 15164, 291, 2835, 558,
  264, 661, 307, 733, 295, 411, 437, 2390, 8474, 286, 727, 5452, 294, 558, 293, 286,
  458, 291, 1074, 362, 534, 665, 14333, 307, 456, 746, 411, 257, 7073, 420, 420, 512,
  733, 295, 1792, 807, 300, 576, 754, 854, 385, 4411, 307, 257, 4195, 437, 366, 264,
  3956, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08503927230834961, "compression_ratio":
  1.6712328767123288, "no_speech_prob": 0.06906592845916748}, {"id": 132, "seek":
  172080, "start": 1721.8, "end": 1736.8, "text": " So we have a couple of different
  different tutorials showing you what kind of notes also you can use like many people
  are not aware of for example options that can do it indexing time that might be
  helpful so.", "tokens": [50414, 407, 321, 362, 257, 1916, 295, 819, 819, 17616,
  4099, 291, 437, 733, 295, 5570, 611, 291, 393, 764, 411, 867, 561, 366, 406, 3650,
  295, 337, 1365, 3956, 300, 393, 360, 309, 8186, 278, 565, 300, 1062, 312, 4961,
  370, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13306047605431598, "compression_ratio":
  1.4822695035460993, "no_speech_prob": 0.005889758467674255}, {"id": 133, "seek":
  173680, "start": 1737.8, "end": 1754.8, "text": " For example, like enriching your
  documents with metadata can be incredibly powerful later at search time because
  you can then filter on your search space to make more categories that that you''re
  interested in.", "tokens": [50414, 1171, 1365, 11, 411, 18849, 278, 428, 8512, 365,
  26603, 393, 312, 6252, 4005, 1780, 412, 3164, 565, 570, 291, 393, 550, 6608, 322,
  428, 3164, 1901, 281, 652, 544, 10479, 300, 300, 291, 434, 3102, 294, 13, 51264],
  "temperature": 0.0, "avg_logprob": -0.2171345211210705, "compression_ratio": 1.4217687074829932,
  "no_speech_prob": 0.03299427404999733}, {"id": 134, "seek": 175480, "start": 1754.8,
  "end": 1759.8, "text": " And there we have for example, the stories that show you
  how easily you can.", "tokens": [50364, 400, 456, 321, 362, 337, 1365, 11, 264,
  3676, 300, 855, 291, 577, 3612, 291, 393, 13, 50614], "temperature": 0.0, "avg_logprob":
  -0.23244542208584873, "compression_ratio": 1.6352201257861636, "no_speech_prob":
  0.0044640968553721905}, {"id": 135, "seek": 175480, "start": 1759.8, "end": 1773.8,
  "text": " For example, classify documents that you index to certain categories and
  then later on at query time use these categories to narrow down your search space
  filter for these categories.", "tokens": [50614, 1171, 1365, 11, 33872, 8512, 300,
  291, 8186, 281, 1629, 10479, 293, 550, 1780, 322, 412, 14581, 565, 764, 613, 10479,
  281, 9432, 760, 428, 3164, 1901, 6608, 337, 613, 10479, 13, 51314], "temperature":
  0.0, "avg_logprob": -0.23244542208584873, "compression_ratio": 1.6352201257861636,
  "no_speech_prob": 0.0044640968553721905}, {"id": 136, "seek": 177380, "start": 1774.8,
  "end": 1785.8, "text": " And on the model side, say if you are now you know that
  you want to have a say QA model reader and you know interested in what model you
  want.", "tokens": [50414, 400, 322, 264, 2316, 1252, 11, 584, 498, 291, 366, 586,
  291, 458, 300, 291, 528, 281, 362, 257, 584, 1249, 32, 2316, 15149, 293, 291, 458,
  3102, 294, 437, 2316, 291, 528, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1968604496547154,
  "compression_ratio": 1.5743589743589743, "no_speech_prob": 0.020196301862597466},
  {"id": 137, "seek": 177380, "start": 1785.8, "end": 1799.8, "text": " I would probably
  suggest you just go to our benchmarks page which is linked from documentation there
  we have a couple of comparisons in terms of accuracy and speed.", "tokens": [50964,
  286, 576, 1391, 3402, 291, 445, 352, 281, 527, 43751, 3028, 597, 307, 9408, 490,
  14333, 456, 321, 362, 257, 1916, 295, 33157, 294, 2115, 295, 14170, 293, 3073, 13,
  51664], "temperature": 0.0, "avg_logprob": -0.1968604496547154, "compression_ratio":
  1.5743589743589743, "no_speech_prob": 0.020196301862597466}, {"id": 138, "seek":
  179980, "start": 1799.8, "end": 1808.8, "text": " But also we have most of our own
  models on the hackenface model hub which appears to find this information and model
  cards.", "tokens": [50364, 583, 611, 321, 362, 881, 295, 527, 1065, 5245, 322, 264,
  10339, 268, 2868, 2316, 11838, 597, 7038, 281, 915, 341, 1589, 293, 2316, 5632,
  13, 50814], "temperature": 0.0, "avg_logprob": -0.23439445495605468, "compression_ratio":
  1.6566523605150214, "no_speech_prob": 0.011053085327148438}, {"id": 139, "seek":
  179980, "start": 1809.8, "end": 1826.8, "text": " Yeah, that''s awesome. So you
  guys in addition to open source version that I could I presume could host completely
  myself right I still have a bunch of questions on that open source side but still
  you also offer the cloud version you call deep set cloud is right.", "tokens": [50864,
  865, 11, 300, 311, 3476, 13, 407, 291, 1074, 294, 4500, 281, 1269, 4009, 3037, 300,
  286, 727, 286, 43283, 727, 3975, 2584, 2059, 558, 286, 920, 362, 257, 3840, 295,
  1651, 322, 300, 1269, 4009, 1252, 457, 920, 291, 611, 2626, 264, 4588, 3037, 291,
  818, 2452, 992, 4588, 307, 558, 13, 51714], "temperature": 0.0, "avg_logprob": -0.23439445495605468,
  "compression_ratio": 1.6566523605150214, "no_speech_prob": 0.011053085327148438},
  {"id": 140, "seek": 182680, "start": 1826.8, "end": 1839.8, "text": " Can you explain
  what users get with that I presume scalability but maybe something else and I think
  we can we can leave a link to in the show notes as well for those users who want
  to try it out.", "tokens": [50364, 1664, 291, 2903, 437, 5022, 483, 365, 300, 286,
  43283, 15664, 2310, 457, 1310, 746, 1646, 293, 286, 519, 321, 393, 321, 393, 1856,
  257, 2113, 281, 294, 264, 855, 5570, 382, 731, 337, 729, 5022, 567, 528, 281, 853,
  309, 484, 13, 51014], "temperature": 0.0, "avg_logprob": -0.21587915530149965, "compression_ratio":
  1.6198347107438016, "no_speech_prob": 0.01978258602321148}, {"id": 141, "seek":
  182680, "start": 1839.8, "end": 1852.8, "text": " Yeah, basically hey stack the
  open source predictors will be a Python framework and you can do everything you
  want there to prototype the experiments and if you want also go to production with
  it.", "tokens": [51014, 865, 11, 1936, 4177, 8630, 264, 1269, 4009, 6069, 830, 486,
  312, 257, 15329, 8388, 293, 291, 393, 360, 1203, 291, 528, 456, 281, 19475, 264,
  12050, 293, 498, 291, 528, 611, 352, 281, 4265, 365, 309, 13, 51664], "temperature":
  0.0, "avg_logprob": -0.21587915530149965, "compression_ratio": 1.6198347107438016,
  "no_speech_prob": 0.01978258602321148}, {"id": 142, "seek": 185280, "start": 1852.8,
  "end": 1881.8, "text": " But you also found in basically in addition to that people
  want something more like they want to really host the platform where it''s really
  end to end and basically you have faster workflows so really what''s covering the
  whole lifecycle of an application from early prototyping to running many experiments
  and parallel getting more guidance.", "tokens": [50364, 583, 291, 611, 1352, 294,
  1936, 294, 4500, 281, 300, 561, 528, 746, 544, 411, 436, 528, 281, 534, 3975, 264,
  3663, 689, 309, 311, 534, 917, 281, 917, 293, 1936, 291, 362, 4663, 43461, 370,
  534, 437, 311, 10322, 264, 1379, 45722, 295, 364, 3861, 490, 2440, 46219, 3381,
  281, 2614, 867, 12050, 293, 8952, 1242, 544, 10056, 13, 51814], "temperature": 0.0,
  "avg_logprob": -0.27022168040275574, "compression_ratio": 1.6394230769230769, "no_speech_prob":
  0.028763873502612114}, {"id": 143, "seek": 188180, "start": 1881.8, "end": 1891.8,
  "text": " What''s on from your eye perspective on what to launch investigating certain
  documents in a faster way.", "tokens": [50364, 708, 311, 322, 490, 428, 3313, 4585,
  322, 437, 281, 4025, 22858, 1629, 8512, 294, 257, 4663, 636, 13, 50864], "temperature":
  0.0, "avg_logprob": -0.2527727782726288, "compression_ratio": 1.4973821989528795,
  "no_speech_prob": 0.02090139500796795}, {"id": 144, "seek": 188180, "start": 1891.8,
  "end": 1906.8, "text": " Then to OK now I did all these experiments and I want ever
  kind of one click path to production and I don''t want to bother with any scaling
  and basically a productionizing on my side.", "tokens": [50864, 1396, 281, 2264,
  586, 286, 630, 439, 613, 12050, 293, 286, 528, 1562, 733, 295, 472, 2052, 3100,
  281, 4265, 293, 286, 500, 380, 528, 281, 8677, 365, 604, 21589, 293, 1936, 257,
  4265, 3319, 322, 452, 1252, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2527727782726288,
  "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.02090139500796795},
  {"id": 145, "seek": 190680, "start": 1906.8, "end": 1933.8, "text": " And this is
  basically what what we do with these at cloud so if you imagine as a host the platform
  the cloud the SaaS platform where you develop your NAP applications and can easily
  bring them to production and monitor them afterwards so really the I would say whole
  life cycle and especially what''s going on getting your.", "tokens": [50364, 400,
  341, 307, 1936, 437, 437, 321, 360, 365, 613, 412, 4588, 370, 498, 291, 3811, 382,
  257, 3975, 264, 3663, 264, 4588, 264, 49733, 3663, 689, 291, 1499, 428, 426, 4715,
  5821, 293, 393, 3612, 1565, 552, 281, 4265, 293, 6002, 552, 10543, 370, 534, 264,
  286, 576, 584, 1379, 993, 6586, 293, 2318, 437, 311, 516, 322, 1242, 428, 13, 51714],
  "temperature": 0.0, "avg_logprob": -0.3750101725260417, "compression_ratio": 1.5707317073170732,
  "no_speech_prob": 0.006859649904072285}, {"id": 146, "seek": 193380, "start": 1933.8,
  "end": 1948.8, "text": " Getting your NAP pipelines faster to production as you
  would probably do it on a just Python level and then continue monitoring them and
  having this close group is to later want to maintain them.", "tokens": [50364, 13674,
  428, 426, 4715, 40168, 4663, 281, 4265, 382, 291, 576, 1391, 360, 309, 322, 257,
  445, 15329, 1496, 293, 550, 2354, 11028, 552, 293, 1419, 341, 1998, 1594, 307, 281,
  1780, 528, 281, 6909, 552, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3244152999505764,
  "compression_ratio": 1.3829787234042554, "no_speech_prob": 0.019420376047492027},
  {"id": 147, "seek": 194880, "start": 1948.8, "end": 1977.8, "text": " So it sounds
  cool and since it''s kind of like so with open source version I presume I could
  do kind of a local development on my PC right and then go and use some deployment
  pipeline to deploy with cloud version I have sort of like managed haystack right
  and now thinking about developer experience are you guys moving more towards cloud
  tools as well you know like for example.", "tokens": [50364, 407, 309, 3263, 1627,
  293, 1670, 309, 311, 733, 295, 411, 370, 365, 1269, 4009, 3037, 286, 43283, 286,
  727, 360, 733, 295, 257, 2654, 3250, 322, 452, 6465, 558, 293, 550, 352, 293, 764,
  512, 19317, 15517, 281, 7274, 365, 4588, 3037, 286, 362, 1333, 295, 411, 6453, 4842,
  372, 501, 558, 293, 586, 1953, 466, 10754, 1752, 366, 291, 1074, 2684, 544, 3030,
  4588, 3873, 382, 731, 291, 458, 411, 337, 1365, 13, 51814], "temperature": 0.0,
  "avg_logprob": -0.15316334253625025, "compression_ratio": 1.6578947368421053, "no_speech_prob":
  0.7715344429016113}, {"id": 148, "seek": 197780, "start": 1977.8, "end": 1990.8,
  "text": " A code editor could be in the clouds or the changes and click click the
  button and off it goes I don''t even need to download it locally right or or do
  you see some other trend with your users.", "tokens": [50364, 316, 3089, 9839, 727,
  312, 294, 264, 12193, 420, 264, 2962, 293, 2052, 2052, 264, 2960, 293, 766, 309,
  1709, 286, 500, 380, 754, 643, 281, 5484, 309, 16143, 558, 420, 420, 360, 291, 536,
  512, 661, 6028, 365, 428, 5022, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16123879474142325,
  "compression_ratio": 1.411764705882353, "no_speech_prob": 0.007400870323181152},
  {"id": 149, "seek": 199080, "start": 1991.8, "end": 2017.8, "text": " No like we
  maybe that''s also an important point so it''s still a developer platform right
  so we are not in a low code no code space and what we really try is basically giving
  developers the option to customize components and that then goes through coding
  and and there we have for example editors directly on the platform where you can.",
  "tokens": [50414, 883, 411, 321, 1310, 300, 311, 611, 364, 1021, 935, 370, 309,
  311, 920, 257, 10754, 3663, 558, 370, 321, 366, 406, 294, 257, 2295, 3089, 572,
  3089, 1901, 293, 437, 321, 534, 853, 307, 1936, 2902, 8849, 264, 3614, 281, 19734,
  6677, 293, 300, 550, 1709, 807, 17720, 293, 293, 456, 321, 362, 337, 1365, 31446,
  3838, 322, 264, 3663, 689, 291, 393, 13, 51714], "temperature": 0.0, "avg_logprob":
  -0.21557789954586307, "compression_ratio": 1.6262135922330097, "no_speech_prob":
  0.06608215719461441}, {"id": 150, "seek": 201780, "start": 2017.8, "end": 2039.8,
  "text": " Edit for example just the young definition of pipelines and quickly switch
  certain parameters if you want to do that and then it''s basically there''s a hosted
  notebooks where you can also easily kind of open these resources like a pipeline
  and we automatically create some Python code of it in notebook that you can then.",
  "tokens": [50364, 33241, 337, 1365, 445, 264, 2037, 7123, 295, 40168, 293, 2661,
  3679, 1629, 9834, 498, 291, 528, 281, 360, 300, 293, 550, 309, 311, 1936, 456, 311,
  257, 19204, 43782, 689, 291, 393, 611, 3612, 733, 295, 1269, 613, 3593, 411, 257,
  15517, 293, 321, 6772, 1884, 512, 15329, 3089, 295, 309, 294, 21060, 300, 291, 393,
  550, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15287343282548208, "compression_ratio":
  1.5841584158415842, "no_speech_prob": 0.0018619955517351627}, {"id": 151, "seek":
  203980, "start": 2039.8, "end": 2044.8, "text": " Then edit as you as you know it
  also from haystack open source.", "tokens": [50364, 1396, 8129, 382, 291, 382, 291,
  458, 309, 611, 490, 4842, 372, 501, 1269, 4009, 13, 50614], "temperature": 0.0,
  "avg_logprob": -0.2969833427751568, "compression_ratio": 1.618811881188119, "no_speech_prob":
  0.0009909897344186902}, {"id": 152, "seek": 203980, "start": 2045.8, "end": 2064.8,
  "text": " Adjust the sort of certain component debug it maybe at another one and
  then it''s basically just one Python line again to move away from the Python code
  in your notebook to the production artifacts to the pipeline that is then deployed
  and then can run production.", "tokens": [50664, 34049, 264, 1333, 295, 1629, 6542,
  24083, 309, 1310, 412, 1071, 472, 293, 550, 309, 311, 1936, 445, 472, 15329, 1622,
  797, 281, 1286, 1314, 490, 264, 15329, 3089, 294, 428, 21060, 281, 264, 4265, 24617,
  281, 264, 15517, 300, 307, 550, 17826, 293, 550, 393, 1190, 4265, 13, 51614], "temperature":
  0.0, "avg_logprob": -0.2969833427751568, "compression_ratio": 1.618811881188119,
  "no_speech_prob": 0.0009909897344186902}, {"id": 153, "seek": 206480, "start": 2065.8,
  "end": 2080.8, "text": " Yeah sounds cool and if a user has some as a user I mean
  it could be a company right so let''s say they have an established tool set you
  know maybe if the usage maker maybe they don''t maybe use something else.", "tokens":
  [50414, 865, 3263, 1627, 293, 498, 257, 4195, 575, 512, 382, 257, 4195, 286, 914,
  309, 727, 312, 257, 2237, 558, 370, 718, 311, 584, 436, 362, 364, 7545, 2290, 992,
  291, 458, 1310, 498, 264, 14924, 17127, 1310, 436, 500, 380, 1310, 764, 746, 1646,
  13, 51164], "temperature": 0.0, "avg_logprob": -0.1538731431307858, "compression_ratio":
  1.5891891891891892, "no_speech_prob": 0.013970565982162952}, {"id": 154, "seek":
  206480, "start": 2081.8, "end": 2089.8, "text": " How do you reach these tools said
  that is kind of outside of haystack do you have to.", "tokens": [51214, 1012, 360,
  291, 2524, 613, 3873, 848, 300, 307, 733, 295, 2380, 295, 4842, 372, 501, 360, 291,
  362, 281, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1538731431307858, "compression_ratio":
  1.5891891891891892, "no_speech_prob": 0.013970565982162952}, {"id": 155, "seek":
  208980, "start": 2089.8, "end": 2118.8, "text": " I would say in most cases not
  so you will I mean what were very very basically stop I would say with with the
  cloud is when you have your pipeline to NAP service and you have your rest API that
  you expose that''s kind of where we stop so there''s a lot of I would say stuff
  in a company that is built around it when you''re into your product and also on
  the other side of where do the files come from where does that.", "tokens": [50414,
  286, 576, 584, 294, 881, 3331, 406, 370, 291, 486, 286, 914, 437, 645, 588, 588,
  1936, 1590, 286, 576, 584, 365, 365, 264, 4588, 307, 562, 291, 362, 428, 15517,
  281, 426, 4715, 2643, 293, 291, 362, 428, 1472, 9362, 300, 291, 19219, 300, 311,
  733, 295, 689, 321, 1590, 370, 456, 311, 257, 688, 295, 286, 576, 584, 1507, 294,
  257, 2237, 300, 307, 3094, 926, 309, 562, 291, 434, 666, 428, 1674, 293, 611, 322,
  264, 661, 1252, 295, 689, 360, 264, 7098, 808, 490, 689, 775, 300, 13, 51814], "temperature":
  0.0, "avg_logprob": -0.3394919144479852, "compression_ratio": 1.7468354430379747,
  "no_speech_prob": 0.006194399204105139}, {"id": 156, "seek": 211980, "start": 2119.8,
  "end": 2123.8, "text": " Data come from how you think it into into a deep set cloud.",
  "tokens": [50364, 11888, 808, 490, 577, 291, 519, 309, 666, 666, 257, 2452, 992,
  4588, 13, 50564], "temperature": 0.0, "avg_logprob": -0.318569540977478, "compression_ratio":
  1.3795620437956204, "no_speech_prob": 0.0023937236983329058}, {"id": 157, "seek":
  211980, "start": 2124.8, "end": 2129.8, "text": " But within that space we rather
  see people.", "tokens": [50614, 583, 1951, 300, 1901, 321, 2831, 536, 561, 13, 50864],
  "temperature": 0.0, "avg_logprob": -0.318569540977478, "compression_ratio": 1.3795620437956204,
  "no_speech_prob": 0.0023937236983329058}, {"id": 158, "seek": 211980, "start": 2130.8,
  "end": 2137.8, "text": " Customers who appreciate it that''s like fully integrated
  and they don''t usually then.", "tokens": [50914, 16649, 433, 567, 4449, 309, 300,
  311, 411, 4498, 10919, 293, 436, 500, 380, 2673, 550, 13, 51264], "temperature":
  0.0, "avg_logprob": -0.318569540977478, "compression_ratio": 1.3795620437956204,
  "no_speech_prob": 0.0023937236983329058}, {"id": 159, "seek": 213780, "start": 2138.8,
  "end": 2146.8, "text": " Want to stay on on sage maker if they are on it for these
  NAP use cases so from our perspective.", "tokens": [50414, 11773, 281, 1754, 322,
  322, 19721, 17127, 498, 436, 366, 322, 309, 337, 613, 426, 4715, 764, 3331, 370,
  490, 527, 4585, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3936958659778942,
  "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.08074244111776352},
  {"id": 160, "seek": 213780, "start": 2147.8, "end": 2155.8, "text": " There are
  the other are these more generic solutions that are not specific for NAP the car
  work for any kind of machine learning.", "tokens": [50864, 821, 366, 264, 661, 366,
  613, 544, 19577, 6547, 300, 366, 406, 2685, 337, 426, 4715, 264, 1032, 589, 337,
  604, 733, 295, 3479, 2539, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3936958659778942,
  "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.08074244111776352},
  {"id": 161, "seek": 215580, "start": 2155.8, "end": 2161.8, "text": " But if you
  really have cases where you want to be faster on your NAP use cases.", "tokens":
  [50364, 583, 498, 291, 534, 362, 3331, 689, 291, 528, 281, 312, 4663, 322, 428,
  426, 4715, 764, 3331, 13, 50664], "temperature": 0.0, "avg_logprob": -0.29431327444608096,
  "compression_ratio": 1.511111111111111, "no_speech_prob": 0.051327504217624664},
  {"id": 162, "seek": 215580, "start": 2161.8, "end": 2178.8, "text": " Want to have
  more say support on that side that''s basically where where deep set cloud and comes
  into play and to give you an example your think of experiments should evaluate these
  pipelines.", "tokens": [50664, 11773, 281, 362, 544, 584, 1406, 322, 300, 1252,
  300, 311, 1936, 689, 689, 2452, 992, 4588, 293, 1487, 666, 862, 293, 281, 976, 291,
  364, 1365, 428, 519, 295, 12050, 820, 13059, 613, 40168, 13, 51514], "temperature":
  0.0, "avg_logprob": -0.29431327444608096, "compression_ratio": 1.511111111111111,
  "no_speech_prob": 0.051327504217624664}, {"id": 163, "seek": 217880, "start": 2178.8,
  "end": 2195.8, "text": " And then you have to do give basically a lot of options
  to investigate predictions and what do these metrics actually say and this is a
  thing is something that is usually missing and solutions like sage maker.", "tokens":
  [50364, 400, 550, 291, 362, 281, 360, 976, 1936, 257, 688, 295, 3956, 281, 15013,
  21264, 293, 437, 360, 613, 16367, 767, 584, 293, 341, 307, 257, 551, 307, 746, 300,
  307, 2673, 5361, 293, 6547, 411, 19721, 17127, 13, 51214], "temperature": 0.0, "avg_logprob":
  -0.31479220390319823, "compression_ratio": 1.7168949771689497, "no_speech_prob":
  0.07646393030881882}, {"id": 164, "seek": 217880, "start": 2195.8, "end": 2202.8,
  "text": " You have to then really combine with many other tools and build in there
  like a lot of extra stuff.", "tokens": [51214, 509, 362, 281, 550, 534, 10432, 365,
  867, 661, 3873, 293, 1322, 294, 456, 411, 257, 688, 295, 2857, 1507, 13, 51564],
  "temperature": 0.0, "avg_logprob": -0.31479220390319823, "compression_ratio": 1.7168949771689497,
  "no_speech_prob": 0.07646393030881882}, {"id": 165, "seek": 217880, "start": 2202.8,
  "end": 2206.8, "text": " And that basically comes all together already with deep
  set cloud.", "tokens": [51564, 400, 300, 1936, 1487, 439, 1214, 1217, 365, 2452,
  992, 4588, 13, 51764], "temperature": 0.0, "avg_logprob": -0.31479220390319823,
  "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.07646393030881882},
  {"id": 166, "seek": 220680, "start": 2206.8, "end": 2213.8, "text": " So get it
  right so deep set cloud with offer me sort of an evaluation tool set right.", "tokens":
  [50364, 407, 483, 309, 558, 370, 2452, 992, 4588, 365, 2626, 385, 1333, 295, 364,
  13344, 2290, 992, 558, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18572496564200755,
  "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.001505466760136187},
  {"id": 167, "seek": 220680, "start": 2213.8, "end": 2219.8, "text": " Can I get
  the same in the open source version or it''s not present there.", "tokens": [50714,
  1664, 286, 483, 264, 912, 294, 264, 1269, 4009, 3037, 420, 309, 311, 406, 1974,
  456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18572496564200755, "compression_ratio":
  1.7173913043478262, "no_speech_prob": 0.001505466760136187}, {"id": 168, "seek":
  220680, "start": 2219.8, "end": 2224.8, "text": " You can basically evaluate single
  pipelines also in the open source version.", "tokens": [51014, 509, 393, 1936, 13059,
  2167, 40168, 611, 294, 264, 1269, 4009, 3037, 13, 51264], "temperature": 0.0, "avg_logprob":
  -0.18572496564200755, "compression_ratio": 1.7173913043478262, "no_speech_prob":
  0.001505466760136187}, {"id": 169, "seek": 220680, "start": 2224.8, "end": 2234.8,
  "text": " The difference is that basically in deep set cloud you have a full overview
  over your project where we track all your experiments you can kind of compare them.",
  "tokens": [51264, 440, 2649, 307, 300, 1936, 294, 2452, 992, 4588, 291, 362, 257,
  1577, 12492, 670, 428, 1716, 689, 321, 2837, 439, 428, 12050, 291, 393, 733, 295,
  6794, 552, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18572496564200755,
  "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.001505466760136187},
  {"id": 170, "seek": 223480, "start": 2234.8, "end": 2250.8, "text": " Launch easily
  20 experiments in parallel and this is actually on large data sets and with open
  source I think and generally you would need to provision a lot of machines GPUs
  to run that in parallel.", "tokens": [50364, 28119, 3612, 945, 12050, 294, 8952,
  293, 341, 307, 767, 322, 2416, 1412, 6352, 293, 365, 1269, 4009, 286, 519, 293,
  5101, 291, 576, 643, 281, 17225, 257, 688, 295, 8379, 18407, 82, 281, 1190, 300,
  294, 8952, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2792954112208167, "compression_ratio":
  1.3724137931034484, "no_speech_prob": 0.004351471550762653}, {"id": 171, "seek":
  225080, "start": 2250.8, "end": 2260.8, "text": " And that''s basically what one
  thing that we offer and deep set cloud and the other is basically the I would say
  just the you I love layer over it.", "tokens": [50364, 400, 300, 311, 1936, 437,
  472, 551, 300, 321, 2626, 293, 2452, 992, 4588, 293, 264, 661, 307, 1936, 264, 286,
  576, 584, 445, 264, 291, 286, 959, 4583, 670, 309, 13, 50864], "temperature": 0.0,
  "avg_logprob": -0.26469106259553327, "compression_ratio": 1.5549738219895288, "no_speech_prob":
  0.24975942075252533}, {"id": 172, "seek": 225080, "start": 2260.8, "end": 2273.8,
  "text": " So of course I can work with what Hey stack on and get basically a report
  around my experiments again maybe a panel state of frame I get some metrics.", "tokens":
  [50864, 407, 295, 1164, 286, 393, 589, 365, 437, 1911, 8630, 322, 293, 483, 1936,
  257, 2275, 926, 452, 12050, 797, 1310, 257, 4831, 1785, 295, 3920, 286, 483, 512,
  16367, 13, 51514], "temperature": 0.0, "avg_logprob": -0.26469106259553327, "compression_ratio":
  1.5549738219895288, "no_speech_prob": 0.24975942075252533}, {"id": 173, "seek":
  227380, "start": 2273.8, "end": 2287.8, "text": " What we do when as you on top
  in deep set cloud is allowing people to interact with this kind of data more easily
  like finding examples of queries that fail that.", "tokens": [50364, 708, 321, 360,
  562, 382, 291, 322, 1192, 294, 2452, 992, 4588, 307, 8293, 561, 281, 4648, 365,
  341, 733, 295, 1412, 544, 3612, 411, 5006, 5110, 295, 24109, 300, 3061, 300, 13,
  51064], "temperature": 0.0, "avg_logprob": -0.2974823624340456, "compression_ratio":
  1.5326633165829147, "no_speech_prob": 0.0484265498816967}, {"id": 174, "seek": 227380,
  "start": 2287.8, "end": 2292.8, "text": " Or that are successful getting feedback
  from also end users so collaborating.", "tokens": [51064, 1610, 300, 366, 4406,
  1242, 5824, 490, 611, 917, 5022, 370, 30188, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.2974823624340456, "compression_ratio": 1.5326633165829147, "no_speech_prob":
  0.0484265498816967}, {"id": 175, "seek": 227380, "start": 2292.8, "end": 2297.8,
  "text": " Basically the the persons who use that search system at the end.", "tokens":
  [51314, 8537, 264, 264, 14453, 567, 764, 300, 3164, 1185, 412, 264, 917, 13, 51564],
  "temperature": 0.0, "avg_logprob": -0.2974823624340456, "compression_ratio": 1.5326633165829147,
  "no_speech_prob": 0.0484265498816967}, {"id": 176, "seek": 229780, "start": 2297.8,
  "end": 2315.8, "text": " And now that''s also what I think what we what we saw a
  lot that yeah you can extract your predictions and maybe it''s like a CSV and then
  you shared with your next colleague who then I''m kind of rates or give say human
  evaluation if these queries makes sense or not.", "tokens": [50364, 400, 586, 300,
  311, 611, 437, 286, 519, 437, 321, 437, 321, 1866, 257, 688, 300, 1338, 291, 393,
  8947, 428, 21264, 293, 1310, 309, 311, 411, 257, 48814, 293, 550, 291, 5507, 365,
  428, 958, 13532, 567, 550, 286, 478, 733, 295, 6846, 420, 976, 584, 1952, 13344,
  498, 613, 24109, 1669, 2020, 420, 406, 13, 51264], "temperature": 0.0, "avg_logprob":
  -0.2676256836437788, "compression_ratio": 1.4887640449438202, "no_speech_prob":
  0.012977584265172482}, {"id": 177, "seek": 231580, "start": 2315.8, "end": 2322.8,
  "text": " But again this is like a lot of friction you have them a lot of these
  sees these are exifies floating around.", "tokens": [50364, 583, 797, 341, 307,
  411, 257, 688, 295, 17710, 291, 362, 552, 257, 688, 295, 613, 8194, 613, 366, 454,
  11221, 12607, 926, 13, 50714], "temperature": 0.0, "avg_logprob": -0.3132982616183124,
  "compression_ratio": 1.6945812807881773, "no_speech_prob": 0.011917325668036938},
  {"id": 178, "seek": 231580, "start": 2322.8, "end": 2338.8, "text": " And what we
  would be what we do is I think bring this together again having it in one place
  that you can also in future easily reuse that for other experiments and even use
  it for training and and have it in this in one central place.", "tokens": [50714,
  400, 437, 321, 576, 312, 437, 321, 360, 307, 286, 519, 1565, 341, 1214, 797, 1419,
  309, 294, 472, 1081, 300, 291, 393, 611, 294, 2027, 3612, 26225, 300, 337, 661,
  12050, 293, 754, 764, 309, 337, 3097, 293, 293, 362, 309, 294, 341, 294, 472, 5777,
  1081, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3132982616183124, "compression_ratio":
  1.6945812807881773, "no_speech_prob": 0.011917325668036938}, {"id": 179, "seek":
  233880, "start": 2338.8, "end": 2348.8, "text": " Yeah sounds amazing from what
  I gather this sounds like a end to end ML ops platforms specifically for an LP neural
  search right.", "tokens": [50364, 865, 3263, 2243, 490, 437, 286, 5448, 341, 3263,
  411, 257, 917, 281, 917, 21601, 44663, 9473, 4682, 337, 364, 38095, 18161, 3164,
  558, 13, 50864], "temperature": 0.0, "avg_logprob": -0.250203937292099, "compression_ratio":
  1.5299539170506913, "no_speech_prob": 0.03293554112315178}, {"id": 180, "seek":
  233880, "start": 2348.8, "end": 2362.8, "text": " Exactly you have thought through
  so many things not only the developer side of things like experimentation but also
  you know debugging and actually going through the feedback from stakeholders or
  users.", "tokens": [50864, 7587, 291, 362, 1194, 807, 370, 867, 721, 406, 787, 264,
  10754, 1252, 295, 721, 411, 37142, 457, 611, 291, 458, 45592, 293, 767, 516, 807,
  264, 5824, 490, 17779, 420, 5022, 13, 51564], "temperature": 0.0, "avg_logprob":
  -0.250203937292099, "compression_ratio": 1.5299539170506913, "no_speech_prob": 0.03293554112315178},
  {"id": 181, "seek": 236280, "start": 2362.8, "end": 2365.8, "text": " And then communicating
  with them.", "tokens": [50364, 400, 550, 17559, 365, 552, 13, 50514], "temperature":
  0.0, "avg_logprob": -0.190742990863857, "compression_ratio": 1.6532663316582914,
  "no_speech_prob": 0.018567346036434174}, {"id": 182, "seek": 236280, "start": 2365.8,
  "end": 2387.8, "text": " Yeah and I think this is like something that is missed
  in many projects like this like end user collaboration and from our experience this
  should really happen in in a very early stage of a project that also kind of continuously
  when when you move to production and even when you are production.", "tokens": [50514,
  865, 293, 286, 519, 341, 307, 411, 746, 300, 307, 6721, 294, 867, 4455, 411, 341,
  411, 917, 4195, 9363, 293, 490, 527, 1752, 341, 820, 534, 1051, 294, 294, 257, 588,
  2440, 3233, 295, 257, 1716, 300, 611, 733, 295, 15684, 562, 562, 291, 1286, 281,
  4265, 293, 754, 562, 291, 366, 4265, 13, 51614], "temperature": 0.0, "avg_logprob":
  -0.190742990863857, "compression_ratio": 1.6532663316582914, "no_speech_prob": 0.018567346036434174},
  {"id": 183, "seek": 238780, "start": 2387.8, "end": 2399.8, "text": " And I think
  this is something which is if you don''t have the right tooling that''s very annoying
  to you probably like just building a demo like a UI for some search system.", "tokens":
  [50364, 400, 286, 519, 341, 307, 746, 597, 307, 498, 291, 500, 380, 362, 264, 558,
  46593, 300, 311, 588, 11304, 281, 291, 1391, 411, 445, 2390, 257, 10723, 411, 257,
  15682, 337, 512, 3164, 1185, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3199255923007397,
  "compression_ratio": 1.6745098039215687, "no_speech_prob": 0.06848172098398209},
  {"id": 184, "seek": 238780, "start": 2399.8, "end": 2416.8, "text": " If you are
  not a front end developer if you''re an LP engineer it takes some extra time and
  even with something extremely these days it''s still is then annoying to do it properly
  and if you''re an enterprise maybe draft some access to it''s permission words.",
  "tokens": [50964, 759, 291, 366, 406, 257, 1868, 917, 10754, 498, 291, 434, 364,
  38095, 11403, 309, 2516, 512, 2857, 565, 293, 754, 365, 746, 4664, 613, 1708, 309,
  311, 920, 307, 550, 11304, 281, 360, 309, 6108, 293, 498, 291, 434, 364, 14132,
  1310, 11206, 512, 2105, 281, 309, 311, 11226, 2283, 13, 51814], "temperature": 0.0,
  "avg_logprob": -0.3199255923007397, "compression_ratio": 1.6745098039215687, "no_speech_prob":
  0.06848172098398209}, {"id": 185, "seek": 241680, "start": 2416.8, "end": 2426.8,
  "text": " But it''s so important I think when you look at what projects work out
  at the end what pipelines more customers go to production.", "tokens": [50364, 583,
  309, 311, 370, 1021, 286, 519, 562, 291, 574, 412, 437, 4455, 589, 484, 412, 264,
  917, 437, 40168, 544, 4581, 352, 281, 4265, 13, 50864], "temperature": 0.0, "avg_logprob":
  -0.1524914332798549, "compression_ratio": 1.7027027027027026, "no_speech_prob":
  0.008107648231089115}, {"id": 186, "seek": 241680, "start": 2426.8, "end": 2443.8,
  "text": " It''s really a big criteria I think in the early days like sharing a demo
  with your colleagues and end users really the first pipeline you have more or less
  giving it to the hands of users and seeing what what they think about it and how
  they use it.", "tokens": [50864, 467, 311, 534, 257, 955, 11101, 286, 519, 294,
  264, 2440, 1708, 411, 5414, 257, 10723, 365, 428, 7734, 293, 917, 5022, 534, 264,
  700, 15517, 291, 362, 544, 420, 1570, 2902, 309, 281, 264, 2377, 295, 5022, 293,
  2577, 437, 437, 436, 519, 466, 309, 293, 577, 436, 764, 309, 13, 51714], "temperature":
  0.0, "avg_logprob": -0.1524914332798549, "compression_ratio": 1.7027027027027026,
  "no_speech_prob": 0.008107648231089115}, {"id": 187, "seek": 244380, "start": 2443.8,
  "end": 2462.8, "text": " And there were so many examples where NLP engineers thought
  they they knew what people were were searching but after these kind of demo sessions
  or like sharing it I want to see what what people actually do there.", "tokens":
  [50364, 400, 456, 645, 370, 867, 5110, 689, 426, 45196, 11955, 1194, 436, 436, 2586,
  437, 561, 645, 645, 10808, 457, 934, 613, 733, 295, 10723, 11081, 420, 411, 5414,
  309, 286, 528, 281, 536, 437, 437, 561, 767, 360, 456, 13, 51314], "temperature":
  0.0, "avg_logprob": -0.3400594923231337, "compression_ratio": 1.4791666666666667,
  "no_speech_prob": 0.007221141830086708}, {"id": 188, "seek": 246280, "start": 2462.8,
  "end": 2473.8, "text": " And then they realized oh like they use a lot of key work
  queries or they never put a question mark at the end or they have a lot of misspellings
  what else.", "tokens": [50364, 400, 550, 436, 5334, 1954, 411, 436, 764, 257, 688,
  295, 2141, 589, 24109, 420, 436, 1128, 829, 257, 1168, 1491, 412, 264, 917, 420,
  436, 362, 257, 688, 295, 1713, 49241, 1109, 437, 1646, 13, 50914], "temperature":
  0.0, "avg_logprob": -0.20001360949348002, "compression_ratio": 1.5681818181818181,
  "no_speech_prob": 0.14753645658493042}, {"id": 189, "seek": 246280, "start": 2473.8,
  "end": 2482.8, "text": " So I think there''s a lot of early learnings that you can
  make as a developer from these demos and understanding it out.", "tokens": [50914,
  407, 286, 519, 456, 311, 257, 688, 295, 2440, 2539, 82, 300, 291, 393, 652, 382,
  257, 10754, 490, 613, 33788, 293, 3701, 309, 484, 13, 51364], "temperature": 0.0,
  "avg_logprob": -0.20001360949348002, "compression_ratio": 1.5681818181818181, "no_speech_prob":
  0.14753645658493042}, {"id": 190, "seek": 248280, "start": 2482.8, "end": 2495.8,
  "text": " And also I think on the other side just creating this early aha moment
  this kind of wow effect and some trust on the end user side is also crucial.", "tokens":
  [50364, 400, 611, 286, 519, 322, 264, 661, 1252, 445, 4084, 341, 2440, 47340, 1623,
  341, 733, 295, 6076, 1802, 293, 512, 3361, 322, 264, 917, 4195, 1252, 307, 611,
  11462, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17290538339053882, "compression_ratio":
  1.6837606837606838, "no_speech_prob": 0.03404049575328827}, {"id": 191, "seek":
  248280, "start": 2495.8, "end": 2511.8, "text": " So I would say that''s a cycle
  one point very early demo getting this initial feedback and then probably the second
  point that we see often is when you then had a time of running your experiments
  tuning your pipeline kind of the way to production.", "tokens": [51014, 407, 286,
  576, 584, 300, 311, 257, 6586, 472, 935, 588, 2440, 10723, 1242, 341, 5883, 5824,
  293, 550, 1391, 264, 1150, 935, 300, 321, 536, 2049, 307, 562, 291, 550, 632, 257,
  565, 295, 2614, 428, 12050, 15164, 428, 15517, 733, 295, 264, 636, 281, 4265, 13,
  51814], "temperature": 0.0, "avg_logprob": -0.17290538339053882, "compression_ratio":
  1.6837606837606838, "no_speech_prob": 0.03404049575328827}, {"id": 192, "seek":
  251180, "start": 2511.8, "end": 2527.8, "text": " I think then at some point a second
  phase where you you just do again some manual evaluation with end user so not completely
  relying on on machine learning metrics.", "tokens": [50364, 286, 519, 550, 412,
  512, 935, 257, 1150, 5574, 689, 291, 291, 445, 360, 797, 512, 9688, 13344, 365,
  917, 4195, 370, 406, 2584, 24140, 322, 322, 3479, 2539, 16367, 13, 51164], "temperature":
  0.0, "avg_logprob": -0.25162489754813055, "compression_ratio": 1.3666666666666667,
  "no_speech_prob": 0.003285330254584551}, {"id": 193, "seek": 252780, "start": 2527.8,
  "end": 2545.8, "text": " Because we think there''s some kind of metric blindness
  in the industry sometimes you just kind of get obsessed with your one metric that
  you optimize in these experiments and whatever it is just increasing it from experiment
  experiment.", "tokens": [50364, 1436, 321, 519, 456, 311, 512, 733, 295, 20678,
  46101, 294, 264, 3518, 2171, 291, 445, 733, 295, 483, 16923, 365, 428, 472, 20678,
  300, 291, 19719, 294, 613, 12050, 293, 2035, 309, 307, 445, 5662, 309, 490, 5120,
  5120, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14726734161376953, "compression_ratio":
  1.5945945945945945, "no_speech_prob": 0.16876477003097534}, {"id": 194, "seek":
  254580, "start": 2545.8, "end": 2557.8, "text": " And you go to production and you
  realize wow okay this metric is doesn''t say say anything about the user set of
  satisfaction that I have in the end.", "tokens": [50364, 400, 291, 352, 281, 4265,
  293, 291, 4325, 6076, 1392, 341, 20678, 307, 1177, 380, 584, 584, 1340, 466, 264,
  4195, 992, 295, 18715, 300, 286, 362, 294, 264, 917, 13, 50964], "temperature":
  0.0, "avg_logprob": -0.21130960052077835, "compression_ratio": 1.59375, "no_speech_prob":
  0.08091925084590912}, {"id": 195, "seek": 254580, "start": 2557.8, "end": 2574.8,
  "text": " And there are so many examples from our customers where just handing out
  this pipeline showing kind of like search queries and results and then collecting
  some easy kind of thumbs up thumbs down feedback and.", "tokens": [50964, 400, 456,
  366, 370, 867, 5110, 490, 527, 4581, 689, 445, 34774, 484, 341, 15517, 4099, 733,
  295, 411, 3164, 24109, 293, 3542, 293, 550, 12510, 512, 1858, 733, 295, 8838, 493,
  8838, 760, 5824, 293, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21130960052077835,
  "compression_ratio": 1.59375, "no_speech_prob": 0.08091925084590912}, {"id": 196,
  "seek": 257480, "start": 2574.8, "end": 2592.8, "text": " And then trying to correlate
  is that really what we also saw in our experiments in our metrics and in the thing
  in many cases was that either the pipeline was not yet ready for production and
  they were like it''s.", "tokens": [50364, 400, 550, 1382, 281, 48742, 307, 300,
  534, 437, 321, 611, 1866, 294, 527, 12050, 294, 527, 16367, 293, 294, 264, 551,
  294, 867, 3331, 390, 300, 2139, 264, 15517, 390, 406, 1939, 1919, 337, 4265, 293,
  436, 645, 411, 309, 311, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2779214940172561,
  "compression_ratio": 1.50354609929078, "no_speech_prob": 0.0039782715030014515},
  {"id": 197, "seek": 259280, "start": 2592.8, "end": 2613.8, "text": " The far less
  accurate than we thought or also case where it was the other way around where teams
  thought are stuck we will never go beyond and like a for a for one score of 60%
  we do not here it''s it''s not working.", "tokens": [50364, 440, 1400, 1570, 8559,
  813, 321, 1194, 420, 611, 1389, 689, 309, 390, 264, 661, 636, 926, 689, 5491, 1194,
  366, 5541, 321, 486, 1128, 352, 4399, 293, 411, 257, 337, 257, 337, 472, 6175, 295,
  4060, 4, 321, 360, 406, 510, 309, 311, 309, 311, 406, 1364, 13, 51414], "temperature":
  0.0, "avg_logprob": -0.40870486565355985, "compression_ratio": 1.4758620689655173,
  "no_speech_prob": 0.21634607017040253}, {"id": 198, "seek": 261380, "start": 2613.8,
  "end": 2623.8, "text": " And they kind of handed out this this predictions or like
  get this demo and then people actually don''t like notes like these predictions
  are perfectly fine.", "tokens": [50364, 400, 436, 733, 295, 16013, 484, 341, 341,
  21264, 420, 411, 483, 341, 10723, 293, 550, 561, 767, 500, 380, 411, 5570, 411,
  613, 21264, 366, 6239, 2489, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2570039524751551,
  "compression_ratio": 1.627659574468085, "no_speech_prob": 0.12585090100765228},
  {"id": 199, "seek": 261380, "start": 2623.8, "end": 2637.8, "text": " And when you
  then dig deeper I think it''s often that engineers not look enough into the data
  I think I''m just kind of rely on this high level metric.", "tokens": [50864, 400,
  562, 291, 550, 2528, 7731, 286, 519, 309, 311, 2049, 300, 11955, 406, 574, 1547,
  666, 264, 1412, 286, 519, 286, 478, 445, 733, 295, 10687, 322, 341, 1090, 1496,
  20678, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2570039524751551, "compression_ratio":
  1.627659574468085, "no_speech_prob": 0.12585090100765228}, {"id": 200, "seek": 263780,
  "start": 2637.8, "end": 2640.8, "text": " And the thing especially nowadays.", "tokens":
  [50364, 400, 264, 551, 2318, 13434, 13, 50514], "temperature": 0.0, "avg_logprob":
  -0.29621750011778714, "compression_ratio": 1.5029940119760479, "no_speech_prob":
  0.00845425482839346}, {"id": 201, "seek": 263780, "start": 2640.8, "end": 2650.8,
  "text": " These metrics only tell the part of the story because you''re like for
  question answering also for search.", "tokens": [50514, 1981, 16367, 787, 980, 264,
  644, 295, 264, 1657, 570, 291, 434, 411, 337, 1168, 13430, 611, 337, 3164, 13, 51014],
  "temperature": 0.0, "avg_logprob": -0.29621750011778714, "compression_ratio": 1.5029940119760479,
  "no_speech_prob": 0.00845425482839346}, {"id": 202, "seek": 263780, "start": 2650.8,
  "end": 2661.8, "text": " If you have a relation data set and let''s say you always
  label the exact answer for certain question or query.", "tokens": [51014, 759, 291,
  362, 257, 9721, 1412, 992, 293, 718, 311, 584, 291, 1009, 7645, 264, 1900, 1867,
  337, 1629, 1168, 420, 14581, 13, 51564], "temperature": 0.0, "avg_logprob": -0.29621750011778714,
  "compression_ratio": 1.5029940119760479, "no_speech_prob": 0.00845425482839346},
  {"id": 203, "seek": 266180, "start": 2661.8, "end": 2665.8, "text": " There''s just
  so many ways how you know.", "tokens": [50364, 821, 311, 445, 370, 867, 2098, 577,
  291, 458, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23199141515444402, "compression_ratio":
  1.5441176470588236, "no_speech_prob": 0.00421946682035923}, {"id": 204, "seek":
  266180, "start": 2665.8, "end": 2673.8, "text": " Can give a correct answer for
  for question that is different to this label so to give an example.", "tokens":
  [50564, 1664, 976, 257, 3006, 1867, 337, 337, 1168, 300, 307, 819, 281, 341, 7645,
  370, 281, 976, 364, 1365, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23199141515444402,
  "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.00421946682035923},
  {"id": 205, "seek": 266180, "start": 2673.8, "end": 2678.8, "text": " And we have
  many customers financially domain so typical question there is.", "tokens": [50964,
  400, 321, 362, 867, 4581, 20469, 9274, 370, 7476, 1168, 456, 307, 13, 51214], "temperature":
  0.0, "avg_logprob": -0.23199141515444402, "compression_ratio": 1.5441176470588236,
  "no_speech_prob": 0.00421946682035923}, {"id": 206, "seek": 266180, "start": 2678.8,
  "end": 2686.8, "text": " How will revenue evolve next year and maybe in your data
  set and the evaluation data set you labeled.", "tokens": [51214, 1012, 486, 9324,
  16693, 958, 1064, 293, 1310, 294, 428, 1412, 992, 293, 264, 13344, 1412, 992, 291,
  21335, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23199141515444402, "compression_ratio":
  1.5441176470588236, "no_speech_prob": 0.00421946682035923}, {"id": 207, "seek":
  268680, "start": 2686.8, "end": 2690.8, "text": " It will increase by 12%.", "tokens":
  [50364, 467, 486, 3488, 538, 2272, 6856, 50564], "temperature": 0.0, "avg_logprob":
  -0.17184983662196568, "compression_ratio": 1.3032786885245902, "no_speech_prob":
  0.00619047274813056}, {"id": 208, "seek": 268680, "start": 2690.8, "end": 2703.8,
  "text": " And now at the prediction time your model maybe finds another passage
  or generates the answer and says it will significantly increase.", "tokens": [50564,
  400, 586, 412, 264, 17630, 565, 428, 2316, 1310, 10704, 1071, 11497, 420, 23815,
  264, 1867, 293, 1619, 309, 486, 10591, 3488, 13, 51214], "temperature": 0.0, "avg_logprob":
  -0.17184983662196568, "compression_ratio": 1.3032786885245902, "no_speech_prob":
  0.00619047274813056}, {"id": 209, "seek": 270380, "start": 2703.8, "end": 2716.8,
  "text": " So like there''s no overlap at all from a lexical side still both answers
  make sense and and are correct and we can probably debate now which one is more
  accurate.", "tokens": [50364, 407, 411, 456, 311, 572, 19959, 412, 439, 490, 257,
  476, 87, 804, 1252, 920, 1293, 6338, 652, 2020, 293, 293, 366, 3006, 293, 321, 393,
  1391, 7958, 586, 597, 472, 307, 544, 8559, 13, 51014], "temperature": 0.0, "avg_logprob":
  -0.16805486511765866, "compression_ratio": 1.5061728395061729, "no_speech_prob":
  0.10800694674253464}, {"id": 210, "seek": 270380, "start": 2716.8, "end": 2722.8,
  "text": " But in many cases there is they basically give the same same answer semantically.",
  "tokens": [51014, 583, 294, 867, 3331, 456, 307, 436, 1936, 976, 264, 912, 912,
  1867, 4361, 49505, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16805486511765866,
  "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.10800694674253464},
  {"id": 211, "seek": 272280, "start": 2722.8, "end": 2728.8, "text": " But they''re
  just formulated very differently and that''s where I would say traditional metrics
  fail.", "tokens": [50364, 583, 436, 434, 445, 48936, 588, 7614, 293, 300, 311, 689,
  286, 576, 584, 5164, 16367, 3061, 13, 50664], "temperature": 0.0, "avg_logprob":
  -0.23958090812929214, "compression_ratio": 1.4919786096256684, "no_speech_prob":
  0.06622686982154846}, {"id": 212, "seek": 272280, "start": 2728.8, "end": 2741.8,
  "text": " So yeah, we need better metrics and we basically did some research work
  on that and also part of the haystack where you can do like more semantic answer
  similarity or as a metric.", "tokens": [50664, 407, 1338, 11, 321, 643, 1101, 16367,
  293, 321, 1936, 630, 512, 2132, 589, 322, 300, 293, 611, 644, 295, 264, 4842, 372,
  501, 689, 291, 393, 360, 411, 544, 47982, 1867, 32194, 420, 382, 257, 20678, 13,
  51314], "temperature": 0.0, "avg_logprob": -0.23958090812929214, "compression_ratio":
  1.4919786096256684, "no_speech_prob": 0.06622686982154846}, {"id": 213, "seek":
  274180, "start": 2741.8, "end": 2760.8, "text": " But it''s of course also just
  I think looking at your data and looking at these predictions and seeing if they''re
  really wrong on or if they''re actually okay and maybe it''s some problem of metrics
  or you are labeling process where maybe you need to collect more different options
  that are okay.", "tokens": [50364, 583, 309, 311, 295, 1164, 611, 445, 286, 519,
  1237, 412, 428, 1412, 293, 1237, 412, 613, 21264, 293, 2577, 498, 436, 434, 534,
  2085, 322, 420, 498, 436, 434, 767, 1392, 293, 1310, 309, 311, 512, 1154, 295, 16367,
  420, 291, 366, 40244, 1399, 689, 1310, 291, 643, 281, 2500, 544, 819, 3956, 300,
  366, 1392, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16979574388073337,
  "compression_ratio": 1.6368715083798884, "no_speech_prob": 0.10116980969905853},
  {"id": 214, "seek": 276080, "start": 2760.8, "end": 2779.8, "text": " Yeah, I totally
  agree it''s like it''s it''s a challenge of intersecting user language with whatever
  machinery you have to answer that right be it''s part search be dense search doesn''t
  matter like users don''t care what they care is that their language is understood
  and often enough it''s not.", "tokens": [50364, 865, 11, 286, 3879, 3986, 309, 311,
  411, 309, 311, 309, 311, 257, 3430, 295, 27815, 278, 4195, 2856, 365, 2035, 27302,
  291, 362, 281, 1867, 300, 558, 312, 309, 311, 644, 3164, 312, 18011, 3164, 1177,
  380, 1871, 411, 5022, 500, 380, 1127, 437, 436, 1127, 307, 300, 641, 2856, 307,
  7320, 293, 2049, 1547, 309, 311, 406, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.1133083701133728, "compression_ratio": 1.5879120879120878, "no_speech_prob":
  0.009111515246331692}, {"id": 215, "seek": 277980, "start": 2779.8, "end": 2790.8,
  "text": " Especially around things like bird if we go dance bird model doesn''t
  understand engagements right there was a research paper on that and that might actually
  harm.", "tokens": [50364, 8545, 926, 721, 411, 5255, 498, 321, 352, 4489, 5255,
  2316, 1177, 380, 1223, 44978, 558, 456, 390, 257, 2132, 3035, 322, 300, 293, 300,
  1062, 767, 6491, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13662292843773252,
  "compression_ratio": 1.564516129032258, "no_speech_prob": 0.23053868114948273},
  {"id": 216, "seek": 277980, "start": 2790.8, "end": 2800.8, "text": " There was
  even a Google example where it''s showing the opposite like you say I don''t want
  that but they say yes you actually do.", "tokens": [50914, 821, 390, 754, 257, 3329,
  1365, 689, 309, 311, 4099, 264, 6182, 411, 291, 584, 286, 500, 380, 528, 300, 457,
  436, 584, 2086, 291, 767, 360, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13662292843773252,
  "compression_ratio": 1.564516129032258, "no_speech_prob": 0.23053868114948273},
  {"id": 217, "seek": 280080, "start": 2800.8, "end": 2805.8, "text": " And then take
  that medicine which might be harmful.", "tokens": [50364, 400, 550, 747, 300, 7195,
  597, 1062, 312, 19727, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15870278222220285,
  "compression_ratio": 1.6319018404907975, "no_speech_prob": 0.10229014605283737},
  {"id": 218, "seek": 280080, "start": 2805.8, "end": 2818.8, "text": " And then the
  metrics is essentially what I get from what you just described essentially you might
  have offline metrics right let''s say and DCG or precision or recall whatever and
  then you have online metrics right.", "tokens": [50614, 400, 550, 264, 16367, 307,
  4476, 437, 286, 483, 490, 437, 291, 445, 7619, 4476, 291, 1062, 362, 21857, 16367,
  558, 718, 311, 584, 293, 9114, 38, 420, 18356, 420, 9901, 2035, 293, 550, 291, 362,
  2950, 16367, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15870278222220285,
  "compression_ratio": 1.6319018404907975, "no_speech_prob": 0.10229014605283737},
  {"id": 219, "seek": 281880, "start": 2818.8, "end": 2836.8, "text": " And actually
  crafting the online metrics is is also our an art and it''s never ending journey
  and just recently I came across one blog post which was shared by a former Netflix
  engineer.", "tokens": [50364, 400, 767, 29048, 264, 2950, 16367, 307, 307, 611,
  527, 364, 1523, 293, 309, 311, 1128, 8121, 4671, 293, 445, 3938, 286, 1361, 2108,
  472, 6968, 2183, 597, 390, 5507, 538, 257, 5819, 12778, 11403, 13, 51264], "temperature":
  0.0, "avg_logprob": -0.14310580492019653, "compression_ratio": 1.3214285714285714,
  "no_speech_prob": 0.1011769026517868}, {"id": 220, "seek": 283680, "start": 2836.8,
  "end": 2861.8, "text": " I will make sure to link it in the show notes as well describing
  click residual metric right so it''s what is you expected success on on on on that
  let''s say segment of your market whatever on the queries versus what you got and
  then people still keep trying and trying and trying but just doesn''t deliver so
  you could have these as a low hanging fruit to fix your system right and so.", "tokens":
  [50364, 286, 486, 652, 988, 281, 2113, 309, 294, 264, 855, 5570, 382, 731, 16141,
  2052, 27980, 20678, 558, 370, 309, 311, 437, 307, 291, 5176, 2245, 322, 322, 322,
  322, 300, 718, 311, 584, 9469, 295, 428, 2142, 2035, 322, 264, 24109, 5717, 437,
  291, 658, 293, 550, 561, 920, 1066, 1382, 293, 1382, 293, 1382, 457, 445, 1177,
  380, 4239, 370, 291, 727, 362, 613, 382, 257, 2295, 8345, 6773, 281, 3191, 428,
  1185, 558, 293, 370, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11219907094197101,
  "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.3428858816623688},
  {"id": 221, "seek": 286180, "start": 2862.8, "end": 2885.8, "text": " Do you see
  that maybe that''s already happening in haystack or do you see that that might happen
  that I as a user might be able to describe my metric let''s say in the form of Python
  or JavaScript code whatever plug it into haystack and let it measure what I want
  and kind of mimic the online metric in substance.", "tokens": [50414, 1144, 291,
  536, 300, 1310, 300, 311, 1217, 2737, 294, 4842, 372, 501, 420, 360, 291, 536, 300,
  300, 1062, 1051, 300, 286, 382, 257, 4195, 1062, 312, 1075, 281, 6786, 452, 20678,
  718, 311, 584, 294, 264, 1254, 295, 15329, 420, 15778, 3089, 2035, 5452, 309, 666,
  4842, 372, 501, 293, 718, 309, 3481, 437, 286, 528, 293, 733, 295, 31075, 264, 2950,
  20678, 294, 12961, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12431187099880642,
  "compression_ratio": 1.61139896373057, "no_speech_prob": 0.015542225912213326},
  {"id": 222, "seek": 288580, "start": 2885.8, "end": 2909.8, "text": " So I think
  like providing kind of custom metrics yeah yeah yeah yeah and you can can can do
  that to some degree already like plugging in basically like a Python function and
  forwarding it that''s the one way I think the other is probably on a on a note level
  so you can imagine this pipeline they''re providing at some point", "tokens": [50364,
  407, 286, 519, 411, 6530, 733, 295, 2375, 16367, 1338, 1338, 1338, 1338, 293, 291,
  393, 393, 393, 360, 300, 281, 512, 4314, 1217, 411, 42975, 294, 1936, 411, 257,
  15329, 2445, 293, 2128, 278, 309, 300, 311, 264, 472, 636, 286, 519, 264, 661, 307,
  1391, 322, 257, 322, 257, 3637, 1496, 370, 291, 393, 3811, 341, 15517, 436, 434,
  6530, 412, 512, 935, 51564], "temperature": 0.0, "avg_logprob": -0.3537139339723449,
  "compression_ratio": 1.7127659574468086, "no_speech_prob": 0.13700120151042938},
  {"id": 223, "seek": 290980, "start": 2909.8, "end": 2936.8, "text": " that you can
  have a lot of connections be it answers or documents so you can also easily kind
  of add custom notes various have like this this no check now I''ll compare it to
  whatever you want or like maybe on an online setting kind of write some locks somewhere
  like take some some signals from from from the early query", "tokens": [50364, 300,
  291, 393, 362, 257, 688, 295, 9271, 312, 309, 6338, 420, 8512, 370, 291, 393, 611,
  3612, 733, 295, 909, 2375, 5570, 3683, 362, 411, 341, 341, 572, 1520, 586, 286,
  603, 6794, 309, 281, 2035, 291, 528, 420, 411, 1310, 322, 364, 2950, 3287, 733,
  295, 2464, 512, 20703, 4079, 411, 747, 512, 512, 12354, 490, 490, 490, 264, 2440,
  14581, 51714], "temperature": 0.4, "avg_logprob": -0.6333732604980469, "compression_ratio":
  1.675392670157068, "no_speech_prob": 0.08251698315143585}, {"id": 224, "seek": 293680,
  "start": 2936.8, "end": 2966.76, "text": " to an extensive the way you can monitor
  it. So yeah I think there''s that''s probably one of the kind of next steps where
  we see it''s more and more online metrics more and more online experiments I would
  say right now where we see big parts of the market I think that''s the more in that
  phase of developing experimenting finding the pipeline getting it initially to production
  and having", "tokens": [50364, 281, 364, 13246, 264, 636, 291, 393, 6002, 309, 13,
  407, 1338, 286, 519, 456, 311, 300, 311, 1391, 472, 295, 264, 733, 295, 958, 4439,
  689, 321, 536, 309, 311, 544, 293, 544, 2950, 16367, 544, 293, 544, 2950, 12050,
  286, 576, 584, 558, 586, 689, 321, 536, 955, 3166, 295, 264, 2142, 286, 519, 300,
  311, 264, 544, 294, 300, 5574, 295, 6416, 29070, 5006, 264, 15517, 1242, 309, 9105,
  281, 4265, 293, 1419, 51862], "temperature": 0.0, "avg_logprob": -0.26680656626254695,
  "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.05011191591620445},
  {"id": 225, "seek": 296680, "start": 2966.8, "end": 2981.8, "text": " their radio
  would say smooth journey and having a fast path to production kind of high success
  rates for these projects and I would say it''s very right now focused on more.",
  "tokens": [50364, 641, 6477, 576, 584, 5508, 4671, 293, 1419, 257, 2370, 3100, 281,
  4265, 733, 295, 1090, 2245, 6846, 337, 613, 4455, 293, 286, 576, 584, 309, 311,
  588, 558, 586, 5178, 322, 544, 13, 51114], "temperature": 0.0, "avg_logprob": -0.34129288322047185,
  "compression_ratio": 1.376, "no_speech_prob": 0.0011878025252372026}, {"id": 226,
  "seek": 298180, "start": 2981.8, "end": 3009.8, "text": " But yeah I would say further
  down the road if you really think about the whole and add up life cycle I think
  on the monitoring side there''s this logic and one online metrics but also then
  things like data drift my queries actually shift into a different direction to these
  things a lot of our query profiles and we think like what I actually these use case
  how how are how can we describe this", "tokens": [50364, 583, 1338, 286, 576, 584,
  3052, 760, 264, 3060, 498, 291, 534, 519, 466, 264, 1379, 293, 909, 493, 993, 6586,
  286, 519, 322, 264, 11028, 1252, 456, 311, 341, 9952, 293, 472, 2950, 16367, 457,
  611, 550, 721, 411, 1412, 19699, 452, 24109, 767, 5513, 666, 257, 819, 3513, 281,
  613, 721, 257, 688, 295, 527, 14581, 23693, 293, 321, 519, 411, 437, 286, 767, 613,
  764, 1389, 577, 577, 366, 577, 393, 321, 6786, 341, 51764], "temperature": 0.0,
  "avg_logprob": -0.4757468612105758, "compression_ratio": 1.7161572052401746, "no_speech_prob":
  0.7357689142227173}, {"id": 227, "seek": 300980, "start": 3009.8, "end": 3032.8,
  "text": " query distribution and this can be on a formal level like say again questions
  for those keyboard queries but could be also on a topic level to understand what
  is a profile at point a I mean we can match it with certain pipelines but also is
  that kind of changing over time.", "tokens": [50364, 14581, 7316, 293, 341, 393,
  312, 322, 257, 9860, 1496, 411, 584, 797, 1651, 337, 729, 10186, 24109, 457, 727,
  312, 611, 322, 257, 4829, 1496, 281, 1223, 437, 307, 257, 7964, 412, 935, 257, 286,
  914, 321, 393, 2995, 309, 365, 1629, 40168, 457, 611, 307, 300, 733, 295, 4473,
  670, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.31107313879605, "compression_ratio":
  1.5423728813559323, "no_speech_prob": 0.020919276401400566}, {"id": 228, "seek":
  303280, "start": 3033.4, "end": 3057.8, "text": " Yeah yeah you you somewhat anticipate
  like expected my question or sort of partly answered my question and my next question
  about where do you see the biggest effort in haystack and and deep set cloud going
  let''s say beyond ML ops you know tightening the knobs and making sure that this
  flies and works correctly.", "tokens": [50394, 865, 1338, 291, 291, 8344, 21685,
  411, 5176, 452, 1168, 420, 1333, 295, 17031, 10103, 452, 1168, 293, 452, 958, 1168,
  466, 689, 360, 291, 536, 264, 3880, 4630, 294, 4842, 372, 501, 293, 293, 2452, 992,
  4588, 516, 718, 311, 584, 4399, 21601, 44663, 291, 458, 42217, 264, 46999, 293,
  1455, 988, 300, 341, 17414, 293, 1985, 8944, 13, 51614], "temperature": 0.0, "avg_logprob":
  -0.2557430565357208, "compression_ratio": 1.6051282051282052, "no_speech_prob":
  0.07559827715158463}, {"id": 229, "seek": 305780, "start": 3057.8, "end": 3087.6000000000004,
  "text": " More towards I know you guys also hiring a product manager so sort of
  like more on vision side and connected to that if you will what do you think is
  missing on the market today still maybe in understanding maybe in perception level
  maybe in tooling you already alluded also to things like metric blindness right
  and and and maybe when users get stuck and thinking that this is a wrong system
  but actually it''s not they just didn''t look the right way and things like that.",
  "tokens": [50394, 5048, 3030, 286, 458, 291, 1074, 611, 15335, 257, 1674, 6598,
  370, 1333, 295, 411, 544, 322, 5201, 1252, 293, 4582, 281, 300, 498, 291, 486, 437,
  360, 291, 519, 307, 5361, 322, 264, 2142, 965, 920, 1310, 294, 3701, 1310, 294,
  12860, 1496, 1310, 294, 46593, 291, 1217, 33919, 611, 281, 721, 411, 20678, 46101,
  558, 293, 293, 293, 1310, 562, 5022, 483, 5541, 293, 1953, 300, 341, 307, 257, 2085,
  1185, 457, 767, 309, 311, 406, 436, 445, 994, 380, 574, 264, 558, 636, 293, 721,
  411, 300, 13, 51854], "temperature": 0.0, "avg_logprob": -0.13475291272427173, "compression_ratio":
  1.7481481481481482, "no_speech_prob": 0.011465130373835564}, {"id": 230, "seek":
  308780, "start": 3088.4, "end": 3109.8, "text": " Yeah and there''s I think the
  ton of works to left I think we are we already talked about it I think things progressed
  a lot in the last years it''s crazy to see but still I feel it''s with the in the
  middle of it or just starting and so much more work and things you can improve and
  do better.", "tokens": [50394, 865, 293, 456, 311, 286, 519, 264, 2952, 295, 1985,
  281, 1411, 286, 519, 321, 366, 321, 1217, 2825, 466, 309, 286, 519, 721, 36789,
  257, 688, 294, 264, 1036, 924, 309, 311, 3219, 281, 536, 457, 920, 286, 841, 309,
  311, 365, 264, 294, 264, 2808, 295, 309, 420, 445, 2891, 293, 370, 709, 544, 589,
  293, 721, 291, 393, 3470, 293, 360, 1101, 13, 51464], "temperature": 0.0, "avg_logprob":
  -0.2778404780796596, "compression_ratio": 1.6166666666666667, "no_speech_prob":
  0.011329004541039467}, {"id": 231, "seek": 310980, "start": 3110.8, "end": 3129.8,
  "text": " Yeah I would say for us right now there''s like a lot of different directions
  but I think especially on the on the open source side we want to improve the developer
  experience also like simplifying the first steps within haystack I think it can
  be still overwhelming and I really want to make sure that", "tokens": [50414, 865,
  286, 576, 584, 337, 505, 558, 586, 456, 311, 411, 257, 688, 295, 819, 11095, 457,
  286, 519, 2318, 322, 264, 322, 264, 1269, 4009, 1252, 321, 528, 281, 3470, 264,
  10754, 1752, 611, 411, 6883, 5489, 264, 700, 4439, 1951, 4842, 372, 501, 286, 519,
  309, 393, 312, 920, 13373, 293, 286, 534, 528, 281, 652, 988, 300, 51364], "temperature":
  0.0, "avg_logprob": -0.18127349019050598, "compression_ratio": 1.5759162303664922,
  "no_speech_prob": 0.13318483531475067}, {"id": 232, "seek": 312980, "start": 3129.8,
  "end": 3145.8, "text": " get as many people to the first aha moment like using all
  your own data asking a few questions comparing sparse to dense retrieval and really
  experiencing this first hand I think this is one of the things we work on.", "tokens":
  [50364, 483, 382, 867, 561, 281, 264, 700, 47340, 1623, 411, 1228, 439, 428, 1065,
  1412, 3365, 257, 1326, 1651, 15763, 637, 11668, 281, 18011, 19817, 3337, 293, 534,
  11139, 341, 700, 1011, 286, 519, 341, 307, 472, 295, 264, 721, 321, 589, 322, 13,
  51164], "temperature": 0.0, "avg_logprob": -0.3492608865102132, "compression_ratio":
  1.4896551724137932, "no_speech_prob": 0.09030165523290634}, {"id": 233, "seek":
  314580, "start": 3145.8, "end": 3174.8, "text": " Then a lot around multi model
  so we recently added support for tables within haystack so I think one interesting
  direction right now that you can really query into these kind of tables in your
  documents but maybe also further down the road into your SQL database as another
  data source and then of course everything around images videos audio and it''s also
  interesting for us I think for our customers.", "tokens": [50364, 1396, 257, 688,
  926, 4825, 2316, 370, 321, 3938, 3869, 1406, 337, 8020, 1951, 4842, 372, 501, 370,
  286, 519, 472, 1880, 3513, 558, 586, 300, 291, 393, 534, 14581, 666, 613, 733, 295,
  8020, 294, 428, 8512, 457, 1310, 611, 3052, 760, 264, 3060, 666, 428, 19200, 8149,
  382, 1071, 1412, 4009, 293, 550, 295, 1164, 1203, 926, 5267, 2145, 6278, 293, 309,
  311, 611, 1880, 337, 505, 286, 519, 337, 527, 4581, 13, 51814], "temperature": 0.0,
  "avg_logprob": -0.17817988762488732, "compression_ratio": 1.654320987654321, "no_speech_prob":
  0.40422317385673523}, {"id": 234, "seek": 317580, "start": 3175.8, "end": 3185.8,
  "text": " Because it''s typically less important than kind of tax in tables but
  still I think it''s interesting interesting options that you can do there.", "tokens":
  [50364, 1436, 309, 311, 5850, 1570, 1021, 813, 733, 295, 3366, 294, 8020, 457, 920,
  286, 519, 309, 311, 1880, 1880, 3956, 300, 291, 393, 360, 456, 13, 50864], "temperature":
  0.0, "avg_logprob": -0.23722707489390432, "compression_ratio": 1.646808510638298,
  "no_speech_prob": 0.0018835271475836635}, {"id": 235, "seek": 317580, "start": 3185.8,
  "end": 3204.8, "text": " So yeah I think that''s like a lot on on open source side
  and deep set cloud are we recently launched basically the experiments module that
  was one big step forward there and now it''s a lot around giving there also guidance
  and suggestions like.", "tokens": [50864, 407, 1338, 286, 519, 300, 311, 411, 257,
  688, 322, 322, 1269, 4009, 1252, 293, 2452, 992, 4588, 366, 321, 3938, 8730, 1936,
  264, 12050, 10088, 300, 390, 472, 955, 1823, 2128, 456, 293, 586, 309, 311, 257,
  688, 926, 2902, 456, 611, 10056, 293, 13396, 411, 13, 51814], "temperature": 0.0,
  "avg_logprob": -0.23722707489390432, "compression_ratio": 1.646808510638298, "no_speech_prob":
  0.0018835271475836635}, {"id": 236, "seek": 320480, "start": 3204.8, "end": 3233.8,
  "text": " Like for example now I have the experiment I ran an experiment I''ve like
  a lot of these metrics I have a lot of data that was somehow generated but as it''s
  not a single model anymore it''s like a pipeline I really want to understand as
  a data scientist okay like where should I not focus on or like where what''s probably
  a good way forward to improve this pipeline is a rather the retrieval problem is
  a rather.", "tokens": [50364, 1743, 337, 1365, 586, 286, 362, 264, 5120, 286, 5872,
  364, 5120, 286, 600, 411, 257, 688, 295, 613, 16367, 286, 362, 257, 688, 295, 1412,
  300, 390, 6063, 10833, 457, 382, 309, 311, 406, 257, 2167, 2316, 3602, 309, 311,
  411, 257, 15517, 286, 534, 528, 281, 1223, 382, 257, 1412, 12662, 1392, 411, 689,
  820, 286, 406, 1879, 322, 420, 411, 689, 437, 311, 1391, 257, 665, 636, 2128, 281,
  3470, 341, 15517, 307, 257, 2831, 264, 19817, 3337, 1154, 307, 257, 2831, 13, 51814],
  "temperature": 0.0, "avg_logprob": -0.23560462527804904, "compression_ratio": 1.7446808510638299,
  "no_speech_prob": 0.010290063917636871}, {"id": 237, "seek": 323380, "start": 3233.8,
  "end": 3246.8, "text": " Another note that I should improve is maybe something wrong
  with my evaluation data set should I go back to labeling and like giving these kind
  of at least making these kind of analysis easier.", "tokens": [50364, 3996, 3637,
  300, 286, 820, 3470, 307, 1310, 746, 2085, 365, 452, 13344, 1412, 992, 820, 286,
  352, 646, 281, 40244, 293, 411, 2902, 613, 733, 295, 412, 1935, 1455, 613, 733,
  295, 5215, 3571, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2450826911516087,
  "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.0007211799384094775},
  {"id": 238, "seek": 323380, "start": 3246.8, "end": 3262.8, "text": " It''s something
  that we work on right now and then I think further down the road that will be for
  us a lot expanding in this world ML of life cycles what we talk about right monitoring
  without just making it simpler to integrate it at both ends so.", "tokens": [51014,
  467, 311, 746, 300, 321, 589, 322, 558, 586, 293, 550, 286, 519, 3052, 760, 264,
  3060, 300, 486, 312, 337, 505, 257, 688, 14702, 294, 341, 1002, 21601, 295, 993,
  17796, 437, 321, 751, 466, 558, 11028, 1553, 445, 1455, 309, 18587, 281, 13365,
  309, 412, 1293, 5314, 370, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2450826911516087,
  "compression_ratio": 1.6961538461538461, "no_speech_prob": 0.0007211799384094775},
  {"id": 239, "seek": 326280, "start": 3262.8, "end": 3279.8, "text": " Basically
  on the one side ingesting your source data more easily and thinking it more easily
  into into deep set cloud so that you can say I know either maybe I have a wiki system
  that I use maybe I don''t know I use notion or maybe I use.", "tokens": [50364,
  8537, 322, 264, 472, 1252, 3957, 8714, 428, 4009, 1412, 544, 3612, 293, 1953, 309,
  544, 3612, 666, 666, 2452, 992, 4588, 370, 300, 291, 393, 584, 286, 458, 2139, 1310,
  286, 362, 257, 261, 9850, 1185, 300, 286, 764, 1310, 286, 500, 380, 458, 286, 764,
  10710, 420, 1310, 286, 764, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1941745824981154,
  "compression_ratio": 1.6013513513513513, "no_speech_prob": 0.020814063027501106},
  {"id": 240, "seek": 327980, "start": 3279.8, "end": 3304.8, "text": " So I think
  it''s just a little bit more conflict or I have a not another elastic such class
  that we already my my documents that I''m interested in so we having there kind
  of smooth connectors that you can can import your data and directly work on it and
  then on the other end the API now how can I easily get now a kind of search bar
  or search functionality in my final product.", "tokens": [50364, 407, 286, 519,
  309, 311, 445, 257, 707, 857, 544, 6596, 420, 286, 362, 257, 406, 1071, 17115, 1270,
  1508, 300, 321, 1217, 452, 452, 8512, 300, 286, 478, 3102, 294, 370, 321, 1419,
  456, 733, 295, 5508, 31865, 300, 291, 393, 393, 974, 428, 1412, 293, 3838, 589,
  322, 309, 293, 550, 322, 264, 661, 917, 264, 9362, 586, 577, 393, 286, 3612, 483,
  586, 257, 733, 295, 3164, 2159, 420, 3164, 14980, 294, 452, 2572, 1674, 13, 51614],
  "temperature": 0.4, "avg_logprob": -0.5478533089879047, "compression_ratio": 1.6391304347826088,
  "no_speech_prob": 0.3419419825077057}, {"id": 241, "seek": 330480, "start": 3304.8,
  "end": 3330.8, "text": " So there''s a lot of things and then everything around
  fine tuning few short learning with large language models that something we are
  quite excited about because I think mentioned I think right now there''s already
  made a big step forward that you there are a lot of use cases where you don''t need
  to train at all anymore and then maybe that''s a misperception that you also see
  in the market.", "tokens": [50364, 407, 456, 311, 257, 688, 295, 721, 293, 550,
  1203, 926, 2489, 15164, 1326, 2099, 2539, 365, 2416, 2856, 5245, 300, 746, 321,
  366, 1596, 2919, 466, 570, 286, 519, 2835, 286, 519, 558, 586, 456, 311, 1217, 1027,
  257, 955, 1823, 2128, 300, 291, 456, 366, 257, 688, 295, 764, 3331, 689, 291, 500,
  380, 643, 281, 3847, 412, 439, 3602, 293, 550, 1310, 300, 311, 257, 3346, 610, 7311,
  300, 291, 611, 536, 294, 264, 2142, 13, 51664], "temperature": 0.0, "avg_logprob":
  -0.2031043868467032, "compression_ratio": 1.7074235807860263, "no_speech_prob":
  0.0442170575261116}, {"id": 242, "seek": 333080, "start": 3330.8, "end": 3359.8,
  "text": " I think to the typical users come to us and say like oh yeah this use
  case how can I train and then we usually ask did you really need to train your own
  model like have you tried this and that take these kind of combinations and kind
  of models that are out there certain sentence transformers certain pre trained QA
  models or rank models and that no no but like our use cases are different and that
  won''t work and.", "tokens": [50364, 286, 519, 281, 264, 7476, 5022, 808, 281, 505,
  293, 584, 411, 1954, 1338, 341, 764, 1389, 577, 393, 286, 3847, 293, 550, 321, 2673,
  1029, 630, 291, 534, 643, 281, 3847, 428, 1065, 2316, 411, 362, 291, 3031, 341,
  293, 300, 747, 613, 733, 295, 21267, 293, 733, 295, 5245, 300, 366, 484, 456, 1629,
  8174, 4088, 433, 1629, 659, 8895, 1249, 32, 5245, 420, 6181, 5245, 293, 300, 572,
  572, 457, 411, 527, 764, 3331, 366, 819, 293, 300, 1582, 380, 589, 293, 13, 51814],
  "temperature": 0.0, "avg_logprob": -0.3410816616482205, "compression_ratio": 1.8157894736842106,
  "no_speech_prob": 0.02054857648909092}, {"id": 243, "seek": 335980, "start": 3359.8,
  "end": 3381.8, "text": " In many cases it does or at least they''re surprised how
  good it is already and maybe it''s enough to get started on it and so I think that''s
  one misperception still I think there are then also these cases to be fair where
  fine tuning still helps right and where you really care about if you.", "tokens":
  [50364, 682, 867, 3331, 309, 775, 420, 412, 1935, 436, 434, 6100, 577, 665, 309,
  307, 1217, 293, 1310, 309, 311, 1547, 281, 483, 1409, 322, 309, 293, 370, 286, 519,
  300, 311, 472, 3346, 610, 7311, 920, 286, 519, 456, 366, 550, 611, 613, 3331, 281,
  312, 3143, 689, 2489, 15164, 920, 3665, 558, 293, 689, 291, 534, 1127, 466, 498,
  291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14951805570232335, "compression_ratio":
  1.5846994535519126, "no_speech_prob": 0.032929182052612305}, {"id": 244, "seek":
  338180, "start": 3381.8, "end": 3410.8, "text": " So you can go to percentage points
  better accuracy and where you then go down and say let''s now start labeling let''s
  collect either like we in this manual labeling process or maybe from some more noisy
  maybe real time like a production data where you saw what people search what they
  clicked how can we use that maybe for training that''s something where we see big
  potential probably for next year.", "tokens": [50364, 407, 291, 393, 352, 281, 9668,
  2793, 1101, 14170, 293, 689, 291, 550, 352, 760, 293, 584, 718, 311, 586, 722, 40244,
  718, 311, 2500, 2139, 411, 321, 294, 341, 9688, 40244, 1399, 420, 1310, 490, 512,
  544, 24518, 1310, 957, 565, 411, 257, 4265, 1412, 689, 291, 1866, 437, 561, 3164,
  437, 436, 23370, 577, 393, 321, 764, 300, 1310, 337, 3097, 300, 311, 746, 689, 321,
  536, 955, 3995, 1391, 337, 958, 1064, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.2801186525368992, "compression_ratio": 1.7008547008547008, "no_speech_prob":
  0.3999885022640228}, {"id": 245, "seek": 341180, "start": 3411.8, "end": 3431.8,
  "text": " And basically want to simplify this domain adaptation to have less manual
  effort and basically more automated way of of training it and that I think was also
  that in the direction of maybe large language models.", "tokens": [50364, 400, 1936,
  528, 281, 20460, 341, 9274, 21549, 281, 362, 1570, 9688, 4630, 293, 1936, 544, 18473,
  636, 295, 295, 3097, 309, 293, 300, 286, 519, 390, 611, 300, 294, 264, 3513, 295,
  1310, 2416, 2856, 5245, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2421353885105678,
  "compression_ratio": 1.4652777777777777, "no_speech_prob": 0.00703832320868969},
  {"id": 246, "seek": 343180, "start": 3431.8, "end": 3456.8, "text": " Yeah sounds
  cool and if we go in even in look even further into the future would say I don''t
  know 5 10 years out do you think that haystack at some point may even start suggesting
  the user what to try you know if you go and set up a key PI for yourself right you
  end goal and then through the chain and that I see click graph it looks like finds
  a weak node and say yes something is going on there.", "tokens": [50364, 865, 3263,
  1627, 293, 498, 321, 352, 294, 754, 294, 574, 754, 3052, 666, 264, 2027, 576, 584,
  286, 500, 380, 458, 1025, 1266, 924, 484, 360, 291, 519, 300, 4842, 372, 501, 412,
  512, 935, 815, 754, 722, 18094, 264, 4195, 437, 281, 853, 291, 458, 498, 291, 352,
  293, 992, 493, 257, 2141, 27176, 337, 1803, 558, 291, 917, 3387, 293, 550, 807,
  264, 5021, 293, 300, 286, 536, 2052, 4295, 309, 1542, 411, 10704, 257, 5336, 9984,
  293, 584, 2086, 746, 307, 516, 322, 456, 13, 51614], "temperature": 0.0, "avg_logprob":
  -0.24444268339423722, "compression_ratio": 1.6446280991735538, "no_speech_prob":
  0.09969537705183029}, {"id": 247, "seek": 345680, "start": 3456.8, "end": 3471.8,
  "text": " Then it would actually suggest you also to try some other model do you
  think it''s possible or do you think it''s a wrong direction at all like to you
  drive and leave this to the creativity of your users.", "tokens": [50364, 1396,
  309, 576, 767, 3402, 291, 611, 281, 853, 512, 661, 2316, 360, 291, 519, 309, 311,
  1944, 420, 360, 291, 519, 309, 311, 257, 2085, 3513, 412, 439, 411, 281, 291, 3332,
  293, 1856, 341, 281, 264, 12915, 295, 428, 5022, 13, 51114], "temperature": 0.0,
  "avg_logprob": -0.16148829967417616, "compression_ratio": 1.4744525547445255, "no_speech_prob":
  0.009724569506943226}, {"id": 248, "seek": 347180, "start": 3471.8, "end": 3485.8,
  "text": " I think it''s a combination of both so I definitely think that helps to
  accelerate and certain parts of your work so especially I think suggesting what
  experiment to run next or what it could be something you can try.", "tokens": [50364,
  286, 519, 309, 311, 257, 6562, 295, 1293, 370, 286, 2138, 519, 300, 3665, 281, 21341,
  293, 1629, 3166, 295, 428, 589, 370, 2318, 286, 519, 18094, 437, 5120, 281, 1190,
  958, 420, 437, 309, 727, 312, 746, 291, 393, 853, 13, 51064], "temperature": 0.0,
  "avg_logprob": -0.2008727775825249, "compression_ratio": 1.598360655737705, "no_speech_prob":
  0.0762307271361351}, {"id": 249, "seek": 347180, "start": 3485.8, "end": 3496.8,
  "text": " So I''m a big fan of that and I think we don''t need to go probably like
  5 or 10 years down the road that is happening already sooner so I can and haystack
  and deep set cloud.", "tokens": [51064, 407, 286, 478, 257, 955, 3429, 295, 300,
  293, 286, 519, 321, 500, 380, 643, 281, 352, 1391, 411, 1025, 420, 1266, 924, 760,
  264, 3060, 300, 307, 2737, 1217, 15324, 370, 286, 393, 293, 4842, 372, 501, 293,
  2452, 992, 4588, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2008727775825249,
  "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0762307271361351}, {"id":
  250, "seek": 349680, "start": 3496.8, "end": 3525.8, "text": " And maybe just like
  one thing we are so we have our company something called Hockey Friday so it''s
  like one Friday every month where every person the company can work on whatever
  they want so really hacking on crazy ideas trying stuff out and I know that this
  Friday people are working on a generative model where you basically give in you
  describe what you want like what kind of pipeline so you can type in.", "tokens":
  [50364, 400, 1310, 445, 411, 472, 551, 321, 366, 370, 321, 362, 527, 2237, 746,
  1219, 389, 46164, 6984, 370, 309, 311, 411, 472, 6984, 633, 1618, 689, 633, 954,
  264, 2237, 393, 589, 322, 2035, 436, 528, 370, 534, 31422, 322, 3219, 3487, 1382,
  1507, 484, 293, 286, 458, 300, 341, 6984, 561, 366, 1364, 322, 257, 1337, 1166,
  2316, 689, 291, 1936, 976, 294, 291, 6786, 437, 291, 528, 411, 437, 733, 295, 15517,
  370, 291, 393, 2010, 294, 13, 51814], "temperature": 0.0, "avg_logprob": -0.23581998488482306,
  "compression_ratio": 1.718487394957983, "no_speech_prob": 0.14596544206142426},
  {"id": 251, "seek": 352580, "start": 3525.8, "end": 3551.8, "text": " And let''s
  say I want documents such pipeline that works on legal data that is very fast something
  like that and the output is basically a YAML file that describes this haystack pipeline
  which you can then easily kind of load and Python try out and also write a load
  and then deep set cloud and run it there.", "tokens": [50364, 400, 718, 311, 584,
  286, 528, 8512, 1270, 15517, 300, 1985, 322, 5089, 1412, 300, 307, 588, 2370, 746,
  411, 300, 293, 264, 5598, 307, 1936, 257, 398, 2865, 43, 3991, 300, 15626, 341,
  4842, 372, 501, 15517, 597, 291, 393, 550, 3612, 733, 295, 3677, 293, 15329, 853,
  484, 293, 611, 2464, 257, 3677, 293, 550, 2452, 992, 4588, 293, 1190, 309, 456,
  13, 51664], "temperature": 0.0, "avg_logprob": -0.15587123926135077, "compression_ratio":
  1.5505050505050506, "no_speech_prob": 0.00043805621680803597}, {"id": 252, "seek":
  355180, "start": 3551.8, "end": 3571.8, "text": " So that''s actually we are experimenting
  with right now and and of course some time for the down the road I could see that
  you can take also like signals from from what we know from what worked on certain
  domains and and basically fuse that in into this maybe a generative process.", "tokens":
  [50364, 407, 300, 311, 767, 321, 366, 29070, 365, 558, 586, 293, 293, 295, 1164,
  512, 565, 337, 264, 760, 264, 3060, 286, 727, 536, 300, 291, 393, 747, 611, 411,
  12354, 490, 490, 437, 321, 458, 490, 437, 2732, 322, 1629, 25514, 293, 293, 1936,
  31328, 300, 294, 666, 341, 1310, 257, 1337, 1166, 1399, 13, 51364], "temperature":
  0.0, "avg_logprob": -0.2281182289123535, "compression_ratio": 1.5555555555555556,
  "no_speech_prob": 0.018495267257094383}, {"id": 253, "seek": 357180, "start": 3571.8,
  "end": 3581.8, "text": " Yeah, it sounds cool actually reminded me of the time when
  I was doing my PhD something like 12 years ago a bit more.", "tokens": [50364, 865,
  11, 309, 3263, 1627, 767, 15920, 385, 295, 264, 565, 562, 286, 390, 884, 452, 14476,
  746, 411, 2272, 924, 2057, 257, 857, 544, 13, 50864], "temperature": 0.0, "avg_logprob":
  -0.13349807780721915, "compression_ratio": 1.6041666666666667, "no_speech_prob":
  0.046169932931661606}, {"id": 254, "seek": 357180, "start": 3581.8, "end": 3600.8,
  "text": " I had a collaborator who wrote a paper on taking taking the user text
  and converging that into C++ code and the use case I don''t remember exactly all
  the details of the use case but I remember it was some way in the airport so like
  they do a lot of this routine work.", "tokens": [50864, 286, 632, 257, 5091, 1639,
  567, 4114, 257, 3035, 322, 1940, 1940, 264, 4195, 2487, 293, 9652, 3249, 300, 666,
  383, 25472, 3089, 293, 264, 764, 1389, 286, 500, 380, 1604, 2293, 439, 264, 4365,
  295, 264, 764, 1389, 457, 286, 1604, 309, 390, 512, 636, 294, 264, 10155, 370, 411,
  436, 360, 257, 688, 295, 341, 9927, 589, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.13349807780721915, "compression_ratio": 1.6041666666666667, "no_speech_prob":
  0.046169932931661606}, {"id": 255, "seek": 360080, "start": 3600.8, "end": 3615.8,
  "text": " And instead of repeating it you could actually build a smarter system
  right so you think this could be the future of haystack or maybe the industry at
  large.", "tokens": [50364, 400, 2602, 295, 18617, 309, 291, 727, 767, 1322, 257,
  20294, 1185, 558, 370, 291, 519, 341, 727, 312, 264, 2027, 295, 4842, 372, 501,
  420, 1310, 264, 3518, 412, 2416, 13, 51114], "temperature": 0.0, "avg_logprob":
  -0.10084089967939588, "compression_ratio": 1.3652173913043477, "no_speech_prob":
  0.01292424462735653}, {"id": 256, "seek": 361580, "start": 3616.8, "end": 3633.8,
  "text": " Yeah, at least I think it''s like one if you want element that helps accelerating
  right so if you also if you look at the core pilot right now I like it a lot for
  calling and I''m still in many cases surprised what what co pilot suggests you''re
  as a as a note on the code level.", "tokens": [50414, 865, 11, 412, 1935, 286, 519,
  309, 311, 411, 472, 498, 291, 528, 4478, 300, 3665, 34391, 558, 370, 498, 291, 611,
  498, 291, 574, 412, 264, 4965, 9691, 558, 586, 286, 411, 309, 257, 688, 337, 5141,
  293, 286, 478, 920, 294, 867, 3331, 6100, 437, 437, 598, 9691, 13409, 291, 434,
  382, 257, 382, 257, 3637, 322, 264, 3089, 1496, 13, 51264], "temperature": 0.0,
  "avg_logprob": -0.29271122946668027, "compression_ratio": 1.5771428571428572, "no_speech_prob":
  0.10706516355276108}, {"id": 257, "seek": 363380, "start": 3633.8, "end": 3647.8,
  "text": " And I think something similar as also positive on the machine learning
  side and you are not only generates a correct code but really something that fits
  for for use case and to describe it.", "tokens": [50364, 400, 286, 519, 746, 2531,
  382, 611, 3353, 322, 264, 3479, 2539, 1252, 293, 291, 366, 406, 787, 23815, 257,
  3006, 3089, 457, 534, 746, 300, 9001, 337, 337, 764, 1389, 293, 281, 6786, 309,
  13, 51064], "temperature": 0.0, "avg_logprob": -0.26022115434919085, "compression_ratio":
  1.6210526315789473, "no_speech_prob": 0.009107493795454502}, {"id": 258, "seek":
  363380, "start": 3647.8, "end": 3654.8, "text": " I mean I think it''s like if you
  think about the big up picture I think it''s one piece that helps you in your workflow.",
  "tokens": [51064, 286, 914, 286, 519, 309, 311, 411, 498, 291, 519, 466, 264, 955,
  493, 3036, 286, 519, 309, 311, 472, 2522, 300, 3665, 291, 294, 428, 20993, 13, 51414],
  "temperature": 0.0, "avg_logprob": -0.26022115434919085, "compression_ratio": 1.6210526315789473,
  "no_speech_prob": 0.009107493795454502}, {"id": 259, "seek": 365480, "start": 3654.8,
  "end": 3664.8, "text": " I think it''s there''s still like many many other pieces
  that we need to get right and that won''t be that''s it a holy grail I think at
  the end.", "tokens": [50364, 286, 519, 309, 311, 456, 311, 920, 411, 867, 867, 661,
  3755, 300, 321, 643, 281, 483, 558, 293, 300, 1582, 380, 312, 300, 311, 309, 257,
  10622, 1295, 388, 286, 519, 412, 264, 917, 13, 50864], "temperature": 0.0, "avg_logprob":
  -0.22905368390290634, "compression_ratio": 1.7212389380530972, "no_speech_prob":
  0.022865615785121918}, {"id": 260, "seek": 365480, "start": 3664.8, "end": 3683.8,
  "text": " What I really believe in is that you need a framework or a platform where
  we want to call it where you can easily compare things on your data and and I think
  this helps a lot then and creating transparency in the market creating also like
  kind of.", "tokens": [50864, 708, 286, 534, 1697, 294, 307, 300, 291, 643, 257,
  8388, 420, 257, 3663, 689, 321, 528, 281, 818, 309, 689, 291, 393, 3612, 6794, 721,
  322, 428, 1412, 293, 293, 286, 519, 341, 3665, 257, 688, 550, 293, 4084, 17131,
  294, 264, 2142, 4084, 611, 411, 733, 295, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.22905368390290634, "compression_ratio": 1.7212389380530972, "no_speech_prob":
  0.022865615785121918}, {"id": 261, "seek": 368380, "start": 3683.8, "end": 3712.8,
  "text": " Trust for your own use case that you are not basically doing a technology
  choice before you actually started working on your use case and that I think holds
  for vector databases where maybe today this is a good choice for you but maybe I
  know one year down the road maybe you want to switch this I think this market is
  so early that it''s very hard to place a batch right now on one of these technologies
  and similarly I think this is on the model.", "tokens": [50364, 11580, 337, 428,
  1065, 764, 1389, 300, 291, 366, 406, 1936, 884, 257, 2899, 3922, 949, 291, 767,
  1409, 1364, 322, 428, 764, 1389, 293, 300, 286, 519, 9190, 337, 8062, 22380, 689,
  1310, 965, 341, 307, 257, 665, 3922, 337, 291, 457, 1310, 286, 458, 472, 1064, 760,
  264, 3060, 1310, 291, 528, 281, 3679, 341, 286, 519, 341, 2142, 307, 370, 2440,
  300, 309, 311, 588, 1152, 281, 1081, 257, 15245, 558, 586, 322, 472, 295, 613, 7943,
  293, 14138, 286, 519, 341, 307, 322, 264, 2316, 13, 51814], "temperature": 0.0,
  "avg_logprob": -0.19237743540013091, "compression_ratio": 1.8024193548387097, "no_speech_prob":
  0.013304518535733223}, {"id": 262, "seek": 371280, "start": 3712.8, "end": 3741.8,
  "text": " Modeling side there''s like so so much crazy bus around large language
  models and can firstly see the trend going there but it''s also I think very important
  to to understand if that''s really useful for your use case now how it compares
  to much smaller models and and this should be easy right this shouldn''t this shouldn''t
  be big part of your project it should be rather.", "tokens": [50364, 6583, 11031,
  1252, 456, 311, 411, 370, 370, 709, 3219, 1255, 926, 2416, 2856, 5245, 293, 393,
  27376, 536, 264, 6028, 516, 456, 457, 309, 311, 611, 286, 519, 588, 1021, 281, 281,
  1223, 498, 300, 311, 534, 4420, 337, 428, 764, 1389, 586, 577, 309, 38334, 281,
  709, 4356, 5245, 293, 293, 341, 820, 312, 1858, 558, 341, 4659, 380, 341, 4659,
  380, 312, 955, 644, 295, 428, 1716, 309, 820, 312, 2831, 13, 51814], "temperature":
  0.0, "avg_logprob": -0.23783538914933988, "compression_ratio": 1.7289719626168225,
  "no_speech_prob": 0.004387346561998129}, {"id": 263, "seek": 374180, "start": 3741.8,
  "end": 3756.8, "text": " You were trying to think about options you want to try
  maybe getting some suggestions as well there but this would be I think this is a
  human creativity part as well and then the the actual.", "tokens": [50364, 509,
  645, 1382, 281, 519, 466, 3956, 291, 528, 281, 853, 1310, 1242, 512, 13396, 382,
  731, 456, 457, 341, 576, 312, 286, 519, 341, 307, 257, 1952, 12915, 644, 382, 731,
  293, 550, 264, 264, 3539, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18044514723227056,
  "compression_ratio": 1.6633165829145728, "no_speech_prob": 0.02033403143286705},
  {"id": 264, "seek": 374180, "start": 3756.8, "end": 3767.8, "text": " So a swapping
  of components and comparing their making them comparable I think that''s nothing
  where you should spend time as a developer on.", "tokens": [51114, 407, 257, 1693,
  10534, 295, 6677, 293, 15763, 641, 1455, 552, 25323, 286, 519, 300, 311, 1825, 689,
  291, 820, 3496, 565, 382, 257, 10754, 322, 13, 51664], "temperature": 0.0, "avg_logprob":
  -0.18044514723227056, "compression_ratio": 1.6633165829145728, "no_speech_prob":
  0.02033403143286705}, {"id": 265, "seek": 376780, "start": 3767.8, "end": 3796.8,
  "text": " And like connected to the question about future maybe causing off of on
  that we recently built with my colleague are netalman a multi model and multilingual
  search demo right where we used clip model of the shelf without any fine tuning
  on web data and it showed us really really amazing results right so like where keyword
  search cannot find because simply.", "tokens": [50414, 400, 411, 4582, 281, 264,
  1168, 466, 2027, 1310, 9853, 766, 295, 322, 300, 321, 3938, 3094, 365, 452, 13532,
  366, 2533, 304, 1601, 257, 4825, 2316, 293, 2120, 38219, 3164, 10723, 558, 689,
  321, 1143, 7353, 2316, 295, 264, 15222, 1553, 604, 2489, 15164, 322, 3670, 1412,
  293, 309, 4712, 505, 534, 534, 2243, 3542, 558, 370, 411, 689, 20428, 3164, 2644,
  915, 570, 2935, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2451728003365653,
  "compression_ratio": 1.634703196347032, "no_speech_prob": 0.03368542715907097},
  {"id": 266, "seek": 379780, "start": 3797.8, "end": 3808.8, "text": " We metadata
  doesn''t have it and it''s multilingual right so and it type it the same query with
  neural retrieval and it gets it.", "tokens": [50364, 492, 26603, 1177, 380, 362,
  309, 293, 309, 311, 2120, 38219, 558, 370, 293, 309, 2010, 309, 264, 912, 14581,
  365, 18161, 19817, 3337, 293, 309, 2170, 309, 13, 50914], "temperature": 0.0, "avg_logprob":
  -0.1457685743059431, "compression_ratio": 1.6396396396396395, "no_speech_prob":
  0.02699311263859272}, {"id": 267, "seek": 379780, "start": 3808.8, "end": 3823.8,
  "text": " Is there anything stopping high stack to move into that direction as well
  sort of like crossing the boundary of only text right so like you did say multi
  model in the context of let''s say queering a table but I could also query an image.",
  "tokens": [50914, 1119, 456, 1340, 12767, 1090, 8630, 281, 1286, 666, 300, 3513,
  382, 731, 1333, 295, 411, 14712, 264, 12866, 295, 787, 2487, 558, 370, 411, 291,
  630, 584, 4825, 2316, 294, 264, 4319, 295, 718, 311, 584, 631, 1794, 257, 3199,
  457, 286, 727, 611, 14581, 364, 3256, 13, 51664], "temperature": 0.0, "avg_logprob":
  -0.1457685743059431, "compression_ratio": 1.6396396396396395, "no_speech_prob":
  0.02699311263859272}, {"id": 268, "seek": 382380, "start": 3823.8, "end": 3827.8,
  "text": " So the same with the test stack is going in that direction as well.",
  "tokens": [50364, 407, 264, 912, 365, 264, 1500, 8630, 307, 516, 294, 300, 3513,
  382, 731, 13, 50564], "temperature": 0.0, "avg_logprob": -0.363587605584528, "compression_ratio":
  1.6732283464566928, "no_speech_prob": 0.012012441642582417}, {"id": 269, "seek":
  382380, "start": 3827.8, "end": 3852.8, "text": " Yeah so we are actually like real
  right now working on it so we have a first case where we want to support where you
  have a text query but you can query also into images from the right side side and
  then basically now other way around would be probably one of the later ones they
  have an image as a query until I want to find different media types, I''d say.",
  "tokens": [50564, 865, 370, 321, 366, 767, 411, 957, 558, 586, 1364, 322, 309, 370,
  321, 362, 257, 700, 1389, 689, 321, 528, 281, 1406, 689, 291, 362, 257, 2487, 14581,
  457, 291, 393, 14581, 611, 666, 5267, 490, 264, 558, 1252, 1252, 293, 550, 1936,
  586, 661, 636, 926, 576, 312, 1391, 472, 295, 264, 1780, 2306, 436, 362, 364, 3256,
  382, 257, 14581, 1826, 286, 528, 281, 915, 819, 3021, 3467, 11, 286, 1116, 584,
  13, 51814], "temperature": 0.0, "avg_logprob": -0.363587605584528, "compression_ratio":
  1.6732283464566928, "no_speech_prob": 0.012012441642582417}, {"id": 270, "seek":
  385280, "start": 3852.8, "end": 3858.8, "text": " But yeah this is like definitely
  what we right now working on.", "tokens": [50364, 583, 1338, 341, 307, 411, 2138,
  437, 321, 558, 586, 1364, 322, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2822669681749846,
  "compression_ratio": 1.5925925925925926, "no_speech_prob": 0.007458253763616085},
  {"id": 271, "seek": 385280, "start": 3858.8, "end": 3868.8, "text": " I think I
  also think we need to think always see what are the big use cases and what kind
  of customers you have and how do we use it.", "tokens": [50664, 286, 519, 286, 611,
  519, 321, 643, 281, 519, 1009, 536, 437, 366, 264, 955, 764, 3331, 293, 437, 733,
  295, 4581, 291, 362, 293, 577, 360, 321, 764, 309, 13, 51164], "temperature": 0.0,
  "avg_logprob": -0.2822669681749846, "compression_ratio": 1.5925925925925926, "no_speech_prob":
  0.007458253763616085}, {"id": 272, "seek": 385280, "start": 3868.8, "end": 3877.8,
  "text": " I think with images there''s a lot of interesting use cases mainly in
  e-commerce I would say that''s cool.", "tokens": [51164, 286, 519, 365, 5267, 456,
  311, 257, 688, 295, 1880, 764, 3331, 8704, 294, 308, 12, 26926, 286, 576, 584, 300,
  311, 1627, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2822669681749846, "compression_ratio":
  1.5925925925925926, "no_speech_prob": 0.007458253763616085}, {"id": 273, "seek":
  387780, "start": 3877.8, "end": 3883.8, "text": " Yeah, we are already supported
  to some degree and will support more I think in the next month.", "tokens": [50364,
  865, 11, 321, 366, 1217, 8104, 281, 512, 4314, 293, 486, 1406, 544, 286, 519, 294,
  264, 958, 1618, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18296910336143093,
  "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.1519637256860733},
  {"id": 274, "seek": 387780, "start": 3883.8, "end": 3905.8, "text": " That''s great
  to learn and that also means that I need to adjust my classification because I''ve
  been presenting what I know about the players in in vector database and neural frameworks
  and specifically for haystack I put NLP as the main vertical and I think largely
  you guys still advertise that as the main vertical but I think nothing stops you
  from.", "tokens": [50664, 663, 311, 869, 281, 1466, 293, 300, 611, 1355, 300, 286,
  643, 281, 4369, 452, 21538, 570, 286, 600, 668, 15578, 437, 286, 458, 466, 264,
  4150, 294, 294, 8062, 8149, 293, 18161, 29834, 293, 4682, 337, 4842, 372, 501, 286,
  829, 426, 45196, 382, 264, 2135, 9429, 293, 286, 519, 11611, 291, 1074, 920, 35379,
  300, 382, 264, 2135, 9429, 457, 286, 519, 1825, 10094, 291, 490, 13, 51764], "temperature":
  0.0, "avg_logprob": -0.18296910336143093, "compression_ratio": 1.6931818181818181,
  "no_speech_prob": 0.1519637256860733}, {"id": 275, "seek": 390580, "start": 3905.8,
  "end": 3913.8, "text": " Switching that to multi modality right so NLP computer
  vision and maybe even speech at some point.", "tokens": [50364, 13893, 278, 300,
  281, 4825, 1072, 1860, 558, 370, 426, 45196, 3820, 5201, 293, 1310, 754, 6218, 412,
  512, 935, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17239184319218503, "compression_ratio":
  1.5260869565217392, "no_speech_prob": 0.01824888400733471}, {"id": 276, "seek":
  390580, "start": 3913.8, "end": 3931.8, "text": " Yeah totally I think our approaches
  there''s just a bit like doing one thing to quite a depth first and then moving
  on to the next rather than let''s say starting with very high level basic support
  for all modalities and then kind of growing all of them.", "tokens": [50764, 865,
  3879, 286, 519, 527, 11587, 456, 311, 445, 257, 857, 411, 884, 472, 551, 281, 1596,
  257, 7161, 700, 293, 550, 2684, 322, 281, 264, 958, 2831, 813, 718, 311, 584, 2891,
  365, 588, 1090, 1496, 3875, 1406, 337, 439, 1072, 16110, 293, 550, 733, 295, 4194,
  439, 295, 552, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17239184319218503,
  "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.01824888400733471},
  {"id": 277, "seek": 393180, "start": 3931.8, "end": 3950.8, "text": " So what we
  rather did in the past and still doing is very deep support for texts and we haven''t
  there everything in place before kind of moving on to the next that''s a bit of
  a philosophy question maybe a strategic questions what you want to do it.", "tokens":
  [50364, 407, 437, 321, 2831, 630, 294, 264, 1791, 293, 920, 884, 307, 588, 2452,
  1406, 337, 15765, 293, 321, 2378, 380, 456, 1203, 294, 1081, 949, 733, 295, 2684,
  322, 281, 264, 958, 300, 311, 257, 857, 295, 257, 10675, 1168, 1310, 257, 10924,
  1651, 437, 291, 528, 281, 360, 309, 13, 51314], "temperature": 0.0, "avg_logprob":
  -0.3619434152330671, "compression_ratio": 1.5121951219512195, "no_speech_prob":
  0.030860206112265587}, {"id": 278, "seek": 395080, "start": 3950.8, "end": 3968.8,
  "text": " So this field multi is changing quite a lot right so a lot of things generative
  models really big large models models that I don''t know even how to use yet you
  know like dali.", "tokens": [50364, 407, 341, 2519, 4825, 307, 4473, 1596, 257,
  688, 558, 370, 257, 688, 295, 721, 1337, 1166, 5245, 534, 955, 2416, 5245, 5245,
  300, 286, 500, 380, 458, 754, 577, 281, 764, 1939, 291, 458, 411, 274, 5103, 13,
  51264], "temperature": 0.0, "avg_logprob": -0.2677145669626635, "compression_ratio":
  1.4112903225806452, "no_speech_prob": 0.3014005124568939}, {"id": 279, "seek": 396880,
  "start": 3968.8, "end": 3982.8, "text": " Of course beyond just kind of experimental
  interest but probably there will be some use cases where do you think else the trends
  are going in this space.", "tokens": [50364, 2720, 1164, 4399, 445, 733, 295, 17069,
  1179, 457, 1391, 456, 486, 312, 512, 764, 3331, 689, 360, 291, 519, 1646, 264, 13892,
  366, 516, 294, 341, 1901, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10289331638451779,
  "compression_ratio": 1.3076923076923077, "no_speech_prob": 0.12392525374889374},
  {"id": 280, "seek": 398280, "start": 3982.8, "end": 4003.8, "text": " Yeah so we
  like want one big trend I think for sure is these large language models and everything
  around it and as I talked earlier about it the questions like where is it right
  now and is it already today really usable is it already kind of worth investigating
  them comparing them for for your own use cases.", "tokens": [50364, 865, 370, 321,
  411, 528, 472, 955, 6028, 286, 519, 337, 988, 307, 613, 2416, 2856, 5245, 293, 1203,
  926, 309, 293, 382, 286, 2825, 3071, 466, 309, 264, 1651, 411, 689, 307, 309, 558,
  586, 293, 307, 309, 1217, 965, 534, 29975, 307, 309, 1217, 733, 295, 3163, 22858,
  552, 15763, 552, 337, 337, 428, 1065, 764, 3331, 13, 51414], "temperature": 0.0,
  "avg_logprob": -0.16247457265853882, "compression_ratio": 1.6263157894736842, "no_speech_prob":
  0.12506522238254547}, {"id": 281, "seek": 400380, "start": 4003.8, "end": 4032.8,
  "text": " I think there we are I would say still in an early phase it''s look at
  for example GPT 3 and and I think it''s high months to the quite nice analysis earlier
  this year where compared embedding some GPT 3 and will more standard size transformers
  and there we think we saw it''s the performance is it''s not bad but it''s also
  definitely not our performing.", "tokens": [50364, 286, 519, 456, 321, 366, 286,
  576, 584, 920, 294, 364, 2440, 5574, 309, 311, 574, 412, 337, 1365, 26039, 51, 805,
  293, 293, 286, 519, 309, 311, 1090, 2493, 281, 264, 1596, 1481, 5215, 3071, 341,
  1064, 689, 5347, 12240, 3584, 512, 26039, 51, 805, 293, 486, 544, 3832, 2744, 4088,
  433, 293, 456, 321, 519, 321, 1866, 309, 311, 264, 3389, 307, 309, 311, 406, 1578,
  457, 309, 311, 611, 2138, 406, 527, 10205, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.3959727817111545, "compression_ratio": 1.6129032258064515, "no_speech_prob":
  0.0796964168548584}, {"id": 282, "seek": 403280, "start": 4032.8, "end": 4045.8,
  "text": " You say regular size models which are a thousand times smaller cost a
  few dollars in not thousands and tens of thousands of dollars for for your influence
  costs.", "tokens": [50364, 509, 584, 3890, 2744, 5245, 597, 366, 257, 4714, 1413,
  4356, 2063, 257, 1326, 3808, 294, 406, 5383, 293, 10688, 295, 5383, 295, 3808, 337,
  337, 428, 6503, 5497, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22282164805644267,
  "compression_ratio": 1.6313131313131313, "no_speech_prob": 0.005298241041600704},
  {"id": 283, "seek": 403280, "start": 4045.8, "end": 4058.8, "text": " So I think
  that''s it''s basically right now as to let''s see case by case that it makes sense
  for use case but if you think look a bit further into the next years.", "tokens":
  [51014, 407, 286, 519, 300, 311, 309, 311, 1936, 558, 586, 382, 281, 718, 311, 536,
  1389, 538, 1389, 300, 309, 1669, 2020, 337, 764, 1389, 457, 498, 291, 519, 574,
  257, 857, 3052, 666, 264, 958, 924, 13, 51664], "temperature": 0.0, "avg_logprob":
  -0.22282164805644267, "compression_ratio": 1.6313131313131313, "no_speech_prob":
  0.005298241041600704}, {"id": 284, "seek": 405880, "start": 4058.8, "end": 4087.8,
  "text": " I''m pretty sure and convinced that this is only a matter of time until
  we see more and more large language models really in production also in search pipelines
  in production and think that now it''s this phase of figuring out how can we make
  them really more efficient more more reliable so we really can trust these these
  results there.", "tokens": [50364, 286, 478, 1238, 988, 293, 12561, 300, 341, 307,
  787, 257, 1871, 295, 565, 1826, 321, 536, 544, 293, 544, 2416, 2856, 5245, 534,
  294, 4265, 611, 294, 3164, 40168, 294, 4265, 293, 519, 300, 586, 309, 311, 341,
  5574, 295, 15213, 484, 577, 393, 321, 652, 552, 534, 544, 7148, 544, 544, 12924,
  370, 321, 534, 393, 3361, 613, 613, 3542, 456, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.14773211759679458, "compression_ratio": 1.6884422110552764, "no_speech_prob":
  0.027059154585003853}, {"id": 285, "seek": 408780, "start": 4087.8, "end": 4116.8,
  "text": " Not going to be an easier way update to new knowledge and I would really
  but now look a lot into and what I''m personally quite excited about is now this
  I think area of research around retrieval based NLP so yes on the one hand side
  kind of scaling up the models making them bigger because we think learned and over
  last years that they are good few short learners and I think that''s really good.",
  "tokens": [50364, 1726, 516, 281, 312, 364, 3571, 636, 5623, 281, 777, 3601, 293,
  286, 576, 534, 457, 586, 574, 257, 688, 666, 293, 437, 286, 478, 5665, 1596, 2919,
  466, 307, 586, 341, 286, 519, 1859, 295, 2132, 926, 19817, 3337, 2361, 426, 45196,
  370, 2086, 322, 264, 472, 1011, 1252, 733, 295, 21589, 493, 264, 5245, 1455, 552,
  3801, 570, 321, 519, 3264, 293, 670, 1036, 924, 300, 436, 366, 665, 1326, 2099,
  23655, 293, 286, 519, 300, 311, 534, 665, 13, 51814], "temperature": 0.0, "avg_logprob":
  -0.34569773563118866, "compression_ratio": 1.6363636363636365, "no_speech_prob":
  0.025945376604795456}, {"id": 286, "seek": 411680, "start": 4116.8, "end": 4135.8,
  "text": " And that''s of course exciting because you can just take these models
  and kind of throw a task at them and they will perform so less manual work of of
  annotating data creating domain specific data sets and so on.", "tokens": [50364,
  400, 300, 311, 295, 1164, 4670, 570, 291, 393, 445, 747, 613, 5245, 293, 733, 295,
  3507, 257, 5633, 412, 552, 293, 436, 486, 2042, 370, 1570, 9688, 589, 295, 295,
  25339, 990, 1412, 4084, 9274, 2685, 1412, 6352, 293, 370, 322, 13, 51314], "temperature":
  0.0, "avg_logprob": -0.21598241684284616, "compression_ratio": 1.4452054794520548,
  "no_speech_prob": 0.01452100370079279}, {"id": 287, "seek": 413580, "start": 4135.8,
  "end": 4164.8, "text": " But I think we also saw that they are not very efficient
  and there are these other problems. How do you how do you actually now teach not
  to be free about recent events or about your own domain knowledge and typically
  I think these these data sets that you that you want to search in they''re not static
  right so there''s a constantly evolving and you really want to retrain these crazy
  models every few days or weeks just to kind of catch up with us.", "tokens": [50364,
  583, 286, 519, 321, 611, 1866, 300, 436, 366, 406, 588, 7148, 293, 456, 366, 613,
  661, 2740, 13, 1012, 360, 291, 577, 360, 291, 767, 586, 2924, 406, 281, 312, 1737,
  466, 5162, 3931, 420, 466, 428, 1065, 9274, 3601, 293, 5850, 286, 519, 613, 613,
  1412, 6352, 300, 291, 300, 291, 528, 281, 3164, 294, 436, 434, 406, 13437, 558,
  370, 456, 311, 257, 6460, 21085, 293, 291, 534, 528, 281, 1533, 7146, 613, 3219,
  5245, 633, 1326, 1708, 420, 3259, 445, 281, 733, 295, 3745, 493, 365, 505, 13, 51814],
  "temperature": 0.0, "avg_logprob": -0.18238230253520765, "compression_ratio": 1.7470817120622568,
  "no_speech_prob": 0.03828652203083038}, {"id": 288, "seek": 416580, "start": 4166.8,
  "end": 4180.8, "text": " And I think that''s like where this stream of retrieval
  based or achievement at models is super interesting and I think there''s a lot of
  cool work.", "tokens": [50414, 400, 286, 519, 300, 311, 411, 689, 341, 4309, 295,
  19817, 3337, 2361, 420, 15838, 412, 5245, 307, 1687, 1880, 293, 286, 519, 456, 311,
  257, 688, 295, 1627, 589, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3552790233067104,
  "compression_ratio": 1.3394495412844036, "no_speech_prob": 0.0035831511486321688},
  {"id": 289, "seek": 418080, "start": 4180.8, "end": 4189.8, "text": " So just this
  week we''re back from from Patrick Lewis publication around the Atlas model.", "tokens":
  [50364, 407, 445, 341, 1243, 321, 434, 646, 490, 490, 13980, 17412, 19953, 926,
  264, 32485, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3222217082977295,
  "compression_ratio": 1.569377990430622, "no_speech_prob": 0.03529032692313194},
  {"id": 290, "seek": 418080, "start": 4189.8, "end": 4193.8, "text": " Sure if you
  saw it. But there''s basically the idea.", "tokens": [50814, 4894, 498, 291, 1866,
  309, 13, 583, 456, 311, 1936, 264, 1558, 13, 51014], "temperature": 0.0, "avg_logprob":
  -0.3222217082977295, "compression_ratio": 1.569377990430622, "no_speech_prob": 0.03529032692313194},
  {"id": 291, "seek": 418080, "start": 4193.8, "end": 4208.8, "text": " Can we can
  we somehow remove the say the memory part from these big models and it kind of outsource
  it to a database to an index and then at a query time we still have like a large
  model.", "tokens": [51014, 1664, 321, 393, 321, 6063, 4159, 264, 584, 264, 4675,
  644, 490, 613, 955, 5245, 293, 309, 733, 295, 14758, 2948, 309, 281, 257, 8149,
  281, 364, 8186, 293, 550, 412, 257, 14581, 565, 321, 920, 362, 411, 257, 2416, 2316,
  13, 51764], "temperature": 0.0, "avg_logprob": -0.3222217082977295, "compression_ratio":
  1.569377990430622, "no_speech_prob": 0.03529032692313194}, {"id": 292, "seek": 420880,
  "start": 4208.8, "end": 4237.8, "text": " Can we look complex reasoning, but it''s
  kind of basing the generation on some retrieve documents and that can be useful
  for search but can be also for an effect checking or other use cases and and long
  story short, I think they have interesting they did love interesting experiments
  and that paper that show that you can actually outsource quite a bit of these parameters
  of this memory into into a", "tokens": [50364, 1664, 321, 574, 3997, 21577, 11,
  457, 309, 311, 733, 295, 987, 278, 264, 5125, 322, 512, 30254, 8512, 293, 300, 393,
  312, 4420, 337, 3164, 457, 393, 312, 611, 337, 364, 1802, 8568, 420, 661, 764, 3331,
  293, 293, 938, 1657, 2099, 11, 286, 519, 436, 362, 1880, 436, 630, 959, 1880, 12050,
  293, 300, 3035, 300, 855, 300, 291, 393, 767, 14758, 2948, 1596, 257, 857, 295,
  613, 9834, 295, 341, 4675, 666, 666, 257, 51814], "temperature": 0.0, "avg_logprob":
  -0.2299939379279996, "compression_ratio": 1.7155172413793103, "no_speech_prob":
  0.005177072249352932}, {"id": 293, "seek": 423780, "start": 4237.8, "end": 4246.8,
  "text": " say a vector like the database and and still keep the few shot capabilities
  of these giant language models.", "tokens": [50364, 584, 257, 8062, 411, 264, 8149,
  293, 293, 920, 1066, 264, 1326, 3347, 10862, 295, 613, 7410, 2856, 5245, 13, 50814],
  "temperature": 0.0, "avg_logprob": -0.2909165721828655, "compression_ratio": 1.5664739884393064,
  "no_speech_prob": 0.0033793686889111996}, {"id": 294, "seek": 423780, "start": 4246.8,
  "end": 4262.8, "text": " And I think this is like a super cool route like larger
  models but still not putting everything in it, not not blowing up parameters, parameters
  size unreasonably.", "tokens": [50814, 400, 286, 519, 341, 307, 411, 257, 1687,
  1627, 7955, 411, 4833, 5245, 457, 920, 406, 3372, 1203, 294, 309, 11, 406, 406,
  15068, 493, 9834, 11, 9834, 2744, 20584, 1258, 1188, 13, 51614], "temperature":
  0.0, "avg_logprob": -0.2909165721828655, "compression_ratio": 1.5664739884393064,
  "no_speech_prob": 0.0033793686889111996}, {"id": 295, "seek": 426280, "start": 4263.8,
  "end": 4270.8, "text": " Let''s do combining it with now let''s say an external
  document base or knowledge base.", "tokens": [50414, 961, 311, 360, 21928, 309,
  365, 586, 718, 311, 584, 364, 8320, 4166, 3096, 420, 3601, 3096, 13, 50764], "temperature":
  0.0, "avg_logprob": -0.2332797604937886, "compression_ratio": 1.6291079812206573,
  "no_speech_prob": 0.16929569840431213}, {"id": 296, "seek": 426280, "start": 4270.8,
  "end": 4286.8, "text": " Yeah, I think it''s the topic attached upon it''s fascinating
  that on one hand, let''s say you have a model, right? And if you if you keep retraining
  it or fine tuning it on on latest data, you may run into this. I think it''s called
  catastrophic forgetting, right?", "tokens": [50764, 865, 11, 286, 519, 309, 311,
  264, 4829, 8570, 3564, 309, 311, 10343, 300, 322, 472, 1011, 11, 718, 311, 584,
  291, 362, 257, 2316, 11, 558, 30, 400, 498, 291, 498, 291, 1066, 49356, 1760, 309,
  420, 2489, 15164, 309, 322, 322, 6792, 1412, 11, 291, 815, 1190, 666, 341, 13, 286,
  519, 309, 311, 1219, 34915, 25428, 11, 558, 30, 51564], "temperature": 0.0, "avg_logprob":
  -0.2332797604937886, "compression_ratio": 1.6291079812206573, "no_speech_prob":
  0.16929569840431213}, {"id": 297, "seek": 428680, "start": 4286.8, "end": 4294.8,
  "text": " Like things that we as humans know that I don''t know what is liquid kind
  of on high level without going into chemistry.", "tokens": [50364, 1743, 721, 300,
  321, 382, 6255, 458, 300, 286, 500, 380, 458, 437, 307, 6553, 733, 295, 322, 1090,
  1496, 1553, 516, 666, 12558, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16509604166789227,
  "compression_ratio": 1.6339285714285714, "no_speech_prob": 0.38385775685310364},
  {"id": 298, "seek": 428680, "start": 4294.8, "end": 4309.8, "text": " And it''s
  not that we think about it every single day when we drink water, but like it''s
  not that we actually forget it if somebody asks us right no matter how many news
  or papers, whatever the red books right we still remember the basic facts and",
  "tokens": [50764, 400, 309, 311, 406, 300, 321, 519, 466, 309, 633, 2167, 786, 562,
  321, 2822, 1281, 11, 457, 411, 309, 311, 406, 300, 321, 767, 2870, 309, 498, 2618,
  8962, 505, 558, 572, 1871, 577, 867, 2583, 420, 10577, 11, 2035, 264, 2182, 3642,
  558, 321, 920, 1604, 264, 3875, 9130, 293, 51514], "temperature": 0.0, "avg_logprob":
  -0.16509604166789227, "compression_ratio": 1.6339285714285714, "no_speech_prob":
  0.38385775685310364}, {"id": 299, "seek": 430980, "start": 4309.8, "end": 4322.8,
  "text": " and I think what you just said with the Atlas model right so approach
  outsourcing that memory into some database that you can maybe even control and say,
  okay, these facts need to stay.", "tokens": [50364, 293, 286, 519, 437, 291, 445,
  848, 365, 264, 32485, 2316, 558, 370, 3109, 14758, 41849, 300, 4675, 666, 512, 8149,
  300, 291, 393, 1310, 754, 1969, 293, 584, 11, 1392, 11, 613, 9130, 643, 281, 1754,
  13, 51014], "temperature": 0.0, "avg_logprob": -0.17950237424750076, "compression_ratio":
  1.5953488372093023, "no_speech_prob": 0.09019295126199722}, {"id": 300, "seek":
  430980, "start": 4322.8, "end": 4332.8, "text": " I never want them to go away no
  matter what right, these are like basic principles and maybe they exist in every
  domain like finance or healthcare and so on.", "tokens": [51014, 286, 1128, 528,
  552, 281, 352, 1314, 572, 1871, 437, 558, 11, 613, 366, 411, 3875, 9156, 293, 1310,
  436, 2514, 294, 633, 9274, 411, 10719, 420, 8884, 293, 370, 322, 13, 51514], "temperature":
  0.0, "avg_logprob": -0.17950237424750076, "compression_ratio": 1.5953488372093023,
  "no_speech_prob": 0.09019295126199722}, {"id": 301, "seek": 433280, "start": 4332.8,
  "end": 4337.8, "text": " And yeah, I think this is interesting direction.", "tokens":
  [50364, 400, 1338, 11, 286, 519, 341, 307, 1880, 3513, 13, 50614], "temperature":
  0.0, "avg_logprob": -0.31480225920677185, "compression_ratio": 1.5210526315789474,
  "no_speech_prob": 0.1434401124715805}, {"id": 302, "seek": 433280, "start": 4337.8,
  "end": 4354.8, "text": " Yeah, absolutely all these facts change right can also
  be that over time you have to adjust facts or knowledge and this is way easier I
  think if you have it explicitly somewhere in documents, not so much in the just
  in the parameters model.", "tokens": [50614, 865, 11, 3122, 439, 613, 9130, 1319,
  558, 393, 611, 312, 300, 670, 565, 291, 362, 281, 4369, 9130, 420, 3601, 293, 341,
  307, 636, 3571, 286, 519, 498, 291, 362, 309, 20803, 4079, 294, 8512, 11, 406, 370,
  709, 294, 264, 445, 294, 264, 9834, 2316, 13, 51464], "temperature": 0.0, "avg_logprob":
  -0.31480225920677185, "compression_ratio": 1.5210526315789474, "no_speech_prob":
  0.1434401124715805}, {"id": 303, "seek": 435480, "start": 4354.8, "end": 4370.8,
  "text": " Yeah, exactly exactly and like maybe just one example that comes to my
  mind is like CT CT''s change names right and so you could still go back and say
  what was the name of that CD between you know 1995 and 2000 right something like
  that.", "tokens": [50364, 865, 11, 2293, 2293, 293, 411, 1310, 445, 472, 1365, 300,
  1487, 281, 452, 1575, 307, 411, 19529, 19529, 311, 1319, 5288, 558, 293, 370, 291,
  727, 920, 352, 646, 293, 584, 437, 390, 264, 1315, 295, 300, 6743, 1296, 291, 458,
  22601, 293, 8132, 558, 746, 411, 300, 13, 51164], "temperature": 0.0, "avg_logprob":
  -0.12375709745619032, "compression_ratio": 1.4478527607361964, "no_speech_prob":
  0.05547681078314781}, {"id": 304, "seek": 437080, "start": 4370.8, "end": 4383.8,
  "text": " Yeah, or presidents of nations also change right so for this kind of queries
  I think you want to make sure that you''re up to date and change it.", "tokens":
  [50364, 865, 11, 420, 27611, 295, 11035, 611, 1319, 558, 370, 337, 341, 733, 295,
  24109, 286, 519, 291, 528, 281, 652, 988, 300, 291, 434, 493, 281, 4002, 293, 1319,
  309, 13, 51014], "temperature": 0.0, "avg_logprob": -0.23905001746283638, "compression_ratio":
  1.2743362831858407, "no_speech_prob": 0.1190975084900856}, {"id": 305, "seek": 438380,
  "start": 4383.8, "end": 4397.8, "text": " Yeah, and I think maybe coming back to
  search understanding the context will place such a huge role once these models become
  even more mature and available and knowledge aware.", "tokens": [50364, 865, 11,
  293, 286, 519, 1310, 1348, 646, 281, 3164, 3701, 264, 4319, 486, 1081, 1270, 257,
  2603, 3090, 1564, 613, 5245, 1813, 754, 544, 14442, 293, 2435, 293, 3601, 3650,
  13, 51064], "temperature": 0.0, "avg_logprob": -0.12950106461842856, "compression_ratio":
  1.3435114503816794, "no_speech_prob": 0.1525879204273224}, {"id": 306, "seek": 439780,
  "start": 4397.8, "end": 4424.8, "text": " But but the challenge of extracting contacts
  from the query still is there if I say who is the president of the United States,
  it might you know conclude that i''m asking about now present but if I was couple
  programs above already saying setting the stage about specific period of time in
  the past it could actually reason that I''m maybe not asking about presence right.",
  "tokens": [50364, 583, 457, 264, 3430, 295, 49844, 15836, 490, 264, 14581, 920,
  307, 456, 498, 286, 584, 567, 307, 264, 3868, 295, 264, 2824, 3040, 11, 309, 1062,
  291, 458, 16886, 300, 741, 478, 3365, 466, 586, 1974, 457, 498, 286, 390, 1916,
  4268, 3673, 1217, 1566, 3287, 264, 3233, 466, 2685, 2896, 295, 565, 294, 264, 1791,
  309, 727, 767, 1778, 300, 286, 478, 1310, 406, 3365, 466, 6814, 558, 13, 51714],
  "temperature": 0.0, "avg_logprob": -0.16103317260742187, "compression_ratio": 1.6228070175438596,
  "no_speech_prob": 0.48550447821617126}, {"id": 307, "seek": 442480, "start": 4424.8,
  "end": 4436.8, "text": " Exactly could do this reasoning or could you ask a clarifying
  question right or say like all like here are a couple of options that you mean this
  like as you may want more to win a human conversation.", "tokens": [50364, 7587,
  727, 360, 341, 21577, 420, 727, 291, 1029, 257, 6093, 5489, 1168, 558, 420, 584,
  411, 439, 411, 510, 366, 257, 1916, 295, 3956, 300, 291, 914, 341, 411, 382, 291,
  815, 528, 544, 281, 1942, 257, 1952, 3761, 13, 50964], "temperature": 0.0, "avg_logprob":
  -0.2118764321009318, "compression_ratio": 1.778225806451613, "no_speech_prob": 0.013676443137228489},
  {"id": 308, "seek": 442480, "start": 4436.8, "end": 4453.8, "text": " Yeah, so I
  think it''s called conversational information retrieval right and I think that we
  might start seeing this blend of what probably today is called chatbot and a search
  engine but it could be a search engine which is just clarifying.", "tokens": [50964,
  865, 11, 370, 286, 519, 309, 311, 1219, 2615, 1478, 1589, 19817, 3337, 558, 293,
  286, 519, 300, 321, 1062, 722, 2577, 341, 10628, 295, 437, 1391, 965, 307, 1219,
  5081, 18870, 293, 257, 3164, 2848, 457, 309, 727, 312, 257, 3164, 2848, 597, 307,
  445, 6093, 5489, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2118764321009318,
  "compression_ratio": 1.778225806451613, "no_speech_prob": 0.013676443137228489},
  {"id": 309, "seek": 445380, "start": 4453.8, "end": 4471.8, "text": " Yeah, I mean
  overall I think it''s it''s that''s a thing also is in the field I think we are
  seeing that the search what we understand undersurge is evolving right so it''s
  not so much anymore so I think about web search engines.", "tokens": [50364, 865,
  11, 286, 914, 4787, 286, 519, 309, 311, 309, 311, 300, 311, 257, 551, 611, 307,
  294, 264, 2519, 286, 519, 321, 366, 2577, 300, 264, 3164, 437, 321, 1223, 16692,
  374, 432, 307, 21085, 558, 370, 309, 311, 406, 370, 709, 3602, 370, 286, 519, 466,
  3670, 3164, 12982, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3456519671848842,
  "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.006885879673063755},
  {"id": 310, "seek": 447180, "start": 4471.8, "end": 4480.8, "text": " Yeah, in few
  cases you were still you search search and you click on the website and then you''ll
  search somewhere your information.", "tokens": [50364, 865, 11, 294, 1326, 3331,
  291, 645, 920, 291, 3164, 3164, 293, 291, 2052, 322, 264, 3144, 293, 550, 291, 603,
  3164, 4079, 428, 1589, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2595294189453125,
  "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.08032990247011185},
  {"id": 311, "seek": 447180, "start": 4480.8, "end": 4497.8, "text": " But in many
  cases we will kind of zero click search now where you have your query and within
  the search results you already find what you want at and i think this is just yeah
  getting more and more popular that.", "tokens": [50814, 583, 294, 867, 3331, 321,
  486, 733, 295, 4018, 2052, 3164, 586, 689, 291, 362, 428, 14581, 293, 1951, 264,
  3164, 3542, 291, 1217, 915, 437, 291, 528, 412, 293, 741, 519, 341, 307, 445, 1338,
  1242, 544, 293, 544, 3743, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2595294189453125,
  "compression_ratio": 1.7064676616915422, "no_speech_prob": 0.08032990247011185},
  {"id": 312, "seek": 449780, "start": 4497.8, "end": 4512.8, "text": " Yeah, you''re
  not providing say there''s the route to go to another knowledge source but you''re
  trying to really answer the query directly and there''s no need to go further.",
  "tokens": [50364, 865, 11, 291, 434, 406, 6530, 584, 456, 311, 264, 7955, 281, 352,
  281, 1071, 3601, 4009, 457, 291, 434, 1382, 281, 534, 1867, 264, 14581, 3838, 293,
  456, 311, 572, 643, 281, 352, 3052, 13, 51114], "temperature": 0.0, "avg_logprob":
  -0.2128302574157715, "compression_ratio": 1.4132231404958677, "no_speech_prob":
  0.009024864062666893}, {"id": 313, "seek": 451280, "start": 4512.8, "end": 4526.8,
  "text": " I will also try to remember to link one paper maybe it''s like a series
  of papers from Microsoft where they try to embed knowledge into the language model
  and that''s.", "tokens": [50364, 286, 486, 611, 853, 281, 1604, 281, 2113, 472,
  3035, 1310, 309, 311, 411, 257, 2638, 295, 10577, 490, 8116, 689, 436, 853, 281,
  12240, 3601, 666, 264, 2856, 2316, 293, 300, 311, 13, 51064], "temperature": 0.0,
  "avg_logprob": -0.12951921161852384, "compression_ratio": 1.3306451612903225, "no_speech_prob":
  0.3616473376750946}, {"id": 314, "seek": 452680, "start": 4527.8, "end": 4543.8,
  "text": " I think it''s a very interesting direction as well as also embedding knowledge
  graphs into the model right because one way, as you said, and I think that trend
  probably still there that yeah you can keep adding parameters more and more billion
  trillions.", "tokens": [50414, 286, 519, 309, 311, 257, 588, 1880, 3513, 382, 731,
  382, 611, 12240, 3584, 3601, 24877, 666, 264, 2316, 558, 570, 472, 636, 11, 382,
  291, 848, 11, 293, 286, 519, 300, 6028, 1391, 920, 456, 300, 1338, 291, 393, 1066,
  5127, 9834, 544, 293, 544, 5218, 504, 46279, 13, 51214], "temperature": 0.0, "avg_logprob":
  -0.130066297672413, "compression_ratio": 1.5149700598802396, "no_speech_prob": 0.15515585243701935},
  {"id": 315, "seek": 454380, "start": 4543.8, "end": 4553.8, "text": " But at some
  point it just simply becomes an on practical in practical right to to have such
  a large model in production and then how do you find unit.", "tokens": [50364, 583,
  412, 512, 935, 309, 445, 2935, 3643, 364, 322, 8496, 294, 8496, 558, 281, 281, 362,
  1270, 257, 2416, 2316, 294, 4265, 293, 550, 577, 360, 291, 915, 4985, 13, 50864],
  "temperature": 0.0, "avg_logprob": -0.18731724132191052, "compression_ratio": 1.5157232704402517,
  "no_speech_prob": 0.01904473640024662}, {"id": 316, "seek": 454380, "start": 4553.8,
  "end": 4559.8, "text": " But again it doesn''t capture the relationships well enough
  right if you didn''t explain it.", "tokens": [50864, 583, 797, 309, 1177, 380, 7983,
  264, 6159, 731, 1547, 558, 498, 291, 994, 380, 2903, 309, 13, 51164], "temperature":
  0.0, "avg_logprob": -0.18731724132191052, "compression_ratio": 1.5157232704402517,
  "no_speech_prob": 0.01904473640024662}, {"id": 317, "seek": 455980, "start": 4560.8,
  "end": 4582.8, "text": " Absolutely and just thinking of it we have actually a meetup
  end of September so if you''re interested or anyone who''s listening where a needs
  Rimas will also talk about exactly that topic how do you kind of incorporate knowledge
  into a language model and and that would be our end of September as well just look
  for maybe can link it in the show notes.", "tokens": [50414, 7021, 293, 445, 1953,
  295, 309, 321, 362, 767, 257, 1677, 1010, 917, 295, 7216, 370, 498, 291, 434, 3102,
  420, 2878, 567, 311, 4764, 689, 257, 2203, 497, 17957, 486, 611, 751, 466, 2293,
  300, 4829, 577, 360, 291, 733, 295, 16091, 3601, 666, 257, 2856, 2316, 293, 293,
  300, 576, 312, 527, 917, 295, 7216, 382, 731, 445, 574, 337, 1310, 393, 2113, 309,
  294, 264, 855, 5570, 13, 51514], "temperature": 0.0, "avg_logprob": -0.32280543009440105,
  "compression_ratio": 1.5855855855855856, "no_speech_prob": 0.033445827662944794},
  {"id": 318, "seek": 458280, "start": 4582.8, "end": 4594.8, "text": " So I will
  manage you to experience absolutely will do that gladly my favorite question I know
  you touched many times as well during this podcast which I really really enjoyed.",
  "tokens": [50364, 407, 286, 486, 3067, 291, 281, 1752, 3122, 486, 360, 300, 47307,
  452, 2954, 1168, 286, 458, 291, 9828, 867, 1413, 382, 731, 1830, 341, 7367, 597,
  286, 534, 534, 4626, 13, 50964], "temperature": 0.0, "avg_logprob": -0.25164263407389326,
  "compression_ratio": 1.681159420289855, "no_speech_prob": 0.33660757541656494},
  {"id": 319, "seek": 458280, "start": 4594.8, "end": 4609.8, "text": " But what what
  else drives you beyond you know you have a role as a city or you have a role as
  a pioneer in this space and maybe educating and reaching more and more people.",
  "tokens": [50964, 583, 437, 437, 1646, 11754, 291, 4399, 291, 458, 291, 362, 257,
  3090, 382, 257, 2307, 420, 291, 362, 257, 3090, 382, 257, 37668, 294, 341, 1901,
  293, 1310, 28835, 293, 9906, 544, 293, 544, 561, 13, 51714], "temperature": 0.0,
  "avg_logprob": -0.25164263407389326, "compression_ratio": 1.681159420289855, "no_speech_prob":
  0.33660757541656494}, {"id": 320, "seek": 460980, "start": 4610.8, "end": 4617.8,
  "text": " Is there something else that drives you sort of beyond the tech itself
  in this field.", "tokens": [50414, 1119, 456, 746, 1646, 300, 11754, 291, 1333,
  295, 4399, 264, 7553, 2564, 294, 341, 2519, 13, 50764], "temperature": 0.0, "avg_logprob":
  -0.2299879921807183, "compression_ratio": 1.6965811965811965, "no_speech_prob":
  0.07786066085100174}, {"id": 321, "seek": 460980, "start": 4617.8, "end": 4623.8,
  "text": " Yeah like I mean that I think my make sight of my passion for an appears
  here.", "tokens": [50764, 865, 411, 286, 914, 300, 286, 519, 452, 652, 7860, 295,
  452, 5418, 337, 364, 7038, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2299879921807183,
  "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.07786066085100174},
  {"id": 322, "seek": 460980, "start": 4623.8, "end": 4626.8, "text": " I hope that
  came came through.", "tokens": [51064, 286, 1454, 300, 1361, 1361, 807, 13, 51214],
  "temperature": 0.0, "avg_logprob": -0.2299879921807183, "compression_ratio": 1.6965811965811965,
  "no_speech_prob": 0.07786066085100174}, {"id": 323, "seek": 460980, "start": 4626.8,
  "end": 4638.8, "text": " But for me like the technology is the one thing but then
  really seeing how you solve problems with that like how you can make annoying work
  of financial analyst faster and better like just seeing that.", "tokens": [51214,
  583, 337, 385, 411, 264, 2899, 307, 264, 472, 551, 457, 550, 534, 2577, 577, 291,
  5039, 2740, 365, 300, 411, 577, 291, 393, 652, 11304, 589, 295, 4669, 19085, 4663,
  293, 1101, 411, 445, 2577, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2299879921807183,
  "compression_ratio": 1.6965811965811965, "no_speech_prob": 0.07786066085100174},
  {"id": 324, "seek": 463880, "start": 4638.8, "end": 4646.8, "text": " Either say
  first and because they are customer or it''s a more indirectly.", "tokens": [50364,
  13746, 584, 700, 293, 570, 436, 366, 5474, 420, 309, 311, 257, 544, 37779, 13, 50764],
  "temperature": 0.0, "avg_logprob": -0.22878915385196083, "compression_ratio": 1.5792079207920793,
  "no_speech_prob": 0.013838470913469791}, {"id": 325, "seek": 463880, "start": 4646.8,
  "end": 4649.8, "text": " If you know that this is now kind of possible.", "tokens":
  [50764, 759, 291, 458, 300, 341, 307, 586, 733, 295, 1944, 13, 50914], "temperature":
  0.0, "avg_logprob": -0.22878915385196083, "compression_ratio": 1.5792079207920793,
  "no_speech_prob": 0.013838470913469791}, {"id": 326, "seek": 463880, "start": 4649.8,
  "end": 4664.8, "text": " So I think it''s like still a big driver for me personally
  and I think I want to think I absolutely love about open source that it''s not just
  paying users commercial users where you kind of see that.", "tokens": [50914, 407,
  286, 519, 309, 311, 411, 920, 257, 955, 6787, 337, 385, 5665, 293, 286, 519, 286,
  528, 281, 519, 286, 3122, 959, 466, 1269, 4009, 300, 309, 311, 406, 445, 6229, 5022,
  6841, 5022, 689, 291, 733, 295, 536, 300, 13, 51664], "temperature": 0.0, "avg_logprob":
  -0.22878915385196083, "compression_ratio": 1.5792079207920793, "no_speech_prob":
  0.013838470913469791}, {"id": 327, "seek": 466480, "start": 4664.8, "end": 4678.8,
  "text": " But we are really this huge community by now from haystack where there''s
  so many different people with different backgrounds, different use cases and it''s.",
  "tokens": [50364, 583, 321, 366, 534, 341, 2603, 1768, 538, 586, 490, 4842, 372,
  501, 689, 456, 311, 370, 867, 819, 561, 365, 819, 17336, 11, 819, 764, 3331, 293,
  309, 311, 13, 51064], "temperature": 0.0, "avg_logprob": -0.29184861864362444, "compression_ratio":
  1.5522388059701493, "no_speech_prob": 0.16269415616989136}, {"id": 328, "seek":
  466480, "start": 4678.8, "end": 4689.8, "text": " For me often like just end of
  the day really like scrolling through and all get up issues kind of questions that
  come in or on Slack when are we on discord.", "tokens": [51064, 1171, 385, 2049,
  411, 445, 917, 295, 264, 786, 534, 411, 29053, 807, 293, 439, 483, 493, 2663, 733,
  295, 1651, 300, 808, 294, 420, 322, 37211, 562, 366, 321, 322, 32989, 13, 51614],
  "temperature": 0.0, "avg_logprob": -0.29184861864362444, "compression_ratio": 1.5522388059701493,
  "no_speech_prob": 0.16269415616989136}, {"id": 329, "seek": 468980, "start": 4689.8,
  "end": 4701.8, "text": " Like what what people are actually building with that and
  and it''s really cool to see what they kind of use case come up with but also how
  far this actually got that it''s.", "tokens": [50364, 1743, 437, 437, 561, 366,
  767, 2390, 365, 300, 293, 293, 309, 311, 534, 1627, 281, 536, 437, 436, 733, 295,
  764, 1389, 808, 493, 365, 457, 611, 577, 1400, 341, 767, 658, 300, 309, 311, 13,
  50964], "temperature": 0.0, "avg_logprob": -0.2533611437169517, "compression_ratio":
  1.4491525423728813, "no_speech_prob": 0.03317677974700928}, {"id": 330, "seek":
  470180, "start": 4701.8, "end": 4711.8, "text": " And using so many companies all
  around the world from big tech to classical enterprise to start up to build their
  products on top.", "tokens": [50364, 400, 1228, 370, 867, 3431, 439, 926, 264, 1002,
  490, 955, 7553, 281, 13735, 14132, 281, 722, 493, 281, 1322, 641, 3383, 322, 1192,
  13, 50864], "temperature": 0.0, "avg_logprob": -0.28388368672338027, "compression_ratio":
  1.5263157894736843, "no_speech_prob": 0.2008688747882843}, {"id": 331, "seek": 470180,
  "start": 4711.8, "end": 4722.8, "text": " And that thing is a stick to one of my
  biggest motivation boosters that you can get at seeing the community appreciating
  using it.", "tokens": [50864, 400, 300, 551, 307, 257, 2897, 281, 472, 295, 452,
  3880, 12335, 748, 40427, 300, 291, 393, 483, 412, 2577, 264, 1768, 3616, 990, 1228,
  309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.28388368672338027, "compression_ratio":
  1.5263157894736843, "no_speech_prob": 0.2008688747882843}, {"id": 332, "seek": 472280,
  "start": 4722.8, "end": 4741.8, "text": " And and probably also like on tip beyond
  get up just recently ran into a guy in a bar who he and Berlin who who use taste
  that and let''s definitely something I never would have imagined a few years ago.",
  "tokens": [50364, 400, 293, 1391, 611, 411, 322, 4125, 4399, 483, 493, 445, 3938,
  5872, 666, 257, 2146, 294, 257, 2159, 567, 415, 293, 13848, 567, 567, 764, 3939,
  300, 293, 718, 311, 2138, 746, 286, 1128, 576, 362, 16590, 257, 1326, 924, 2057,
  13, 51314], "temperature": 0.0, "avg_logprob": -0.3452634608491938, "compression_ratio":
  1.3835616438356164, "no_speech_prob": 0.0756370946764946}, {"id": 333, "seek": 474180,
  "start": 4741.8, "end": 4752.8, "text": " And this kind of happens or what we said
  of a glass here when we find a bit of a vision and thought about some goals at the
  company offside.", "tokens": [50364, 400, 341, 733, 295, 2314, 420, 437, 321, 848,
  295, 257, 4276, 510, 562, 321, 915, 257, 857, 295, 257, 5201, 293, 1194, 466, 512,
  5493, 412, 264, 2237, 766, 1812, 13, 50914], "temperature": 0.0, "avg_logprob":
  -0.3188341547933857, "compression_ratio": 1.6134453781512605, "no_speech_prob":
  0.289371520280838}, {"id": 334, "seek": 474180, "start": 4752.8, "end": 4769.8,
  "text": " I think one of us for the open source side that people start putting say
  haystack experience into their job requirements or the other way around people putting
  that in the CVs and we thought oh, I guess this is maybe three years down the road.",
  "tokens": [50914, 286, 519, 472, 295, 505, 337, 264, 1269, 4009, 1252, 300, 561,
  722, 3372, 584, 4842, 372, 501, 1752, 666, 641, 1691, 7728, 420, 264, 661, 636,
  926, 561, 3372, 300, 294, 264, 22995, 82, 293, 321, 1194, 1954, 11, 286, 2041, 341,
  307, 1310, 1045, 924, 760, 264, 3060, 13, 51764], "temperature": 0.0, "avg_logprob":
  -0.3188341547933857, "compression_ratio": 1.6134453781512605, "no_speech_prob":
  0.289371520280838}, {"id": 335, "seek": 476980, "start": 4769.8, "end": 4780.8,
  "text": " But then a few weeks afterwards we saw these first job postings where
  this was required and also TVs where this was mentioned.", "tokens": [50364, 583,
  550, 257, 1326, 3259, 10543, 321, 1866, 613, 700, 1691, 2183, 1109, 689, 341, 390,
  4739, 293, 611, 38085, 689, 341, 390, 2835, 13, 50914], "temperature": 0.0, "avg_logprob":
  -0.22198439263678216, "compression_ratio": 1.5809523809523809, "no_speech_prob":
  0.0012213498121127486}, {"id": 336, "seek": 476980, "start": 4780.8, "end": 4796.8,
  "text": " So I think it''s just cool to see how you can leave a footprint and beyond
  let''s say you are immediate bubble but really kind of spreads it''s open it''s
  all digital it''s kind of connected in the world right.", "tokens": [50914, 407,
  286, 519, 309, 311, 445, 1627, 281, 536, 577, 291, 393, 1856, 257, 24222, 293, 4399,
  718, 311, 584, 291, 366, 11629, 12212, 457, 534, 733, 295, 25728, 309, 311, 1269,
  309, 311, 439, 4562, 309, 311, 733, 295, 4582, 294, 264, 1002, 558, 13, 51714],
  "temperature": 0.0, "avg_logprob": -0.22198439263678216, "compression_ratio": 1.5809523809523809,
  "no_speech_prob": 0.0012213498121127486}, {"id": 337, "seek": 479680, "start": 4796.8,
  "end": 4802.8, "text": " And leaving those kind of footprint is what I enjoy.",
  "tokens": [50364, 400, 5012, 729, 733, 295, 24222, 307, 437, 286, 2103, 13, 50664],
  "temperature": 0.0, "avg_logprob": -0.31329286420667496, "compression_ratio": 1.5353535353535352,
  "no_speech_prob": 0.09440034627914429}, {"id": 338, "seek": 479680, "start": 4802.8,
  "end": 4823.8, "text": " And yeah, search in I think as a domain in us just for
  me really interesting because it''s so diverse as you can go in many directions
  can dive very deep into an IP can think a lot about the user side at the end for
  what use cases you can make it work.", "tokens": [50664, 400, 1338, 11, 3164, 294,
  286, 519, 382, 257, 9274, 294, 505, 445, 337, 385, 534, 1880, 570, 309, 311, 370,
  9521, 382, 291, 393, 352, 294, 867, 11095, 393, 9192, 588, 2452, 666, 364, 8671,
  393, 519, 257, 688, 466, 264, 4195, 1252, 412, 264, 917, 337, 437, 764, 3331, 291,
  393, 652, 309, 589, 13, 51714], "temperature": 0.0, "avg_logprob": -0.31329286420667496,
  "compression_ratio": 1.5353535353535352, "no_speech_prob": 0.09440034627914429},
  {"id": 339, "seek": 482380, "start": 4823.8, "end": 4827.8, "text": " And can think
  a lot about scalability.", "tokens": [50364, 400, 393, 519, 257, 688, 466, 15664,
  2310, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22565984725952148, "compression_ratio":
  1.6402116402116402, "no_speech_prob": 0.07637202739715576}, {"id": 340, "seek":
  482380, "start": 4827.8, "end": 4837.8, "text": " It''s just I think the one of
  the most for my point most exciting and diverse applications of technology right
  now.", "tokens": [50564, 467, 311, 445, 286, 519, 264, 472, 295, 264, 881, 337,
  452, 935, 881, 4670, 293, 9521, 5821, 295, 2899, 558, 586, 13, 51064], "temperature":
  0.0, "avg_logprob": -0.22565984725952148, "compression_ratio": 1.6402116402116402,
  "no_speech_prob": 0.07637202739715576}, {"id": 341, "seek": 482380, "start": 4837.8,
  "end": 4849.8, "text": " And and one way I think you can really relate to like really
  can think okay what what is actually possible what kind of information you can make
  accessible.", "tokens": [51064, 400, 293, 472, 636, 286, 519, 291, 393, 534, 10961,
  281, 411, 534, 393, 519, 1392, 437, 437, 307, 767, 1944, 437, 733, 295, 1589, 291,
  393, 652, 9515, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22565984725952148,
  "compression_ratio": 1.6402116402116402, "no_speech_prob": 0.07637202739715576},
  {"id": 342, "seek": 484980, "start": 4849.8, "end": 4853.8, "text": " And that''s
  that''s obviously the beauty of it.", "tokens": [50364, 400, 300, 311, 300, 311,
  2745, 264, 6643, 295, 309, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22360794127933561,
  "compression_ratio": 1.567251461988304, "no_speech_prob": 0.057433754205703735},
  {"id": 343, "seek": 484980, "start": 4853.8, "end": 4869.8, "text": " Yeah, it''s
  beautifully put thanks thanks for sharing I know some some of the guests that I
  asked this question would probably think hey why is this philosophical question
  I''m just you know doing it I like it but that''s it.", "tokens": [50564, 865, 11,
  309, 311, 16525, 829, 3231, 3231, 337, 5414, 286, 458, 512, 512, 295, 264, 9804,
  300, 286, 2351, 341, 1168, 576, 1391, 519, 4177, 983, 307, 341, 25066, 1168, 286,
  478, 445, 291, 458, 884, 309, 286, 411, 309, 457, 300, 311, 309, 13, 51364], "temperature":
  0.0, "avg_logprob": -0.22360794127933561, "compression_ratio": 1.567251461988304,
  "no_speech_prob": 0.057433754205703735}, {"id": 344, "seek": 486980, "start": 4869.8,
  "end": 4887.8, "text": " But I think it gives so much to towards you know you reflecting
  on what you do because that might also influence your choices in in the tech or
  in how you approach your users what message you send and so on and so forth and
  maybe reconsider some things as well.", "tokens": [50364, 583, 286, 519, 309, 2709,
  370, 709, 281, 3030, 291, 458, 291, 23543, 322, 437, 291, 360, 570, 300, 1062, 611,
  6503, 428, 7994, 294, 294, 264, 7553, 420, 294, 577, 291, 3109, 428, 5022, 437,
  3636, 291, 2845, 293, 370, 322, 293, 370, 5220, 293, 1310, 40497, 512, 721, 382,
  731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10283544607329786, "compression_ratio":
  1.535294117647059, "no_speech_prob": 0.09484151005744934}, {"id": 345, "seek": 488780,
  "start": 4887.8, "end": 4916.8, "text": " And an open source part you reminded me
  of one story when it was my first time visit in the US I think it was 2015 and it
  was a patchy coin I was crossing on the traffic light you know on the pedestrian
  crossing and it was like this narrow avenue you know not narrow white and every
  right and select takes on my account like few minutes but it''s of course not minutes
  maybe 20 seconds.", "tokens": [50364, 400, 364, 1269, 4009, 644, 291, 15920, 385,
  295, 472, 1657, 562, 309, 390, 452, 700, 565, 3441, 294, 264, 2546, 286, 519, 309,
  390, 7546, 293, 309, 390, 257, 9972, 88, 11464, 286, 390, 14712, 322, 264, 6419,
  1442, 291, 458, 322, 264, 33947, 14712, 293, 309, 390, 411, 341, 9432, 39230, 291,
  458, 406, 9432, 2418, 293, 633, 558, 293, 3048, 2516, 322, 452, 2696, 411, 1326,
  2077, 457, 309, 311, 295, 1164, 406, 2077, 1310, 945, 3949, 13, 51814], "temperature":
  0.0, "avg_logprob": -0.18648285585291247, "compression_ratio": 1.6594827586206897,
  "no_speech_prob": 0.41918399930000305}, {"id": 346, "seek": 491680, "start": 4916.8,
  "end": 4927.8, "text": " And I think it''s really bouncing to me from the other
  side of the road saying I know you I was like no it''s impossible it''s my first
  time visit you know I don''t I''m not a public figure.", "tokens": [50364, 400,
  286, 519, 309, 311, 534, 27380, 281, 385, 490, 264, 661, 1252, 295, 264, 3060, 1566,
  286, 458, 291, 286, 390, 411, 572, 309, 311, 6243, 309, 311, 452, 700, 565, 3441,
  291, 458, 286, 500, 380, 286, 478, 406, 257, 1908, 2573, 13, 50914], "temperature":
  0.0, "avg_logprob": -0.28622476914349726, "compression_ratio": 1.5922330097087378,
  "no_speech_prob": 0.0880960151553154}, {"id": 347, "seek": 491680, "start": 4927.8,
  "end": 4937.8, "text": " How is it possible and he said he because you build look
  it''s one of the open source kind of you seen in the extriders that I used to work
  on.", "tokens": [50914, 1012, 307, 309, 1944, 293, 415, 848, 415, 570, 291, 1322,
  574, 309, 311, 472, 295, 264, 1269, 4009, 733, 295, 291, 1612, 294, 264, 16455,
  6936, 300, 286, 1143, 281, 589, 322, 13, 51414], "temperature": 0.0, "avg_logprob":
  -0.28622476914349726, "compression_ratio": 1.5922330097087378, "no_speech_prob":
  0.0880960151553154}, {"id": 348, "seek": 493780, "start": 4937.8, "end": 4957.8,
  "text": " You know which I inherited from its original creator Andrej Blyetski and
  that''s it he didn''t stop to say anything else but but he made my day you know
  and I think what you felt in the bar was probably similar knowing that that person
  uses haystack and you know it''s amazing.", "tokens": [50364, 509, 458, 597, 286,
  27091, 490, 1080, 3380, 14181, 20667, 73, 363, 356, 1385, 2984, 293, 300, 311, 309,
  415, 994, 380, 1590, 281, 584, 1340, 1646, 457, 457, 415, 1027, 452, 786, 291, 458,
  293, 286, 519, 437, 291, 2762, 294, 264, 2159, 390, 1391, 2531, 5276, 300, 300,
  954, 4960, 4842, 372, 501, 293, 291, 458, 309, 311, 2243, 13, 51364], "temperature":
  0.0, "avg_logprob": -0.18135158943407464, "compression_ratio": 1.4972677595628416,
  "no_speech_prob": 0.28599128127098083}, {"id": 349, "seek": 495780, "start": 4957.8,
  "end": 4973.8, "text": " Absolutely because it''s just it feels very honest right
  it feels like it is is not because we know it''s crazy marketing or anything like
  that it''s just like a really like a natural community thing and and just building
  something that''s useful for others.", "tokens": [50364, 7021, 570, 309, 311, 445,
  309, 3417, 588, 3245, 558, 309, 3417, 411, 309, 307, 307, 406, 570, 321, 458, 309,
  311, 3219, 6370, 420, 1340, 411, 300, 309, 311, 445, 411, 257, 534, 411, 257, 3303,
  1768, 551, 293, 293, 445, 2390, 746, 300, 311, 4420, 337, 2357, 13, 51164], "temperature":
  0.0, "avg_logprob": -0.19310691621568468, "compression_ratio": 1.6178343949044587,
  "no_speech_prob": 0.29695138335227966}, {"id": 350, "seek": 497380, "start": 4973.8,
  "end": 4991.8, "text": " Yeah exactly which probably reinforces you and gives you
  these well in this case direct feedback well not only specifics of your of your
  platform but actually the fact that they''re using it and relying on and building
  a business and that tells the two decisions you made in the architecture and so
  on and so forth that''s amazing.", "tokens": [50364, 865, 2293, 597, 1391, 20520,
  887, 291, 293, 2709, 291, 613, 731, 294, 341, 1389, 2047, 5824, 731, 406, 787, 28454,
  295, 428, 295, 428, 3663, 457, 767, 264, 1186, 300, 436, 434, 1228, 309, 293, 24140,
  322, 293, 2390, 257, 1606, 293, 300, 5112, 264, 732, 5327, 291, 1027, 294, 264,
  9482, 293, 370, 322, 293, 370, 5220, 300, 311, 2243, 13, 51264], "temperature":
  0.0, "avg_logprob": -0.16160714448387944, "compression_ratio": 1.645, "no_speech_prob":
  0.1877872794866562}, {"id": 351, "seek": 499180, "start": 4991.8, "end": 5006.8,
  "text": " Yeah I mean like from a company perspective that''s one of the fastest
  feedback cycles you can have right and like seeing diverse use cases diverse developer
  person on us how they approach things what they''re struggling with.", "tokens":
  [50364, 865, 286, 914, 411, 490, 257, 2237, 4585, 300, 311, 472, 295, 264, 14573,
  5824, 17796, 291, 393, 362, 558, 293, 411, 2577, 9521, 764, 3331, 9521, 10754, 954,
  322, 505, 577, 436, 3109, 721, 437, 436, 434, 9314, 365, 13, 51114], "temperature":
  0.0, "avg_logprob": -0.27308099023227034, "compression_ratio": 1.5, "no_speech_prob":
  0.022672582417726517}, {"id": 352, "seek": 499180, "start": 5006.8, "end": 5011.8,
  "text": " Yeah also that angle it was fast yeah absolutely crucial.", "tokens":
  [51114, 865, 611, 300, 5802, 309, 390, 2370, 1338, 3122, 11462, 13, 51364], "temperature":
  0.0, "avg_logprob": -0.27308099023227034, "compression_ratio": 1.5, "no_speech_prob":
  0.022672582417726517}, {"id": 353, "seek": 501180, "start": 5011.8, "end": 5029.8,
  "text": " I think it''s the best and it''s like I think it''s Elon Musk who said
  the best setting is when your user fell in love with your product and once you just
  succeed so yeah there you go amazing and I''ve enjoyed this podcast so much is there
  anything you want to announce to our listeners.", "tokens": [50364, 286, 519, 309,
  311, 264, 1151, 293, 309, 311, 411, 286, 519, 309, 311, 28498, 26019, 567, 848,
  264, 1151, 3287, 307, 562, 428, 4195, 5696, 294, 959, 365, 428, 1674, 293, 1564,
  291, 445, 7754, 370, 1338, 456, 291, 352, 2243, 293, 286, 600, 4626, 341, 7367,
  370, 709, 307, 456, 1340, 291, 528, 281, 7478, 281, 527, 23274, 13, 51264], "temperature":
  0.0, "avg_logprob": -0.08603833271906926, "compression_ratio": 1.5810055865921788,
  "no_speech_prob": 0.11487273871898651}, {"id": 354, "seek": 502980, "start": 5029.8,
  "end": 5052.8, "text": " Yeah we''ve just the meet up I already mentioned so if
  you''re interested in LP that''s happening in September it will be hybrid so you
  can join online but if you''re if you happen to be in Berlin we also have a small
  on site event and then yeah of course if you haven''t tried hastag yet maybe check
  it out on GitHub.", "tokens": [50364, 865, 321, 600, 445, 264, 1677, 493, 286, 1217,
  2835, 370, 498, 291, 434, 3102, 294, 38095, 300, 311, 2737, 294, 7216, 309, 486,
  312, 13051, 370, 291, 393, 3917, 2950, 457, 498, 291, 434, 498, 291, 1051, 281,
  312, 294, 13848, 321, 611, 362, 257, 1359, 322, 3621, 2280, 293, 550, 1338, 295,
  1164, 498, 291, 2378, 380, 3031, 6581, 559, 1939, 1310, 1520, 309, 484, 322, 23331,
  13, 51514], "temperature": 0.2, "avg_logprob": -0.18091154098510742, "compression_ratio":
  1.5120772946859904, "no_speech_prob": 0.49246010184288025}, {"id": 355, "seek":
  505280, "start": 5052.8, "end": 5080.8, "text": " As a prompt every promise you
  can get an easy first pipeline up and running and just give it a try to try to question
  answering if you haven''t if you''re more coming from traditional search and down
  on deep set cloud as mentioned we just released a big new model on experiments with
  still an early stage with with the product but we have an early access program so
  if you''re interested if you''re", "tokens": [50364, 1018, 257, 12391, 633, 6228,
  291, 393, 483, 364, 1858, 700, 15517, 493, 293, 2614, 293, 445, 976, 309, 257, 853,
  281, 853, 281, 1168, 13430, 498, 291, 2378, 380, 498, 291, 434, 544, 1348, 490,
  5164, 3164, 293, 760, 322, 2452, 992, 4588, 382, 2835, 321, 445, 4736, 257, 955,
  777, 2316, 322, 12050, 365, 920, 364, 2440, 3233, 365, 365, 264, 1674, 457, 321,
  362, 364, 2440, 2105, 1461, 370, 498, 291, 434, 3102, 498, 291, 434, 51764], "temperature":
  0.0, "avg_logprob": -0.2995092323027461, "compression_ratio": 1.7356828193832599,
  "no_speech_prob": 0.2435733675956726}, {"id": 356, "seek": 508080, "start": 5080.8,
  "end": 5101.8, "text": " having a lot of use case that you want to bring to production
  in a fast way where you think about how to scale it how to actually find that pipeline
  how to collaborate with with your end users and get some feedback there just reach
  out to us and then we can can get you on the on the early access program.", "tokens":
  [50364, 1419, 257, 688, 295, 764, 1389, 300, 291, 528, 281, 1565, 281, 4265, 294,
  257, 2370, 636, 689, 291, 519, 466, 577, 281, 4373, 309, 577, 281, 767, 915, 300,
  15517, 577, 281, 18338, 365, 365, 428, 917, 5022, 293, 483, 512, 5824, 456, 445,
  2524, 484, 281, 505, 293, 550, 321, 393, 393, 483, 291, 322, 264, 322, 264, 2440,
  2105, 1461, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22766752804026885,
  "compression_ratio": 1.6576086956521738, "no_speech_prob": 0.041459083557128906},
  {"id": 357, "seek": 510180, "start": 5101.8, "end": 5130.8, "text": " Amazing thanks
  so much multi have enjoyed again saying this and this was deep and thoughtful and
  we will make sure to link all the all the goodies that you mentioned in the show
  notes and I hope to meet some day maybe in Berlin maybe somewhere else but absolutely
  yeah let''s make that happen and I totally enjoyed our conversation as well so thanks
  but for having me.", "tokens": [50384, 14165, 3231, 370, 709, 4825, 362, 4626, 797,
  1566, 341, 293, 341, 390, 2452, 293, 21566, 293, 321, 486, 652, 988, 281, 2113,
  439, 264, 439, 264, 44072, 300, 291, 2835, 294, 264, 855, 5570, 293, 286, 1454,
  281, 1677, 512, 786, 1310, 294, 13848, 1310, 4079, 1646, 457, 3122, 1338, 718, 311,
  652, 300, 1051, 293, 286, 3879, 4626, 527, 3761, 382, 731, 370, 3231, 457, 337,
  1419, 385, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19059837186658704,
  "compression_ratio": 1.660633484162896, "no_speech_prob": 0.0754031240940094}, {"id":
  358, "seek": 513180, "start": 5131.8, "end": 5139.8, "text": " It''s definitely
  interesting fantastic all the best with haystack and and with your research and
  development.", "tokens": [50364, 467, 311, 2138, 1880, 5456, 439, 264, 1151, 365,
  4842, 372, 501, 293, 293, 365, 428, 2132, 293, 3250, 13, 50764], "temperature":
  0.0, "avg_logprob": -0.354872076851981, "compression_ratio": 1.3545454545454545,
  "no_speech_prob": 0.007043542806059122}, {"id": 359, "seek": 513180, "start": 5140.6,
  "end": 5143.8, "text": " Thanks a lot thanks all the bye bye bye.", "tokens": [50804,
  2561, 257, 688, 3231, 439, 264, 6543, 6543, 6543, 13, 50964], "temperature": 0.0,
  "avg_logprob": -0.354872076851981, "compression_ratio": 1.3545454545454545, "no_speech_prob":
  0.007043542806059122}, {"id": 360, "seek": 516180, "start": 5161.8, "end": 5163.8,
  "text": " you", "tokens": [50364, 291, 50464], "temperature": 0.0, "avg_logprob":
  -0.7324387431144714, "compression_ratio": 0.2727272727272727, "no_speech_prob":
  0.705066442489624}]'
---

Hello there, Vector Podcast. Season 2, we are relaunching after summer and it was a little bit of break last episode was from Berlin buzzwords and today, coincidentally, we have a guest from Berlin, multi-peach, a studio of deep set, the company behind Haystack.
So we're going to be diving into what I call a neural framework, but I wonder if Malta would give a different picture there, but still very interested to learn and dive into multiple topics there. Hey, Malta, how you doing? I'm good doing great. Thanks for having me today.
How are you doing? I'm good. I'm great. It's still summer. It's super hot as we were exchanging before the recording. It's super, super hot, but I like it.
So yeah, I think before we dive into what is Haystack, I really like to learn about yourself and what is your background and how did you find yourself in this space of what we call Vector Search? I wonder if you describe it differently, but I call it Vector Search, Vector Search players.
So can you tell a bit about that? Yeah, I'm sure I'm happy. So I would say my background is mostly in NLP engineering, what I would call probably these days. And during my studies, I basically had no clue about NLP. I think it wasn't really any part of our coursework or something really a thing.
And for me, all then started basically after my studies, went to the research project in the US, which was at the intersection of machine learning and healthcare. And the big, big focus there was on numerical data.
So we were basically trying to find signals, patterns, and laboratory measurements for kidney disease patients to predict some kind of risks. And there was all the kind of numerical data.
And NLP wasn't really really scope of that project, but there was for me, that basically one kind of event that made me then get in touch with NLP and eventually fell at fall in love.
And it was really in this project, we tried to predict a lot of these risk factors through a lot of, I would say, quite fancy modeling to get some good signals. And at the end, it kind of worked.
We were able to predict some risks, but when we then talked to doctors and showed them these results or asked for their feedback, they said, yeah, yeah, that's all correct. Yeah, but it's not really new. We knew that before. But this part here, this is like, this is an interesting one.
And this is what we do there. And that was basically the only small part where we, where we looked at written notes of of doctors during treatments. And from a modeling perspective, that was really, I would say, nothing fancy, nothing advanced, nothing where we spend a lot of time.
But at the end, it was the point, I think, where the, the doctor's physician saw the biggest value. And that kind of got me to think again, thought, okay, well, like, just this kind of data source, it was something they couldn't really access before.
And now with this, like, very simple, native methods, they somehow saw a value, a new thing. And that's basically where I thought, oh, what, it's cool.
What can you actually then do with more advanced methods of, if you have more fancy models, how can you make this kind of unused data source than accessible. And yeah, basically, realizing this, the power of it.
And that's basically when it then started digging deeper, working more on energy, at some point, then set left research, because I was really interested in seeing these models working the real world.
How do they work at scale? How can they really then solve problems every day? And basically, and came back to Germany, worked in a couple of startups, always just say, NAP at scale, kind of intersection, a lot in online advertisement, recommend our systems.
And then eventually four years ago, then we started sort of deep set. And together with two colleagues, we found the deep set basically because we saw this big motion appeared was kind of piling up.
There was a whole like still pre transformers, but there were early science, I think, on research that, that things are becoming more feasible and super interesting things became possible.
At the same time, we also saw that there's this big gap, you know, like things becoming possible on research side, didn't really mean people were using it in production in the industry. And I think we were at this, this interesting bubble back then.
We did it, we applied deep learning models at scale, saw how that worked, but also saw how much of work it actually is of manual work to get it done.
And basically up the early days of deep set were mainly around, how can we bridge that gap, how can we get latest models from research into production in the industry, what kind of product tooling can we do. And can we build to make that transition easier.
Yeah, and that's basically how we, we ended up in the, in the startup world building building out deep set. And, yeah, initially, that was really more about we saw this problem. We had a couple of product hypothesis, but we didn't, didn't like say place a bet on directly on one of them.
We rather said, okay, let's, let's go out there. Let's really try to understand for one year what are really repetitive use cases out there. What are really the pain points of other enterprise teams that are working in that field and then kind of settling on a product and then building it out.
Yeah, that's basically after one year, how we ended up in search.
 And of course, I would say really the one use case, the dominant use case, there was present in every company that we worked with and that was really a big say, valuable use case, where the push not only came from the developers who wanted to do something better, but also actually from the, from the business side where people saw big value inside Eric.
I use Google every day, where can't we have something similar in our product or our internal data sets and and that thing was something that got us done really interested.
And on the same time that on the the tech side, basically learning more and more about the pain points, why is it actually so difficult for for people in these and these enterprises to build modern search systems, what could you actually do to help them. Yeah, that's fascinating.
Actually four or five years ago, could you have imagined that an L.P.
would cross paths with search because like in many ways, this bar search, which existed for many, many years before was in some sense, I sense it that way in mailing list, let's say a patch is solar mailing list, people were dreaming about applying an L.P.
in some way, compared to what is happening right now.
 I don't want to downplay those efforts, but I'm saying things like you could embed a post tag, part of speech tag on on term level, and then use that during search again, you need to run some kind of parser on the query, and then use that payload information to filter through let's say adjectives and verbs or something bad, you know, I don't know if there was any practical application in place, probably there was.
But again, if you compare that to what is happening today, you basically have a vast array of models right in deep learning models that can be applied directly to search using vector search approach, could you have imagined this happening when you when you were about to start the company.
No, I would say I was I think what we we had big big say dreams about N.A.P.
and we we were true believers that that things become easier and say more feasible in production, but that was more actually under I would say transfer learning side and making models to say more easily adoptable to certain domains for search, I think that was for us.
And only then on our journey where we kind of realized, oh, like that's actually two interesting different fields kind of connecting over time right and also I felt from at least from my perspective, from a community side from the people who worked on information retrieval.
 I think for a long time, a big, like a lot of skeptic people, I wouldn't be talking about any key or dance dance retrieval for good reason right because I think there was also like a lot of hype around deep learning and still what's a lot of promises that were made like that it will just outperform space retrieval out of the box.
And then I think many of these promises were not hold for a long time.
But I think then basically there was another phase where I think people realized, oh, actually now it's kind of starting to work and not only just in research and these ivory towers and lab settings but actually also in reality at scale.
 And I think that was then fast also here, the moment where I've got really interesting and I think since then just crazy to see how things are progressing when thinking about a multi model search or now just was like more I say going away from document retrieval to maybe something like question answering which we do a lot.
And really really crazy to see what's possible these days and I couldn't have imagined that it's going so fast. Yeah, and there are a lot of contributors as well, of course. I just happened to give a talk about players in vector search.
I will link it in the show notes, which was just published with C's. London IR meet up, but even that during that presentation, I felt like I'm scratching the the tip of the iceberg in some sense, I know there is so much happening.
And in Heystack, like did you have a vision for the product, like you said, you didn't know what the product will be, but you knew sort of the repetitive use cases in a way, right, and also challenges, can you share some of the early day challenges that you saw.
And do you think that they are solved today or are they still kind of like in the mix of we need to fix something's there. So I think that was basically all about this first year of deep set, where we did these learnings where wasn't that clear.
But after that year, I think we had a lot of clear insights and at least for us, a clear vision also for Heystack, what we want to want to solve there.
And I would say the big challenge, the big problem that we focused on that we saw in the industry was having just all these get up technologies and.
And basically Heystack is trying and always as I would say as a design philosophy design principle has two things in place that try to bring these data technologies together in a meaningful way.
And what I mean with that is basically if you think about search it's what say really it's a lot more than then model right and it typically you have factor databases.
And you may be chained together multiple models, you have something you want to do at indexing time, you have other things you want to do a query time.
And for each of these say kind of components that you need at the end, there are so many different options that you're that you can plug in and often it's hard to say in the early days.
And then you know, do I go for elastic search or something like pine cone electrical database, do I go for this model or that model, do I need a, I don't know, just the retriever in my pipeline or do I actually also need to add a re rank or something else.
And we just saw that teams are aware of actually spending a lot of time on. And then we're doing these things together manually. And even when they had it once there was and constant or maintenance work or iterations where they have to exchange one component of the system.
And that was really just slowing them down a lot and sometimes even then causing that a project got. So over time, not really ending up in production, but kind of dying at the prototyping stage, because it just took so long and and things got kind of sidetracked.
And with hastag, we basically tried to solve that and having very clear building blocks like, for example, the retriever, which very clean their face.
And within that you can swap a lot of different technology models and the same for a slew vector database document stores where you can very easily change between something like elastic search, pine cone, we veate and whatnot.
So I would say that's the was the one thing this building blocks and trying to get the focus of developers back on making these creative decisions what they actually want to have in their pipeline, trying it out with with anti users, rather than just spending time on doing things together.
And the second thing is I would say very deep concept also in hastag up pipelines. So really what we saw is it's not just one model. It's typically a couple of steps that you want to have there.
So in hastag we started early on having direct as to click graphs where you can have different notes and basically when you have a query or indexing time file that kind of hits the pipeline, you can root it for this graph. That can be very easy. There is a set of a query.
I do put it to a retriever and I get back my documents or can go basically quite complex where you say all like depending on the query type.
If it's a keyword query, I rooted a certain path in my graph, my pipeline, or if it's a question, maybe I go a different way and I have different models, I'm basically involved in my in my search request. And these two, I was here, the core principles in hastag up. That's very interesting.
So that second thing they are cyclic graph with a love for very complex scenarios, right. Like as you explained, we couldn't principle support question answering use case side by side with the kind of like normal search with theory, rankers and stuff, right. Is that correct. Exactly.
So that's what we basically learned from customers like when we saw there was a big interest in something like question answering and people say, wow, that's amazing. Can we use that for our website or for our product here. But doing that switch in a production case is quite tough, right.
Like if people are used to do keyword queries and they know I know I have to enter your keywords to get basically my results.
And then from one day to the other, you switch to more semantic queries, maybe more questions or also I think dance retrieval, if you really have more sentences that you use.
It takes some time for people to adjust and we saw that in a couple of scenarios that basically the traffic kind of requests that come in. Start a lot with keyword queries and then over time slowly shift towards more semantic queries.
When people realize, oh, I can actually also ask a question and all this like, like Google.
 And then there's a trend, but you need everything to have an option your system to allow both for certain time and and hasty basically with the query classifier where you can initially basically classify is that a question or a keyword query or you could go with also semantically like what a topic level saying all like this is a query for certain type of category in my my document set.
And then maybe do something different. And like early on Hey stack did it integrate with any database per se was it like the last search back then. Yeah, like the basically starting point was the last search was the very first document store we had.
But the last search back then didn't I believe didn't support neural search right so how did you actually gel these things together. Yeah, that was just that kind of coming in over time right so it was. Think the the era where elastic search was for us was really.
We came from a question answering use cases a lot and there was really like how do we scale that how can we now. Ask questions not on a single document and single small passage, but how can we do it actually on millions of files and.
And the 25 work as a retriever step before that was was okay was not not too bad and that's kind of how it started and then very fast evolved into into a say back to search direction. Where we had them a files basically as a as a next document store.
In combination with some some SQL database for for the metadata and so on and then it basically kind of. I think took off on the lecture database side with the nervous we via a pine cone and so on and so forth open search today is also part of the face deck. But that was I think then just.
Half half here after we launched a stick. Oh yeah, that's awesome. That sounds quite quick. I know that BBA was also emerging about the same time. And then and then neighbors I guess as well. Yeah, that's that's that sounds super cool.
And was there any as you were approaching your clients or like prospects was there any specific use case that you would be demoing with because you knew this would trigger the aha moment like question answering or maybe a specific domain where you did that.
Yeah, I would say we were for us it was a lot around question answering back then that was really very great that I think many of these aha moments. As to remember we were at one client and when this meeting and it was like on the in the financial domain.
So we're interested in asking questions on financial reports of certain companies and basically accelerating their analysis.
And at one point in this meeting we showed what you can do with question answering ask these questions and they also like suggested own questions that we should ask and they work so they were that point and convinced oh like that's not fake. And like smoke and mirror here.
 And the basically the boss of the department was standing up and shouting like wow that's that's amazing and went out of the office and at the office next door and and carried over colleagues and said like you have to see that and that was actually even before we started building hastag but was these kind of moments were very important to see like this is something.
That is not just fascinating for for techies like we were but also say business people and users see that value and see value and their work for it.
 I can imagine that and it's like a class of what we call knowledge workers right it's something that you spend so much time on crafting this queries and I have spent some time in the full text finance I would say at alpha sense and remember some of the clients they had accumulated Boolean queries over a period of 20 years right and they were like so long it's like several pages.
 When you when you when you slap that into solar it runs for three minutes because our index layout was not what it is today and was not very optimal and it's crazy to see what what people kind of start doing as work around right so we are at a similar case with a with an airplane manufacturer was not financial domain but really on some more maintenance level analyzing basically issues that come up maybe in certain technical areas and they also have like this crazy Boolean search queries and people just became experts and crafting that but it took them really long like asking for sending one query creating this query I was taking easily like minutes.
 Yeah exactly and so what hey stack is today can you can you elaborate a bit on the architecture and maybe if it's possible if you find it easy if you put if you pick what say use case actually I recently I was talking to one stakeholder who wanted to build a chatbot but it was a very specific domain so that chatbot would actually ask you some kind of philosophical question.
 So I think it's a very difficult then like questions sort of a little bit like distracting you from from what's going on let's say you are on a conference and in a lot of things go through your mind but you don't register maybe what's going on you don't get see the value and and that Zenbot might kind of ask you and well essentially allow you to pause and reflect right.
What I realized is that yeah I could pick another shelf model let's say question answering bird or something but it probably wouldn't work on what I want right my domain is different and I had an electronic book with this Zen type of statements.
So this one question I'm hinting to is kind of fine tuning or maybe even right retraining right but where would I start with hey stack and can you walk me through the architecture.
 So as mentioned earlier into core principles are these building blocks and using this building blocks to assemble pipelines and I would say the core we come from is question answering and search but by now I would say the framework has evolved a lot in that direction if you have a lot of different notes and can support a lot of different use cases going to translation zero short classification.
And you could produce these notes in isolation or you can kind of assemble them and use them within your search pipeline.
So usually I think what what our users through and how they start is now they often come with a kind of search use case pick one of the standard pipelines that we have so we can very easily the few lines of Python created pipeline for no it's a question answering or maybe dance retrieval.
 Pick a document store you pick one model from for example the hackenface model hub and and we give some recommendations on which models might be my people starting point and then it's very easy actually to just put your files into a into a pipeline can be PDF files we do the conversion basically for you there's a note for it.
And just have a basic say demo system up and running in a few minutes and that's often already I think a good good starting point if you are maybe also new to that field if you just want to try it quickly out on this kind of ebooks that you mentioned.
 And get a get a first let's say quality of understanding how good piece of the shelf pipelines for my use case get this first data point and then basically enter the I would say next next steps typically in your project if you see all like this is promising but not enough for really going to production.
And then typically go more in this experimentation mode they say all it's now maybe evaluate compare a couple of different models let's maybe adjust this pipeline a bit or add a re-ranker maybe or go maybe to the to a hybrid retriever pipeline where we come.
Basically have a 25 retriever in parallel to a dense retriever and we join these documents and hastic has a lot of functionality that makes that easy to to basically change a pipeline as you wonder very quickly and then evaluate if that gives you any any benefit.
 If these say of the shelf options and combinations are not enough for use case then yeah you can go down the fine tuning route I would say we have also have a source the notation tool labeling tool where you can create training data and basically fine tune parts of your pipeline retriever or reader for question answering.
So basically I would say everything from a quick prototype tool. Let's do some some experiments here and there to then going and production and deploying it with a with a basic rest API until basically.
 Sounds cool and so in that experimentation mode I guess one one one aspect is like fine tuning you mentioned right the other is kind of like what building blocks I could plug in right and I know you guys have really good documentation is there something like a tutorial or or some kind of walk through that would even help me discover is a user what are the options.
So we have a couple of different different tutorials showing you what kind of notes also you can use like many people are not aware of for example options that can do it indexing time that might be helpful so.
For example, like enriching your documents with metadata can be incredibly powerful later at search time because you can then filter on your search space to make more categories that that you're interested in. And there we have for example, the stories that show you how easily you can.
For example, classify documents that you index to certain categories and then later on at query time use these categories to narrow down your search space filter for these categories.
And on the model side, say if you are now you know that you want to have a say QA model reader and you know interested in what model you want.
I would probably suggest you just go to our benchmarks page which is linked from documentation there we have a couple of comparisons in terms of accuracy and speed. But also we have most of our own models on the hackenface model hub which appears to find this information and model cards.
Yeah, that's awesome. So you guys in addition to open source version that I could I presume could host completely myself right I still have a bunch of questions on that open source side but still you also offer the cloud version you call deep set cloud is right.
Can you explain what users get with that I presume scalability but maybe something else and I think we can we can leave a link to in the show notes as well for those users who want to try it out.
Yeah, basically hey stack the open source predictors will be a Python framework and you can do everything you want there to prototype the experiments and if you want also go to production with it.
 But you also found in basically in addition to that people want something more like they want to really host the platform where it's really end to end and basically you have faster workflows so really what's covering the whole lifecycle of an application from early prototyping to running many experiments and parallel getting more guidance.
What's on from your eye perspective on what to launch investigating certain documents in a faster way. Then to OK now I did all these experiments and I want ever kind of one click path to production and I don't want to bother with any scaling and basically a productionizing on my side.
 And this is basically what what we do with these at cloud so if you imagine as a host the platform the cloud the SaaS platform where you develop your NAP applications and can easily bring them to production and monitor them afterwards so really the I would say whole life cycle and especially what's going on getting your.
Getting your NAP pipelines faster to production as you would probably do it on a just Python level and then continue monitoring them and having this close group is to later want to maintain them.
 So it sounds cool and since it's kind of like so with open source version I presume I could do kind of a local development on my PC right and then go and use some deployment pipeline to deploy with cloud version I have sort of like managed haystack right and now thinking about developer experience are you guys moving more towards cloud tools as well you know like for example.
A code editor could be in the clouds or the changes and click click the button and off it goes I don't even need to download it locally right or or do you see some other trend with your users.
 No like we maybe that's also an important point so it's still a developer platform right so we are not in a low code no code space and what we really try is basically giving developers the option to customize components and that then goes through coding and and there we have for example editors directly on the platform where you can.
 Edit for example just the young definition of pipelines and quickly switch certain parameters if you want to do that and then it's basically there's a hosted notebooks where you can also easily kind of open these resources like a pipeline and we automatically create some Python code of it in notebook that you can then.
Then edit as you as you know it also from haystack open source.
Adjust the sort of certain component debug it maybe at another one and then it's basically just one Python line again to move away from the Python code in your notebook to the production artifacts to the pipeline that is then deployed and then can run production.
Yeah sounds cool and if a user has some as a user I mean it could be a company right so let's say they have an established tool set you know maybe if the usage maker maybe they don't maybe use something else. How do you reach these tools said that is kind of outside of haystack do you have to.
 I would say in most cases not so you will I mean what were very very basically stop I would say with with the cloud is when you have your pipeline to NAP service and you have your rest API that you expose that's kind of where we stop so there's a lot of I would say stuff in a company that is built around it when you're into your product and also on the other side of where do the files come from where does that.
Data come from how you think it into into a deep set cloud. But within that space we rather see people. Customers who appreciate it that's like fully integrated and they don't usually then. Want to stay on on sage maker if they are on it for these NAP use cases so from our perspective.
There are the other are these more generic solutions that are not specific for NAP the car work for any kind of machine learning. But if you really have cases where you want to be faster on your NAP use cases.
Want to have more say support on that side that's basically where where deep set cloud and comes into play and to give you an example your think of experiments should evaluate these pipelines.
And then you have to do give basically a lot of options to investigate predictions and what do these metrics actually say and this is a thing is something that is usually missing and solutions like sage maker.
You have to then really combine with many other tools and build in there like a lot of extra stuff. And that basically comes all together already with deep set cloud. So get it right so deep set cloud with offer me sort of an evaluation tool set right.
Can I get the same in the open source version or it's not present there. You can basically evaluate single pipelines also in the open source version.
The difference is that basically in deep set cloud you have a full overview over your project where we track all your experiments you can kind of compare them.
Launch easily 20 experiments in parallel and this is actually on large data sets and with open source I think and generally you would need to provision a lot of machines GPUs to run that in parallel.
And that's basically what one thing that we offer and deep set cloud and the other is basically the I would say just the you I love layer over it. So of course I can work with what Hey stack on and get basically a report around my experiments again maybe a panel state of frame I get some metrics.
What we do when as you on top in deep set cloud is allowing people to interact with this kind of data more easily like finding examples of queries that fail that. Or that are successful getting feedback from also end users so collaborating.
Basically the the persons who use that search system at the end.
And now that's also what I think what we what we saw a lot that yeah you can extract your predictions and maybe it's like a CSV and then you shared with your next colleague who then I'm kind of rates or give say human evaluation if these queries makes sense or not.
But again this is like a lot of friction you have them a lot of these sees these are exifies floating around.
And what we would be what we do is I think bring this together again having it in one place that you can also in future easily reuse that for other experiments and even use it for training and and have it in this in one central place.
Yeah sounds amazing from what I gather this sounds like a end to end ML ops platforms specifically for an LP neural search right.
Exactly you have thought through so many things not only the developer side of things like experimentation but also you know debugging and actually going through the feedback from stakeholders or users. And then communicating with them.
Yeah and I think this is like something that is missed in many projects like this like end user collaboration and from our experience this should really happen in in a very early stage of a project that also kind of continuously when when you move to production and even when you are production.
And I think this is something which is if you don't have the right tooling that's very annoying to you probably like just building a demo like a UI for some search system.
If you are not a front end developer if you're an LP engineer it takes some extra time and even with something extremely these days it's still is then annoying to do it properly and if you're an enterprise maybe draft some access to it's permission words.
But it's so important I think when you look at what projects work out at the end what pipelines more customers go to production.
It's really a big criteria I think in the early days like sharing a demo with your colleagues and end users really the first pipeline you have more or less giving it to the hands of users and seeing what what they think about it and how they use it.
And there were so many examples where NLP engineers thought they they knew what people were were searching but after these kind of demo sessions or like sharing it I want to see what what people actually do there.
And then they realized oh like they use a lot of key work queries or they never put a question mark at the end or they have a lot of misspellings what else. So I think there's a lot of early learnings that you can make as a developer from these demos and understanding it out.
And also I think on the other side just creating this early aha moment this kind of wow effect and some trust on the end user side is also crucial.
So I would say that's a cycle one point very early demo getting this initial feedback and then probably the second point that we see often is when you then had a time of running your experiments tuning your pipeline kind of the way to production.
I think then at some point a second phase where you you just do again some manual evaluation with end user so not completely relying on on machine learning metrics.
Because we think there's some kind of metric blindness in the industry sometimes you just kind of get obsessed with your one metric that you optimize in these experiments and whatever it is just increasing it from experiment experiment.
And you go to production and you realize wow okay this metric is doesn't say say anything about the user set of satisfaction that I have in the end.
And there are so many examples from our customers where just handing out this pipeline showing kind of like search queries and results and then collecting some easy kind of thumbs up thumbs down feedback and.
And then trying to correlate is that really what we also saw in our experiments in our metrics and in the thing in many cases was that either the pipeline was not yet ready for production and they were like it's.
The far less accurate than we thought or also case where it was the other way around where teams thought are stuck we will never go beyond and like a for a for one score of 60% we do not here it's it's not working.
And they kind of handed out this this predictions or like get this demo and then people actually don't like notes like these predictions are perfectly fine.
And when you then dig deeper I think it's often that engineers not look enough into the data I think I'm just kind of rely on this high level metric. And the thing especially nowadays. These metrics only tell the part of the story because you're like for question answering also for search.
If you have a relation data set and let's say you always label the exact answer for certain question or query. There's just so many ways how you know. Can give a correct answer for for question that is different to this label so to give an example.
And we have many customers financially domain so typical question there is. How will revenue evolve next year and maybe in your data set and the evaluation data set you labeled. It will increase by 12%.
And now at the prediction time your model maybe finds another passage or generates the answer and says it will significantly increase. So like there's no overlap at all from a lexical side still both answers make sense and and are correct and we can probably debate now which one is more accurate.
But in many cases there is they basically give the same same answer semantically. But they're just formulated very differently and that's where I would say traditional metrics fail.
So yeah, we need better metrics and we basically did some research work on that and also part of the haystack where you can do like more semantic answer similarity or as a metric.
But it's of course also just I think looking at your data and looking at these predictions and seeing if they're really wrong on or if they're actually okay and maybe it's some problem of metrics or you are labeling process where maybe you need to collect more different options that are okay.
Yeah, I totally agree it's like it's it's a challenge of intersecting user language with whatever machinery you have to answer that right be it's part search be dense search doesn't matter like users don't care what they care is that their language is understood and often enough it's not.
Especially around things like bird if we go dance bird model doesn't understand engagements right there was a research paper on that and that might actually harm. There was even a Google example where it's showing the opposite like you say I don't want that but they say yes you actually do.
And then take that medicine which might be harmful. And then the metrics is essentially what I get from what you just described essentially you might have offline metrics right let's say and DCG or precision or recall whatever and then you have online metrics right.
And actually crafting the online metrics is is also our an art and it's never ending journey and just recently I came across one blog post which was shared by a former Netflix engineer.
 I will make sure to link it in the show notes as well describing click residual metric right so it's what is you expected success on on on on that let's say segment of your market whatever on the queries versus what you got and then people still keep trying and trying and trying but just doesn't deliver so you could have these as a low hanging fruit to fix your system right and so.
 Do you see that maybe that's already happening in haystack or do you see that that might happen that I as a user might be able to describe my metric let's say in the form of Python or JavaScript code whatever plug it into haystack and let it measure what I want and kind of mimic the online metric in substance.
 So I think like providing kind of custom metrics yeah yeah yeah yeah and you can can can do that to some degree already like plugging in basically like a Python function and forwarding it that's the one way I think the other is probably on a on a note level so you can imagine this pipeline they're providing at some point that you can have a lot of connections be it answers or documents so you can also easily kind of add custom notes various have like this this no check now I'll compare it to whatever you want or like maybe on an online setting kind of write some locks somewhere like take some some signals from from from the early query to an extensive the way you can monitor it.
 So yeah I think there's that's probably one of the kind of next steps where we see it's more and more online metrics more and more online experiments I would say right now where we see big parts of the market I think that's the more in that phase of developing experimenting finding the pipeline getting it initially to production and having their radio would say smooth journey and having a fast path to production kind of high success rates for these projects and I would say it's very right now focused on more.
 But yeah I would say further down the road if you really think about the whole and add up life cycle I think on the monitoring side there's this logic and one online metrics but also then things like data drift my queries actually shift into a different direction to these things a lot of our query profiles and we think like what I actually these use case how how are how can we describe this query distribution and this can be on a formal level like say again questions for those keyboard queries but could be also on a topic level to understand what is a profile at point a I mean we can match it with certain pipelines but also is that kind of changing over time.
 Yeah yeah you you somewhat anticipate like expected my question or sort of partly answered my question and my next question about where do you see the biggest effort in haystack and and deep set cloud going let's say beyond ML ops you know tightening the knobs and making sure that this flies and works correctly.
 More towards I know you guys also hiring a product manager so sort of like more on vision side and connected to that if you will what do you think is missing on the market today still maybe in understanding maybe in perception level maybe in tooling you already alluded also to things like metric blindness right and and and maybe when users get stuck and thinking that this is a wrong system but actually it's not they just didn't look the right way and things like that.
Yeah and there's I think the ton of works to left I think we are we already talked about it I think things progressed a lot in the last years it's crazy to see but still I feel it's with the in the middle of it or just starting and so much more work and things you can improve and do better.
 Yeah I would say for us right now there's like a lot of different directions but I think especially on the on the open source side we want to improve the developer experience also like simplifying the first steps within haystack I think it can be still overwhelming and I really want to make sure that get as many people to the first aha moment like using all your own data asking a few questions comparing sparse to dense retrieval and really experiencing this first hand I think this is one of the things we work on.
 Then a lot around multi model so we recently added support for tables within haystack so I think one interesting direction right now that you can really query into these kind of tables in your documents but maybe also further down the road into your SQL database as another data source and then of course everything around images videos audio and it's also interesting for us I think for our customers.
Because it's typically less important than kind of tax in tables but still I think it's interesting interesting options that you can do there.
So yeah I think that's like a lot on on open source side and deep set cloud are we recently launched basically the experiments module that was one big step forward there and now it's a lot around giving there also guidance and suggestions like.
 Like for example now I have the experiment I ran an experiment I've like a lot of these metrics I have a lot of data that was somehow generated but as it's not a single model anymore it's like a pipeline I really want to understand as a data scientist okay like where should I not focus on or like where what's probably a good way forward to improve this pipeline is a rather the retrieval problem is a rather.
Another note that I should improve is maybe something wrong with my evaluation data set should I go back to labeling and like giving these kind of at least making these kind of analysis easier.
It's something that we work on right now and then I think further down the road that will be for us a lot expanding in this world ML of life cycles what we talk about right monitoring without just making it simpler to integrate it at both ends so.
Basically on the one side ingesting your source data more easily and thinking it more easily into into deep set cloud so that you can say I know either maybe I have a wiki system that I use maybe I don't know I use notion or maybe I use.
 So I think it's just a little bit more conflict or I have a not another elastic such class that we already my my documents that I'm interested in so we having there kind of smooth connectors that you can can import your data and directly work on it and then on the other end the API now how can I easily get now a kind of search bar or search functionality in my final product.
 So there's a lot of things and then everything around fine tuning few short learning with large language models that something we are quite excited about because I think mentioned I think right now there's already made a big step forward that you there are a lot of use cases where you don't need to train at all anymore and then maybe that's a misperception that you also see in the market.
 I think to the typical users come to us and say like oh yeah this use case how can I train and then we usually ask did you really need to train your own model like have you tried this and that take these kind of combinations and kind of models that are out there certain sentence transformers certain pre trained QA models or rank models and that no no but like our use cases are different and that won't work and.
In many cases it does or at least they're surprised how good it is already and maybe it's enough to get started on it and so I think that's one misperception still I think there are then also these cases to be fair where fine tuning still helps right and where you really care about if you.
 So you can go to percentage points better accuracy and where you then go down and say let's now start labeling let's collect either like we in this manual labeling process or maybe from some more noisy maybe real time like a production data where you saw what people search what they clicked how can we use that maybe for training that's something where we see big potential probably for next year.
And basically want to simplify this domain adaptation to have less manual effort and basically more automated way of of training it and that I think was also that in the direction of maybe large language models.
 Yeah sounds cool and if we go in even in look even further into the future would say I don't know 5 10 years out do you think that haystack at some point may even start suggesting the user what to try you know if you go and set up a key PI for yourself right you end goal and then through the chain and that I see click graph it looks like finds a weak node and say yes something is going on there.
Then it would actually suggest you also to try some other model do you think it's possible or do you think it's a wrong direction at all like to you drive and leave this to the creativity of your users.
I think it's a combination of both so I definitely think that helps to accelerate and certain parts of your work so especially I think suggesting what experiment to run next or what it could be something you can try.
So I'm a big fan of that and I think we don't need to go probably like 5 or 10 years down the road that is happening already sooner so I can and haystack and deep set cloud.
 And maybe just like one thing we are so we have our company something called Hockey Friday so it's like one Friday every month where every person the company can work on whatever they want so really hacking on crazy ideas trying stuff out and I know that this Friday people are working on a generative model where you basically give in you describe what you want like what kind of pipeline so you can type in.
 And let's say I want documents such pipeline that works on legal data that is very fast something like that and the output is basically a YAML file that describes this haystack pipeline which you can then easily kind of load and Python try out and also write a load and then deep set cloud and run it there.
So that's actually we are experimenting with right now and and of course some time for the down the road I could see that you can take also like signals from from what we know from what worked on certain domains and and basically fuse that in into this maybe a generative process.
Yeah, it sounds cool actually reminded me of the time when I was doing my PhD something like 12 years ago a bit more.
I had a collaborator who wrote a paper on taking taking the user text and converging that into C++ code and the use case I don't remember exactly all the details of the use case but I remember it was some way in the airport so like they do a lot of this routine work.
And instead of repeating it you could actually build a smarter system right so you think this could be the future of haystack or maybe the industry at large.
Yeah, at least I think it's like one if you want element that helps accelerating right so if you also if you look at the core pilot right now I like it a lot for calling and I'm still in many cases surprised what what co pilot suggests you're as a as a note on the code level.
And I think something similar as also positive on the machine learning side and you are not only generates a correct code but really something that fits for for use case and to describe it.
I mean I think it's like if you think about the big up picture I think it's one piece that helps you in your workflow. I think it's there's still like many many other pieces that we need to get right and that won't be that's it a holy grail I think at the end.
What I really believe in is that you need a framework or a platform where we want to call it where you can easily compare things on your data and and I think this helps a lot then and creating transparency in the market creating also like kind of.
 Trust for your own use case that you are not basically doing a technology choice before you actually started working on your use case and that I think holds for vector databases where maybe today this is a good choice for you but maybe I know one year down the road maybe you want to switch this I think this market is so early that it's very hard to place a batch right now on one of these technologies and similarly I think this is on the model.
 Modeling side there's like so so much crazy bus around large language models and can firstly see the trend going there but it's also I think very important to to understand if that's really useful for your use case now how it compares to much smaller models and and this should be easy right this shouldn't this shouldn't be big part of your project it should be rather.
You were trying to think about options you want to try maybe getting some suggestions as well there but this would be I think this is a human creativity part as well and then the the actual.
So a swapping of components and comparing their making them comparable I think that's nothing where you should spend time as a developer on.
 And like connected to the question about future maybe causing off of on that we recently built with my colleague are netalman a multi model and multilingual search demo right where we used clip model of the shelf without any fine tuning on web data and it showed us really really amazing results right so like where keyword search cannot find because simply.
We metadata doesn't have it and it's multilingual right so and it type it the same query with neural retrieval and it gets it.
Is there anything stopping high stack to move into that direction as well sort of like crossing the boundary of only text right so like you did say multi model in the context of let's say queering a table but I could also query an image.
So the same with the test stack is going in that direction as well.
 Yeah so we are actually like real right now working on it so we have a first case where we want to support where you have a text query but you can query also into images from the right side side and then basically now other way around would be probably one of the later ones they have an image as a query until I want to find different media types, I'd say.
But yeah this is like definitely what we right now working on. I think I also think we need to think always see what are the big use cases and what kind of customers you have and how do we use it.
I think with images there's a lot of interesting use cases mainly in e-commerce I would say that's cool. Yeah, we are already supported to some degree and will support more I think in the next month.
 That's great to learn and that also means that I need to adjust my classification because I've been presenting what I know about the players in in vector database and neural frameworks and specifically for haystack I put NLP as the main vertical and I think largely you guys still advertise that as the main vertical but I think nothing stops you from.
Switching that to multi modality right so NLP computer vision and maybe even speech at some point.
Yeah totally I think our approaches there's just a bit like doing one thing to quite a depth first and then moving on to the next rather than let's say starting with very high level basic support for all modalities and then kind of growing all of them.
So what we rather did in the past and still doing is very deep support for texts and we haven't there everything in place before kind of moving on to the next that's a bit of a philosophy question maybe a strategic questions what you want to do it.
So this field multi is changing quite a lot right so a lot of things generative models really big large models models that I don't know even how to use yet you know like dali.
Of course beyond just kind of experimental interest but probably there will be some use cases where do you think else the trends are going in this space.
 Yeah so we like want one big trend I think for sure is these large language models and everything around it and as I talked earlier about it the questions like where is it right now and is it already today really usable is it already kind of worth investigating them comparing them for for your own use cases.
 I think there we are I would say still in an early phase it's look at for example GPT 3 and and I think it's high months to the quite nice analysis earlier this year where compared embedding some GPT 3 and will more standard size transformers and there we think we saw it's the performance is it's not bad but it's also definitely not our performing.
You say regular size models which are a thousand times smaller cost a few dollars in not thousands and tens of thousands of dollars for for your influence costs.
So I think that's it's basically right now as to let's see case by case that it makes sense for use case but if you think look a bit further into the next years.
 I'm pretty sure and convinced that this is only a matter of time until we see more and more large language models really in production also in search pipelines in production and think that now it's this phase of figuring out how can we make them really more efficient more more reliable so we really can trust these these results there.
 Not going to be an easier way update to new knowledge and I would really but now look a lot into and what I'm personally quite excited about is now this I think area of research around retrieval based NLP so yes on the one hand side kind of scaling up the models making them bigger because we think learned and over last years that they are good few short learners and I think that's really good.
And that's of course exciting because you can just take these models and kind of throw a task at them and they will perform so less manual work of of annotating data creating domain specific data sets and so on.
But I think we also saw that they are not very efficient and there are these other problems.
 How do you how do you actually now teach not to be free about recent events or about your own domain knowledge and typically I think these these data sets that you that you want to search in they're not static right so there's a constantly evolving and you really want to retrain these crazy models every few days or weeks just to kind of catch up with us.
And I think that's like where this stream of retrieval based or achievement at models is super interesting and I think there's a lot of cool work. So just this week we're back from from Patrick Lewis publication around the Atlas model. Sure if you saw it. But there's basically the idea.
Can we can we somehow remove the say the memory part from these big models and it kind of outsource it to a database to an index and then at a query time we still have like a large model.
 Can we look complex reasoning, but it's kind of basing the generation on some retrieve documents and that can be useful for search but can be also for an effect checking or other use cases and and long story short, I think they have interesting they did love interesting experiments and that paper that show that you can actually outsource quite a bit of these parameters of this memory into into a say a vector like the database and and still keep the few shot capabilities of these giant language models.
And I think this is like a super cool route like larger models but still not putting everything in it, not not blowing up parameters, parameters size unreasonably. Let's do combining it with now let's say an external document base or knowledge base.
Yeah, I think it's the topic attached upon it's fascinating that on one hand, let's say you have a model, right? And if you if you keep retraining it or fine tuning it on on latest data, you may run into this.
I think it's called catastrophic forgetting, right? Like things that we as humans know that I don't know what is liquid kind of on high level without going into chemistry.
 And it's not that we think about it every single day when we drink water, but like it's not that we actually forget it if somebody asks us right no matter how many news or papers, whatever the red books right we still remember the basic facts and and I think what you just said with the Atlas model right so approach outsourcing that memory into some database that you can maybe even control and say, okay, these facts need to stay.
I never want them to go away no matter what right, these are like basic principles and maybe they exist in every domain like finance or healthcare and so on. And yeah, I think this is interesting direction.
Yeah, absolutely all these facts change right can also be that over time you have to adjust facts or knowledge and this is way easier I think if you have it explicitly somewhere in documents, not so much in the just in the parameters model.
Yeah, exactly exactly and like maybe just one example that comes to my mind is like CT CT's change names right and so you could still go back and say what was the name of that CD between you know 1995 and 2000 right something like that.
Yeah, or presidents of nations also change right so for this kind of queries I think you want to make sure that you're up to date and change it.
Yeah, and I think maybe coming back to search understanding the context will place such a huge role once these models become even more mature and available and knowledge aware.
 But but the challenge of extracting contacts from the query still is there if I say who is the president of the United States, it might you know conclude that i'm asking about now present but if I was couple programs above already saying setting the stage about specific period of time in the past it could actually reason that I'm maybe not asking about presence right.
Exactly could do this reasoning or could you ask a clarifying question right or say like all like here are a couple of options that you mean this like as you may want more to win a human conversation.
Yeah, so I think it's called conversational information retrieval right and I think that we might start seeing this blend of what probably today is called chatbot and a search engine but it could be a search engine which is just clarifying.
Yeah, I mean overall I think it's it's that's a thing also is in the field I think we are seeing that the search what we understand undersurge is evolving right so it's not so much anymore so I think about web search engines.
Yeah, in few cases you were still you search search and you click on the website and then you'll search somewhere your information.
But in many cases we will kind of zero click search now where you have your query and within the search results you already find what you want at and i think this is just yeah getting more and more popular that.
Yeah, you're not providing say there's the route to go to another knowledge source but you're trying to really answer the query directly and there's no need to go further.
I will also try to remember to link one paper maybe it's like a series of papers from Microsoft where they try to embed knowledge into the language model and that's.
I think it's a very interesting direction as well as also embedding knowledge graphs into the model right because one way, as you said, and I think that trend probably still there that yeah you can keep adding parameters more and more billion trillions.
But at some point it just simply becomes an on practical in practical right to to have such a large model in production and then how do you find unit. But again it doesn't capture the relationships well enough right if you didn't explain it.
 Absolutely and just thinking of it we have actually a meetup end of September so if you're interested or anyone who's listening where a needs Rimas will also talk about exactly that topic how do you kind of incorporate knowledge into a language model and and that would be our end of September as well just look for maybe can link it in the show notes.
So I will manage you to experience absolutely will do that gladly my favorite question I know you touched many times as well during this podcast which I really really enjoyed.
But what what else drives you beyond you know you have a role as a city or you have a role as a pioneer in this space and maybe educating and reaching more and more people. Is there something else that drives you sort of beyond the tech itself in this field.
Yeah like I mean that I think my make sight of my passion for an appears here. I hope that came came through.
But for me like the technology is the one thing but then really seeing how you solve problems with that like how you can make annoying work of financial analyst faster and better like just seeing that. Either say first and because they are customer or it's a more indirectly.
If you know that this is now kind of possible. So I think it's like still a big driver for me personally and I think I want to think I absolutely love about open source that it's not just paying users commercial users where you kind of see that.
But we are really this huge community by now from haystack where there's so many different people with different backgrounds, different use cases and it's.
For me often like just end of the day really like scrolling through and all get up issues kind of questions that come in or on Slack when are we on discord.
Like what what people are actually building with that and and it's really cool to see what they kind of use case come up with but also how far this actually got that it's.
And using so many companies all around the world from big tech to classical enterprise to start up to build their products on top. And that thing is a stick to one of my biggest motivation boosters that you can get at seeing the community appreciating using it.
And and probably also like on tip beyond get up just recently ran into a guy in a bar who he and Berlin who who use taste that and let's definitely something I never would have imagined a few years ago.
And this kind of happens or what we said of a glass here when we find a bit of a vision and thought about some goals at the company offside.
I think one of us for the open source side that people start putting say haystack experience into their job requirements or the other way around people putting that in the CVs and we thought oh, I guess this is maybe three years down the road.
But then a few weeks afterwards we saw these first job postings where this was required and also TVs where this was mentioned.
So I think it's just cool to see how you can leave a footprint and beyond let's say you are immediate bubble but really kind of spreads it's open it's all digital it's kind of connected in the world right. And leaving those kind of footprint is what I enjoy.
And yeah, search in I think as a domain in us just for me really interesting because it's so diverse as you can go in many directions can dive very deep into an IP can think a lot about the user side at the end for what use cases you can make it work. And can think a lot about scalability.
It's just I think the one of the most for my point most exciting and diverse applications of technology right now. And and one way I think you can really relate to like really can think okay what what is actually possible what kind of information you can make accessible.
And that's that's obviously the beauty of it. Yeah, it's beautifully put thanks thanks for sharing I know some some of the guests that I asked this question would probably think hey why is this philosophical question I'm just you know doing it I like it but that's it.
But I think it gives so much to towards you know you reflecting on what you do because that might also influence your choices in in the tech or in how you approach your users what message you send and so on and so forth and maybe reconsider some things as well.
 And an open source part you reminded me of one story when it was my first time visit in the US I think it was 2015 and it was a patchy coin I was crossing on the traffic light you know on the pedestrian crossing and it was like this narrow avenue you know not narrow white and every right and select takes on my account like few minutes but it's of course not minutes maybe 20 seconds.
And I think it's really bouncing to me from the other side of the road saying I know you I was like no it's impossible it's my first time visit you know I don't I'm not a public figure.
How is it possible and he said he because you build look it's one of the open source kind of you seen in the extriders that I used to work on.
You know which I inherited from its original creator Andrej Blyetski and that's it he didn't stop to say anything else but but he made my day you know and I think what you felt in the bar was probably similar knowing that that person uses haystack and you know it's amazing.
Absolutely because it's just it feels very honest right it feels like it is is not because we know it's crazy marketing or anything like that it's just like a really like a natural community thing and and just building something that's useful for others.
 Yeah exactly which probably reinforces you and gives you these well in this case direct feedback well not only specifics of your of your platform but actually the fact that they're using it and relying on and building a business and that tells the two decisions you made in the architecture and so on and so forth that's amazing.
Yeah I mean like from a company perspective that's one of the fastest feedback cycles you can have right and like seeing diverse use cases diverse developer person on us how they approach things what they're struggling with. Yeah also that angle it was fast yeah absolutely crucial.
I think it's the best and it's like I think it's Elon Musk who said the best setting is when your user fell in love with your product and once you just succeed so yeah there you go amazing and I've enjoyed this podcast so much is there anything you want to announce to our listeners.
 Yeah we've just the meet up I already mentioned so if you're interested in LP that's happening in September it will be hybrid so you can join online but if you're if you happen to be in Berlin we also have a small on site event and then yeah of course if you haven't tried hastag yet maybe check it out on GitHub.
 As a prompt every promise you can get an easy first pipeline up and running and just give it a try to try to question answering if you haven't if you're more coming from traditional search and down on deep set cloud as mentioned we just released a big new model on experiments with still an early stage with with the product but we have an early access program so if you're interested if you're having a lot of use case that you want to bring to production in a fast way where you think about how to scale it how to actually find that pipeline how to collaborate with with your end users and get some feedback there just reach out to us and then we can can get you on the on the early access program.
 Amazing thanks so much multi have enjoyed again saying this and this was deep and thoughtful and we will make sure to link all the all the goodies that you mentioned in the show notes and I hope to meet some day maybe in Berlin maybe somewhere else but absolutely yeah let's make that happen and I totally enjoyed our conversation as well so thanks but for having me.
It's definitely interesting fantastic all the best with haystack and and with your research and development. Thanks a lot thanks all the bye bye bye. you