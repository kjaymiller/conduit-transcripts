---
description: '<p>YouTube: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=AU0O_6-EY6s">https://www.youtube.com/watch?v=AU0O_6-EY6s</a></p><p>Topics:</p><p>00:00
  Intro</p><p>01:03 Yusuf’s background</p><p>03:00 Multimodal search in tech and humans</p><p>08:53
  CLIP: discovering hidden semantics</p><p>13:02 Where to start to apply metric learning
  in practice. AutoEncoder architecture included!</p><p>19:00 Unpacking it further:
  what is metric learning and the difference with deep metric learning?</p><p>28:50
  How Deep Learning allowed us to transition from pixels to meaning in the images</p><p>32:05
  Increasing efficiency: vector compression and quantization aspects</p><p>34:25 Yusuf
  gives a practical use-case with Conversational AI of where metric learning can prove
  to be useful. And tools!</p><p>40:59 A few words on how the podcast is made :) Yusuf’s
  explanation of how Gmail smart reply feature works internally</p><p>51:19 Metric
  learning helps us learn the best vector representation for the given task</p><p>52:16
  Metric learning shines in data scarce regimes. Positive impact on the planet</p><p>58:30
  Yusuf’s motivation to work in the space of vector search, Qdrant, deep learning
  and metric learning — the question of Why</p><p>1:05:02 Announcements from Yusuf</p><p>-
  Join discussions at Discord: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://discord.qdrant.tech">https://discord.qdrant.tech</a></p><p>- Yusuf''s
  Medium: <a target="_blank" rel="noopener noreferrer nofollow" href="https://medium.com/@yusufsarigoz">https://medium.com/@yusufsarigoz</a>
  and LinkedIn: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.linkedin.com/in/yusufsarigoz/">https://www.linkedin.com/in/yusufsarigoz/</a></p><p>-
  GSOC 2022: TensorFlow Similarity - project led by Yusuf: <a target="_blank" rel="noopener
  noreferrer nofollow" href="https://docs.google.com/document/d/1fLDLwIhnwDUz3uUV8RyUZiOlmTN9Uzy5ZuvI8iDDFf8/edit#heading=h.zftd93u5hfnp">https://docs.google.com/document/d/1fLDLwIhnwDUz3uUV8RyUZiOlmTN9Uzy5ZuvI8iDDFf8/edit#heading=h.zftd93u5hfnp</a></p><p>-
  Dmitry''s Twitter: <a target="_blank" rel="noopener noreferrer nofollow" href="https://twitter.com/DmitryKan">https://twitter.com/DmitryKan</a></p><p>Full
  Show Notes: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=AU0O_6-EY6s">https://www.youtube.com/watch?v=AU0O_6-EY6s</a></p>'
image_url: https://media.rss.com/vector-podcast/20220507_080542_57009c58f961b6d0713e057b9a5a4832.jpg
pub_date: Sat, 07 May 2022 20:37:42 GMT
title: Yusuf Sarıgöz - AI Research Engineer, Qdrant - Getting to know your data with
  metric learning
url: https://rss.com/podcasts/vector-podcast/479453
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 24.72, "text": " Hello,
  today we have a new episode of the Vector Podcast and today I''m super happy to
  have", "tokens": [50364, 2425, 11, 965, 321, 362, 257, 777, 3500, 295, 264, 691,
  20814, 29972, 293, 965, 286, 478, 1687, 2055, 281, 362, 51600], "temperature": 0.0,
  "avg_logprob": -0.3523604583740234, "compression_ratio": 1.0714285714285714, "no_speech_prob":
  0.16972854733467102}, {"id": 1, "seek": 2472, "start": 24.72, "end": 32.64, "text":
  " you, Suf Sangos, with me. He holds the role of AI Research Engineer at Quadrant.
  It''s a", "tokens": [50364, 291, 11, 2746, 69, 19037, 329, 11, 365, 385, 13, 634,
  9190, 264, 3090, 295, 7318, 10303, 15808, 412, 29619, 7541, 13, 467, 311, 257, 50760],
  "temperature": 0.0, "avg_logprob": -0.2194515677059398, "compression_ratio": 1.5743801652892562,
  "no_speech_prob": 0.2924881875514984}, {"id": 2, "seek": 2472, "start": 32.64, "end":
  38.32, "text": " Vector Search Database Company and you might remember we had an
  episode with Tom Lackner,", "tokens": [50760, 691, 20814, 17180, 40461, 651, 13918,
  293, 291, 1062, 1604, 321, 632, 364, 3500, 365, 5041, 441, 501, 1193, 11, 51044],
  "temperature": 0.0, "avg_logprob": -0.2194515677059398, "compression_ratio": 1.5743801652892562,
  "no_speech_prob": 0.2924881875514984}, {"id": 3, "seek": 2472, "start": 38.32, "end":
  44.16, "text": " who is the user of Quadrant today. We have an episode and discussion
  with you, Suf, who works for", "tokens": [51044, 567, 307, 264, 4195, 295, 29619,
  7541, 965, 13, 492, 362, 364, 3500, 293, 5017, 365, 291, 11, 2746, 69, 11, 567,
  1985, 337, 51336], "temperature": 0.0, "avg_logprob": -0.2194515677059398, "compression_ratio":
  1.5743801652892562, "no_speech_prob": 0.2924881875514984}, {"id": 4, "seek": 2472,
  "start": 44.16, "end": 50.8, "text": " Quadrant. And one of the core topics today
  we''re going to be discussing metric learning, but before that,", "tokens": [51336,
  29619, 7541, 13, 400, 472, 295, 264, 4965, 8378, 965, 321, 434, 516, 281, 312, 10850,
  20678, 2539, 11, 457, 949, 300, 11, 51668], "temperature": 0.0, "avg_logprob": -0.2194515677059398,
  "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.2924881875514984},
  {"id": 5, "seek": 5080, "start": 50.8, "end": 60.4, "text": " hey, you Suf, how
  are you doing? I''m very excited to join you in this episode to discuss metric learning",
  "tokens": [50364, 4177, 11, 291, 2746, 69, 11, 577, 366, 291, 884, 30, 286, 478,
  588, 2919, 281, 3917, 291, 294, 341, 3500, 281, 2248, 20678, 2539, 50844], "temperature":
  0.0, "avg_logprob": -0.1496942705578274, "compression_ratio": 1.5265957446808511,
  "no_speech_prob": 0.00567870307713747}, {"id": 6, "seek": 5080, "start": 60.4, "end":
  67.12, "text": " and thank you for having me. Yeah, thanks for coming up. Really,
  I think this topic is", "tokens": [50844, 293, 1309, 291, 337, 1419, 385, 13, 865,
  11, 3231, 337, 1348, 493, 13, 4083, 11, 286, 519, 341, 4829, 307, 51180], "temperature":
  0.0, "avg_logprob": -0.1496942705578274, "compression_ratio": 1.5265957446808511,
  "no_speech_prob": 0.00567870307713747}, {"id": 7, "seek": 5080, "start": 67.67999999999999,
  "end": 74.64, "text": " something that has been crossing my area of focus and also
  some of the questions that users are", "tokens": [51208, 746, 300, 575, 668, 14712,
  452, 1859, 295, 1879, 293, 611, 512, 295, 264, 1651, 300, 5022, 366, 51556], "temperature":
  0.0, "avg_logprob": -0.1496942705578274, "compression_ratio": 1.5265957446808511,
  "no_speech_prob": 0.00567870307713747}, {"id": 8, "seek": 7464, "start": 74.64,
  "end": 80.64, "text": " asking, you know, okay, if I have this data set, how can
  I be sure that it will work with neural", "tokens": [50364, 3365, 11, 291, 458,
  11, 1392, 11, 498, 286, 362, 341, 1412, 992, 11, 577, 393, 286, 312, 988, 300, 309,
  486, 589, 365, 18161, 50664], "temperature": 0.0, "avg_logprob": -0.2478970399836904,
  "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.013338115066289902},
  {"id": 9, "seek": 7464, "start": 80.64, "end": 85.76, "text": " search, right? And
  I think metric learning seems to be one of the answers. But before we start", "tokens":
  [50664, 3164, 11, 558, 30, 400, 286, 519, 20678, 2539, 2544, 281, 312, 472, 295,
  264, 6338, 13, 583, 949, 321, 722, 50920], "temperature": 0.0, "avg_logprob": -0.2478970399836904,
  "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.013338115066289902},
  {"id": 10, "seek": 7464, "start": 85.76, "end": 91.12, "text": " discussing this
  in deep, in depth, I was thinking, could you please introduce yourself to our audience?",
  "tokens": [50920, 10850, 341, 294, 2452, 11, 294, 7161, 11, 286, 390, 1953, 11,
  727, 291, 1767, 5366, 1803, 281, 527, 4034, 30, 51188], "temperature": 0.0, "avg_logprob":
  -0.2478970399836904, "compression_ratio": 1.5378486055776892, "no_speech_prob":
  0.013338115066289902}, {"id": 11, "seek": 7464, "start": 92.64, "end": 101.04, "text":
  " Yes, sure. Armist has told Suf''s software developer and AI researcher with a
  background in", "tokens": [51264, 1079, 11, 988, 13, 11893, 468, 575, 1907, 2746,
  69, 311, 4722, 10754, 293, 7318, 21751, 365, 257, 3678, 294, 51684], "temperature":
  0.0, "avg_logprob": -0.2478970399836904, "compression_ratio": 1.5378486055776892,
  "no_speech_prob": 0.013338115066289902}, {"id": 12, "seek": 10104, "start": 101.12,
  "end": 112.08000000000001, "text": " linguistics at the university. Actually, I''ve
  been developing software since my high school years.", "tokens": [50368, 21766,
  6006, 412, 264, 5454, 13, 5135, 11, 286, 600, 668, 6416, 4722, 1670, 452, 1090,
  1395, 924, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1662550083426542, "compression_ratio":
  1.3706293706293706, "no_speech_prob": 0.006533720064908266}, {"id": 13, "seek":
  10104, "start": 113.04, "end": 123.04, "text": " During my master''s study, I combined
  my experience and my education to study machine translation.", "tokens": [50964,
  6842, 452, 4505, 311, 2979, 11, 286, 9354, 452, 1752, 293, 452, 3309, 281, 2979,
  3479, 12853, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1662550083426542,
  "compression_ratio": 1.3706293706293706, "no_speech_prob": 0.006533720064908266},
  {"id": 14, "seek": 12304, "start": 123.28, "end": 134.4, "text": " After several
  years of experience in different roles and at different startups,", "tokens": [50376,
  2381, 2940, 924, 295, 1752, 294, 819, 9604, 293, 412, 819, 28041, 11, 50932], "temperature":
  0.0, "avg_logprob": -0.22620956521285207, "compression_ratio": 1.416, "no_speech_prob":
  0.03279435634613037}, {"id": 15, "seek": 12304, "start": 138.24, "end": 151.20000000000002,
  "text": " I ended up with the multi model retrieval because I had a long experience
  in both computer vision", "tokens": [51124, 286, 4590, 493, 365, 264, 4825, 2316,
  19817, 3337, 570, 286, 632, 257, 938, 1752, 294, 1293, 3820, 5201, 51772], "temperature":
  0.0, "avg_logprob": -0.22620956521285207, "compression_ratio": 1.416, "no_speech_prob":
  0.03279435634613037}, {"id": 16, "seek": 15120, "start": 151.2, "end": 159.92, "text":
  " and measured language processing. So for some time, my main focus is metric learning.",
  "tokens": [50364, 293, 12690, 2856, 9007, 13, 407, 337, 512, 565, 11, 452, 2135,
  1879, 307, 20678, 2539, 13, 50800], "temperature": 0.0, "avg_logprob": -0.31396156549453735,
  "compression_ratio": 1.4619883040935673, "no_speech_prob": 0.003455967642366886},
  {"id": 17, "seek": 15120, "start": 161.11999999999998, "end": 171.51999999999998,
  "text": " I was already a user of co-advent, even before joining co-advent and I
  thought it would be very cool", "tokens": [50860, 286, 390, 1217, 257, 4195, 295,
  598, 12, 345, 2475, 11, 754, 949, 5549, 598, 12, 345, 2475, 293, 286, 1194, 309,
  576, 312, 588, 1627, 51380], "temperature": 0.0, "avg_logprob": -0.31396156549453735,
  "compression_ratio": 1.4619883040935673, "no_speech_prob": 0.003455967642366886},
  {"id": 18, "seek": 15120, "start": 172.48, "end": 178.88, "text": " to work for
  an open source project that I find valuable myself.", "tokens": [51428, 281, 589,
  337, 364, 1269, 4009, 1716, 300, 286, 915, 8263, 2059, 13, 51748], "temperature":
  0.0, "avg_logprob": -0.31396156549453735, "compression_ratio": 1.4619883040935673,
  "no_speech_prob": 0.003455967642366886}, {"id": 19, "seek": 17888, "start": 179.6,
  "end": 186.32, "text": " Yeah, sounds awesome. Sounds cool. You just mentioned multi
  model. So you mean like multi model search,", "tokens": [50400, 865, 11, 3263, 3476,
  13, 14576, 1627, 13, 509, 445, 2835, 4825, 2316, 13, 407, 291, 914, 411, 4825, 2316,
  3164, 11, 50736], "temperature": 0.0, "avg_logprob": -0.2481926781790597, "compression_ratio":
  1.541237113402062, "no_speech_prob": 0.025137102231383324}, {"id": 20, "seek": 17888,
  "start": 186.32, "end": 194.0, "text": " right? And I think this field is still
  kind of in many ways shaping up and many people are still", "tokens": [50736, 558,
  30, 400, 286, 519, 341, 2519, 307, 920, 733, 295, 294, 867, 2098, 25945, 493, 293,
  867, 561, 366, 920, 51120], "temperature": 0.0, "avg_logprob": -0.2481926781790597,
  "compression_ratio": 1.541237113402062, "no_speech_prob": 0.025137102231383324},
  {"id": 21, "seek": 17888, "start": 194.0, "end": 199.44, "text": " learning and
  kind of scratching their heads like what is multi model? Like maybe if you could
  give", "tokens": [51120, 2539, 293, 733, 295, 29699, 641, 8050, 411, 437, 307, 4825,
  2316, 30, 1743, 1310, 498, 291, 727, 976, 51392], "temperature": 0.0, "avg_logprob":
  -0.2481926781790597, "compression_ratio": 1.541237113402062, "no_speech_prob": 0.025137102231383324},
  {"id": 22, "seek": 19944, "start": 199.52, "end": 212.64, "text": " an example or
  a little bit explain what is multi model. Yes, sure. Actually, as you just said,
  multi model is quite a", "tokens": [50368, 364, 1365, 420, 257, 707, 857, 2903,
  437, 307, 4825, 2316, 13, 1079, 11, 988, 13, 5135, 11, 382, 291, 445, 848, 11, 4825,
  2316, 307, 1596, 257, 51024], "temperature": 0.0, "avg_logprob": -0.34644007215312883,
  "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.006710630841553211},
  {"id": 23, "seek": 19944, "start": 213.2, "end": 224.48, "text": " new topic actually.
  Actually, it''s resurrecting with developments in deep metric learning.", "tokens":
  [51052, 777, 4829, 767, 13, 5135, 11, 309, 311, 34338, 278, 365, 20862, 294, 2452,
  20678, 2539, 13, 51616], "temperature": 0.0, "avg_logprob": -0.34644007215312883,
  "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.006710630841553211},
  {"id": 24, "seek": 22448, "start": 224.56, "end": 236.39999999999998, "text": "
  One of the most famous applications is a clip by OpenAI, short for contrastive language
  image,", "tokens": [50368, 1485, 295, 264, 881, 4618, 5821, 307, 257, 7353, 538,
  7238, 48698, 11, 2099, 337, 8712, 488, 2856, 3256, 11, 50960], "temperature": 0.0,
  "avg_logprob": -0.1859737237294515, "compression_ratio": 1.3642857142857143, "no_speech_prob":
  0.00890433695167303}, {"id": 25, "seek": 22448, "start": 238.0, "end": 251.28, "text":
  " the pre-training. In the most basic term, they train a model to construct a unified
  vector space", "tokens": [51040, 264, 659, 12, 17227, 1760, 13, 682, 264, 881, 3875,
  1433, 11, 436, 3847, 257, 2316, 281, 7690, 257, 26787, 8062, 1901, 51704], "temperature":
  0.0, "avg_logprob": -0.1859737237294515, "compression_ratio": 1.3642857142857143,
  "no_speech_prob": 0.00890433695167303}, {"id": 26, "seek": 25128, "start": 251.28,
  "end": 262.32, "text": " for both images and tests. Basically, they have two encoders,
  one for images and one for", "tokens": [50364, 337, 1293, 5267, 293, 6921, 13, 8537,
  11, 436, 362, 732, 2058, 378, 433, 11, 472, 337, 5267, 293, 472, 337, 50916], "temperature":
  0.0, "avg_logprob": -0.20743141617885855, "compression_ratio": 1.4424778761061947,
  "no_speech_prob": 0.0070470524951815605}, {"id": 27, "seek": 25128, "start": 263.28,
  "end": 272.8, "text": " tests, support that you have a pair of images and its textual
  description.", "tokens": [50964, 6921, 11, 1406, 300, 291, 362, 257, 6119, 295,
  5267, 293, 1080, 2487, 901, 3855, 13, 51440], "temperature": 0.0, "avg_logprob":
  -0.20743141617885855, "compression_ratio": 1.4424778761061947, "no_speech_prob":
  0.0070470524951815605}, {"id": 28, "seek": 27280, "start": 273.12, "end": 285.36,
  "text": " When you see this image and that textual description to these encoders,
  you are supposed to get", "tokens": [50380, 1133, 291, 536, 341, 3256, 293, 300,
  2487, 901, 3855, 281, 613, 2058, 378, 433, 11, 291, 366, 3442, 281, 483, 50992],
  "temperature": 0.0, "avg_logprob": -0.23163982232411703, "compression_ratio": 1.476923076923077,
  "no_speech_prob": 0.04262159764766693}, {"id": 29, "seek": 27280, "start": 286.56,
  "end": 301.2, "text": " very similar vectors, vector output from these encoders.
  So you can search images with a textual", "tokens": [51052, 588, 2531, 18875, 11,
  8062, 5598, 490, 613, 2058, 378, 433, 13, 407, 291, 393, 3164, 5267, 365, 257, 2487,
  901, 51784], "temperature": 0.0, "avg_logprob": -0.23163982232411703, "compression_ratio":
  1.476923076923077, "no_speech_prob": 0.04262159764766693}, {"id": 30, "seek": 30120,
  "start": 301.2, "end": 311.92, "text": " query or Y-14. So you sort of crossed the,
  so in a way with one modalities text or image is another", "tokens": [50364, 14581,
  420, 398, 12, 7271, 13, 407, 291, 1333, 295, 14622, 264, 11, 370, 294, 257, 636,
  365, 472, 1072, 16110, 2487, 420, 3256, 307, 1071, 50900], "temperature": 0.0, "avg_logprob":
  -0.3519118513379778, "compression_ratio": 1.4202898550724639, "no_speech_prob":
  0.007251732051372528}, {"id": 31, "seek": 30120, "start": 311.92, "end": 319.91999999999996,
  "text": " modality, but in this case, we kind of like cross go across modalities.
  I think we can cross the", "tokens": [50900, 1072, 1860, 11, 457, 294, 341, 1389,
  11, 321, 733, 295, 411, 3278, 352, 2108, 1072, 16110, 13, 286, 519, 321, 393, 3278,
  264, 51300], "temperature": 0.0, "avg_logprob": -0.3519118513379778, "compression_ratio":
  1.4202898550724639, "no_speech_prob": 0.007251732051372528}, {"id": 32, "seek":
  31992, "start": 319.92, "end": 330.40000000000003, "text": " border of modalities
  with this. Yeah, which I think to many users will sound like a magic because", "tokens":
  [50364, 7838, 295, 1072, 16110, 365, 341, 13, 865, 11, 597, 286, 519, 281, 867,
  5022, 486, 1626, 411, 257, 5585, 570, 50888], "temperature": 0.0, "avg_logprob":
  -0.2309054798550076, "compression_ratio": 1.5957446808510638, "no_speech_prob":
  0.004148249980062246}, {"id": 33, "seek": 31992, "start": 331.28000000000003, "end":
  339.44, "text": " you essentially, if you view an image like a set of pixels and
  if you query textual queries a set of", "tokens": [50932, 291, 4476, 11, 498, 291,
  1910, 364, 3256, 411, 257, 992, 295, 18668, 293, 498, 291, 14581, 2487, 901, 24109,
  257, 992, 295, 51340], "temperature": 0.0, "avg_logprob": -0.2309054798550076, "compression_ratio":
  1.5957446808510638, "no_speech_prob": 0.004148249980062246}, {"id": 34, "seek":
  31992, "start": 339.44, "end": 347.76, "text": " words, now you sort of somehow
  magically search your words in pixels, but actually that''s not exactly", "tokens":
  [51340, 2283, 11, 586, 291, 1333, 295, 6063, 39763, 3164, 428, 2283, 294, 18668,
  11, 457, 767, 300, 311, 406, 2293, 51756], "temperature": 0.0, "avg_logprob": -0.2309054798550076,
  "compression_ratio": 1.5957446808510638, "no_speech_prob": 0.004148249980062246},
  {"id": 35, "seek": 34776, "start": 347.76, "end": 354.48, "text": " what''s happening.
  Of course, we do the embedding and so on, but in a nutshell, it kind of sounds like",
  "tokens": [50364, 437, 311, 2737, 13, 2720, 1164, 11, 321, 360, 264, 12240, 3584,
  293, 370, 322, 11, 457, 294, 257, 37711, 11, 309, 733, 295, 3263, 411, 50700], "temperature":
  0.0, "avg_logprob": -0.2396084947406121, "compression_ratio": 1.356164383561644,
  "no_speech_prob": 0.003481998573988676}, {"id": 36, "seek": 34776, "start": 354.48,
  "end": 365.52, "text": " this magical cross model search there. Yes, I expected
  for newcomers is a little bit like magic,", "tokens": [50700, 341, 12066, 3278,
  2316, 3164, 456, 13, 1079, 11, 286, 5176, 337, 40014, 433, 307, 257, 707, 857, 411,
  5585, 11, 51252], "temperature": 0.0, "avg_logprob": -0.2396084947406121, "compression_ratio":
  1.356164383561644, "no_speech_prob": 0.003481998573988676}, {"id": 37, "seek": 36552,
  "start": 366.08, "end": 381.03999999999996, "text": " but from quite a long time,
  we have already been using vector search in the context of image search,", "tokens":
  [50392, 457, 490, 1596, 257, 938, 565, 11, 321, 362, 1217, 668, 1228, 8062, 3164,
  294, 264, 4319, 295, 3256, 3164, 11, 51140], "temperature": 0.0, "avg_logprob":
  -0.23621658325195313, "compression_ratio": 1.1764705882352942, "no_speech_prob":
  0.005519893951714039}, {"id": 38, "seek": 38104, "start": 381.12, "end": 396.16,
  "text": " but in that case, we search for images with a query which is image if
  that, but in this case,", "tokens": [50368, 457, 294, 300, 1389, 11, 321, 3164,
  337, 5267, 365, 257, 14581, 597, 307, 3256, 498, 300, 11, 457, 294, 341, 1389, 11,
  51120], "temperature": 0.0, "avg_logprob": -0.24256067073091547, "compression_ratio":
  1.4296875, "no_speech_prob": 0.0025529435370117426}, {"id": 39, "seek": 38104, "start":
  396.8, "end": 409.76, "text": " we make a connection between two modalities actually.
  This is also how our human brain is", "tokens": [51152, 321, 652, 257, 4984, 1296,
  732, 1072, 16110, 767, 13, 639, 307, 611, 577, 527, 1952, 3567, 307, 51800], "temperature":
  0.0, "avg_logprob": -0.24256067073091547, "compression_ratio": 1.4296875, "no_speech_prob":
  0.0025529435370117426}, {"id": 40, "seek": 40976, "start": 409.76, "end": 421.12,
  "text": " functioning. For the most of the time, we don''t consume the information
  from a single", "tokens": [50364, 18483, 13, 1171, 264, 881, 295, 264, 565, 11,
  321, 500, 380, 14732, 264, 1589, 490, 257, 2167, 50932], "temperature": 0.0, "avg_logprob":
  -0.19399917957394622, "compression_ratio": 1.3636363636363635, "no_speech_prob":
  0.00391565402969718}, {"id": 41, "seek": 40976, "start": 421.59999999999997, "end":
  439.36, "text": " modality actually when we try to understand our environment, we
  both take it as a visual input", "tokens": [50956, 1072, 1860, 767, 562, 321, 853,
  281, 1223, 527, 2823, 11, 321, 1293, 747, 309, 382, 257, 5056, 4846, 51844], "temperature":
  0.0, "avg_logprob": -0.19399917957394622, "compression_ratio": 1.3636363636363635,
  "no_speech_prob": 0.00391565402969718}, {"id": 42, "seek": 43936, "start": 439.36,
  "end": 451.92, "text": " and also an audio input and we also talk to people around
  them for it gives us a better", "tokens": [50364, 293, 611, 364, 6278, 4846, 293,
  321, 611, 751, 281, 561, 926, 552, 337, 309, 2709, 505, 257, 1101, 50992], "temperature":
  0.0, "avg_logprob": -0.16713650660081344, "compression_ratio": 1.3671875, "no_speech_prob":
  0.007281173951923847}, {"id": 43, "seek": 43936, "start": 452.8, "end": 465.12,
  "text": " understanding of the environment. So if we want to make our AI smarter,
  we also need to", "tokens": [51036, 3701, 295, 264, 2823, 13, 407, 498, 321, 528,
  281, 652, 527, 7318, 20294, 11, 321, 611, 643, 281, 51652], "temperature": 0.0,
  "avg_logprob": -0.16713650660081344, "compression_ratio": 1.3671875, "no_speech_prob":
  0.007281173951923847}, {"id": 44, "seek": 46512, "start": 466.0, "end": 477.92,
  "text": " help them gain this ability as well. So beyond searching for images with
  a textual query,", "tokens": [50408, 854, 552, 6052, 341, 3485, 382, 731, 13, 407,
  4399, 10808, 337, 5267, 365, 257, 2487, 901, 14581, 11, 51004], "temperature": 0.0,
  "avg_logprob": -0.2102330525716146, "compression_ratio": 1.3970588235294117, "no_speech_prob":
  0.010370674543082714}, {"id": 45, "seek": 46512, "start": 478.88, "end": 493.12,
  "text": " this also helps us to combine information from different sources. So in
  this case, maybe we can also", "tokens": [51052, 341, 611, 3665, 505, 281, 10432,
  1589, 490, 819, 7139, 13, 407, 294, 341, 1389, 11, 1310, 321, 393, 611, 51764],
  "temperature": 0.0, "avg_logprob": -0.2102330525716146, "compression_ratio": 1.3970588235294117,
  "no_speech_prob": 0.010370674543082714}, {"id": 46, "seek": 49312, "start": 493.12,
  "end": 504.64, "text": " have AI better understand its environment by combining,
  for example, a stream from the", "tokens": [50364, 362, 7318, 1101, 1223, 1080,
  2823, 538, 21928, 11, 337, 1365, 11, 257, 4309, 490, 264, 50940], "temperature":
  0.0, "avg_logprob": -0.27706934276380035, "compression_ratio": 1.3643410852713178,
  "no_speech_prob": 0.007500975858420134}, {"id": 47, "seek": 49312, "start": 506.32,
  "end": 517.6800000000001, "text": " camera and also maybe an output from a speech
  recognition and encoding them into a vector", "tokens": [51024, 2799, 293, 611,
  1310, 364, 5598, 490, 257, 6218, 11150, 293, 43430, 552, 666, 257, 8062, 51592],
  "temperature": 0.0, "avg_logprob": -0.27706934276380035, "compression_ratio": 1.3643410852713178,
  "no_speech_prob": 0.007500975858420134}, {"id": 48, "seek": 51768, "start": 518.4799999999999,
  "end": 532.3199999999999, "text": " we can combine these two vectors to fit into
  that encoder. So this also opens such new opportunities.", "tokens": [50404, 321,
  393, 10432, 613, 732, 18875, 281, 3318, 666, 300, 2058, 19866, 13, 407, 341, 611,
  9870, 1270, 777, 4786, 13, 51096], "temperature": 0.0, "avg_logprob": -0.2814934507329413,
  "compression_ratio": 1.3776223776223777, "no_speech_prob": 0.017302606254816055},
  {"id": 49, "seek": 51768, "start": 533.28, "end": 540.88, "text": " Yeah, that''s
  a great intro there also like how you gave analogy with how human brain functions,",
  "tokens": [51144, 865, 11, 300, 311, 257, 869, 12897, 456, 611, 411, 577, 291, 2729,
  21663, 365, 577, 1952, 3567, 6828, 11, 51524], "temperature": 0.0, "avg_logprob":
  -0.2814934507329413, "compression_ratio": 1.3776223776223777, "no_speech_prob":
  0.017302606254816055}, {"id": 50, "seek": 54088, "start": 540.88, "end": 549.2,
  "text": " so like how we take so many signals into our decision making. And specifically,
  like what you", "tokens": [50364, 370, 411, 577, 321, 747, 370, 867, 12354, 666,
  527, 3537, 1455, 13, 400, 4682, 11, 411, 437, 291, 50780], "temperature": 0.0, "avg_logprob":
  -0.1895873719367428, "compression_ratio": 1.6057142857142856, "no_speech_prob":
  0.033799298107624054}, {"id": 51, "seek": 54088, "start": 549.2, "end": 556.96,
  "text": " mentioned about clip, I like the fact that in practical settings, let''s
  say if you have images,", "tokens": [50780, 2835, 466, 7353, 11, 286, 411, 264,
  1186, 300, 294, 8496, 6257, 11, 718, 311, 584, 498, 291, 362, 5267, 11, 51168],
  "temperature": 0.0, "avg_logprob": -0.1895873719367428, "compression_ratio": 1.6057142857142856,
  "no_speech_prob": 0.033799298107624054}, {"id": 52, "seek": 54088, "start": 556.96,
  "end": 562.88, "text": " let''s say of some goods and you want to make a search
  in those goods and you also have some", "tokens": [51168, 718, 311, 584, 295, 512,
  10179, 293, 291, 528, 281, 652, 257, 3164, 294, 729, 10179, 293, 291, 611, 362,
  512, 51464], "temperature": 0.0, "avg_logprob": -0.1895873719367428, "compression_ratio":
  1.6057142857142856, "no_speech_prob": 0.033799298107624054}, {"id": 53, "seek":
  56288, "start": 562.88, "end": 571.6, "text": " metadata, let''s say titles or descriptions,
  right? It may be that some human decided what to put", "tokens": [50364, 26603,
  11, 718, 311, 584, 12992, 420, 24406, 11, 558, 30, 467, 815, 312, 300, 512, 1952,
  3047, 437, 281, 829, 50800], "temperature": 0.0, "avg_logprob": -0.13722216116415487,
  "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.0037804818712174892},
  {"id": 54, "seek": 56288, "start": 571.6, "end": 576.96, "text": " in that text,
  but they didn''t put everything that there is on the image, right? And so I think",
  "tokens": [50800, 294, 300, 2487, 11, 457, 436, 994, 380, 829, 1203, 300, 456, 307,
  322, 264, 3256, 11, 558, 30, 400, 370, 286, 519, 51068], "temperature": 0.0, "avg_logprob":
  -0.13722216116415487, "compression_ratio": 1.5824175824175823, "no_speech_prob":
  0.0037804818712174892}, {"id": 55, "seek": 56288, "start": 576.96, "end": 584.64,
  "text": " clip helps us to find sort of semantics that''s hidden inside the image
  itself, right? So I think", "tokens": [51068, 7353, 3665, 505, 281, 915, 1333, 295,
  4361, 45298, 300, 311, 7633, 1854, 264, 3256, 2564, 11, 558, 30, 407, 286, 519,
  51452], "temperature": 0.0, "avg_logprob": -0.13722216116415487, "compression_ratio":
  1.5824175824175823, "no_speech_prob": 0.0037804818712174892}, {"id": 56, "seek":
  58464, "start": 584.64, "end": 589.36, "text": " that''s kind of like has practical
  impact on what we built.", "tokens": [50364, 300, 311, 733, 295, 411, 575, 8496,
  2712, 322, 437, 321, 3094, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2164562463760376,
  "compression_ratio": 1.2704918032786885, "no_speech_prob": 0.004923407919704914},
  {"id": 57, "seek": 58464, "start": 591.6, "end": 602.4, "text": " Yeah, exactly.
  Actually, in the traditional source, for example, let''s get the product source
  as", "tokens": [50712, 865, 11, 2293, 13, 5135, 11, 294, 264, 5164, 4009, 11, 337,
  1365, 11, 718, 311, 483, 264, 1674, 4009, 382, 51252], "temperature": 0.0, "avg_logprob":
  -0.2164562463760376, "compression_ratio": 1.2704918032786885, "no_speech_prob":
  0.004923407919704914}, {"id": 58, "seek": 60240, "start": 602.72, "end": 615.4399999999999,
  "text": " example, when you want to develop a product source for, for example, an
  e-commerce website,", "tokens": [50380, 1365, 11, 562, 291, 528, 281, 1499, 257,
  1674, 4009, 337, 11, 337, 1365, 11, 364, 308, 12, 26926, 3144, 11, 51016], "temperature":
  0.0, "avg_logprob": -0.1745330344798953, "compression_ratio": 1.4132231404958677,
  "no_speech_prob": 0.006605126429349184}, {"id": 59, "seek": 60240, "start": 615.4399999999999,
  "end": 625.04, "text": " you need to enter different terms that can define that
  product to have a user''s", "tokens": [51016, 291, 643, 281, 3242, 819, 2115, 300,
  393, 6964, 300, 1674, 281, 362, 257, 4195, 311, 51496], "temperature": 0.0, "avg_logprob":
  -0.1745330344798953, "compression_ratio": 1.4132231404958677, "no_speech_prob":
  0.006605126429349184}, {"id": 60, "seek": 62504, "start": 625.04, "end": 637.92,
  "text": " find that product with different wording, but this is not so practical
  because people use very", "tokens": [50364, 915, 300, 1674, 365, 819, 47602, 11,
  457, 341, 307, 406, 370, 8496, 570, 561, 764, 588, 51008], "temperature": 0.0, "avg_logprob":
  -0.17904976436070033, "compression_ratio": 1.3880597014925373, "no_speech_prob":
  0.0028693124186247587}, {"id": 61, "seek": 62504, "start": 637.92, "end": 650.0,
  "text": " different terms to refer to things. And you in the current capacity of
  e-commerce websites,", "tokens": [51008, 819, 2115, 281, 2864, 281, 721, 13, 400,
  291, 294, 264, 2190, 6042, 295, 308, 12, 26926, 12891, 11, 51612], "temperature":
  0.0, "avg_logprob": -0.17904976436070033, "compression_ratio": 1.3880597014925373,
  "no_speech_prob": 0.0028693124186247587}, {"id": 62, "seek": 65000, "start": 650.0,
  "end": 663.28, "text": " we have hundreds of thousands of products and they also
  need to be updated once you add new", "tokens": [50364, 321, 362, 6779, 295, 5383,
  295, 3383, 293, 436, 611, 643, 281, 312, 10588, 1564, 291, 909, 777, 51028], "temperature":
  0.0, "avg_logprob": -0.22537077040899367, "compression_ratio": 1.4634146341463414,
  "no_speech_prob": 0.01681429147720337}, {"id": 63, "seek": 65000, "start": 663.28,
  "end": 677.28, "text": " products and remove new products. And also like myths acted
  at typos to this complexity,", "tokens": [51028, 3383, 293, 4159, 777, 3383, 13,
  400, 611, 411, 28205, 20359, 412, 2125, 329, 281, 341, 14024, 11, 51728], "temperature":
  0.0, "avg_logprob": -0.22537077040899367, "compression_ratio": 1.4634146341463414,
  "no_speech_prob": 0.01681429147720337}, {"id": 64, "seek": 67728, "start": 678.0,
  "end": 688.8, "text": " is actually explored to millions, maybe a tens of millions
  of possibilities. This is beyond the", "tokens": [50400, 307, 767, 24016, 281, 6803,
  11, 1310, 257, 10688, 295, 6803, 295, 12178, 13, 639, 307, 4399, 264, 50940], "temperature":
  0.0, "avg_logprob": -0.26218256839486054, "compression_ratio": 1.4444444444444444,
  "no_speech_prob": 0.019165612757205963}, {"id": 65, "seek": 67728, "start": 688.8,
  "end": 703.04, "text": " power of humans actually. But once you make connections,
  make a connection between text and images,", "tokens": [50940, 1347, 295, 6255,
  767, 13, 583, 1564, 291, 652, 9271, 11, 652, 257, 4984, 1296, 2487, 293, 5267, 11,
  51652], "temperature": 0.0, "avg_logprob": -0.26218256839486054, "compression_ratio":
  1.4444444444444444, "no_speech_prob": 0.019165612757205963}, {"id": 66, "seek":
  70304, "start": 703.04, "end": 716.4, "text": " you don''t need to enter such descriptive
  text, you only encode images into vectors and index", "tokens": [50364, 291, 500,
  380, 643, 281, 3242, 1270, 42585, 2487, 11, 291, 787, 2058, 1429, 5267, 666, 18875,
  293, 8186, 51032], "temperature": 0.0, "avg_logprob": -0.2243068573322702, "compression_ratio":
  1.4427480916030535, "no_speech_prob": 0.005336532834917307}, {"id": 67, "seek":
  70304, "start": 716.4, "end": 726.4, "text": " time into a vector database. Then
  in the inference time, all you need is just encode the textual", "tokens": [51032,
  565, 666, 257, 8062, 8149, 13, 1396, 294, 264, 38253, 565, 11, 439, 291, 643, 307,
  445, 2058, 1429, 264, 2487, 901, 51532], "temperature": 0.0, "avg_logprob": -0.2243068573322702,
  "compression_ratio": 1.4427480916030535, "no_speech_prob": 0.005336532834917307},
  {"id": 68, "seek": 72640, "start": 726.88, "end": 736.9599999999999, "text": " input
  as well and create that pre-indexed database to get similar results. Actually, this
  also", "tokens": [50388, 4846, 382, 731, 293, 1884, 300, 659, 12, 471, 3121, 292,
  8149, 281, 483, 2531, 3542, 13, 5135, 11, 341, 611, 50892], "temperature": 0.0,
  "avg_logprob": -0.24781280093722874, "compression_ratio": 1.2905982905982907, "no_speech_prob":
  0.0028040348552167416}, {"id": 69, "seek": 72640, "start": 738.8, "end": 743.28,
  "text": " buildings new opportunities, for example, people usually", "tokens": [50984,
  7446, 777, 4786, 11, 337, 1365, 11, 561, 2673, 51208], "temperature": 0.0, "avg_logprob":
  -0.24781280093722874, "compression_ratio": 1.2905982905982907, "no_speech_prob":
  0.0028040348552167416}, {"id": 70, "seek": 74328, "start": 743.36, "end": 758.4,
  "text": " enter some pre-defined textual descriptors in this search engines, but
  some new products may have", "tokens": [50368, 3242, 512, 659, 12, 37716, 2487,
  901, 31280, 830, 294, 341, 3164, 12982, 11, 457, 512, 777, 3383, 815, 362, 51120],
  "temperature": 0.0, "avg_logprob": -0.34983181953430176, "compression_ratio": 1.1547619047619047,
  "no_speech_prob": 0.00809534639120102}, {"id": 71, "seek": 75840, "start": 759.1999999999999,
  "end": 773.1999999999999, "text": " brand new features that people are not accustomed
  to. So even in this case, our vector search based", "tokens": [50404, 3360, 777,
  4122, 300, 561, 366, 406, 35980, 281, 13, 407, 754, 294, 341, 1389, 11, 527, 8062,
  3164, 2361, 51104], "temperature": 0.0, "avg_logprob": -0.22027762234210968, "compression_ratio":
  1.5056179775280898, "no_speech_prob": 0.005204358603805304}, {"id": 72, "seek":
  75840, "start": 774.8, "end": 780.8, "text": " solution that combines images and
  text can be in that image as well.", "tokens": [51184, 3827, 300, 29520, 5267, 293,
  2487, 393, 312, 294, 300, 3256, 382, 731, 13, 51484], "temperature": 0.0, "avg_logprob":
  -0.22027762234210968, "compression_ratio": 1.5056179775280898, "no_speech_prob":
  0.005204358603805304}, {"id": 73, "seek": 75840, "start": 782.0, "end": 787.12,
  "text": " Yeah, that sounds cool. So it kind of opens up a lot of opportunities
  that didn''t exist before when", "tokens": [51544, 865, 11, 300, 3263, 1627, 13,
  407, 309, 733, 295, 9870, 493, 257, 688, 295, 4786, 300, 994, 380, 2514, 949, 562,
  51800], "temperature": 0.0, "avg_logprob": -0.22027762234210968, "compression_ratio":
  1.5056179775280898, "no_speech_prob": 0.005204358603805304}, {"id": 74, "seek":
  78712, "start": 787.68, "end": 795.44, "text": " we modeled our object purely through
  textual representation. Maybe somebody did attempt to", "tokens": [50392, 321, 37140,
  527, 2657, 17491, 807, 2487, 901, 10290, 13, 2704, 2618, 630, 5217, 281, 50780],
  "temperature": 0.0, "avg_logprob": -0.18337147376116583, "compression_ratio": 1.4263959390862944,
  "no_speech_prob": 0.002145607490092516}, {"id": 75, "seek": 78712, "start": 795.44,
  "end": 801.2, "text": " also encode images of some other binary format, but I think
  maybe it wasn''t as efficient or", "tokens": [50780, 611, 2058, 1429, 5267, 295,
  512, 661, 17434, 7877, 11, 457, 286, 519, 1310, 309, 2067, 380, 382, 7148, 420,
  51068], "temperature": 0.0, "avg_logprob": -0.18337147376116583, "compression_ratio":
  1.4263959390862944, "no_speech_prob": 0.002145607490092516}, {"id": 76, "seek":
  78712, "start": 802.08, "end": 811.84, "text": " definitely not multi-model. So
  that sounds so cool. And so how do you connect? Where do you start?", "tokens":
  [51112, 2138, 406, 4825, 12, 8014, 338, 13, 407, 300, 3263, 370, 1627, 13, 400,
  370, 577, 360, 291, 1745, 30, 2305, 360, 291, 722, 30, 51600], "temperature": 0.0,
  "avg_logprob": -0.18337147376116583, "compression_ratio": 1.4263959390862944, "no_speech_prob":
  0.002145607490092516}, {"id": 77, "seek": 81184, "start": 811.84, "end": 818.0,
  "text": " Usually, let''s say if you have a data set, right? And you want to implement
  neural search", "tokens": [50364, 11419, 11, 718, 311, 584, 498, 291, 362, 257,
  1412, 992, 11, 558, 30, 400, 291, 528, 281, 4445, 18161, 3164, 50672], "temperature":
  0.0, "avg_logprob": -0.15680377403002108, "compression_ratio": 1.6519823788546255,
  "no_speech_prob": 0.0053307972848415375}, {"id": 78, "seek": 81184, "start": 818.5600000000001,
  "end": 827.0400000000001, "text": " experience. At one point of time, do you start
  thinking about what the metric is the best for", "tokens": [50700, 1752, 13, 1711,
  472, 935, 295, 565, 11, 360, 291, 722, 1953, 466, 437, 264, 20678, 307, 264, 1151,
  337, 51124], "temperature": 0.0, "avg_logprob": -0.15680377403002108, "compression_ratio":
  1.6519823788546255, "no_speech_prob": 0.0053307972848415375}, {"id": 79, "seek":
  81184, "start": 827.0400000000001, "end": 833.2, "text": " my data set? And also,
  how do you approach it from which angle do you usually approach this?", "tokens":
  [51124, 452, 1412, 992, 30, 400, 611, 11, 577, 360, 291, 3109, 309, 490, 597, 5802,
  360, 291, 2673, 3109, 341, 30, 51432], "temperature": 0.0, "avg_logprob": -0.15680377403002108,
  "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0053307972848415375},
  {"id": 80, "seek": 81184, "start": 833.2, "end": 837.76, "text": " And this is something
  that really helps you to hear your theoretical as well as practical thoughts", "tokens":
  [51432, 400, 341, 307, 746, 300, 534, 3665, 291, 281, 1568, 428, 20864, 382, 731,
  382, 8496, 4598, 51660], "temperature": 0.0, "avg_logprob": -0.15680377403002108,
  "compression_ratio": 1.6519823788546255, "no_speech_prob": 0.0053307972848415375},
  {"id": 81, "seek": 83776, "start": 837.76, "end": 847.92, "text": " of this. Yes,
  you''re actually there are lots of very different techniques and", "tokens": [50364,
  295, 341, 13, 1079, 11, 291, 434, 767, 456, 366, 3195, 295, 588, 819, 7512, 293,
  50872], "temperature": 0.0, "avg_logprob": -0.2619859544854415, "compression_ratio":
  1.328125, "no_speech_prob": 0.004639596678316593}, {"id": 82, "seek": 83776, "start":
  848.56, "end": 859.12, "text": " methods and approaches to metric learning that
  can work for some specific types of problems.", "tokens": [50904, 7150, 293, 11587,
  281, 20678, 2539, 300, 393, 589, 337, 512, 2685, 3467, 295, 2740, 13, 51432], "temperature":
  0.0, "avg_logprob": -0.2619859544854415, "compression_ratio": 1.328125, "no_speech_prob":
  0.004639596678316593}, {"id": 83, "seek": 85912, "start": 859.84, "end": 870.24,
  "text": " But in my practical experience, I usually begin with with with an auto
  encoder, because it''s", "tokens": [50400, 583, 294, 452, 8496, 1752, 11, 286, 2673,
  1841, 365, 365, 365, 364, 8399, 2058, 19866, 11, 570, 309, 311, 50920], "temperature":
  0.0, "avg_logprob": -0.22210170911706012, "compression_ratio": 1.3880597014925373,
  "no_speech_prob": 0.013364981859922409}, {"id": 84, "seek": 85912, "start": 872.0,
  "end": 886.5600000000001, "text": " already very easy to implement and easy to train.
  It can be applied to almost any data track.", "tokens": [51008, 1217, 588, 1858,
  281, 4445, 293, 1858, 281, 3847, 13, 467, 393, 312, 6456, 281, 1920, 604, 1412,
  2837, 13, 51736], "temperature": 0.0, "avg_logprob": -0.22210170911706012, "compression_ratio":
  1.3880597014925373, "no_speech_prob": 0.013364981859922409}, {"id": 85, "seek":
  88656, "start": 887.4399999999999, "end": 896.64, "text": " Basically, in auto encoders,
  we have two models and encoder and the encoders.", "tokens": [50408, 8537, 11, 294,
  8399, 2058, 378, 433, 11, 321, 362, 732, 5245, 293, 2058, 19866, 293, 264, 2058,
  378, 433, 13, 50868], "temperature": 0.0, "avg_logprob": -0.2684889923442494, "compression_ratio":
  1.4234234234234233, "no_speech_prob": 0.008925629779696465}, {"id": 86, "seek":
  88656, "start": 897.4399999999999, "end": 909.28, "text": " The encoders part encodes
  samples into an dimensional vector. This and should be", "tokens": [50908, 440,
  2058, 378, 433, 644, 2058, 4789, 10938, 666, 364, 18795, 8062, 13, 639, 293, 820,
  312, 51500], "temperature": 0.0, "avg_logprob": -0.2684889923442494, "compression_ratio":
  1.4234234234234233, "no_speech_prob": 0.008925629779696465}, {"id": 87, "seek":
  90928, "start": 910.24, "end": 921.1999999999999, "text": " much lower than the
  dimensionality of the input sample. And the decoder is supposed to", "tokens": [50412,
  709, 3126, 813, 264, 10139, 1860, 295, 264, 4846, 6889, 13, 400, 264, 979, 19866,
  307, 3442, 281, 50960], "temperature": 0.0, "avg_logprob": -0.24122115543910436,
  "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.006070391740649939},
  {"id": 88, "seek": 90928, "start": 921.92, "end": 936.72, "text": " reconstruct
  the input sample when this encoded vector is given to it. So this is a", "tokens":
  [50996, 31499, 264, 4846, 6889, 562, 341, 2058, 12340, 8062, 307, 2212, 281, 309,
  13, 407, 341, 307, 257, 51736], "temperature": 0.0, "avg_logprob": -0.24122115543910436,
  "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.006070391740649939},
  {"id": 89, "seek": 93672, "start": 937.6800000000001, "end": 949.28, "text": " the
  self-provised method. So it can be applied to any type of data set. You don''t need
  labels.", "tokens": [50412, 264, 2698, 12, 4318, 24420, 3170, 13, 407, 309, 393,
  312, 6456, 281, 604, 2010, 295, 1412, 992, 13, 509, 500, 380, 643, 16949, 13, 50992],
  "temperature": 0.0, "avg_logprob": -0.33323415120442706, "compression_ratio": 1.2857142857142858,
  "no_speech_prob": 0.012670663185417652}, {"id": 90, "seek": 93672, "start": 950.88,
  "end": 963.6, "text": " It usually gives a very good resource. After training such
  a model, you can visualize", "tokens": [51072, 467, 2673, 2709, 257, 588, 665, 7684,
  13, 2381, 3097, 1270, 257, 2316, 11, 291, 393, 23273, 51708], "temperature": 0.0,
  "avg_logprob": -0.33323415120442706, "compression_ratio": 1.2857142857142858, "no_speech_prob":
  0.012670663185417652}, {"id": 91, "seek": 96360, "start": 964.0, "end": 976.5600000000001,
  "text": " embedding. We call the output of the encoders, vectors embedding. So you
  can visualize such embedding", "tokens": [50384, 12240, 3584, 13, 492, 818, 264,
  5598, 295, 264, 2058, 378, 433, 11, 18875, 12240, 3584, 13, 407, 291, 393, 23273,
  1270, 12240, 3584, 51012], "temperature": 0.0, "avg_logprob": -0.269812992640904,
  "compression_ratio": 1.406015037593985, "no_speech_prob": 0.010135110467672348},
  {"id": 92, "seek": 96360, "start": 976.5600000000001, "end": 990.0, "text": " with
  a tool. This tool can be, for example, TensorFlow projectors and another tool by",
  "tokens": [51012, 365, 257, 2290, 13, 639, 2290, 393, 312, 11, 337, 1365, 11, 37624,
  1716, 830, 293, 1071, 2290, 538, 51684], "temperature": 0.0, "avg_logprob": -0.269812992640904,
  "compression_ratio": 1.406015037593985, "no_speech_prob": 0.010135110467672348},
  {"id": 93, "seek": 99000, "start": 990.0, "end": 998.4, "text": " Yubach. I just
  couldn''t show my word in there. Sorry. No worries. We can find those links later,
  I guess.", "tokens": [50364, 398, 836, 608, 13, 286, 445, 2809, 380, 855, 452, 1349,
  294, 456, 13, 4919, 13, 883, 16340, 13, 492, 393, 915, 729, 6123, 1780, 11, 286,
  2041, 13, 50784], "temperature": 0.0, "avg_logprob": -0.4937224881402377, "compression_ratio":
  1.355263157894737, "no_speech_prob": 0.006493051536381245}, {"id": 94, "seek": 99000,
  "start": 999.6, "end": 1013.28, "text": " Yeah, we can put a link in the description.
  And this visualization tools have us see if our encoders", "tokens": [50844, 865,
  11, 321, 393, 829, 257, 2113, 294, 264, 3855, 13, 400, 341, 25801, 3873, 362, 505,
  536, 498, 527, 2058, 378, 433, 51528], "temperature": 0.0, "avg_logprob": -0.4937224881402377,
  "compression_ratio": 1.355263157894737, "no_speech_prob": 0.006493051536381245},
  {"id": 95, "seek": 101328, "start": 1013.68, "end": 1026.56, "text": " really involve
  similar samples need to each closer to each other than the similar ones.", "tokens":
  [50384, 534, 9494, 2531, 10938, 643, 281, 1184, 4966, 281, 1184, 661, 813, 264,
  2531, 2306, 13, 51028], "temperature": 0.0, "avg_logprob": -0.24338811509152677,
  "compression_ratio": 1.464, "no_speech_prob": 0.060637082904577255}, {"id": 96,
  "seek": 101328, "start": 1027.84, "end": 1038.16, "text": " If it is, we can use
  this encoders part. We can just dispose the decoder part and we can simply", "tokens":
  [51092, 759, 309, 307, 11, 321, 393, 764, 341, 2058, 378, 433, 644, 13, 492, 393,
  445, 42537, 264, 979, 19866, 644, 293, 321, 393, 2935, 51608], "temperature": 0.0,
  "avg_logprob": -0.24338811509152677, "compression_ratio": 1.464, "no_speech_prob":
  0.060637082904577255}, {"id": 97, "seek": 103816, "start": 1039.0400000000002, "end":
  1045.44, "text": " keep the encoder part and use it to encode our samples and index
  them in the", "tokens": [50408, 1066, 264, 2058, 19866, 644, 293, 764, 309, 281,
  2058, 1429, 527, 10938, 293, 8186, 552, 294, 264, 50728], "temperature": 0.0, "avg_logprob":
  -0.18929408146784857, "compression_ratio": 1.3214285714285714, "no_speech_prob":
  0.011630809865891933}, {"id": 98, "seek": 103816, "start": 1046.3200000000002, "end":
  1058.0800000000002, "text": " vector. And we can already start searching semantics.
  But we usually do", "tokens": [50772, 8062, 13, 400, 321, 393, 1217, 722, 10808,
  4361, 45298, 13, 583, 321, 2673, 360, 51360], "temperature": 0.0, "avg_logprob":
  -0.18929408146784857, "compression_ratio": 1.3214285714285714, "no_speech_prob":
  0.011630809865891933}, {"id": 99, "seek": 105808, "start": 1058.08, "end": 1069.12,
  "text": " buzzers than this one with only small set of labeled data. And you actually
  need only", "tokens": [50364, 13036, 433, 813, 341, 472, 365, 787, 1359, 992, 295,
  21335, 1412, 13, 400, 291, 767, 643, 787, 50916], "temperature": 0.0, "avg_logprob":
  -0.23537533623831614, "compression_ratio": 1.3858267716535433, "no_speech_prob":
  0.01939673162996769}, {"id": 100, "seek": 105808, "start": 1070.8799999999999, "end":
  1079.1999999999998, "text": " a few with that one. Actually, we are preparing some
  publications to demonstrate this one.", "tokens": [51004, 257, 1326, 365, 300, 472,
  13, 5135, 11, 321, 366, 10075, 512, 25618, 281, 11698, 341, 472, 13, 51420], "temperature":
  0.0, "avg_logprob": -0.23537533623831614, "compression_ratio": 1.3858267716535433,
  "no_speech_prob": 0.01939673162996769}, {"id": 101, "seek": 107920, "start": 1079.68,
  "end": 1095.3600000000001, "text": " After you train and encoders with a considerable
  number of unlabeled data, all you need to do is", "tokens": [50388, 2381, 291, 3847,
  293, 2058, 378, 433, 365, 257, 24167, 1230, 295, 32118, 18657, 292, 1412, 11, 439,
  291, 643, 281, 360, 307, 51172], "temperature": 0.0, "avg_logprob": -0.4124216842651367,
  "compression_ratio": 1.4511278195488722, "no_speech_prob": 0.020865069702267647},
  {"id": 102, "seek": 107920, "start": 1096.96, "end": 1108.64, "text": " just to
  find to in it with a small set of labeled data. On the supervised site, there are
  really", "tokens": [51252, 445, 281, 915, 281, 294, 309, 365, 257, 1359, 992, 295,
  21335, 1412, 13, 1282, 264, 46533, 3621, 11, 456, 366, 534, 51836], "temperature":
  0.0, "avg_logprob": -0.4124216842651367, "compression_ratio": 1.4511278195488722,
  "no_speech_prob": 0.020865069702267647}, {"id": 103, "seek": 110920, "start": 1109.6000000000001,
  "end": 1121.76, "text": " quite a number of very different approaches to matrix
  learning from more traditional margin-based", "tokens": [50384, 1596, 257, 1230,
  295, 588, 819, 11587, 281, 8141, 2539, 490, 544, 5164, 10270, 12, 6032, 50992],
  "temperature": 0.0, "avg_logprob": -0.26663709298158306, "compression_ratio": 1.4661654135338347,
  "no_speech_prob": 0.004581479821354151}, {"id": 104, "seek": 110920, "start": 1122.48,
  "end": 1138.24, "text": " approaches to newer categorization-based approaches. And
  actually, they deserve a long discussion", "tokens": [51028, 11587, 281, 17628,
  19250, 2144, 12, 6032, 11587, 13, 400, 767, 11, 436, 9948, 257, 938, 5017, 51816],
  "temperature": 0.0, "avg_logprob": -0.26663709298158306, "compression_ratio": 1.4661654135338347,
  "no_speech_prob": 0.004581479821354151}, {"id": 105, "seek": 113824, "start": 1138.24,
  "end": 1144.88, "text": " of data. For sure. Yeah, that''s awesome. But just to
  unpack it a little bit, so", "tokens": [50364, 295, 1412, 13, 1171, 988, 13, 865,
  11, 300, 311, 3476, 13, 583, 445, 281, 26699, 309, 257, 707, 857, 11, 370, 50696],
  "temperature": 0.0, "avg_logprob": -0.3364786207675934, "compression_ratio": 1.4911242603550297,
  "no_speech_prob": 0.0079282121732831}, {"id": 106, "seek": 113824, "start": 1144.88,
  "end": 1152.0, "text": " in a natural metric learning process allows me to learn
  the optimal distance metric for my data.", "tokens": [50696, 294, 257, 3303, 20678,
  2539, 1399, 4045, 385, 281, 1466, 264, 16252, 4560, 20678, 337, 452, 1412, 13, 51052],
  "temperature": 0.0, "avg_logprob": -0.3364786207675934, "compression_ratio": 1.4911242603550297,
  "no_speech_prob": 0.0079282121732831}, {"id": 107, "seek": 113824, "start": 1152.8,
  "end": 1158.48, "text": " So it''s kind of like a function of my dataset properties,
  inner properties.", "tokens": [51092, 407, 309, 311, 733, 295, 411, 257, 2445, 295,
  452, 28872, 7221, 11, 7284, 7221, 13, 51376], "temperature": 0.0, "avg_logprob":
  -0.3364786207675934, "compression_ratio": 1.4911242603550297, "no_speech_prob":
  0.0079282121732831}, {"id": 108, "seek": 115848, "start": 1159.3600000000001, "end":
  1166.56, "text": " Yeah, actually, let''s clarify this metric thing. What does it
  mean in this context?", "tokens": [50408, 865, 11, 767, 11, 718, 311, 17594, 341,
  20678, 551, 13, 708, 775, 309, 914, 294, 341, 4319, 30, 50768], "temperature": 0.0,
  "avg_logprob": -0.1807206796140087, "compression_ratio": 1.3333333333333333, "no_speech_prob":
  0.017488857731223106}, {"id": 109, "seek": 115848, "start": 1167.84, "end": 1181.2,
  "text": " In this context, a metric is a non-negative function with two inputs.
  Let''s say X and Y.", "tokens": [50832, 682, 341, 4319, 11, 257, 20678, 307, 257,
  2107, 12, 28561, 1166, 2445, 365, 732, 15743, 13, 961, 311, 584, 1783, 293, 398,
  13, 51500], "temperature": 0.0, "avg_logprob": -0.1807206796140087, "compression_ratio":
  1.3333333333333333, "no_speech_prob": 0.017488857731223106}, {"id": 110, "seek":
  118120, "start": 1181.76, "end": 1190.88, "text": " And it is used to measure what
  is called the distance between X and Y.", "tokens": [50392, 400, 309, 307, 1143,
  281, 3481, 437, 307, 1219, 264, 4560, 1296, 1783, 293, 398, 13, 50848], "temperature":
  0.0, "avg_logprob": -0.16151898946517554, "compression_ratio": 1.2522522522522523,
  "no_speech_prob": 0.00732125248759985}, {"id": 111, "seek": 118120, "start": 1192.72,
  "end": 1202.8, "text": " When we feed such two inputs, it gives us a scaler''s positive
  value.", "tokens": [50940, 1133, 321, 3154, 1270, 732, 15743, 11, 309, 2709, 505,
  257, 15664, 260, 311, 3353, 2158, 13, 51444], "temperature": 0.0, "avg_logprob":
  -0.16151898946517554, "compression_ratio": 1.2522522522522523, "no_speech_prob":
  0.00732125248759985}, {"id": 112, "seek": 120280, "start": 1203.6, "end": 1216.1599999999999,
  "text": " If this value is closer to zero, then we can assume that those two inputs
  are more", "tokens": [50404, 759, 341, 2158, 307, 4966, 281, 4018, 11, 550, 321,
  393, 6552, 300, 729, 732, 15743, 366, 544, 51032], "temperature": 0.0, "avg_logprob":
  -0.2132264773050944, "compression_ratio": 1.408, "no_speech_prob": 0.011007328517735004},
  {"id": 113, "seek": 120280, "start": 1216.1599999999999, "end": 1230.0, "text":
  " similar to each other with two inputs with a higher distance value. So our whole
  objective in", "tokens": [51032, 2531, 281, 1184, 661, 365, 732, 15743, 365, 257,
  2946, 4560, 2158, 13, 407, 527, 1379, 10024, 294, 51724], "temperature": 0.0, "avg_logprob":
  -0.2132264773050944, "compression_ratio": 1.408, "no_speech_prob": 0.011007328517735004},
  {"id": 114, "seek": 123000, "start": 1230.0, "end": 1245.28, "text": " metric learning
  is to train functions that can give this distance value. On the practical", "tokens":
  [50364, 20678, 2539, 307, 281, 3847, 6828, 300, 393, 976, 341, 4560, 2158, 13, 1282,
  264, 8496, 51128], "temperature": 0.0, "avg_logprob": -0.21928277015686035, "compression_ratio":
  1.141025641025641, "no_speech_prob": 0.0022721486166119576}, {"id": 115, "seek":
  124528, "start": 1246.24, "end": 1256.72, "text": " site, we usually train a model
  that outputs a vector and a dimensional vector.", "tokens": [50412, 3621, 11, 321,
  2673, 3847, 257, 2316, 300, 23930, 257, 8062, 293, 257, 18795, 8062, 13, 50936],
  "temperature": 0.0, "avg_logprob": -0.28207188844680786, "compression_ratio": 1.280373831775701,
  "no_speech_prob": 0.009226060472428799}, {"id": 116, "seek": 124528, "start": 1258.08,
  "end": 1264.24, "text": " And then we can apply different distance functions such
  as", "tokens": [51004, 400, 550, 321, 393, 3079, 819, 4560, 6828, 1270, 382, 51312],
  "temperature": 0.0, "avg_logprob": -0.28207188844680786, "compression_ratio": 1.280373831775701,
  "no_speech_prob": 0.009226060472428799}, {"id": 117, "seek": 126424, "start": 1265.2,
  "end": 1276.64, "text": " Euclidean and cosine distance to get a measurement of
  the distance value.", "tokens": [50412, 462, 1311, 31264, 282, 293, 23565, 4560,
  281, 483, 257, 13160, 295, 264, 4560, 2158, 13, 50984], "temperature": 0.0, "avg_logprob":
  -0.17669592405620374, "compression_ratio": 1.4051724137931034, "no_speech_prob":
  0.005759425926953554}, {"id": 118, "seek": 126424, "start": 1278.0, "end": 1288.16,
  "text": " There is also a term deep metric learning. Actually, the traditional metric
  learning uses", "tokens": [51052, 821, 307, 611, 257, 1433, 2452, 20678, 2539, 13,
  5135, 11, 264, 5164, 20678, 2539, 4960, 51560], "temperature": 0.0, "avg_logprob":
  -0.17669592405620374, "compression_ratio": 1.4051724137931034, "no_speech_prob":
  0.005759425926953554}, {"id": 119, "seek": 128816, "start": 1288.8000000000002,
  "end": 1297.1200000000001, "text": " some linear transformations to project samples
  into an dimensional", "tokens": [50396, 512, 8213, 34852, 281, 1716, 10938, 666,
  364, 18795, 50812], "temperature": 0.0, "avg_logprob": -0.16697173118591307, "compression_ratio":
  1.4220183486238531, "no_speech_prob": 0.00545174814760685}, {"id": 120, "seek":
  128816, "start": 1298.88, "end": 1310.48, "text": " feature space to apply a metric
  function. But this linear aspect of such transformations", "tokens": [50900, 4111,
  1901, 281, 3079, 257, 20678, 2445, 13, 583, 341, 8213, 4171, 295, 1270, 34852, 51480],
  "temperature": 0.0, "avg_logprob": -0.16697173118591307, "compression_ratio": 1.4220183486238531,
  "no_speech_prob": 0.00545174814760685}, {"id": 121, "seek": 131048, "start": 1310.48,
  "end": 1325.2, "text": " limits the use of traditional metric learning using time
  with more richers, data types,", "tokens": [50364, 10406, 264, 764, 295, 5164, 20678,
  2539, 1228, 565, 365, 544, 4593, 433, 11, 1412, 3467, 11, 51100], "temperature":
  0.0, "avg_logprob": -0.42113206444717033, "compression_ratio": 1.4566929133858268,
  "no_speech_prob": 0.00289943628013134}, {"id": 122, "seek": 131048, "start": 1325.2,
  "end": 1336.08, "text": " for example, images and texts. So deep metric learning
  benefits from the methods of deep learning", "tokens": [51100, 337, 1365, 11, 5267,
  293, 15765, 13, 407, 2452, 20678, 2539, 5311, 490, 264, 7150, 295, 2452, 2539, 51644],
  "temperature": 0.0, "avg_logprob": -0.42113206444717033, "compression_ratio": 1.4566929133858268,
  "no_speech_prob": 0.00289943628013134}, {"id": 123, "seek": 133608, "start": 1336.24,
  "end": 1349.4399999999998, "text": " to learn non-linear transformations to project
  samples into a new and dimensional vector space.", "tokens": [50372, 281, 1466,
  2107, 12, 28263, 34852, 281, 1716, 10938, 666, 257, 777, 293, 18795, 8062, 1901,
  13, 51032], "temperature": 0.0, "avg_logprob": -0.16175737613584937, "compression_ratio":
  1.3555555555555556, "no_speech_prob": 0.008436362259089947}, {"id": 124, "seek":
  133608, "start": 1350.6399999999999, "end": 1361.28, "text": " But in this context,
  I usually use metric learning as an umbrella term to refer to both", "tokens": [51092,
  583, 294, 341, 4319, 11, 286, 2673, 764, 20678, 2539, 382, 364, 21925, 1433, 281,
  2864, 281, 1293, 51624], "temperature": 0.0, "avg_logprob": -0.16175737613584937,
  "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.008436362259089947},
  {"id": 125, "seek": 136128, "start": 1361.36, "end": 1370.3999999999999, "text":
  " traditional metric learning and deep metric learning. Just like we do with machine
  learning", "tokens": [50368, 5164, 20678, 2539, 293, 2452, 20678, 2539, 13, 1449,
  411, 321, 360, 365, 3479, 2539, 50820], "temperature": 0.0, "avg_logprob": -0.3222321485861754,
  "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.009160725399851799},
  {"id": 126, "seek": 136128, "start": 1370.3999999999999, "end": 1374.32, "text":
  " to refer to both classical machine learning and deep learning.", "tokens": [50820,
  281, 2864, 281, 1293, 13735, 3479, 2539, 293, 2452, 2539, 13, 51016], "temperature":
  0.0, "avg_logprob": -0.3222321485861754, "compression_ratio": 1.7079207920792079,
  "no_speech_prob": 0.009160725399851799}, {"id": 127, "seek": 136128, "start": 1374.32,
  "end": 1383.2, "text": " Yeah, that makes sense. Thank you. And so essentially,
  in the lay main terms, deep learning allows us to", "tokens": [51016, 865, 11, 300,
  1669, 2020, 13, 1044, 291, 13, 400, 370, 4476, 11, 294, 264, 2360, 2135, 2115, 11,
  2452, 2539, 4045, 505, 281, 51460], "temperature": 0.0, "avg_logprob": -0.3222321485861754,
  "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.009160725399851799},
  {"id": 128, "seek": 136128, "start": 1383.2, "end": 1391.12, "text": " vectorize
  data objects that previously we couldn''t vectorize in a celly, so images or", "tokens":
  [51460, 8062, 1125, 1412, 6565, 300, 8046, 321, 2809, 380, 8062, 1125, 294, 257,
  2815, 88, 11, 370, 5267, 420, 51856], "temperature": 0.0, "avg_logprob": -0.3222321485861754,
  "compression_ratio": 1.7079207920792079, "no_speech_prob": 0.009160725399851799},
  {"id": 129, "seek": 139128, "start": 1391.44, "end": 1397.36, "text": " I don''t
  know. And do it efficiently, because in images, you might have way too many pixels.",
  "tokens": [50372, 286, 500, 380, 458, 13, 400, 360, 309, 19621, 11, 570, 294, 5267,
  11, 291, 1062, 362, 636, 886, 867, 18668, 13, 50668], "temperature": 0.0, "avg_logprob":
  -0.19561427126648606, "compression_ratio": 1.5964125560538116, "no_speech_prob":
  0.004142153076827526}, {"id": 130, "seek": 139128, "start": 1397.36, "end": 1402.8799999999999,
  "text": " So if you just take the vector of all the pixels, it''s way too big of
  an object to deal with.", "tokens": [50668, 407, 498, 291, 445, 747, 264, 8062,
  295, 439, 264, 18668, 11, 309, 311, 636, 886, 955, 295, 364, 2657, 281, 2028, 365,
  13, 50944], "temperature": 0.0, "avg_logprob": -0.19561427126648606, "compression_ratio":
  1.5964125560538116, "no_speech_prob": 0.004142153076827526}, {"id": 131, "seek":
  139128, "start": 1403.68, "end": 1408.0, "text": " And so you vectorize, as you
  said, in the beginning, and you basically sort of", "tokens": [50984, 400, 370,
  291, 8062, 1125, 11, 382, 291, 848, 11, 294, 264, 2863, 11, 293, 291, 1936, 1333,
  295, 51200], "temperature": 0.0, "avg_logprob": -0.19561427126648606, "compression_ratio":
  1.5964125560538116, "no_speech_prob": 0.004142153076827526}, {"id": 132, "seek":
  139128, "start": 1410.32, "end": 1415.92, "text": " project it in a lower dimensional
  space. So now you can actually efficiently operate on it.", "tokens": [51316, 1716,
  309, 294, 257, 3126, 18795, 1901, 13, 407, 586, 291, 393, 767, 19621, 9651, 322,
  309, 13, 51596], "temperature": 0.0, "avg_logprob": -0.19561427126648606, "compression_ratio":
  1.5964125560538116, "no_speech_prob": 0.004142153076827526}, {"id": 133, "seek":
  141592, "start": 1416.88, "end": 1430.8000000000002, "text": " Exactly. Let''s get
  images as an example. Let''s assume that we have images with a size of", "tokens":
  [50412, 7587, 13, 961, 311, 483, 5267, 382, 364, 1365, 13, 961, 311, 6552, 300,
  321, 362, 5267, 365, 257, 2744, 295, 51108], "temperature": 0.0, "avg_logprob":
  -0.23407047271728515, "compression_ratio": 1.1282051282051282, "no_speech_prob":
  0.021925868466496468}, {"id": 134, "seek": 143080, "start": 1431.76, "end": 1447.44,
  "text": " 200 times 200. And we also have a channel value of three. So we end up
  with 200 times 200 times", "tokens": [50412, 2331, 1413, 2331, 13, 400, 321, 611,
  362, 257, 2269, 2158, 295, 1045, 13, 407, 321, 917, 493, 365, 2331, 1413, 2331,
  1413, 51196], "temperature": 0.0, "avg_logprob": -0.2718716132931593, "compression_ratio":
  1.309090909090909, "no_speech_prob": 0.01090193446725607}, {"id": 135, "seek": 143080,
  "start": 1448.0, "end": 1454.8, "text": " three values for a single image. And also,
  let''s", "tokens": [51224, 1045, 4190, 337, 257, 2167, 3256, 13, 400, 611, 11, 718,
  311, 51564], "temperature": 0.0, "avg_logprob": -0.2718716132931593, "compression_ratio":
  1.309090909090909, "no_speech_prob": 0.01090193446725607}, {"id": 136, "seek": 145480,
  "start": 1454.96, "end": 1471.12, "text": " actually, too many values also mean
  a great variance value. So it''s not so practical to make a", "tokens": [50372,
  767, 11, 886, 867, 4190, 611, 914, 257, 869, 21977, 2158, 13, 407, 309, 311, 406,
  370, 8496, 281, 652, 257, 51180], "temperature": 0.0, "avg_logprob": -0.3360712814331055,
  "compression_ratio": 1.146341463414634, "no_speech_prob": 0.0020567975006997585},
  {"id": 137, "seek": 147112, "start": 1471.12, "end": 1485.52, "text": " measurement
  between two images, because those pixel values can include very surface, quite",
  "tokens": [50364, 13160, 1296, 732, 5267, 11, 570, 729, 19261, 4190, 393, 4090,
  588, 3753, 11, 1596, 51084], "temperature": 0.0, "avg_logprob": -0.28118324279785156,
  "compression_ratio": 1.3445378151260505, "no_speech_prob": 0.004205784294754267},
  {"id": 138, "seek": 147112, "start": 1488.32, "end": 1498.0, "text": " shallow surface
  features that do not make any sense in our semantics.", "tokens": [51224, 20488,
  3753, 4122, 300, 360, 406, 652, 604, 2020, 294, 527, 4361, 45298, 13, 51708], "temperature":
  0.0, "avg_logprob": -0.28118324279785156, "compression_ratio": 1.3445378151260505,
  "no_speech_prob": 0.004205784294754267}, {"id": 139, "seek": 149800, "start": 1498.24,
  "end": 1508.16, "text": " But once we encode those high dimensional inputs into
  a low dimensional vector space,", "tokens": [50376, 583, 1564, 321, 2058, 1429,
  729, 1090, 18795, 15743, 666, 257, 2295, 18795, 8062, 1901, 11, 50872], "temperature":
  0.0, "avg_logprob": -0.4275725228445871, "compression_ratio": 1.238532110091743,
  "no_speech_prob": 0.012053826823830605}, {"id": 140, "seek": 149800, "start": 1508.88,
  "end": 1516.08, "text": " for example, we usually have 500 to 12, 10 to the", "tokens":
  [50908, 337, 1365, 11, 321, 2673, 362, 5923, 281, 2272, 11, 1266, 281, 264, 51268],
  "temperature": 0.0, "avg_logprob": -0.4275725228445871, "compression_ratio": 1.238532110091743,
  "no_speech_prob": 0.012053826823830605}, {"id": 141, "seek": 151608, "start": 1516.08,
  "end": 1530.1599999999999, "text": " 12, 12, 12, 14 dimensional vectors. And this
  value is really low when compared to the original", "tokens": [50364, 2272, 11,
  2272, 11, 2272, 11, 3499, 18795, 18875, 13, 400, 341, 2158, 307, 534, 2295, 562,
  5347, 281, 264, 3380, 51068], "temperature": 0.0, "avg_logprob": -0.44789005279541017,
  "compression_ratio": 1.0804597701149425, "no_speech_prob": 0.004807830322533846},
  {"id": 142, "seek": 153016, "start": 1531.1200000000001, "end": 1546.64, "text":
  " dimension of that sample. So in this case, that model should learn, should learn
  a representation", "tokens": [50412, 10139, 295, 300, 6889, 13, 407, 294, 341, 1389,
  11, 300, 2316, 820, 1466, 11, 820, 1466, 257, 10290, 51188], "temperature": 0.0,
  "avg_logprob": -0.24646667812181555, "compression_ratio": 1.2278481012658229, "no_speech_prob":
  0.015870986506342888}, {"id": 143, "seek": 154664, "start": 1547.44, "end": 1562.0800000000002,
  "text": " of high dimensional samples. Actually, we just throw the unnecessary part
  of those samples,", "tokens": [50404, 295, 1090, 18795, 10938, 13, 5135, 11, 321,
  445, 3507, 264, 19350, 644, 295, 729, 10938, 11, 51136], "temperature": 0.0, "avg_logprob":
  -0.20283959893619313, "compression_ratio": 1.2660550458715596, "no_speech_prob":
  0.028912030160427094}, {"id": 144, "seek": 154664, "start": 1562.0800000000002,
  "end": 1568.48, "text": " and we only keep the part that matters for us.", "tokens":
  [51136, 293, 321, 787, 1066, 264, 644, 300, 7001, 337, 505, 13, 51456], "temperature":
  0.0, "avg_logprob": -0.20283959893619313, "compression_ratio": 1.2660550458715596,
  "no_speech_prob": 0.028912030160427094}, {"id": 145, "seek": 156848, "start": 1569.44,
  "end": 1575.68, "text": " Yeah, yeah. So kind of in some sense, you could say it''s
  like signal compression,", "tokens": [50412, 865, 11, 1338, 13, 407, 733, 295, 294,
  512, 2020, 11, 291, 727, 584, 309, 311, 411, 6358, 19355, 11, 50724], "temperature":
  0.0, "avg_logprob": -0.20223540845124618, "compression_ratio": 1.6756756756756757,
  "no_speech_prob": 0.02364165149629116}, {"id": 146, "seek": 156848, "start": 1575.68,
  "end": 1583.52, "text": " right? So in some sense, like using the signal law, like
  the distribution, you could actually", "tokens": [50724, 558, 30, 407, 294, 512,
  2020, 11, 411, 1228, 264, 6358, 2101, 11, 411, 264, 7316, 11, 291, 727, 767, 51116],
  "temperature": 0.0, "avg_logprob": -0.20223540845124618, "compression_ratio": 1.6756756756756757,
  "no_speech_prob": 0.02364165149629116}, {"id": 147, "seek": 156848, "start": 1583.52,
  "end": 1589.84, "text": " compress things, like I don''t know if theoretically speaking
  in an image, you have like one object,", "tokens": [51116, 14778, 721, 11, 411,
  286, 500, 380, 458, 498, 29400, 4124, 294, 364, 3256, 11, 291, 362, 411, 472, 2657,
  11, 51432], "temperature": 0.0, "avg_logprob": -0.20223540845124618, "compression_ratio":
  1.6756756756756757, "no_speech_prob": 0.02364165149629116}, {"id": 148, "seek":
  156848, "start": 1589.84, "end": 1595.28, "text": " and the rest is just the background
  of one color. You really don''t need to pass all these pixels", "tokens": [51432,
  293, 264, 1472, 307, 445, 264, 3678, 295, 472, 2017, 13, 509, 534, 500, 380, 643,
  281, 1320, 439, 613, 18668, 51704], "temperature": 0.0, "avg_logprob": -0.20223540845124618,
  "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.02364165149629116},
  {"id": 149, "seek": 159528, "start": 1595.28, "end": 1601.84, "text": " independently,
  like you could just say, okay, it''s a background I''ve learned that it''s that
  color", "tokens": [50364, 21761, 11, 411, 291, 727, 445, 584, 11, 1392, 11, 309,
  311, 257, 3678, 286, 600, 3264, 300, 309, 311, 300, 2017, 50692], "temperature":
  0.0, "avg_logprob": -0.15224934948815239, "compression_ratio": 1.4923857868020305,
  "no_speech_prob": 0.0006475347327068448}, {"id": 150, "seek": 159528, "start": 1601.84,
  "end": 1608.32, "text": " kind of semantically, I guess, and then what matters is
  the object somewhere there that we focus on", "tokens": [50692, 733, 295, 4361,
  49505, 11, 286, 2041, 11, 293, 550, 437, 7001, 307, 264, 2657, 4079, 456, 300, 321,
  1879, 322, 51016], "temperature": 0.0, "avg_logprob": -0.15224934948815239, "compression_ratio":
  1.4923857868020305, "no_speech_prob": 0.0006475347327068448}, {"id": 151, "seek":
  159528, "start": 1608.32, "end": 1616.56, "text": " when we look at this picture,
  right? Yeah, exactly. Actually, in the original distribution case,", "tokens": [51016,
  562, 321, 574, 412, 341, 3036, 11, 558, 30, 865, 11, 2293, 13, 5135, 11, 294, 264,
  3380, 7316, 1389, 11, 51428], "temperature": 0.0, "avg_logprob": -0.15224934948815239,
  "compression_ratio": 1.4923857868020305, "no_speech_prob": 0.0006475347327068448},
  {"id": 152, "seek": 161656, "start": 1617.52, "end": 1624.8799999999999, "text":
  " for example, of images, we don''t have any connection between the value of a pixel",
  "tokens": [50412, 337, 1365, 11, 295, 5267, 11, 321, 500, 380, 362, 604, 4984, 1296,
  264, 2158, 295, 257, 19261, 50780], "temperature": 0.0, "avg_logprob": -0.12031276835951694,
  "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.0027168949600309134},
  {"id": 153, "seek": 161656, "start": 1626.56, "end": 1640.48, "text": " and the
  semantic counterpart of that pixel one. But once we transform it into a vector space,",
  "tokens": [50864, 293, 264, 47982, 22335, 295, 300, 19261, 472, 13, 583, 1564, 321,
  4088, 309, 666, 257, 8062, 1901, 11, 51560], "temperature": 0.0, "avg_logprob":
  -0.12031276835951694, "compression_ratio": 1.3461538461538463, "no_speech_prob":
  0.0027168949600309134}, {"id": 154, "seek": 164048, "start": 1641.1200000000001,
  "end": 1653.44, "text": " at least theoretically we can make conclusions. For example,
  we have a 1024 dimensional", "tokens": [50396, 412, 1935, 29400, 321, 393, 652,
  22865, 13, 1171, 1365, 11, 321, 362, 257, 1266, 7911, 18795, 51012], "temperature":
  0.0, "avg_logprob": -0.2408162967578785, "compression_ratio": 1.2521008403361344,
  "no_speech_prob": 0.0059758685529232025}, {"id": 155, "seek": 164048, "start": 1654.88,
  "end": 1661.76, "text": " vector as a representation of that image. In this case,
  if we", "tokens": [51084, 8062, 382, 257, 10290, 295, 300, 3256, 13, 682, 341, 1389,
  11, 498, 321, 51428], "temperature": 0.0, "avg_logprob": -0.2408162967578785, "compression_ratio":
  1.2521008403361344, "no_speech_prob": 0.0059758685529232025}, {"id": 156, "seek":
  166176, "start": 1661.76, "end": 1674.72, "text": " examine this vector space, we
  can make conclusions of this value in the index zero,", "tokens": [50364, 17496,
  341, 8062, 1901, 11, 321, 393, 652, 22865, 295, 341, 2158, 294, 264, 8186, 4018,
  11, 51012], "temperature": 0.0, "avg_logprob": -0.2603144231049911, "compression_ratio":
  1.5081967213114753, "no_speech_prob": 0.0028704653959721327}, {"id": 157, "seek":
  166176, "start": 1674.72, "end": 1687.92, "text": " in cause the features of this
  feature of image. For example, it can, in cause the size of a specific", "tokens":
  [51012, 294, 3082, 264, 4122, 295, 341, 4111, 295, 3256, 13, 1171, 1365, 11, 309,
  393, 11, 294, 3082, 264, 2744, 295, 257, 2685, 51672], "temperature": 0.0, "avg_logprob":
  -0.2603144231049911, "compression_ratio": 1.5081967213114753, "no_speech_prob":
  0.0028704653959721327}, {"id": 158, "seek": 168792, "start": 1688.0, "end": 1701.1200000000001,
  "text": " object or the colors value of a specific object or maybe some more abstract
  features of objects.", "tokens": [50368, 2657, 420, 264, 4577, 2158, 295, 257, 2685,
  2657, 420, 1310, 512, 544, 12649, 4122, 295, 6565, 13, 51024], "temperature": 0.0,
  "avg_logprob": -0.2761072256626227, "compression_ratio": 1.4609375, "no_speech_prob":
  0.006052842829376459}, {"id": 159, "seek": 168792, "start": 1702.72, "end": 1716.72,
  "text": " This enables us to search it more efficiently instead of otherwise our
  values are actually", "tokens": [51104, 639, 17077, 505, 281, 3164, 309, 544, 19621,
  2602, 295, 5911, 527, 4190, 366, 767, 51804], "temperature": 0.0, "avg_logprob":
  -0.2761072256626227, "compression_ratio": 1.4609375, "no_speech_prob": 0.006052842829376459},
  {"id": 160, "seek": 171672, "start": 1717.2, "end": 1730.0, "text": " distributed
  to a very wide range. And we don''t have such interpretations in that distribution
  space.", "tokens": [50388, 12631, 281, 257, 588, 4874, 3613, 13, 400, 321, 500,
  380, 362, 1270, 37547, 294, 300, 7316, 1901, 13, 51028], "temperature": 0.0, "avg_logprob":
  -0.2621048927307129, "compression_ratio": 1.5172413793103448, "no_speech_prob":
  0.008603821508586407}, {"id": 161, "seek": 171672, "start": 1730.72, "end": 1736.08,
  "text": " Yeah, that makes sense. It''s a very unique high variant and also in some
  senses,", "tokens": [51064, 865, 11, 300, 1669, 2020, 13, 467, 311, 257, 588, 3845,
  1090, 17501, 293, 611, 294, 512, 17057, 11, 51332], "temperature": 0.0, "avg_logprob":
  -0.2621048927307129, "compression_ratio": 1.5172413793103448, "no_speech_prob":
  0.008603821508586407}, {"id": 162, "seek": 171672, "start": 1736.08, "end": 1741.68,
  "text": " like waste of space because we are not communicating that much more information
  by", "tokens": [51332, 411, 5964, 295, 1901, 570, 321, 366, 406, 17559, 300, 709,
  544, 1589, 538, 51612], "temperature": 0.0, "avg_logprob": -0.2621048927307129,
  "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.008603821508586407},
  {"id": 163, "seek": 174168, "start": 1742.0, "end": 1746.64, "text": " sort of encoding
  all these pixels. But we could actually extract some features and patterns in",
  "tokens": [50380, 1333, 295, 43430, 439, 613, 18668, 13, 583, 321, 727, 767, 8947,
  512, 4122, 293, 8294, 294, 50612], "temperature": 0.0, "avg_logprob": -0.23810322388358737,
  "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.01150743942707777},
  {"id": 164, "seek": 174168, "start": 1746.64, "end": 1752.0, "text": " the image.
  I think some early work on this was done using, if I remember, it was called a",
  "tokens": [50612, 264, 3256, 13, 286, 519, 512, 2440, 589, 322, 341, 390, 1096,
  1228, 11, 498, 286, 1604, 11, 309, 390, 1219, 257, 50880], "temperature": 0.0, "avg_logprob":
  -0.23810322388358737, "compression_ratio": 1.6083333333333334, "no_speech_prob":
  0.01150743942707777}, {"id": 165, "seek": 174168, "start": 1752.0, "end": 1758.24,
  "text": " Godworth filter or some other ways of kind of smoothing your image and
  trying to learn what features", "tokens": [50880, 1265, 13136, 6608, 420, 512, 661,
  2098, 295, 733, 295, 899, 6259, 571, 428, 3256, 293, 1382, 281, 1466, 437, 4122,
  51192], "temperature": 0.0, "avg_logprob": -0.23810322388358737, "compression_ratio":
  1.6083333333333334, "no_speech_prob": 0.01150743942707777}, {"id": 166, "seek":
  174168, "start": 1758.24, "end": 1765.3600000000001, "text": " you have, for instance,
  if you try to differentiate between spruce and widely trees. So like for the", "tokens":
  [51192, 291, 362, 11, 337, 5197, 11, 498, 291, 853, 281, 23203, 1296, 637, 41158,
  293, 13371, 5852, 13, 407, 411, 337, 264, 51548], "temperature": 0.0, "avg_logprob":
  -0.23810322388358737, "compression_ratio": 1.6083333333333334, "no_speech_prob":
  0.01150743942707777}, {"id": 167, "seek": 176536, "start": 1765.36, "end": 1776.3999999999999,
  "text": " purposes of keeping one tree and then maybe removing the others. But I
  think it wasn''t as efficient", "tokens": [50364, 9932, 295, 5145, 472, 4230, 293,
  550, 1310, 12720, 264, 2357, 13, 583, 286, 519, 309, 2067, 380, 382, 7148, 50916],
  "temperature": 0.0, "avg_logprob": -0.20047240257263182, "compression_ratio": 1.606837606837607,
  "no_speech_prob": 0.0010489040287211537}, {"id": 168, "seek": 176536, "start": 1777.12,
  "end": 1781.6799999999998, "text": " perhaps as compared to deep learning because
  deep learning, as far as understanding,", "tokens": [50952, 4317, 382, 5347, 281,
  2452, 2539, 570, 2452, 2539, 11, 382, 1400, 382, 3701, 11, 51180], "temperature":
  0.0, "avg_logprob": -0.20047240257263182, "compression_ratio": 1.606837606837607,
  "no_speech_prob": 0.0010489040287211537}, {"id": 169, "seek": 176536, "start": 1781.6799999999998,
  "end": 1787.6, "text": " basically like learns without features in many ways. It
  learns from the data and then you should", "tokens": [51180, 1936, 411, 27152, 1553,
  4122, 294, 867, 2098, 13, 467, 27152, 490, 264, 1412, 293, 550, 291, 820, 51476],
  "temperature": 0.0, "avg_logprob": -0.20047240257263182, "compression_ratio": 1.606837606837607,
  "no_speech_prob": 0.0010489040287211537}, {"id": 170, "seek": 176536, "start": 1787.6,
  "end": 1794.24, "text": " have some target function that you''re optimizing for
  so it can recreate the weights inside it.", "tokens": [51476, 362, 512, 3779, 2445,
  300, 291, 434, 40425, 337, 370, 309, 393, 25833, 264, 17443, 1854, 309, 13, 51808],
  "temperature": 0.0, "avg_logprob": -0.20047240257263182, "compression_ratio": 1.606837606837607,
  "no_speech_prob": 0.0010489040287211537}, {"id": 171, "seek": 179536, "start": 1796.32,
  "end": 1800.6399999999999, "text": " Exactly. Actually, what is most differentiating",
  "tokens": [50412, 7587, 13, 5135, 11, 437, 307, 881, 27372, 990, 50628], "temperature":
  0.0, "avg_logprob": -0.20722929123909242, "compression_ratio": 1.3557692307692308,
  "no_speech_prob": 0.0030548565555363894}, {"id": 172, "seek": 179536, "start": 1804.24,
  "end": 1816.6399999999999, "text": " feature of deep learning is deep learning is
  actually used to learn the parameters of complex", "tokens": [50808, 4111, 295,
  2452, 2539, 307, 2452, 2539, 307, 767, 1143, 281, 1466, 264, 9834, 295, 3997, 51428],
  "temperature": 0.0, "avg_logprob": -0.20722929123909242, "compression_ratio": 1.3557692307692308,
  "no_speech_prob": 0.0030548565555363894}, {"id": 173, "seek": 181664, "start": 1817.6000000000001,
  "end": 1831.1200000000001, "text": " functions instead of manually tuning them.
  Before deep learning, we already had most of the", "tokens": [50412, 6828, 2602,
  295, 16945, 15164, 552, 13, 4546, 2452, 2539, 11, 321, 1217, 632, 881, 295, 264,
  51088], "temperature": 0.0, "avg_logprob": -0.1290648341178894, "compression_ratio":
  1.4803149606299213, "no_speech_prob": 0.00355158350430429}, {"id": 174, "seek":
  181664, "start": 1831.1200000000001, "end": 1844.5600000000002, "text": " filters
  we currently have. But the parameters of such filters were supposed to be manually
  tuned", "tokens": [51088, 15995, 321, 4362, 362, 13, 583, 264, 9834, 295, 1270,
  15995, 645, 3442, 281, 312, 16945, 10870, 51760], "temperature": 0.0, "avg_logprob":
  -0.1290648341178894, "compression_ratio": 1.4803149606299213, "no_speech_prob":
  0.00355158350430429}, {"id": 175, "seek": 184456, "start": 1844.6399999999999, "end":
  1855.04, "text": " by experts in that domain. But in deep learning, we learn those
  parameters directly from data.", "tokens": [50368, 538, 8572, 294, 300, 9274, 13,
  583, 294, 2452, 2539, 11, 321, 1466, 729, 9834, 3838, 490, 1412, 13, 50888], "temperature":
  0.0, "avg_logprob": -0.1896209239959717, "compression_ratio": 1.3306451612903225,
  "no_speech_prob": 0.004211807157844305}, {"id": 176, "seek": 184456, "start": 1856.1599999999999,
  "end": 1864.56, "text": " And as you said, actually, the beginning of metric learning
  is also in", "tokens": [50944, 400, 382, 291, 848, 11, 767, 11, 264, 2863, 295,
  20678, 2539, 307, 611, 294, 51364], "temperature": 0.0, "avg_logprob": -0.1896209239959717,
  "compression_ratio": 1.3306451612903225, "no_speech_prob": 0.004211807157844305},
  {"id": 177, "seek": 186456, "start": 1865.04, "end": 1876.96, "text": " dimensionality
  reduction. We have most popular contrastive loss, for example. And the first", "tokens":
  [50388, 10139, 1860, 11004, 13, 492, 362, 881, 3743, 8712, 488, 4470, 11, 337, 1365,
  13, 400, 264, 700, 50984], "temperature": 0.0, "avg_logprob": -0.25351572036743164,
  "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.004416854120790958},
  {"id": 178, "seek": 186456, "start": 1876.96, "end": 1888.72, "text": " introduction
  of contrastive loss is in 2005 and the original purpose of that function actually",
  "tokens": [50984, 9339, 295, 8712, 488, 4470, 307, 294, 14394, 293, 264, 3380, 4334,
  295, 300, 2445, 767, 51572], "temperature": 0.0, "avg_logprob": -0.25351572036743164,
  "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.004416854120790958},
  {"id": 179, "seek": 188872, "start": 1889.6000000000001, "end": 1902.64, "text":
  " to reduce dimensionality of high dimensional inputs rather than vector source
  or anything F", "tokens": [50408, 281, 5407, 10139, 1860, 295, 1090, 18795, 15743,
  2831, 813, 8062, 4009, 420, 1340, 479, 51060], "temperature": 0.0, "avg_logprob":
  -0.27132900555928546, "compression_ratio": 1.7333333333333334, "no_speech_prob":
  0.010725550353527069}, {"id": 180, "seek": 188872, "start": 1904.16, "end": 1915.44,
  "text": " for actually another end just tried to reduce the dimensionality of high
  dimensional input", "tokens": [51136, 337, 767, 1071, 917, 445, 3031, 281, 5407,
  264, 10139, 1860, 295, 1090, 18795, 4846, 51700], "temperature": 0.0, "avg_logprob":
  -0.27132900555928546, "compression_ratio": 1.7333333333333334, "no_speech_prob":
  0.010725550353527069}, {"id": 181, "seek": 191544, "start": 1915.52, "end": 1923.6000000000001,
  "text": " to use lower dimensional input F features to other models.", "tokens":
  [50368, 281, 764, 3126, 18795, 4846, 479, 4122, 281, 661, 5245, 13, 50772], "temperature":
  0.0, "avg_logprob": -0.1849643144852076, "compression_ratio": 1.502183406113537,
  "no_speech_prob": 0.0032915824558585882}, {"id": 182, "seek": 191544, "start": 1924.48,
  "end": 1929.92, "text": " Yeah, that sounds exciting. Actually, before you brought
  this up, I didn''t think that way because", "tokens": [50816, 865, 11, 300, 3263,
  4670, 13, 5135, 11, 949, 291, 3038, 341, 493, 11, 286, 994, 380, 519, 300, 636,
  570, 51088], "temperature": 0.0, "avg_logprob": -0.1849643144852076, "compression_ratio":
  1.502183406113537, "no_speech_prob": 0.0032915824558585882}, {"id": 183, "seek":
  191544, "start": 1931.1200000000001, "end": 1938.24, "text": " I was experimenting
  in my team also with things like product quantization. So you do have all", "tokens":
  [51148, 286, 390, 29070, 294, 452, 1469, 611, 365, 721, 411, 1674, 4426, 2144, 13,
  407, 291, 360, 362, 439, 51504], "temperature": 0.0, "avg_logprob": -0.1849643144852076,
  "compression_ratio": 1.502183406113537, "no_speech_prob": 0.0032915824558585882},
  {"id": 184, "seek": 191544, "start": 1938.24, "end": 1943.04, "text": " already
  the vectors computed by the neural network, but you could actually quantize them
  even", "tokens": [51504, 1217, 264, 18875, 40610, 538, 264, 18161, 3209, 11, 457,
  291, 727, 767, 4426, 1125, 552, 754, 51744], "temperature": 0.0, "avg_logprob":
  -0.1849643144852076, "compression_ratio": 1.502183406113537, "no_speech_prob": 0.0032915824558585882},
  {"id": 185, "seek": 194304, "start": 1943.04, "end": 1950.48, "text": " further.
  So you save space and maybe of course you introduce some overlaps that might decrease
  your", "tokens": [50364, 3052, 13, 407, 291, 3155, 1901, 293, 1310, 295, 1164, 291,
  5366, 512, 15986, 2382, 300, 1062, 11514, 428, 50736], "temperature": 0.0, "avg_logprob":
  -0.15778249769068475, "compression_ratio": 1.582010582010582, "no_speech_prob":
  0.009588501416146755}, {"id": 186, "seek": 194304, "start": 1950.48, "end": 1958.56,
  "text": " precision, but slightly, but you''re gonna save a ton of space and make
  your search more efficient.", "tokens": [50736, 18356, 11, 457, 4748, 11, 457, 291,
  434, 799, 3155, 257, 2952, 295, 1901, 293, 652, 428, 3164, 544, 7148, 13, 51140],
  "temperature": 0.0, "avg_logprob": -0.15778249769068475, "compression_ratio": 1.582010582010582,
  "no_speech_prob": 0.009588501416146755}, {"id": 187, "seek": 194304, "start": 1958.56,
  "end": 1964.48, "text": " So it''s almost like you could think of dimensionality
  reduction in so many different levels and ways", "tokens": [51140, 407, 309, 311,
  1920, 411, 291, 727, 519, 295, 10139, 1860, 11004, 294, 370, 867, 819, 4358, 293,
  2098, 51436], "temperature": 0.0, "avg_logprob": -0.15778249769068475, "compression_ratio":
  1.582010582010582, "no_speech_prob": 0.009588501416146755}, {"id": 188, "seek":
  196448, "start": 1965.1200000000001, "end": 1972.8, "text": " as you have the reason
  about your data, right? Yeah, exactly. Actually, metric learning is", "tokens":
  [50396, 382, 291, 362, 264, 1778, 466, 428, 1412, 11, 558, 30, 865, 11, 2293, 13,
  5135, 11, 20678, 2539, 307, 50780], "temperature": 0.0, "avg_logprob": -0.2866149946700695,
  "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.00762192253023386},
  {"id": 189, "seek": 196448, "start": 1973.68, "end": 1984.88, "text": " itself a
  type of dimensionality reduction, but even after you apply metric learning and vector",
  "tokens": [50824, 2564, 257, 2010, 295, 10139, 1860, 11004, 11, 457, 754, 934, 291,
  3079, 20678, 2539, 293, 8062, 51384], "temperature": 0.0, "avg_logprob": -0.2866149946700695,
  "compression_ratio": 1.3703703703703705, "no_speech_prob": 0.00762192253023386},
  {"id": 190, "seek": 198488, "start": 1984.88, "end": 1996.64, "text": " encoding
  to your data, you still have a high dimensional vector. You have, for example, 10,",
  "tokens": [50364, 43430, 281, 428, 1412, 11, 291, 920, 362, 257, 1090, 18795, 8062,
  13, 509, 362, 11, 337, 1365, 11, 1266, 11, 50952], "temperature": 0.0, "avg_logprob":
  -0.24850128173828126, "compression_ratio": 1.0833333333333333, "no_speech_prob":
  0.007246140856295824}, {"id": 191, "seek": 199664, "start": 1996.64, "end": 2015.0400000000002,
  "text": " 10, 10, 10, 4, dimensional data times 32 bits for a single flaw. So it''s
  already a huge data", "tokens": [50364, 1266, 11, 1266, 11, 1266, 11, 1017, 11,
  18795, 1412, 1413, 8858, 9239, 337, 257, 2167, 13717, 13, 407, 309, 311, 1217, 257,
  2603, 1412, 51284], "temperature": 0.0, "avg_logprob": -0.43807647968160696, "compression_ratio":
  1.069767441860465, "no_speech_prob": 0.008578852750360966}, {"id": 192, "seek":
  201504, "start": 2015.04, "end": 2029.52, "text": " when you have, for example,
  millions of samples. So you can still actually apply some quantization", "tokens":
  [50364, 562, 291, 362, 11, 337, 1365, 11, 6803, 295, 10938, 13, 407, 291, 393, 920,
  767, 3079, 512, 4426, 2144, 51088], "temperature": 0.0, "avg_logprob": -0.2164289951324463,
  "compression_ratio": 1.1264367816091954, "no_speech_prob": 0.018948595970869064},
  {"id": 193, "seek": 202952, "start": 2030.32, "end": 2042.8799999999999, "text":
  " methods to get even smaller representations from that one. And this can be also
  hierarchical, meaning that", "tokens": [50404, 7150, 281, 483, 754, 4356, 33358,
  490, 300, 472, 13, 400, 341, 393, 312, 611, 35250, 804, 11, 3620, 300, 51032], "temperature":
  0.0, "avg_logprob": -0.20502420572134164, "compression_ratio": 1.472, "no_speech_prob":
  0.08072381466627121}, {"id": 194, "seek": 202952, "start": 2043.68, "end": 2056.16,
  "text": " you can get several representations of the same sample at different levels
  of", "tokens": [51072, 291, 393, 483, 2940, 33358, 295, 264, 912, 6889, 412, 819,
  4358, 295, 51696], "temperature": 0.0, "avg_logprob": -0.20502420572134164, "compression_ratio":
  1.472, "no_speech_prob": 0.08072381466627121}, {"id": 195, "seek": 205616, "start":
  2057.04, "end": 2066.16, "text": " information encoded in that feature space. Yeah,
  that''s fantastic. So I was also thinking like,", "tokens": [50408, 1589, 2058,
  12340, 294, 300, 4111, 1901, 13, 865, 11, 300, 311, 5456, 13, 407, 286, 390, 611,
  1953, 411, 11, 50864], "temperature": 0.0, "avg_logprob": -0.21805763244628906,
  "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.004282147157937288},
  {"id": 196, "seek": 205616, "start": 2067.6, "end": 2075.7599999999998, "text":
  " if you could give like some practical example or setting where I could start thinking
  about", "tokens": [50936, 498, 291, 727, 976, 411, 512, 8496, 1365, 420, 3287, 689,
  286, 727, 722, 1953, 466, 51344], "temperature": 0.0, "avg_logprob": -0.21805763244628906,
  "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.004282147157937288},
  {"id": 197, "seek": 205616, "start": 2075.7599999999998, "end": 2083.7599999999998,
  "text": " deploying metric learning and also like, could you sort of point us in
  the direction of what tools", "tokens": [51344, 34198, 20678, 2539, 293, 611, 411,
  11, 727, 291, 1333, 295, 935, 505, 294, 264, 3513, 295, 437, 3873, 51744], "temperature":
  0.0, "avg_logprob": -0.21805763244628906, "compression_ratio": 1.5376344086021505,
  "no_speech_prob": 0.004282147157937288}, {"id": 198, "seek": 208376, "start": 2083.76,
  "end": 2089.2000000000003, "text": " are available so that I don''t think we need
  to reinvent everything from scratch, but maybe there are", "tokens": [50364, 366,
  2435, 370, 300, 286, 500, 380, 519, 321, 643, 281, 33477, 1203, 490, 8459, 11, 457,
  1310, 456, 366, 50636], "temperature": 0.0, "avg_logprob": -0.1574177811111229,
  "compression_ratio": 1.5153061224489797, "no_speech_prob": 0.00320336502045393},
  {"id": 199, "seek": 208376, "start": 2089.2000000000003, "end": 2095.92, "text":
  " some practices, also best practices available, you know, to structure this process.
  Can you give", "tokens": [50636, 512, 7525, 11, 611, 1151, 7525, 2435, 11, 291,
  458, 11, 281, 3877, 341, 1399, 13, 1664, 291, 976, 50972], "temperature": 0.0, "avg_logprob":
  -0.1574177811111229, "compression_ratio": 1.5153061224489797, "no_speech_prob":
  0.00320336502045393}, {"id": 200, "seek": 208376, "start": 2095.92, "end": 2106.8,
  "text": " some advice on that? Yeah, sure. For a starter example, actually, metric
  learning is best known for", "tokens": [50972, 512, 5192, 322, 300, 30, 865, 11,
  988, 13, 1171, 257, 22465, 1365, 11, 767, 11, 20678, 2539, 307, 1151, 2570, 337,
  51516], "temperature": 0.0, "avg_logprob": -0.1574177811111229, "compression_ratio":
  1.5153061224489797, "no_speech_prob": 0.00320336502045393}, {"id": 201, "seek":
  210680, "start": 2106.8, "end": 2115.76, "text": " is used in face recognition,
  but personally, I don''t support use of machine learning to process", "tokens":
  [50364, 307, 1143, 294, 1851, 11150, 11, 457, 5665, 11, 286, 500, 380, 1406, 764,
  295, 3479, 2539, 281, 1399, 50812], "temperature": 0.0, "avg_logprob": -0.24581545049493964,
  "compression_ratio": 1.3617021276595744, "no_speech_prob": 0.005657794885337353},
  {"id": 202, "seek": 210680, "start": 2115.76, "end": 2126.6400000000003, "text":
  " biometric information. So I give an example from our everyday life, actually,
  we almost everyday", "tokens": [50812, 3228, 29470, 1589, 13, 407, 286, 976, 364,
  1365, 490, 527, 7429, 993, 11, 767, 11, 321, 1920, 7429, 51356], "temperature":
  0.0, "avg_logprob": -0.24581545049493964, "compression_ratio": 1.3617021276595744,
  "no_speech_prob": 0.005657794885337353}, {"id": 203, "seek": 212664, "start": 2127.44,
  "end": 2139.2, "text": " use it, smart deploy. The feature found in, for example,
  GMA, LinkedIn, and other messaging apps.", "tokens": [50404, 764, 309, 11, 4069,
  7274, 13, 440, 4111, 1352, 294, 11, 337, 1365, 11, 460, 9998, 11, 20657, 11, 293,
  661, 21812, 7733, 13, 50992], "temperature": 0.0, "avg_logprob": -0.23712456744650137,
  "compression_ratio": 1.2993197278911566, "no_speech_prob": 0.00422839168459177},
  {"id": 204, "seek": 212664, "start": 2141.44, "end": 2152.3199999999997, "text":
  " Actually, it is trained from a large collection of conversation histories in these
  platforms.", "tokens": [51104, 5135, 11, 309, 307, 8895, 490, 257, 2416, 5765, 295,
  3761, 30631, 294, 613, 9473, 13, 51648], "temperature": 0.0, "avg_logprob": -0.23712456744650137,
  "compression_ratio": 1.2993197278911566, "no_speech_prob": 0.00422839168459177},
  {"id": 205, "seek": 215232, "start": 2153.2000000000003, "end": 2169.44, "text":
  " Basically, they just like the example we put in the beginning image and textual
  unified vector space,", "tokens": [50408, 8537, 11, 436, 445, 411, 264, 1365, 321,
  829, 294, 264, 2863, 3256, 293, 2487, 901, 26787, 8062, 1901, 11, 51220], "temperature":
  0.0, "avg_logprob": -0.2126744099152394, "compression_ratio": 1.4803149606299213,
  "no_speech_prob": 0.0026586647145450115}, {"id": 206, "seek": 215232, "start": 2169.44,
  "end": 2178.88, "text": " they construct a unified vector space for conversation
  histories and single sentences.", "tokens": [51220, 436, 7690, 257, 26787, 8062,
  1901, 337, 3761, 30631, 293, 2167, 16579, 13, 51692], "temperature": 0.0, "avg_logprob":
  -0.2126744099152394, "compression_ratio": 1.4803149606299213, "no_speech_prob":
  0.0026586647145450115}, {"id": 207, "seek": 217888, "start": 2179.44, "end": 2192.2400000000002,
  "text": " For any moment of conversation, you encode the history of that conversation
  to retrieve", "tokens": [50392, 1171, 604, 1623, 295, 3761, 11, 291, 2058, 1429,
  264, 2503, 295, 300, 3761, 281, 30254, 51032], "temperature": 0.0, "avg_logprob":
  -0.21244478225708008, "compression_ratio": 1.4508196721311475, "no_speech_prob":
  0.007910378277301788}, {"id": 208, "seek": 217888, "start": 2193.28, "end": 2207.76,
  "text": " most relevant replies to that history. And you can show them as suggestions
  to the usage,", "tokens": [51084, 881, 7340, 42289, 281, 300, 2503, 13, 400, 291,
  393, 855, 552, 382, 13396, 281, 264, 14924, 11, 51808], "temperature": 0.0, "avg_logprob":
  -0.21244478225708008, "compression_ratio": 1.4508196721311475, "no_speech_prob":
  0.007910378277301788}, {"id": 209, "seek": 220776, "start": 2207.76, "end": 2218.1600000000003,
  "text": " and users can pitch one of them. And what is exciting with this setup,
  you can also", "tokens": [50364, 293, 5022, 393, 7293, 472, 295, 552, 13, 400, 437,
  307, 4670, 365, 341, 8657, 11, 291, 393, 611, 50884], "temperature": 0.0, "avg_logprob":
  -0.17199566250755674, "compression_ratio": 1.390625, "no_speech_prob": 0.00404054019600153},
  {"id": 210, "seek": 220776, "start": 2219.5200000000004, "end": 2234.8, "text":
  " log the chosen reply, and you can continue improving your model from direct feedback
  from your", "tokens": [50952, 3565, 264, 8614, 16972, 11, 293, 291, 393, 2354, 11470,
  428, 2316, 490, 2047, 5824, 490, 428, 51716], "temperature": 0.0, "avg_logprob":
  -0.17199566250755674, "compression_ratio": 1.390625, "no_speech_prob": 0.00404054019600153},
  {"id": 211, "seek": 223480, "start": 2235.52, "end": 2247.36, "text": " actual users.
  So it''s a really practical use case of metric learning. And for practitioners who
  want to", "tokens": [50400, 3539, 5022, 13, 407, 309, 311, 257, 534, 8496, 764,
  1389, 295, 20678, 2539, 13, 400, 337, 25742, 567, 528, 281, 50992], "temperature":
  0.0, "avg_logprob": -0.259184482485749, "compression_ratio": 1.4402985074626866,
  "no_speech_prob": 0.00505034951493144}, {"id": 212, "seek": 223480, "start": 2248.32,
  "end": 2259.92, "text": " start experimenting with metric learning, actually, there
  are lots of tools to solve very", "tokens": [51040, 722, 29070, 365, 20678, 2539,
  11, 767, 11, 456, 366, 3195, 295, 3873, 281, 5039, 588, 51620], "temperature": 0.0,
  "avg_logprob": -0.259184482485749, "compression_ratio": 1.4402985074626866, "no_speech_prob":
  0.00505034951493144}, {"id": 213, "seek": 225992, "start": 2259.92, "end": 2271.52,
  "text": " few problems in metric learning. So in the context of deep learning model
  development itself,", "tokens": [50364, 1326, 2740, 294, 20678, 2539, 13, 407, 294,
  264, 4319, 295, 2452, 2539, 2316, 3250, 2564, 11, 50944], "temperature": 0.0, "avg_logprob":
  -0.2515622813527177, "compression_ratio": 1.4230769230769231, "no_speech_prob":
  0.0075847068801522255}, {"id": 214, "seek": 225992, "start": 2271.52, "end": 2282.08,
  "text": " we have several libraries, such as high-torch metric learning and transfer
  flow similarity.", "tokens": [50944, 321, 362, 2940, 15148, 11, 1270, 382, 1090,
  12, 21151, 339, 20678, 2539, 293, 5003, 3095, 32194, 13, 51472], "temperature":
  0.0, "avg_logprob": -0.2515622813527177, "compression_ratio": 1.4230769230769231,
  "no_speech_prob": 0.0075847068801522255}, {"id": 215, "seek": 228208, "start": 2282.56,
  "end": 2297.44, "text": " There are other libraries as well, but I think these are
  the most mature libraries and most", "tokens": [50388, 821, 366, 661, 15148, 382,
  731, 11, 457, 286, 519, 613, 366, 264, 881, 14442, 15148, 293, 881, 51132], "temperature":
  0.0, "avg_logprob": -0.3006820028478449, "compression_ratio": 1.1973684210526316,
  "no_speech_prob": 0.017655150964856148}, {"id": 216, "seek": 229744, "start": 2297.52,
  "end": 2310.0, "text": " cultural, how should I say, virtual libraries to tackle
  with different data tasks.", "tokens": [50368, 6988, 11, 577, 820, 286, 584, 11,
  6374, 15148, 281, 14896, 365, 819, 1412, 9608, 13, 50992], "temperature": 0.0, "avg_logprob":
  -0.457450093449773, "compression_ratio": 1.2741935483870968, "no_speech_prob": 0.018973808735609055},
  {"id": 217, "seek": 229744, "start": 2312.16, "end": 2320.8, "text": " On the other
  hand, for visualization, we have this transfer flow projector,", "tokens": [51100,
  1282, 264, 661, 1011, 11, 337, 25801, 11, 321, 362, 341, 5003, 3095, 39792, 11,
  51532], "temperature": 0.0, "avg_logprob": -0.457450093449773, "compression_ratio":
  1.2741935483870968, "no_speech_prob": 0.018973808735609055}, {"id": 218, "seek":
  232080, "start": 2320.8, "end": 2330.0800000000004, "text": " is a browser-based
  tool for you can examine your embedding easily with that one.", "tokens": [50364,
  307, 257, 11185, 12, 6032, 2290, 337, 291, 393, 17496, 428, 12240, 3584, 3612, 365,
  300, 472, 13, 50828], "temperature": 0.0, "avg_logprob": -0.35618030734178496, "compression_ratio":
  1.344, "no_speech_prob": 0.01588754542171955}, {"id": 219, "seek": 232080, "start":
  2332.96, "end": 2343.6000000000004, "text": " There are also vector search databases,
  there are increasing in numbers, but of course,", "tokens": [50972, 821, 366, 611,
  8062, 3164, 22380, 11, 456, 366, 5662, 294, 3547, 11, 457, 295, 1164, 11, 51504],
  "temperature": 0.0, "avg_logprob": -0.35618030734178496, "compression_ratio": 1.344,
  "no_speech_prob": 0.01588754542171955}, {"id": 220, "seek": 234360, "start": 2344.48,
  "end": 2357.2799999999997, "text": " I am a fan of Quaddon because it''s really
  doing a great job with an extensive filtering support", "tokens": [50408, 286, 669,
  257, 3429, 295, 2326, 345, 13966, 570, 309, 311, 534, 884, 257, 869, 1691, 365,
  364, 13246, 30822, 1406, 51048], "temperature": 0.0, "avg_logprob": -0.3162554680032933,
  "compression_ratio": 1.3098591549295775, "no_speech_prob": 0.014685436151921749},
  {"id": 221, "seek": 234360, "start": 2357.2799999999997, "end": 2372.24, "text":
  " for a variety of data tasks. And it''s doing this very efficiently, very elegant
  in only 40", "tokens": [51048, 337, 257, 5673, 295, 1412, 9608, 13, 400, 309, 311,
  884, 341, 588, 19621, 11, 588, 21117, 294, 787, 3356, 51796], "temperature": 0.0,
  "avg_logprob": -0.3162554680032933, "compression_ratio": 1.3098591549295775, "no_speech_prob":
  0.014685436151921749}, {"id": 222, "seek": 237224, "start": 2372.8799999999997,
  "end": 2387.6, "text": " megawise. So it opens up very important is to put your
  metric learning model into production", "tokens": [50396, 10816, 1607, 908, 13,
  407, 309, 9870, 493, 588, 1021, 307, 281, 829, 428, 20678, 2539, 2316, 666, 4265,
  51132], "temperature": 0.0, "avg_logprob": -0.31873972519584326, "compression_ratio":
  1.1358024691358024, "no_speech_prob": 0.013554328121244907}, {"id": 223, "seek":
  238760, "start": 2388.56, "end": 2402.16, "text": " and to combine vector search
  with super search as well. So you can just filter your data based on", "tokens":
  [50412, 293, 281, 10432, 8062, 3164, 365, 1687, 3164, 382, 731, 13, 407, 291, 393,
  445, 6608, 428, 1412, 2361, 322, 51092], "temperature": 0.0, "avg_logprob": -0.3026287749006942,
  "compression_ratio": 1.373913043478261, "no_speech_prob": 0.025765538215637207},
  {"id": 224, "seek": 238760, "start": 2403.6, "end": 2409.8399999999997, "text":
  " their payload information at the same time as vector search.", "tokens": [51164,
  641, 30918, 1589, 412, 264, 912, 565, 382, 8062, 3164, 13, 51476], "temperature":
  0.0, "avg_logprob": -0.3026287749006942, "compression_ratio": 1.373913043478261,
  "no_speech_prob": 0.025765538215637207}, {"id": 225, "seek": 240984, "start": 2410.32,
  "end": 2425.04, "text": " I think these are other than that, beyond beside my research
  and engineering practices,", "tokens": [50388, 286, 519, 613, 366, 661, 813, 300,
  11, 4399, 15726, 452, 2132, 293, 7043, 7525, 11, 51124], "temperature": 0.0, "avg_logprob":
  -0.3671827567251105, "compression_ratio": 1.3615384615384616, "no_speech_prob":
  0.018351832404732704}, {"id": 226, "seek": 240984, "start": 2425.76, "end": 2437.28,
  "text": " I''m also maintaining a repository called Automatic Learning and I''m
  regularly sharing new", "tokens": [51160, 286, 478, 611, 14916, 257, 25841, 1219,
  6049, 13143, 15205, 293, 286, 478, 11672, 5414, 777, 51736], "temperature": 0.0,
  "avg_logprob": -0.3671827567251105, "compression_ratio": 1.3615384615384616, "no_speech_prob":
  0.018351832404732704}, {"id": 227, "seek": 243728, "start": 2438.1600000000003,
  "end": 2448.88, "text": " developments in the domain of metric learning with personal
  annotations. So I think it might be", "tokens": [50408, 20862, 294, 264, 9274, 295,
  20678, 2539, 365, 2973, 25339, 763, 13, 407, 286, 519, 309, 1062, 312, 50944], "temperature":
  0.0, "avg_logprob": -0.20294805673452523, "compression_ratio": 1.3333333333333333,
  "no_speech_prob": 0.011788278818130493}, {"id": 228, "seek": 243728, "start": 2448.88,
  "end": 2459.52, "text": " also quite helpful for those who want to find their ways
  in this domain.", "tokens": [50944, 611, 1596, 4961, 337, 729, 567, 528, 281, 915,
  641, 2098, 294, 341, 9274, 13, 51476], "temperature": 0.0, "avg_logprob": -0.20294805673452523,
  "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.011788278818130493},
  {"id": 229, "seek": 245952, "start": 2459.6, "end": 2465.7599999999998, "text":
  " That''s awesome. Thank you. I will certainly make sure to add all of these links
  in the", "tokens": [50368, 663, 311, 3476, 13, 1044, 291, 13, 286, 486, 3297, 652,
  988, 281, 909, 439, 295, 613, 6123, 294, 264, 50676], "temperature": 0.0, "avg_logprob":
  -0.18772247225739236, "compression_ratio": 1.6666666666666667, "no_speech_prob":
  0.02823718637228012}, {"id": 230, "seek": 245952, "start": 2466.56, "end": 2473.68,
  "text": " description notes, in the notes to this podcast and usually all of these
  podcasts that I do,", "tokens": [50716, 3855, 5570, 11, 294, 264, 5570, 281, 341,
  7367, 293, 2673, 439, 295, 613, 24045, 300, 286, 360, 11, 51072], "temperature":
  0.0, "avg_logprob": -0.18772247225739236, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.02823718637228012}, {"id": 231, "seek": 245952, "start": 2473.68,
  "end": 2478.64, "text": " they have a lot of links that actually you almost can
  use as an educational material. And", "tokens": [51072, 436, 362, 257, 688, 295,
  6123, 300, 767, 291, 1920, 393, 764, 382, 364, 10189, 2527, 13, 400, 51320], "temperature":
  0.0, "avg_logprob": -0.18772247225739236, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.02823718637228012}, {"id": 232, "seek": 245952, "start": 2478.64,
  "end": 2486.72, "text": " thanks so much for adding so much information here. And
  I actually wanted to drill a little bit", "tokens": [51320, 3231, 370, 709, 337,
  5127, 370, 709, 1589, 510, 13, 400, 286, 767, 1415, 281, 11392, 257, 707, 857, 51724],
  "temperature": 0.0, "avg_logprob": -0.18772247225739236, "compression_ratio": 1.6666666666666667,
  "no_speech_prob": 0.02823718637228012}, {"id": 233, "seek": 248672, "start": 2486.72,
  "end": 2492.24, "text": " again into that example that brilliant example you gave
  about predicting sort of what snacks when I", "tokens": [50364, 797, 666, 300, 1365,
  300, 10248, 1365, 291, 2729, 466, 32884, 1333, 295, 437, 16160, 562, 286, 50640],
  "temperature": 0.0, "avg_logprob": -0.2181310551140898, "compression_ratio": 1.6324786324786325,
  "no_speech_prob": 0.0064032007940113544}, {"id": 234, "seek": 248672, "start": 2492.24,
  "end": 2497.04, "text": " type. Actually, I used this feature quite a lot and especially
  like when you''re on the go and", "tokens": [50640, 2010, 13, 5135, 11, 286, 1143,
  341, 4111, 1596, 257, 688, 293, 2318, 411, 562, 291, 434, 322, 264, 352, 293, 50880],
  "temperature": 0.0, "avg_logprob": -0.2181310551140898, "compression_ratio": 1.6324786324786325,
  "no_speech_prob": 0.0064032007940113544}, {"id": 235, "seek": 248672, "start": 2497.04,
  "end": 2503.4399999999996, "text": " today I think I''ve used it somewhere with
  a Gmail, I was on the go and I had only one finger,", "tokens": [50880, 965, 286,
  519, 286, 600, 1143, 309, 4079, 365, 257, 36732, 11, 286, 390, 322, 264, 352, 293,
  286, 632, 787, 472, 5984, 11, 51200], "temperature": 0.0, "avg_logprob": -0.2181310551140898,
  "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0064032007940113544},
  {"id": 236, "seek": 248672, "start": 2503.4399999999996, "end": 2510.3999999999996,
  "text": " right? So just holding my phone as I go and there was a question and the
  answer was something,", "tokens": [51200, 558, 30, 407, 445, 5061, 452, 2593, 382,
  286, 352, 293, 456, 390, 257, 1168, 293, 264, 1867, 390, 746, 11, 51548], "temperature":
  0.0, "avg_logprob": -0.2181310551140898, "compression_ratio": 1.6324786324786325,
  "no_speech_prob": 0.0064032007940113544}, {"id": 237, "seek": 251040, "start": 2510.4,
  "end": 2516.4, "text": " yes, it happened or yes, it did. And maybe it wasn''t the
  best sort of semantical choice or maybe", "tokens": [50364, 2086, 11, 309, 2011,
  420, 2086, 11, 309, 630, 13, 400, 1310, 309, 2067, 380, 264, 1151, 1333, 295, 4361,
  394, 804, 3922, 420, 1310, 50664], "temperature": 0.0, "avg_logprob": -0.1602321261451358,
  "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.005754351615905762},
  {"id": 238, "seek": 251040, "start": 2516.4, "end": 2521.84, "text": " not the most
  elegant choice linguistically, like maybe I would add more color, but because I
  was on", "tokens": [50664, 406, 264, 881, 21117, 3922, 21766, 20458, 11, 411, 1310,
  286, 576, 909, 544, 2017, 11, 457, 570, 286, 390, 322, 50936], "temperature": 0.0,
  "avg_logprob": -0.1602321261451358, "compression_ratio": 1.6527196652719665, "no_speech_prob":
  0.005754351615905762}, {"id": 239, "seek": 251040, "start": 2521.84, "end": 2529.12,
  "text": " the go, it was fine to save that, you know, few minutes and don''t be
  distracted by the phone. So I", "tokens": [50936, 264, 352, 11, 309, 390, 2489,
  281, 3155, 300, 11, 291, 458, 11, 1326, 2077, 293, 500, 380, 312, 21658, 538, 264,
  2593, 13, 407, 286, 51300], "temperature": 0.0, "avg_logprob": -0.1602321261451358,
  "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.005754351615905762},
  {"id": 240, "seek": 251040, "start": 2529.12, "end": 2538.64, "text": " just pressed
  that button and off it goes. And so that''s a fantastic feature. So I wanted to
  sort of", "tokens": [51300, 445, 17355, 300, 2960, 293, 766, 309, 1709, 13, 400,
  370, 300, 311, 257, 5456, 4111, 13, 407, 286, 1415, 281, 1333, 295, 51776], "temperature":
  0.0, "avg_logprob": -0.1602321261451358, "compression_ratio": 1.6527196652719665,
  "no_speech_prob": 0.005754351615905762}, {"id": 241, "seek": 253864, "start": 2539.2,
  "end": 2544.64, "text": " open up the process a little bit of metric learning in
  this case. Basically, I imagine and please", "tokens": [50392, 1269, 493, 264, 1399,
  257, 707, 857, 295, 20678, 2539, 294, 341, 1389, 13, 8537, 11, 286, 3811, 293, 1767,
  50664], "temperature": 0.0, "avg_logprob": -0.14401057215020208, "compression_ratio":
  1.6570247933884297, "no_speech_prob": 0.003972918260842562}, {"id": 242, "seek":
  253864, "start": 2544.64, "end": 2552.0, "text": " correct me if I''m wrong. As
  an input, I would have, let''s say, a pair of sentences that what was the", "tokens":
  [50664, 3006, 385, 498, 286, 478, 2085, 13, 1018, 364, 4846, 11, 286, 576, 362,
  11, 718, 311, 584, 11, 257, 6119, 295, 16579, 300, 437, 390, 264, 51032], "temperature":
  0.0, "avg_logprob": -0.14401057215020208, "compression_ratio": 1.6570247933884297,
  "no_speech_prob": 0.003972918260842562}, {"id": 243, "seek": 253864, "start": 2552.0,
  "end": 2557.7599999999998, "text": " input and what was the prediction and that
  prediction could be either curated by experts or we could", "tokens": [51032, 4846,
  293, 437, 390, 264, 17630, 293, 300, 17630, 727, 312, 2139, 47851, 538, 8572, 420,
  321, 727, 51320], "temperature": 0.0, "avg_logprob": -0.14401057215020208, "compression_ratio":
  1.6570247933884297, "no_speech_prob": 0.003972918260842562}, {"id": 244, "seek":
  253864, "start": 2557.7599999999998, "end": 2565.2, "text": " have minded from the
  logs, whatever. So let''s say we have a corpus like this, right? So we can employ",
  "tokens": [51320, 362, 36707, 490, 264, 20820, 11, 2035, 13, 407, 718, 311, 584,
  321, 362, 257, 1181, 31624, 411, 341, 11, 558, 30, 407, 321, 393, 3188, 51692],
  "temperature": 0.0, "avg_logprob": -0.14401057215020208, "compression_ratio": 1.6570247933884297,
  "no_speech_prob": 0.003972918260842562}, {"id": 245, "seek": 256520, "start": 2565.2,
  "end": 2570.64, "text": " sequence to sequence model or some other model to actually
  train like our first first predictor.", "tokens": [50364, 8310, 281, 8310, 2316,
  420, 512, 661, 2316, 281, 767, 3847, 411, 527, 700, 700, 6069, 284, 13, 50636],
  "temperature": 0.0, "avg_logprob": -0.1332760530359605, "compression_ratio": 1.748878923766816,
  "no_speech_prob": 0.018314525485038757}, {"id": 246, "seek": 256520, "start": 2571.6,
  "end": 2576.8799999999997, "text": " So at which point would you start thinking
  and how exactly would you start thinking about metric", "tokens": [50684, 407, 412,
  597, 935, 576, 291, 722, 1953, 293, 577, 2293, 576, 291, 722, 1953, 466, 20678,
  50948], "temperature": 0.0, "avg_logprob": -0.1332760530359605, "compression_ratio":
  1.748878923766816, "no_speech_prob": 0.018314525485038757}, {"id": 247, "seek":
  256520, "start": 2576.8799999999997, "end": 2585.04, "text": " learning? Like how
  can I change the behavior of my model? Like will I replace like last layer of my",
  "tokens": [50948, 2539, 30, 1743, 577, 393, 286, 1319, 264, 5223, 295, 452, 2316,
  30, 1743, 486, 286, 7406, 411, 1036, 4583, 295, 452, 51356], "temperature": 0.0,
  "avg_logprob": -0.1332760530359605, "compression_ratio": 1.748878923766816, "no_speech_prob":
  0.018314525485038757}, {"id": 248, "seek": 256520, "start": 2585.04, "end": 2589.6,
  "text": " neural network with like different layer that I have learned from metric
  learning? Can you a bit", "tokens": [51356, 18161, 3209, 365, 411, 819, 4583, 300,
  286, 362, 3264, 490, 20678, 2539, 30, 1664, 291, 257, 857, 51584], "temperature":
  0.0, "avg_logprob": -0.1332760530359605, "compression_ratio": 1.748878923766816,
  "no_speech_prob": 0.018314525485038757}, {"id": 249, "seek": 258960, "start": 2589.6,
  "end": 2597.04, "text": " open up this kitchen for me? Thanks. Actually, this smart
  supply has", "tokens": [50364, 1269, 493, 341, 6525, 337, 385, 30, 2561, 13, 5135,
  11, 341, 4069, 5847, 575, 50736], "temperature": 0.0, "avg_logprob": -0.22520437240600585,
  "compression_ratio": 1.2698412698412698, "no_speech_prob": 0.0060164667665958405},
  {"id": 250, "seek": 258960, "start": 2598.4, "end": 2611.04, "text": " its own paper
  by Google as well and they are really doing a great job to describe the whole",
  "tokens": [50804, 1080, 1065, 3035, 538, 3329, 382, 731, 293, 436, 366, 534, 884,
  257, 869, 1691, 281, 6786, 264, 1379, 51436], "temperature": 0.0, "avg_logprob":
  -0.22520437240600585, "compression_ratio": 1.2698412698412698, "no_speech_prob":
  0.0060164667665958405}, {"id": 251, "seek": 261104, "start": 2611.04, "end": 2623.36,
  "text": " logic to whole design decisions behind this feature. As you already said,
  the suggested", "tokens": [50364, 9952, 281, 1379, 1715, 5327, 2261, 341, 4111,
  13, 1018, 291, 1217, 848, 11, 264, 10945, 50980], "temperature": 0.0, "avg_logprob":
  -0.2349872134980701, "compression_ratio": 1.3909774436090225, "no_speech_prob":
  0.005320590455085039}, {"id": 252, "seek": 261104, "start": 2623.36, "end": 2638.48,
  "text": " duplicates are not the best, the most specific replies that you can imagine,
  but this is actually", "tokens": [50980, 17154, 1024, 366, 406, 264, 1151, 11, 264,
  881, 2685, 42289, 300, 291, 393, 3811, 11, 457, 341, 307, 767, 51736], "temperature":
  0.0, "avg_logprob": -0.2349872134980701, "compression_ratio": 1.3909774436090225,
  "no_speech_prob": 0.005320590455085039}, {"id": 253, "seek": 263848, "start": 2639.44,
  "end": 2650.8, "text": " are spied design because they do not generate those replies,
  but they have a large collection of", "tokens": [50412, 366, 637, 1091, 1715, 570,
  436, 360, 406, 8460, 729, 42289, 11, 457, 436, 362, 257, 2416, 5765, 295, 50980],
  "temperature": 0.0, "avg_logprob": -0.2822672681110661, "compression_ratio": 1.4651162790697674,
  "no_speech_prob": 0.009230163879692554}, {"id": 254, "seek": 263848, "start": 2651.44,
  "end": 2663.44, "text": " such replies and they should be as flexible as possible
  to fit into different circumstances.", "tokens": [51012, 1270, 42289, 293, 436,
  820, 312, 382, 11358, 382, 1944, 281, 3318, 666, 819, 9121, 13, 51612], "temperature":
  0.0, "avg_logprob": -0.2822672681110661, "compression_ratio": 1.4651162790697674,
  "no_speech_prob": 0.009230163879692554}, {"id": 255, "seek": 266344, "start": 2663.44,
  "end": 2676.96, "text": " So they shouldn''t have any specific references to a specific
  sentence in the conversation.", "tokens": [50364, 407, 436, 4659, 380, 362, 604,
  2685, 15400, 281, 257, 2685, 8174, 294, 264, 3761, 13, 51040], "temperature": 0.0,
  "avg_logprob": -0.2183765411376953, "compression_ratio": 1.4324324324324325, "no_speech_prob":
  0.004382849670946598}, {"id": 256, "seek": 266344, "start": 2676.96, "end": 2684.96,
  "text": " So that should be a generic enough to apply almost any conversation.",
  "tokens": [51040, 407, 300, 820, 312, 257, 19577, 1547, 281, 3079, 1920, 604, 3761,
  13, 51440], "temperature": 0.0, "avg_logprob": -0.2183765411376953, "compression_ratio":
  1.4324324324324325, "no_speech_prob": 0.004382849670946598}, {"id": 257, "seek":
  268496, "start": 2685.6, "end": 2698.8, "text": " For the training slide, yeah actually,
  they filter a large collection from the different platforms", "tokens": [50396,
  1171, 264, 3097, 4137, 11, 1338, 767, 11, 436, 6608, 257, 2416, 5765, 490, 264,
  819, 9473, 51056], "temperature": 0.0, "avg_logprob": -0.40209515889485675, "compression_ratio":
  1.4957264957264957, "no_speech_prob": 0.032382991164922714}, {"id": 258, "seek":
  268496, "start": 2699.52, "end": 2709.36, "text": " they are running Gmail and other
  platforms and they filter short replies and", "tokens": [51092, 436, 366, 2614,
  36732, 293, 661, 9473, 293, 436, 6608, 2099, 42289, 293, 51584], "temperature":
  0.0, "avg_logprob": -0.40209515889485675, "compression_ratio": 1.4957264957264957,
  "no_speech_prob": 0.032382991164922714}, {"id": 259, "seek": 270936, "start": 2710.0,
  "end": 2727.92, "text": " thematically more broad samples such as as you gave as
  an example. Yes, I did or no, I didn''t", "tokens": [50396, 552, 5030, 544, 4152,
  10938, 1270, 382, 382, 291, 2729, 382, 364, 1365, 13, 1079, 11, 286, 630, 420, 572,
  11, 286, 994, 380, 51292], "temperature": 0.0, "avg_logprob": -0.34926251002720426,
  "compression_ratio": 1.1071428571428572, "no_speech_prob": 0.005918622016906738},
  {"id": 260, "seek": 272792, "start": 2728.8, "end": 2740.0, "text": " does it have
  such examples. And the actual training algorithm works like this. They", "tokens":
  [50408, 775, 309, 362, 1270, 5110, 13, 400, 264, 3539, 3097, 9284, 1985, 411, 341,
  13, 814, 50968], "temperature": 0.0, "avg_logprob": -0.3826844930648804, "compression_ratio":
  1.3410852713178294, "no_speech_prob": 0.015645449981093407}, {"id": 261, "seek":
  272792, "start": 2741.44, "end": 2753.28, "text": " actually come up with a very
  creative, very clever, lost function for just a terrain with", "tokens": [51040,
  767, 808, 493, 365, 257, 588, 5880, 11, 588, 13494, 11, 2731, 2445, 337, 445, 257,
  17674, 365, 51632], "temperature": 0.0, "avg_logprob": -0.3826844930648804, "compression_ratio":
  1.3410852713178294, "no_speech_prob": 0.015645449981093407}, {"id": 262, "seek":
  275328, "start": 2753.28, "end": 2768.4, "text": " this model. They have only a
  pair of two samples and there is no other label or information.", "tokens": [50364,
  341, 2316, 13, 814, 362, 787, 257, 6119, 295, 732, 10938, 293, 456, 307, 572, 661,
  7645, 420, 1589, 13, 51120], "temperature": 0.0, "avg_logprob": -0.27707455555597943,
  "compression_ratio": 1.1219512195121952, "no_speech_prob": 0.01523177232593298},
  {"id": 263, "seek": 276840, "start": 2769.2000000000003, "end": 2783.6800000000003,
  "text": " We only have one input and one ground truth, we have no other scoring,
  no other label or", "tokens": [50404, 492, 787, 362, 472, 4846, 293, 472, 2727,
  3494, 11, 321, 362, 572, 661, 22358, 11, 572, 661, 7645, 420, 51128], "temperature":
  0.0, "avg_logprob": -0.402416189511617, "compression_ratio": 1.1733333333333333,
  "no_speech_prob": 0.04214279353618622}, {"id": 264, "seek": 278368, "start": 2784.3999999999996,
  "end": 2794.96, "text": " anything else. So we only get a batch of, for example,",
  "tokens": [50400, 1340, 1646, 13, 407, 321, 787, 483, 257, 15245, 295, 11, 337,
  1365, 11, 50928], "temperature": 0.0, "avg_logprob": -0.24027713569434914, "compression_ratio":
  1.3557692307692308, "no_speech_prob": 0.029186204075813293}, {"id": 265, "seek":
  278368, "start": 2796.0, "end": 2807.68, "text": " and samples and we encode those
  two and samples because we have two samples first page", "tokens": [50980, 293,
  10938, 293, 321, 2058, 1429, 729, 732, 293, 10938, 570, 321, 362, 732, 10938, 700,
  3028, 51564], "temperature": 0.0, "avg_logprob": -0.24027713569434914, "compression_ratio":
  1.3557692307692308, "no_speech_prob": 0.029186204075813293}, {"id": 266, "seek":
  280768, "start": 2808.56, "end": 2822.24, "text": " and we end up with two and samples.
  And once we encode them with our encoder, we can compute a", "tokens": [50408, 293,
  321, 917, 493, 365, 732, 293, 10938, 13, 400, 1564, 321, 2058, 1429, 552, 365, 527,
  2058, 19866, 11, 321, 393, 14722, 257, 51092], "temperature": 0.0, "avg_logprob":
  -0.26709564364686306, "compression_ratio": 1.5241935483870968, "no_speech_prob":
  0.011978224851191044}, {"id": 267, "seek": 280768, "start": 2822.24, "end": 2834.24,
  "text": " distance matrix between these all posts of the encoder. A distance matrix
  is a two-dimensional", "tokens": [51092, 4560, 8141, 1296, 613, 439, 12300, 295,
  264, 2058, 19866, 13, 316, 4560, 8141, 307, 257, 732, 12, 18759, 51692], "temperature":
  0.0, "avg_logprob": -0.26709564364686306, "compression_ratio": 1.5241935483870968,
  "no_speech_prob": 0.011978224851191044}, {"id": 268, "seek": 283424, "start": 2835.2,
  "end": 2850.24, "text": " matrix to define every distance value between all possible
  pairs in a collection. So we have", "tokens": [50412, 8141, 281, 6964, 633, 4560,
  2158, 1296, 439, 1944, 15494, 294, 257, 5765, 13, 407, 321, 362, 51164], "temperature":
  0.0, "avg_logprob": -0.26274358658563524, "compression_ratio": 1.1358024691358024,
  "no_speech_prob": 0.006556063424795866}, {"id": 269, "seek": 285024, "start": 2850.72,
  "end": 2865.4399999999996, "text": " a matrix of five and times and and we already
  have these samples as pairs. We already know", "tokens": [50388, 257, 8141, 295,
  1732, 293, 1413, 293, 293, 321, 1217, 362, 613, 10938, 382, 15494, 13, 492, 1217,
  458, 51124], "temperature": 0.0, "avg_logprob": -0.36319554370382556, "compression_ratio":
  1.1538461538461537, "no_speech_prob": 0.01102994754910469}, {"id": 270, "seek":
  286544, "start": 2865.92, "end": 2879.12, "text": " that there is a company target
  samples for the sample, for the first sample at index zero,", "tokens": [50388,
  300, 456, 307, 257, 2237, 3779, 10938, 337, 264, 6889, 11, 337, 264, 700, 6889,
  412, 8186, 4018, 11, 51048], "temperature": 0.0, "avg_logprob": -0.37609410840411517,
  "compression_ratio": 1.801980198019802, "no_speech_prob": 0.01228384394198656},
  {"id": 271, "seek": 286544, "start": 2879.12, "end": 2893.6, "text": " the company
  sample should also be at index zero for sample at index one. The company sample",
  "tokens": [51048, 264, 2237, 6889, 820, 611, 312, 412, 8186, 4018, 337, 6889, 412,
  8186, 472, 13, 440, 2237, 6889, 51772], "temperature": 0.0, "avg_logprob": -0.37609410840411517,
  "compression_ratio": 1.801980198019802, "no_speech_prob": 0.01228384394198656},
  {"id": 272, "seek": 289360, "start": 2894.08, "end": 2905.92, "text": " sample should
  be at index one. So we can generate these target labels just based on this", "tokens":
  [50388, 6889, 820, 312, 412, 8186, 472, 13, 407, 321, 393, 8460, 613, 3779, 16949,
  445, 2361, 322, 341, 50980], "temperature": 0.0, "avg_logprob": -0.11794736773468727,
  "compression_ratio": 1.3984375, "no_speech_prob": 0.003321515629068017}, {"id":
  273, "seek": 289360, "start": 2905.92, "end": 2918.24, "text": " information. So
  it''s like a categorical classification now. So for the first sample in the", "tokens":
  [50980, 1589, 13, 407, 309, 311, 411, 257, 19250, 804, 21538, 586, 13, 407, 337,
  264, 700, 6889, 294, 264, 51596], "temperature": 0.0, "avg_logprob": -0.11794736773468727,
  "compression_ratio": 1.3984375, "no_speech_prob": 0.003321515629068017}, {"id":
  274, "seek": 291824, "start": 2919.2, "end": 2932.8799999999997, "text": " pair
  at index zero, the categorical label should be zero and others all index values
  should be", "tokens": [50412, 6119, 412, 8186, 4018, 11, 264, 19250, 804, 7645,
  820, 312, 4018, 293, 2357, 439, 8186, 4190, 820, 312, 51096], "temperature": 0.0,
  "avg_logprob": -0.179537586543871, "compression_ratio": 1.4761904761904763, "no_speech_prob":
  0.00920387264341116}, {"id": 275, "seek": 291824, "start": 2933.68, "end": 2942.7999999999997,
  "text": " wrong. So we can just encode this information as a one-heart encoding
  and we can simply use", "tokens": [51136, 2085, 13, 407, 321, 393, 445, 2058, 1429,
  341, 1589, 382, 257, 472, 12, 12864, 43430, 293, 321, 393, 2935, 764, 51592], "temperature":
  0.0, "avg_logprob": -0.179537586543871, "compression_ratio": 1.4761904761904763,
  "no_speech_prob": 0.00920387264341116}, {"id": 276, "seek": 294280, "start": 2943.76,
  "end": 2952.8, "text": " a Crohn''s entropy loss once we encode this information
  as a one-heart encoding and we can", "tokens": [50412, 257, 18965, 12071, 311, 30867,
  4470, 1564, 321, 2058, 1429, 341, 1589, 382, 257, 472, 12, 12864, 43430, 293, 321,
  393, 50864], "temperature": 0.0, "avg_logprob": -0.2634368159554221, "compression_ratio":
  1.3740458015267176, "no_speech_prob": 0.01413606759160757}, {"id": 277, "seek":
  294280, "start": 2953.52, "end": 2964.1600000000003, "text": " train this model
  with this loss. So it is called multiple negative ranking loss because in", "tokens":
  [50900, 3847, 341, 2316, 365, 341, 4470, 13, 407, 309, 307, 1219, 3866, 3671, 17833,
  4470, 570, 294, 51432], "temperature": 0.0, "avg_logprob": -0.2634368159554221,
  "compression_ratio": 1.3740458015267176, "no_speech_prob": 0.01413606759160757},
  {"id": 278, "seek": 296416, "start": 2965.12, "end": 2980.08, "text": " in some
  way we rank all possible replies in a batch with multiple negatives and only one
  positive", "tokens": [50412, 294, 512, 636, 321, 6181, 439, 1944, 42289, 294, 257,
  15245, 365, 3866, 40019, 293, 787, 472, 3353, 51160], "temperature": 0.0, "avg_logprob":
  -0.2694201252677224, "compression_ratio": 1.4222222222222223, "no_speech_prob":
  0.0041774264536798}, {"id": 279, "seek": 296416, "start": 2980.96, "end": 2993.12,
  "text": " sample. Yeah. And so you would train this network with this with this
  loss function and so the", "tokens": [51204, 6889, 13, 865, 13, 400, 370, 291, 576,
  3847, 341, 3209, 365, 341, 365, 341, 4470, 2445, 293, 370, 264, 51812], "temperature":
  0.0, "avg_logprob": -0.2694201252677224, "compression_ratio": 1.4222222222222223,
  "no_speech_prob": 0.0041774264536798}, {"id": 280, "seek": 299312, "start": 2993.12,
  "end": 3002.08, "text": " output will be what? Like will it be like the optimal
  metric or optimal? Yeah. In one, we train this", "tokens": [50364, 5598, 486, 312,
  437, 30, 1743, 486, 309, 312, 411, 264, 16252, 20678, 420, 16252, 30, 865, 13, 682,
  472, 11, 321, 3847, 341, 50812], "temperature": 0.0, "avg_logprob": -0.2433856208369417,
  "compression_ratio": 1.4565217391304348, "no_speech_prob": 0.006256880704313517},
  {"id": 281, "seek": 299312, "start": 3003.2, "end": 3021.12, "text": " model, we
  end up with a model that can encode a sentence in such a way that that vector can
  retrieve", "tokens": [50868, 2316, 11, 321, 917, 493, 365, 257, 2316, 300, 393,
  2058, 1429, 257, 8174, 294, 1270, 257, 636, 300, 300, 8062, 393, 30254, 51764],
  "temperature": 0.0, "avg_logprob": -0.2433856208369417, "compression_ratio": 1.4565217391304348,
  "no_speech_prob": 0.006256880704313517}, {"id": 282, "seek": 302112, "start": 3021.12,
  "end": 3034.3199999999997, "text": " the most relevant vectors from a collection
  of possible replies. So after we train this model,", "tokens": [50364, 264, 881,
  7340, 18875, 490, 257, 5765, 295, 1944, 42289, 13, 407, 934, 321, 3847, 341, 2316,
  11, 51024], "temperature": 0.0, "avg_logprob": -0.10508264194835316, "compression_ratio":
  1.4538461538461538, "no_speech_prob": 0.008691701106727123}, {"id": 283, "seek":
  302112, "start": 3034.3199999999997, "end": 3046.3199999999997, "text": " we encode
  all possible replies and index them in a vector database. And at the inference time,",
  "tokens": [51024, 321, 2058, 1429, 439, 1944, 42289, 293, 8186, 552, 294, 257, 8062,
  8149, 13, 400, 412, 264, 38253, 565, 11, 51624], "temperature": 0.0, "avg_logprob":
  -0.10508264194835316, "compression_ratio": 1.4538461538461538, "no_speech_prob":
  0.008691701106727123}, {"id": 284, "seek": 304632, "start": 3046.8, "end": 3059.76,
  "text": " we encode the usage input again with this model and make a query, a vector
  source query to that", "tokens": [50388, 321, 2058, 1429, 264, 14924, 4846, 797,
  365, 341, 2316, 293, 652, 257, 14581, 11, 257, 8062, 4009, 14581, 281, 300, 51036],
  "temperature": 0.0, "avg_logprob": -0.32258164648916204, "compression_ratio": 1.4104477611940298,
  "no_speech_prob": 0.008722199127078056}, {"id": 285, "seek": 304632, "start": 3059.76,
  "end": 3070.88, "text": " pre-index database of possible replies and we can get,
  for example, a car, a chain, a nearest", "tokens": [51036, 659, 12, 471, 3121, 8149,
  295, 1944, 42289, 293, 321, 393, 483, 11, 337, 1365, 11, 257, 1032, 11, 257, 5021,
  11, 257, 23831, 51592], "temperature": 0.0, "avg_logprob": -0.32258164648916204,
  "compression_ratio": 1.4104477611940298, "no_speech_prob": 0.008722199127078056},
  {"id": 286, "seek": 307088, "start": 3070.88, "end": 3082.7200000000003, "text":
  " neighbor to that vector to suggest to use it. Yeah. I mean, after you explain
  this, like to me,", "tokens": [50364, 5987, 281, 300, 8062, 281, 3402, 281, 764,
  309, 13, 865, 13, 286, 914, 11, 934, 291, 2903, 341, 11, 411, 281, 385, 11, 50956],
  "temperature": 0.0, "avg_logprob": -0.24846298554364374, "compression_ratio": 1.676300578034682,
  "no_speech_prob": 0.007206438574939966}, {"id": 287, "seek": 307088, "start": 3082.7200000000003,
  "end": 3089.6, "text": " like the mental image that evokes is that we sort of like
  learn rather than learning the metric,", "tokens": [50956, 411, 264, 4973, 3256,
  300, 1073, 8606, 307, 300, 321, 1333, 295, 411, 1466, 2831, 813, 2539, 264, 20678,
  11, 51300], "temperature": 0.0, "avg_logprob": -0.24846298554364374, "compression_ratio":
  1.676300578034682, "no_speech_prob": 0.007206438574939966}, {"id": 288, "seek":
  307088, "start": 3089.6, "end": 3095.36, "text": " we''re actually learning the
  vectors themselves. We''re learning the best vector representation for", "tokens":
  [51300, 321, 434, 767, 2539, 264, 18875, 2969, 13, 492, 434, 2539, 264, 1151, 8062,
  10290, 337, 51588], "temperature": 0.0, "avg_logprob": -0.24846298554364374, "compression_ratio":
  1.676300578034682, "no_speech_prob": 0.007206438574939966}, {"id": 289, "seek":
  309536, "start": 3095.44, "end": 3102.32, "text": " our object to satisfy some goal,
  right? Let''s say that for this sentence, the closest", "tokens": [50368, 527, 2657,
  281, 19319, 512, 3387, 11, 558, 30, 961, 311, 584, 300, 337, 341, 8174, 11, 264,
  13699, 50712], "temperature": 0.0, "avg_logprob": -0.24090723557905716, "compression_ratio":
  1.3235294117647058, "no_speech_prob": 0.004641443956643343}, {"id": 290, "seek":
  309536, "start": 3102.32, "end": 3111.04, "text": " reply should be this in some
  sense. Yeah, exactly. Actually, the model learns a representation", "tokens": [50712,
  16972, 820, 312, 341, 294, 512, 2020, 13, 865, 11, 2293, 13, 5135, 11, 264, 2316,
  27152, 257, 10290, 51148], "temperature": 0.0, "avg_logprob": -0.24090723557905716,
  "compression_ratio": 1.3235294117647058, "no_speech_prob": 0.004641443956643343},
  {"id": 291, "seek": 311104, "start": 3111.12, "end": 3126.64, "text": " that satisfies
  the satisfies our purpose. So in some way, we can fully pick any distance", "tokens":
  [50368, 300, 44271, 264, 44271, 527, 4334, 13, 407, 294, 512, 636, 11, 321, 393,
  4498, 1888, 604, 4560, 51144], "temperature": 0.0, "avg_logprob": -0.24228973388671876,
  "compression_ratio": 1.3360655737704918, "no_speech_prob": 0.02719750814139843},
  {"id": 292, "seek": 311104, "start": 3127.68, "end": 3138.4, "text": " metric based
  on this intuition. Yeah. So the second part of your question,", "tokens": [51196,
  20678, 2361, 322, 341, 24002, 13, 865, 13, 407, 264, 1150, 644, 295, 428, 1168,
  11, 51732], "temperature": 0.0, "avg_logprob": -0.24228973388671876, "compression_ratio":
  1.3360655737704918, "no_speech_prob": 0.02719750814139843}, {"id": 293, "seek":
  313840, "start": 3139.12, "end": 3151.28, "text": " when we can think about metric
  learning. Actually, metric learning can be applied to almost any domain", "tokens":
  [50400, 562, 321, 393, 519, 466, 20678, 2539, 13, 5135, 11, 20678, 2539, 393, 312,
  6456, 281, 1920, 604, 9274, 51008], "temperature": 0.0, "avg_logprob": -0.15340256690979004,
  "compression_ratio": 1.5, "no_speech_prob": 0.0053184013813734055}, {"id": 294,
  "seek": 313840, "start": 3152.4, "end": 3161.92, "text": " of problems, but there
  are some particular cases where metric learning really shines over", "tokens": [51064,
  295, 2740, 11, 457, 456, 366, 512, 1729, 3331, 689, 20678, 2539, 534, 28056, 670,
  51540], "temperature": 0.0, "avg_logprob": -0.15340256690979004, "compression_ratio":
  1.5, "no_speech_prob": 0.0053184013813734055}, {"id": 295, "seek": 316192, "start":
  3162.7200000000003, "end": 3175.2000000000003, "text": " other alternatives. These
  are actually data scarce regimes, especially for labeled, if you are", "tokens":
  [50404, 661, 20478, 13, 1981, 366, 767, 1412, 41340, 45738, 11, 2318, 337, 21335,
  11, 498, 291, 366, 51028], "temperature": 0.0, "avg_logprob": -0.24727450476752388,
  "compression_ratio": 1.441860465116279, "no_speech_prob": 0.004401012323796749},
  {"id": 296, "seek": 316192, "start": 3176.48, "end": 3185.84, "text": " short for
  labeled data, you can still do a pretty good job with, for example, auto encoder,",
  "tokens": [51092, 2099, 337, 21335, 1412, 11, 291, 393, 920, 360, 257, 1238, 665,
  1691, 365, 11, 337, 1365, 11, 8399, 2058, 19866, 11, 51560], "temperature": 0.0,
  "avg_logprob": -0.24727450476752388, "compression_ratio": 1.441860465116279, "no_speech_prob":
  0.004401012323796749}, {"id": 297, "seek": 318584, "start": 3185.84, "end": 3195.6000000000004,
  "text": " as we already discussed previously. And also, if you have", "tokens":
  [50364, 382, 321, 1217, 7152, 8046, 13, 400, 611, 11, 498, 291, 362, 50852], "temperature":
  0.0, "avg_logprob": -0.3226981664958753, "compression_ratio": 1.2767857142857142,
  "no_speech_prob": 0.004759341012686491}, {"id": 298, "seek": 318584, "start": 3197.1200000000003,
  "end": 3210.0, "text": " rapid-changing distributions, it''s again, very helpful.
  And if you have, for example,", "tokens": [50928, 7558, 12, 27123, 37870, 11, 309,
  311, 797, 11, 588, 4961, 13, 400, 498, 291, 362, 11, 337, 1365, 11, 51572], "temperature":
  0.0, "avg_logprob": -0.3226981664958753, "compression_ratio": 1.2767857142857142,
  "no_speech_prob": 0.004759341012686491}, {"id": 299, "seek": 321000, "start": 3210.0,
  "end": 3225.52, "text": " a very, very high number of classes, again, metric learning
  can do a good job. Finally, metric learning", "tokens": [50364, 257, 588, 11, 588,
  1090, 1230, 295, 5359, 11, 797, 11, 20678, 2539, 393, 360, 257, 665, 1691, 13, 6288,
  11, 20678, 2539, 51140], "temperature": 0.0, "avg_logprob": -0.20940260092417398,
  "compression_ratio": 1.507462686567164, "no_speech_prob": 0.01618296280503273},
  {"id": 300, "seek": 321000, "start": 3227.12, "end": 3239.44, "text": " is one of
  the best way to be able to actually increase the performance of machine learning
  models,", "tokens": [51220, 307, 472, 295, 264, 1151, 636, 281, 312, 1075, 281,
  767, 3488, 264, 3389, 295, 3479, 2539, 5245, 11, 51836], "temperature": 0.0, "avg_logprob":
  -0.20940260092417398, "compression_ratio": 1.507462686567164, "no_speech_prob":
  0.01618296280503273}, {"id": 301, "seek": 323944, "start": 3239.52, "end": 3248.88,
  "text": " even after training. In normal deep learning training, there is no way
  to increase the performance", "tokens": [50368, 754, 934, 3097, 13, 682, 2710, 2452,
  2539, 3097, 11, 456, 307, 572, 636, 281, 3488, 264, 3389, 50836], "temperature":
  0.0, "avg_logprob": -0.13123036490546333, "compression_ratio": 1.4887218045112782,
  "no_speech_prob": 0.0019445125944912434}, {"id": 302, "seek": 323944, "start": 3248.88,
  "end": 3261.68, "text": " of a model after training is complete. But in metric learning,
  this is quite possible. For example,", "tokens": [50836, 295, 257, 2316, 934, 3097,
  307, 3566, 13, 583, 294, 20678, 2539, 11, 341, 307, 1596, 1944, 13, 1171, 1365,
  11, 51476], "temperature": 0.0, "avg_logprob": -0.13123036490546333, "compression_ratio":
  1.4887218045112782, "no_speech_prob": 0.0019445125944912434}, {"id": 303, "seek":
  326168, "start": 3261.7599999999998, "end": 3271.68, "text": " instead of just a
  training classification model to make a probability distribution over", "tokens":
  [50368, 2602, 295, 445, 257, 3097, 21538, 2316, 281, 652, 257, 8482, 7316, 670,
  50864], "temperature": 0.0, "avg_logprob": -0.26426038986597306, "compression_ratio":
  1.472, "no_speech_prob": 0.012363242916762829}, {"id": 304, "seek": 326168, "start":
  3273.04, "end": 3284.7999999999997, "text": " set of classes, we can train a metric
  learning model and encode samples with that model to store", "tokens": [50932, 992,
  295, 5359, 11, 321, 393, 3847, 257, 20678, 2539, 2316, 293, 2058, 1429, 10938, 365,
  300, 2316, 281, 3531, 51520], "temperature": 0.0, "avg_logprob": -0.26426038986597306,
  "compression_ratio": 1.472, "no_speech_prob": 0.012363242916762829}, {"id": 305,
  "seek": 328480, "start": 3284.8, "end": 3294.96, "text": " somewhere. And during
  the inference, we can query that store to get more similar", "tokens": [50364, 4079,
  13, 400, 1830, 264, 38253, 11, 321, 393, 14581, 300, 3531, 281, 483, 544, 2531,
  50872], "temperature": 0.0, "avg_logprob": -0.275920481295199, "compression_ratio":
  1.376, "no_speech_prob": 0.01892298273742199}, {"id": 306, "seek": 328480, "start":
  3296.6400000000003, "end": 3309.36, "text": " chain nearest neighbors and decide
  on the predicted category based on the majority of those", "tokens": [50956, 5021,
  23831, 12512, 293, 4536, 322, 264, 19147, 7719, 2361, 322, 264, 6286, 295, 729,
  51592], "temperature": 0.0, "avg_logprob": -0.275920481295199, "compression_ratio":
  1.376, "no_speech_prob": 0.01892298273742199}, {"id": 307, "seek": 330936, "start":
  3309.36, "end": 3320.48, "text": " chain nearest neighbors. This is called chain
  uncostication, in fact. And in the practical", "tokens": [50364, 5021, 23831, 12512,
  13, 639, 307, 1219, 5021, 6219, 555, 8758, 11, 294, 1186, 13, 400, 294, 264, 8496,
  50920], "temperature": 0.0, "avg_logprob": -0.4215660509855851, "compression_ratio":
  1.4651162790697674, "no_speech_prob": 0.007927683182060719}, {"id": 308, "seek":
  330936, "start": 3320.48, "end": 3334.1600000000003, "text": " side, on the practical
  side, you can continue to add new samples to that store without any need to", "tokens":
  [50920, 1252, 11, 322, 264, 8496, 1252, 11, 291, 393, 2354, 281, 909, 777, 10938,
  281, 300, 3531, 1553, 604, 643, 281, 51604], "temperature": 0.0, "avg_logprob":
  -0.4215660509855851, "compression_ratio": 1.4651162790697674, "no_speech_prob":
  0.007927683182060719}, {"id": 309, "seek": 333416, "start": 3334.96, "end": 3345.8399999999997,
  "text": " retrain the model. And once you add new samples to that store your model
  performance", "tokens": [50404, 1533, 7146, 264, 2316, 13, 400, 1564, 291, 909,
  777, 10938, 281, 300, 3531, 428, 2316, 3389, 50948], "temperature": 0.0, "avg_logprob":
  -0.23172660101027714, "compression_ratio": 1.423728813559322, "no_speech_prob":
  0.005843288265168667}, {"id": 310, "seek": 333416, "start": 3346.64, "end": 3357.12,
  "text": " will also increase. And also, there is another use case, for example,
  a more recent", "tokens": [50988, 486, 611, 3488, 13, 400, 611, 11, 456, 307, 1071,
  764, 1389, 11, 337, 1365, 11, 257, 544, 5162, 51512], "temperature": 0.0, "avg_logprob":
  -0.23172660101027714, "compression_ratio": 1.423728813559322, "no_speech_prob":
  0.005843288265168667}, {"id": 311, "seek": 335712, "start": 3357.7599999999998,
  "end": 3373.2, "text": " approach by DeepMind. Up until now, the only way to make
  AI smarter is usually train a bigger", "tokens": [50396, 3109, 538, 14895, 44, 471,
  13, 5858, 1826, 586, 11, 264, 787, 636, 281, 652, 7318, 20294, 307, 2673, 3847,
  257, 3801, 51168], "temperature": 0.0, "avg_logprob": -0.3901444948636569, "compression_ratio":
  1.0333333333333334, "no_speech_prob": 0.017984267324209213}, {"id": 312, "seek":
  337320, "start": 3373.2799999999997, "end": 3388.64, "text": " and bigger language
  model. But in the most recent study by DeepMind, they augment language models",
  "tokens": [50368, 293, 3801, 2856, 2316, 13, 583, 294, 264, 881, 5162, 2979, 538,
  14895, 44, 471, 11, 436, 29919, 2856, 5245, 51136], "temperature": 0.0, "avg_logprob":
  -0.19730119705200194, "compression_ratio": 1.3070175438596492, "no_speech_prob":
  0.019084036350250244}, {"id": 313, "seek": 337320, "start": 3388.64, "end": 3394.56,
  "text": " with retrieval capability. This means actually they", "tokens": [51136,
  365, 19817, 3337, 13759, 13, 639, 1355, 767, 436, 51432], "temperature": 0.0, "avg_logprob":
  -0.19730119705200194, "compression_ratio": 1.3070175438596492, "no_speech_prob":
  0.019084036350250244}, {"id": 314, "seek": 339456, "start": 3395.52, "end": 3409.44,
  "text": " encode and store a large collection of corpus in a Rector''s database.
  And during the inference,", "tokens": [50412, 2058, 1429, 293, 3531, 257, 2416,
  5765, 295, 1181, 31624, 294, 257, 497, 20814, 311, 8149, 13, 400, 1830, 264, 38253,
  11, 51108], "temperature": 0.0, "avg_logprob": -0.32727373563326323, "compression_ratio":
  1.1176470588235294, "no_speech_prob": 0.0066364360973238945}, {"id": 315, "seek":
  340944, "start": 3410.4, "end": 3428.7200000000003, "text": " they query this database
  to get most relevant sentences, most relevant text to the user input.", "tokens":
  [50412, 436, 14581, 341, 8149, 281, 483, 881, 7340, 16579, 11, 881, 7340, 2487,
  281, 264, 4195, 4846, 13, 51328], "temperature": 0.0, "avg_logprob": -0.33846139907836914,
  "compression_ratio": 1.2533333333333334, "no_speech_prob": 0.011745172552764416},
  {"id": 316, "seek": 342872, "start": 3428.7999999999997, "end": 3441.68, "text":
  " And they combine them to feed to the model. And with this technique, they can
  achieve the", "tokens": [50368, 400, 436, 10432, 552, 281, 3154, 281, 264, 2316,
  13, 400, 365, 341, 6532, 11, 436, 393, 4584, 264, 51012], "temperature": 0.0, "avg_logprob":
  -0.21010682799599387, "compression_ratio": 1.2740740740740741, "no_speech_prob":
  0.013064459897577763}, {"id": 317, "seek": 342872, "start": 3442.56, "end": 3458.08,
  "text": " same performance as GPT3 with 25x less parameters. So it''s a very efficient
  way of", "tokens": [51056, 912, 3389, 382, 26039, 51, 18, 365, 3552, 87, 1570, 9834,
  13, 407, 309, 311, 257, 588, 7148, 636, 295, 51832], "temperature": 0.0, "avg_logprob":
  -0.21010682799599387, "compression_ratio": 1.2740740740740741, "no_speech_prob":
  0.013064459897577763}, {"id": 318, "seek": 345872, "start": 3459.04, "end": 3470.72,
  "text": " AI. So I''m also quite happy to see the direction of AI towards a more
  efficient one with", "tokens": [50380, 7318, 13, 407, 286, 478, 611, 1596, 2055,
  281, 536, 264, 3513, 295, 7318, 3030, 257, 544, 7148, 472, 365, 50964], "temperature":
  0.0, "avg_logprob": -0.17484624620894312, "compression_ratio": 1.4583333333333333,
  "no_speech_prob": 0.00731369573622942}, {"id": 319, "seek": 345872, "start": 3471.6,
  "end": 3476.24, "text": " metric learning as well. Yeah, yeah, it''s fantastic.
  And I think it''s like a good impact on the", "tokens": [51008, 20678, 2539, 382,
  731, 13, 865, 11, 1338, 11, 309, 311, 5456, 13, 400, 286, 519, 309, 311, 411, 257,
  665, 2712, 322, 264, 51240], "temperature": 0.0, "avg_logprob": -0.17484624620894312,
  "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.00731369573622942},
  {"id": 320, "seek": 345872, "start": 3476.24, "end": 3481.2799999999997, "text":
  " planning, because I don''t think we want to spend too much electricity on all
  power and training", "tokens": [51240, 5038, 11, 570, 286, 500, 380, 519, 321, 528,
  281, 3496, 886, 709, 10356, 322, 439, 1347, 293, 3097, 51492], "temperature": 0.0,
  "avg_logprob": -0.17484624620894312, "compression_ratio": 1.4583333333333333, "no_speech_prob":
  0.00731369573622942}, {"id": 321, "seek": 348128, "start": 3481.28, "end": 3490.8,
  "text": " neural networks. Yeah, exactly. And it also enables democratization of
  Deep Learning, because", "tokens": [50364, 18161, 9590, 13, 865, 11, 2293, 13, 400,
  309, 611, 17077, 37221, 2144, 295, 14895, 15205, 11, 570, 50840], "temperature":
  0.0, "avg_logprob": -0.37569480718568316, "compression_ratio": 1.282758620689655,
  "no_speech_prob": 0.012573977001011372}, {"id": 322, "seek": 348128, "start": 3491.6800000000003,
  "end": 3503.0400000000004, "text": " not everyone has the same resources as this
  large companies as Google, Facebook, and OpenAI.", "tokens": [50884, 406, 1518,
  575, 264, 912, 3593, 382, 341, 2416, 3431, 382, 3329, 11, 4384, 11, 293, 7238, 48698,
  13, 51452], "temperature": 0.0, "avg_logprob": -0.37569480718568316, "compression_ratio":
  1.282758620689655, "no_speech_prob": 0.012573977001011372}, {"id": 323, "seek":
  350304, "start": 3503.52, "end": 3508.48, "text": " So I think it''s also important
  for that reason as well.", "tokens": [50388, 407, 286, 519, 309, 311, 611, 1021,
  337, 300, 1778, 382, 731, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1723371891493208,
  "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.016916243359446526},
  {"id": 324, "seek": 350304, "start": 3509.12, "end": 3514.08, "text": " Yeah, that''s
  fantastic. I mean, you gave quite a lot of detail on metric learning. Of course,",
  "tokens": [50668, 865, 11, 300, 311, 5456, 13, 286, 914, 11, 291, 2729, 1596, 257,
  688, 295, 2607, 322, 20678, 2539, 13, 2720, 1164, 11, 50916], "temperature": 0.0,
  "avg_logprob": -0.1723371891493208, "compression_ratio": 1.6179245283018868, "no_speech_prob":
  0.016916243359446526}, {"id": 325, "seek": 350304, "start": 3514.08, "end": 3522.4,
  "text": " there is a ton to learn. And I even, I''ve seen like a book cited on one
  of the metric learning pages", "tokens": [50916, 456, 307, 257, 2952, 281, 1466,
  13, 400, 286, 754, 11, 286, 600, 1612, 411, 257, 1446, 30134, 322, 472, 295, 264,
  20678, 2539, 7183, 51332], "temperature": 0.0, "avg_logprob": -0.1723371891493208,
  "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.016916243359446526},
  {"id": 326, "seek": 350304, "start": 3523.2, "end": 3531.12, "text": " that I found
  through your awesome metric learning resource. And now that we touched a bit on",
  "tokens": [51372, 300, 286, 1352, 807, 428, 3476, 20678, 2539, 7684, 13, 400, 586,
  300, 321, 9828, 257, 857, 322, 51768], "temperature": 0.0, "avg_logprob": -0.1723371891493208,
  "compression_ratio": 1.6179245283018868, "no_speech_prob": 0.016916243359446526},
  {"id": 327, "seek": 353112, "start": 3532.08, "end": 3537.44, "text": " where the
  AI is also going and then how to make it more efficient. I also like to ask a question",
  "tokens": [50412, 689, 264, 7318, 307, 611, 516, 293, 550, 577, 281, 652, 309, 544,
  7148, 13, 286, 611, 411, 281, 1029, 257, 1168, 50680], "temperature": 0.0, "avg_logprob":
  -0.17041492462158203, "compression_ratio": 1.6044444444444443, "no_speech_prob":
  0.0015371742192655802}, {"id": 328, "seek": 353112, "start": 3537.44, "end": 3545.3599999999997,
  "text": " of why sort of this magical question which drills into your motivation
  as to why at all you are", "tokens": [50680, 295, 983, 1333, 295, 341, 12066, 1168,
  597, 36126, 666, 428, 12335, 382, 281, 983, 412, 439, 291, 366, 51076], "temperature":
  0.0, "avg_logprob": -0.17041492462158203, "compression_ratio": 1.6044444444444443,
  "no_speech_prob": 0.0015371742192655802}, {"id": 329, "seek": 353112, "start": 3545.3599999999997,
  "end": 3551.8399999999997, "text": " in this space, let''s say deep learning and
  quadrant vector search and also specifically metric", "tokens": [51076, 294, 341,
  1901, 11, 718, 311, 584, 2452, 2539, 293, 46856, 8062, 3164, 293, 611, 4682, 20678,
  51400], "temperature": 0.0, "avg_logprob": -0.17041492462158203, "compression_ratio":
  1.6044444444444443, "no_speech_prob": 0.0015371742192655802}, {"id": 330, "seek":
  353112, "start": 3551.8399999999997, "end": 3556.24, "text": " learning. Can you
  a bit elaborate on the philosophy that drives you here?", "tokens": [51400, 2539,
  13, 1664, 291, 257, 857, 20945, 322, 264, 10675, 300, 11754, 291, 510, 30, 51620],
  "temperature": 0.0, "avg_logprob": -0.17041492462158203, "compression_ratio": 1.6044444444444443,
  "no_speech_prob": 0.0015371742192655802}, {"id": 331, "seek": 355624, "start": 3556.72,
  "end": 3570.24, "text": " Yes, sure. Actually, what motivates me to work with metric
  learning is", "tokens": [50388, 1079, 11, 988, 13, 5135, 11, 437, 42569, 385, 281,
  589, 365, 20678, 2539, 307, 51064], "temperature": 0.0, "avg_logprob": -0.24811282910798726,
  "compression_ratio": 0.958904109589041, "no_speech_prob": 0.009302918799221516},
  {"id": 332, "seek": 357024, "start": 3570.8799999999997, "end": 3595.52, "text":
  " it''s potential to approach many different problems very efficiently. Before metric
  learning actually,", "tokens": [50396, 309, 311, 3995, 281, 3109, 867, 819, 2740,
  588, 19621, 13, 4546, 20678, 2539, 767, 11, 51628], "temperature": 0.0, "avg_logprob":
  -0.43420095443725587, "compression_ratio": 1.1222222222222222, "no_speech_prob":
  0.006233962252736092}, {"id": 333, "seek": 359552, "start": 3596.24, "end": 3611.28,
  "text": " you need to train very different models to solve very different problems.
  But with metric learning,", "tokens": [50400, 291, 643, 281, 3847, 588, 819, 5245,
  281, 5039, 588, 819, 2740, 13, 583, 365, 20678, 2539, 11, 51152], "temperature":
  0.0, "avg_logprob": -0.149908954446966, "compression_ratio": 1.736842105263158,
  "no_speech_prob": 0.006703353486955166}, {"id": 334, "seek": 359552, "start": 3611.28,
  "end": 3624.48, "text": " you can train a single model and you can use the very
  same model to solve very different problems.", "tokens": [51152, 291, 393, 3847,
  257, 2167, 2316, 293, 291, 393, 764, 264, 588, 912, 2316, 281, 5039, 588, 819, 2740,
  13, 51812], "temperature": 0.0, "avg_logprob": -0.149908954446966, "compression_ratio":
  1.736842105263158, "no_speech_prob": 0.006703353486955166}, {"id": 335, "seek":
  362448, "start": 3625.28, "end": 3638.0, "text": " And this is also another fight
  that makes metric learning efficient. Actually, metric learning has a", "tokens":
  [50404, 400, 341, 307, 611, 1071, 2092, 300, 1669, 20678, 2539, 7148, 13, 5135,
  11, 20678, 2539, 575, 257, 51040], "temperature": 0.0, "avg_logprob": -0.22633049488067628,
  "compression_ratio": 1.4344262295081966, "no_speech_prob": 0.0023442262317985296},
  {"id": 336, "seek": 362448, "start": 3638.0, "end": 3646.56, "text": " great potential,
  but you also need a great tool to put it into production.", "tokens": [51040, 869,
  3995, 11, 457, 291, 611, 643, 257, 869, 2290, 281, 829, 309, 666, 4265, 13, 51468],
  "temperature": 0.0, "avg_logprob": -0.22633049488067628, "compression_ratio": 1.4344262295081966,
  "no_speech_prob": 0.0023442262317985296}, {"id": 337, "seek": 364656, "start": 3647.04,
  "end": 3664.16, "text": " For example, upon to now there was no way to combine vector
  search with paid-out information.", "tokens": [50388, 1171, 1365, 11, 3564, 281,
  586, 456, 390, 572, 636, 281, 10432, 8062, 3164, 365, 4835, 12, 346, 1589, 13, 51244],
  "temperature": 0.0, "avg_logprob": -0.38897028836337005, "compression_ratio": 1.4360902255639099,
  "no_speech_prob": 0.03534810617566109}, {"id": 338, "seek": 364656, "start": 3665.52,
  "end": 3675.04, "text": " Even if you make a connection, it was not for practical
  because you lose some information because", "tokens": [51312, 2754, 498, 291, 652,
  257, 4984, 11, 309, 390, 406, 337, 8496, 570, 291, 3624, 512, 1589, 570, 51788],
  "temperature": 0.0, "avg_logprob": -0.38897028836337005, "compression_ratio": 1.4360902255639099,
  "no_speech_prob": 0.03534810617566109}, {"id": 339, "seek": 367504, "start": 3675.7599999999998,
  "end": 3689.92, "text": " you do not, you could not filter the tool systems of information
  at the same time. Quadrant is", "tokens": [50400, 291, 360, 406, 11, 291, 727, 406,
  6608, 264, 2290, 3652, 295, 1589, 412, 264, 912, 565, 13, 29619, 7541, 307, 51108],
  "temperature": 0.0, "avg_logprob": -0.2610823448668135, "compression_ratio": 1.4014598540145986,
  "no_speech_prob": 0.003344888100400567}, {"id": 340, "seek": 367504, "start": 3690.72,
  "end": 3703.2, "text": " doing a great job by combining vector search with filterable
  paid-out information. So it opens up", "tokens": [51148, 884, 257, 869, 1691, 538,
  21928, 8062, 3164, 365, 6608, 712, 4835, 12, 346, 1589, 13, 407, 309, 9870, 493,
  51772], "temperature": 0.0, "avg_logprob": -0.2610823448668135, "compression_ratio":
  1.4014598540145986, "no_speech_prob": 0.003344888100400567}, {"id": 341, "seek":
  370320, "start": 3703.8399999999997, "end": 3719.12, "text": " quite a few new opportunities.
  For that one, you can filter your information based on", "tokens": [50396, 1596,
  257, 1326, 777, 4786, 13, 1171, 300, 472, 11, 291, 393, 6608, 428, 1589, 2361, 322,
  51160], "temperature": 0.0, "avg_logprob": -0.20006409145536877, "compression_ratio":
  1.0625, "no_speech_prob": 0.0070467074401676655}, {"id": 342, "seek": 371912, "start":
  3720.08, "end": 3731.2, "text": " if geographic, geographic place, for example,
  or another sparse category,", "tokens": [50412, 498, 32318, 11, 32318, 1081, 11,
  337, 1365, 11, 420, 1071, 637, 11668, 7719, 11, 50968], "temperature": 0.0, "avg_logprob":
  -0.2995162464323498, "compression_ratio": 1.3387096774193548, "no_speech_prob":
  0.007769379299134016}, {"id": 343, "seek": 371912, "start": 3733.04, "end": 3745.12,
  "text": " numeric value or anything else while at the same time doing a vector search.
  So I think it''s", "tokens": [51060, 7866, 299, 2158, 420, 1340, 1646, 1339, 412,
  264, 912, 565, 884, 257, 8062, 3164, 13, 407, 286, 519, 309, 311, 51664], "temperature":
  0.0, "avg_logprob": -0.2995162464323498, "compression_ratio": 1.3387096774193548,
  "no_speech_prob": 0.007769379299134016}, {"id": 344, "seek": 374512, "start": 3746.08,
  "end": 3760.16, "text": " really exciting. One of the most common problems in AI,
  you actually do the research, but you", "tokens": [50412, 534, 4670, 13, 1485, 295,
  264, 881, 2689, 2740, 294, 7318, 11, 291, 767, 360, 264, 2132, 11, 457, 291, 51116],
  "temperature": 0.0, "avg_logprob": -0.1324567084616803, "compression_ratio": 1.3357142857142856,
  "no_speech_prob": 0.014797184616327286}, {"id": 345, "seek": 374512, "start": 3760.16,
  "end": 3771.8399999999997, "text": " don''t have the required tooling to make it
  practical in the real world. So I think it''s quite", "tokens": [51116, 500, 380,
  362, 264, 4739, 46593, 281, 652, 309, 8496, 294, 264, 957, 1002, 13, 407, 286, 519,
  309, 311, 1596, 51700], "temperature": 0.0, "avg_logprob": -0.1324567084616803,
  "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.014797184616327286},
  {"id": 346, "seek": 377184, "start": 3771.84, "end": 3784.0, "text": " important
  to have such tools as Quadrant to achieve very different, very difficult and challenging",
  "tokens": [50364, 1021, 281, 362, 1270, 3873, 382, 29619, 7541, 281, 4584, 588,
  819, 11, 588, 2252, 293, 7595, 50972], "temperature": 0.0, "avg_logprob": -0.2031852782718719,
  "compression_ratio": 1.5026737967914439, "no_speech_prob": 0.0026681963354349136},
  {"id": 347, "seek": 377184, "start": 3784.0, "end": 3791.52, "text": " problems
  very alien than efficiently. Yeah, absolutely. That''s quite deep. Thank you so
  much", "tokens": [50972, 2740, 588, 12319, 813, 19621, 13, 865, 11, 3122, 13, 663,
  311, 1596, 2452, 13, 1044, 291, 370, 709, 51348], "temperature": 0.0, "avg_logprob":
  -0.2031852782718719, "compression_ratio": 1.5026737967914439, "no_speech_prob":
  0.0026681963354349136}, {"id": 348, "seek": 377184, "start": 3791.52, "end": 3800.4,
  "text": " for sharing this. It also resonates with me because in many ways, you
  know, deep learning", "tokens": [51348, 337, 5414, 341, 13, 467, 611, 41051, 365,
  385, 570, 294, 867, 2098, 11, 291, 458, 11, 2452, 2539, 51792], "temperature": 0.0,
  "avg_logprob": -0.2031852782718719, "compression_ratio": 1.5026737967914439, "no_speech_prob":
  0.0026681963354349136}, {"id": 349, "seek": 380040, "start": 3800.4, "end": 3806.08,
  "text": " on one hand, maybe some people feel like it''s kind of overhyped and there
  is so much material on", "tokens": [50364, 322, 472, 1011, 11, 1310, 512, 561, 841,
  411, 309, 311, 733, 295, 670, 3495, 3452, 293, 456, 307, 370, 709, 2527, 322, 50648],
  "temperature": 0.0, "avg_logprob": -0.11098207877232479, "compression_ratio": 1.6610169491525424,
  "no_speech_prob": 0.025757860392332077}, {"id": 350, "seek": 380040, "start": 3806.08,
  "end": 3812.1600000000003, "text": " the web. On the other hand, when you start
  doing it yourself, you might end up, you know, going into", "tokens": [50648, 264,
  3670, 13, 1282, 264, 661, 1011, 11, 562, 291, 722, 884, 309, 1803, 11, 291, 1062,
  917, 493, 11, 291, 458, 11, 516, 666, 50952], "temperature": 0.0, "avg_logprob":
  -0.11098207877232479, "compression_ratio": 1.6610169491525424, "no_speech_prob":
  0.025757860392332077}, {"id": 351, "seek": 380040, "start": 3812.1600000000003,
  "end": 3816.32, "text": " down the rabbit hole and you don''t know all the tools
  as you said. You don''t know all the best", "tokens": [50952, 760, 264, 19509, 5458,
  293, 291, 500, 380, 458, 439, 264, 3873, 382, 291, 848, 13, 509, 500, 380, 458,
  439, 264, 1151, 51160], "temperature": 0.0, "avg_logprob": -0.11098207877232479,
  "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.025757860392332077},
  {"id": 352, "seek": 380040, "start": 3816.32, "end": 3824.8, "text": " practices.
  And also, like, before we had vector databases, you couldn''t actually, well, apply
  this,", "tokens": [51160, 7525, 13, 400, 611, 11, 411, 11, 949, 321, 632, 8062,
  22380, 11, 291, 2809, 380, 767, 11, 731, 11, 3079, 341, 11, 51584], "temperature":
  0.0, "avg_logprob": -0.11098207877232479, "compression_ratio": 1.6610169491525424,
  "no_speech_prob": 0.025757860392332077}, {"id": 353, "seek": 382480, "start": 3824.8,
  "end": 3831.28, "text": " like, okay, you could of course build some nice demo and,
  you know, throw a web page and just ask", "tokens": [50364, 411, 11, 1392, 11, 291,
  727, 295, 1164, 1322, 512, 1481, 10723, 293, 11, 291, 458, 11, 3507, 257, 3670,
  3028, 293, 445, 1029, 50688], "temperature": 0.0, "avg_logprob": -0.12215572042563527,
  "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.013267750851809978},
  {"id": 354, "seek": 382480, "start": 3831.28, "end": 3836.8, "text": " somebody,
  okay, type something here and my neural network will do something. But now, like,
  you could", "tokens": [50688, 2618, 11, 1392, 11, 2010, 746, 510, 293, 452, 18161,
  3209, 486, 360, 746, 13, 583, 586, 11, 411, 11, 291, 727, 50964], "temperature":
  0.0, "avg_logprob": -0.12215572042563527, "compression_ratio": 1.6610169491525424,
  "no_speech_prob": 0.013267750851809978}, {"id": 355, "seek": 382480, "start": 3836.8,
  "end": 3842.4, "text": " kind of scale this further and index your embeddings and
  see the end result of what you''re doing", "tokens": [50964, 733, 295, 4373, 341,
  3052, 293, 8186, 428, 12240, 29432, 293, 536, 264, 917, 1874, 295, 437, 291, 434,
  884, 51244], "temperature": 0.0, "avg_logprob": -0.12215572042563527, "compression_ratio":
  1.6610169491525424, "no_speech_prob": 0.013267750851809978}, {"id": 356, "seek":
  382480, "start": 3842.4, "end": 3851.2000000000003, "text": " through the retrieval
  process. So I think that opens up a lot of opportunities. So that''s super", "tokens":
  [51244, 807, 264, 19817, 3337, 1399, 13, 407, 286, 519, 300, 9870, 493, 257, 688,
  295, 4786, 13, 407, 300, 311, 1687, 51684], "temperature": 0.0, "avg_logprob": -0.12215572042563527,
  "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.013267750851809978},
  {"id": 357, "seek": 385120, "start": 3851.2, "end": 3869.9199999999996, "text":
  " cool. Yeah, exactly. Actually, once we have such tooling, the domain is also improving
  more rapidly", "tokens": [50364, 1627, 13, 865, 11, 2293, 13, 5135, 11, 1564, 321,
  362, 1270, 46593, 11, 264, 9274, 307, 611, 11470, 544, 12910, 51300], "temperature":
  0.0, "avg_logprob": -0.2701752853393555, "compression_ratio": 1.1, "no_speech_prob":
  0.017540913075208664}, {"id": 358, "seek": 386992, "start": 3869.92, "end": 3882.8,
  "text": " and also the improvements in the domain also foster development of such
  tools. So I think it''s like", "tokens": [50364, 293, 611, 264, 13797, 294, 264,
  9274, 611, 17114, 3250, 295, 1270, 3873, 13, 407, 286, 519, 309, 311, 411, 51008],
  "temperature": 0.0, "avg_logprob": -0.2383765345034392, "compression_ratio": 1.4393939393939394,
  "no_speech_prob": 0.05824998393654823}, {"id": 359, "seek": 386992, "start": 3883.6,
  "end": 3896.48, "text": " too far and it will be a metric learning will be in a
  better place in the future with this", "tokens": [51048, 886, 1400, 293, 309, 486,
  312, 257, 20678, 2539, 486, 312, 294, 257, 1101, 1081, 294, 264, 2027, 365, 341,
  51692], "temperature": 0.0, "avg_logprob": -0.2383765345034392, "compression_ratio":
  1.4393939393939394, "no_speech_prob": 0.05824998393654823}, {"id": 360, "seek":
  389648, "start": 3897.2, "end": 3904.56, "text": " rapid developments in the domain.
  Yeah, absolutely. And I was thinking, like, there''s like a ton of", "tokens": [50400,
  7558, 20862, 294, 264, 9274, 13, 865, 11, 3122, 13, 400, 286, 390, 1953, 11, 411,
  11, 456, 311, 411, 257, 2952, 295, 50768], "temperature": 0.0, "avg_logprob": -0.1799595850818562,
  "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.016853876411914825},
  {"id": 361, "seek": 389648, "start": 3904.56, "end": 3910.64, "text": " material,
  I''m sure we''ll have to digest, at least I will have to digest a lot of it and
  see how I can", "tokens": [50768, 2527, 11, 286, 478, 988, 321, 603, 362, 281, 13884,
  11, 412, 1935, 286, 486, 362, 281, 13884, 257, 688, 295, 309, 293, 536, 577, 286,
  393, 51072], "temperature": 0.0, "avg_logprob": -0.1799595850818562, "compression_ratio":
  1.6129032258064515, "no_speech_prob": 0.016853876411914825}, {"id": 362, "seek":
  389648, "start": 3910.64, "end": 3917.36, "text": " apply this. And thankfully,
  you have, you know, you have this awesome metric learning resource on", "tokens":
  [51072, 3079, 341, 13, 400, 27352, 11, 291, 362, 11, 291, 458, 11, 291, 362, 341,
  3476, 20678, 2539, 7684, 322, 51408], "temperature": 0.0, "avg_logprob": -0.1799595850818562,
  "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.016853876411914825},
  {"id": 363, "seek": 389648, "start": 3917.36, "end": 3924.88, "text": " GitHub that
  we can check out. We''ll make sure to leave it in the notes. And if if some of us
  want to", "tokens": [51408, 23331, 300, 321, 393, 1520, 484, 13, 492, 603, 652,
  988, 281, 1856, 309, 294, 264, 5570, 13, 400, 498, 498, 512, 295, 505, 528, 281,
  51784], "temperature": 0.0, "avg_logprob": -0.1799595850818562, "compression_ratio":
  1.6129032258064515, "no_speech_prob": 0.016853876411914825}, {"id": 364, "seek":
  392488, "start": 3924.88, "end": 3931.6, "text": " kind of work with you or interact
  with you, can you make like a little announcement where we can", "tokens": [50364,
  733, 295, 589, 365, 291, 420, 4648, 365, 291, 11, 393, 291, 652, 411, 257, 707,
  12847, 689, 321, 393, 50700], "temperature": 0.0, "avg_logprob": -0.21776157809842017,
  "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.005566722247749567},
  {"id": 365, "seek": 392488, "start": 3931.6, "end": 3937.52, "text": " join forces
  and kind of learn more about metric learning and maybe contribute to this field
  together", "tokens": [50700, 3917, 5874, 293, 733, 295, 1466, 544, 466, 20678, 2539,
  293, 1310, 10586, 281, 341, 2519, 1214, 50996], "temperature": 0.0, "avg_logprob":
  -0.21776157809842017, "compression_ratio": 1.5371428571428571, "no_speech_prob":
  0.005566722247749567}, {"id": 366, "seek": 392488, "start": 3937.52, "end": 3949.04,
  "text": " with you? Yes, you''re right. I have several announcements maybe. First,",
  "tokens": [50996, 365, 291, 30, 1079, 11, 291, 434, 558, 13, 286, 362, 2940, 23785,
  1310, 13, 2386, 11, 51572], "temperature": 0.0, "avg_logprob": -0.21776157809842017,
  "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.005566722247749567},
  {"id": 367, "seek": 394904, "start": 3949.2, "end": 3962.32, "text": " beyond my
  resource and engineering site, I''m also a community guide guide and we have a",
  "tokens": [50372, 4399, 452, 7684, 293, 7043, 3621, 11, 286, 478, 611, 257, 1768,
  5934, 5934, 293, 321, 362, 257, 51028], "temperature": 0.0, "avg_logprob": -0.33519233976091656,
  "compression_ratio": 1.328358208955224, "no_speech_prob": 0.010248702950775623},
  {"id": 368, "seek": 394904, "start": 3962.32, "end": 3974.64, "text": " difficult
  server at Quadrant where we hold paper reading class. We had the first one about",
  "tokens": [51028, 2252, 7154, 412, 29619, 7541, 689, 321, 1797, 3035, 3760, 1508,
  13, 492, 632, 264, 700, 472, 466, 51644], "temperature": 0.0, "avg_logprob": -0.33519233976091656,
  "compression_ratio": 1.328358208955224, "no_speech_prob": 0.010248702950775623},
  {"id": 369, "seek": 397464, "start": 3974.64, "end": 3985.12, "text": " contrastive
  laws and we will also have another fashion about triplet laws. And I also", "tokens":
  [50364, 8712, 488, 6064, 293, 321, 486, 611, 362, 1071, 6700, 466, 1376, 14657,
  6064, 13, 400, 286, 611, 50888], "temperature": 0.0, "avg_logprob": -0.39745194571358816,
  "compression_ratio": 1.3706896551724137, "no_speech_prob": 0.006165626924484968},
  {"id": 370, "seek": 397464, "start": 3986.24, "end": 3996.64, "text": " wrote a
  wrote an intuitional triplet law post. Our approach will be like,", "tokens": [50944,
  4114, 257, 4114, 364, 560, 84, 2628, 1376, 14657, 2101, 2183, 13, 2621, 3109, 486,
  312, 411, 11, 51464], "temperature": 0.0, "avg_logprob": -0.39745194571358816, "compression_ratio":
  1.3706896551724137, "no_speech_prob": 0.006165626924484968}, {"id": 371, "seek":
  399664, "start": 3997.12, "end": 4007.6, "text": " after I will write such intuitional
  post about papers and then we will hold", "tokens": [50388, 934, 286, 486, 2464,
  1270, 560, 84, 2628, 2183, 466, 10577, 293, 550, 321, 486, 1797, 50912], "temperature":
  0.0, "avg_logprob": -0.18127928972244262, "compression_ratio": 1.3253968253968254,
  "no_speech_prob": 0.006581358145922422}, {"id": 372, "seek": 399664, "start": 4008.48,
  "end": 4019.7599999999998, "text": " Q&A sessions in our discourse servers. So everyone
  who is curious about metric learning can", "tokens": [50956, 1249, 5, 32, 11081,
  294, 527, 23938, 15909, 13, 407, 1518, 567, 307, 6369, 466, 20678, 2539, 393, 51520],
  "temperature": 0.0, "avg_logprob": -0.18127928972244262, "compression_ratio": 1.3253968253968254,
  "no_speech_prob": 0.006581358145922422}, {"id": 373, "seek": 401976, "start": 4019.76,
  "end": 4033.28, "text": " join the discourse server to enjoy this discussion. Apart
  from that one, beside my professional", "tokens": [50364, 3917, 264, 23938, 7154,
  281, 2103, 341, 5017, 13, 24111, 490, 300, 472, 11, 15726, 452, 4843, 51040], "temperature":
  0.0, "avg_logprob": -0.16468121455265924, "compression_ratio": 1.300751879699248,
  "no_speech_prob": 0.003287998028099537}, {"id": 374, "seek": 401976, "start": 4033.28,
  "end": 4041.28, "text": " life, I''m recognized as a Google developer expert on
  machine learning, on the", "tokens": [51040, 993, 11, 286, 478, 9823, 382, 257,
  3329, 10754, 5844, 322, 3479, 2539, 11, 322, 264, 51440], "temperature": 0.0, "avg_logprob":
  -0.16468121455265924, "compression_ratio": 1.300751879699248, "no_speech_prob":
  0.003287998028099537}, {"id": 375, "seek": 404128, "start": 4041.76, "end": 4054.0800000000004,
  "text": " volunteering site, community site. And this year at Google''s thunder
  off call, I will serve", "tokens": [50388, 33237, 3621, 11, 1768, 3621, 13, 400,
  341, 1064, 412, 3329, 311, 19898, 766, 818, 11, 286, 486, 4596, 51004], "temperature":
  0.0, "avg_logprob": -0.6347226036919488, "compression_ratio": 1.3768115942028984,
  "no_speech_prob": 0.003868973348289728}, {"id": 376, "seek": 404128, "start": 4054.7200000000003,
  "end": 4063.84, "text": " as a transfer flow mentor for the transfer flow, the library
  take Python package, if a package for", "tokens": [51036, 382, 257, 5003, 3095,
  14478, 337, 264, 5003, 3095, 11, 264, 6405, 747, 15329, 7372, 11, 498, 257, 7372,
  337, 51492], "temperature": 0.0, "avg_logprob": -0.6347226036919488, "compression_ratio":
  1.3768115942028984, "no_speech_prob": 0.003868973348289728}, {"id": 377, "seek":
  406384, "start": 4064.56, "end": 4072.8, "text": " metric learning in the transfer
  flow ecosystem. So university students and fresh", "tokens": [50400, 20678, 2539,
  294, 264, 5003, 3095, 11311, 13, 407, 5454, 1731, 293, 4451, 50812], "temperature":
  0.0, "avg_logprob": -0.170440673828125, "compression_ratio": 1.5371428571428571,
  "no_speech_prob": 0.011202441528439522}, {"id": 378, "seek": 406384, "start": 4073.6000000000004,
  "end": 4083.6000000000004, "text": " graduates can apply to Google''s thunder off
  call if they want to work with me in this effort and", "tokens": [50852, 13577,
  393, 3079, 281, 3329, 311, 19898, 766, 818, 498, 436, 528, 281, 589, 365, 385, 294,
  341, 4630, 293, 51352], "temperature": 0.0, "avg_logprob": -0.170440673828125, "compression_ratio":
  1.5371428571428571, "no_speech_prob": 0.011202441528439522}, {"id": 379, "seek":
  406384, "start": 4083.6000000000004, "end": 4092.6400000000003, "text": " contribute
  to the field. That''s fantastic. I think Google''s thunder off code is an exciting",
  "tokens": [51352, 10586, 281, 264, 2519, 13, 663, 311, 5456, 13, 286, 519, 3329,
  311, 19898, 766, 3089, 307, 364, 4670, 51804], "temperature": 0.0, "avg_logprob":
  -0.170440673828125, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.011202441528439522},
  {"id": 380, "seek": 409264, "start": 4092.72, "end": 4097.599999999999, "text":
  " place to be and there are so many projects but it''s great to learn that you are
  leading the", "tokens": [50368, 1081, 281, 312, 293, 456, 366, 370, 867, 4455, 457,
  309, 311, 869, 281, 1466, 300, 291, 366, 5775, 264, 50612], "temperature": 0.0,
  "avg_logprob": -0.14781305064325748, "compression_ratio": 1.6740088105726871, "no_speech_prob":
  0.011664592660963535}, {"id": 381, "seek": 409264, "start": 4097.599999999999, "end":
  4104.48, "text": " metric learning exploration there and I''m sure there will be
  interest towards it and I will make", "tokens": [50612, 20678, 2539, 16197, 456,
  293, 286, 478, 988, 456, 486, 312, 1179, 3030, 309, 293, 286, 486, 652, 50956],
  "temperature": 0.0, "avg_logprob": -0.14781305064325748, "compression_ratio": 1.6740088105726871,
  "no_speech_prob": 0.011664592660963535}, {"id": 382, "seek": 409264, "start": 4104.48,
  "end": 4112.5599999999995, "text": " sure to also leave the relevant link in the
  show notes on this. Yeah, thanks so much. Use of this", "tokens": [50956, 988, 281,
  611, 1856, 264, 7340, 2113, 294, 264, 855, 5570, 322, 341, 13, 865, 11, 3231, 370,
  709, 13, 8278, 295, 341, 51360], "temperature": 0.0, "avg_logprob": -0.14781305064325748,
  "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.011664592660963535},
  {"id": 383, "seek": 409264, "start": 4113.76, "end": 4120.72, "text": " this was
  a pleasure to discuss with you. I feel like I dipped some of my fingers in the water",
  "tokens": [51420, 341, 390, 257, 6834, 281, 2248, 365, 291, 13, 286, 841, 411, 286,
  45162, 512, 295, 452, 7350, 294, 264, 1281, 51768], "temperature": 0.0, "avg_logprob":
  -0.14781305064325748, "compression_ratio": 1.6740088105726871, "no_speech_prob":
  0.011664592660963535}, {"id": 384, "seek": 412072, "start": 4120.8, "end": 4125.360000000001,
  "text": " of metric learning. I think there is still a ton to learn and thanks so
  much for", "tokens": [50368, 295, 20678, 2539, 13, 286, 519, 456, 307, 920, 257,
  2952, 281, 1466, 293, 3231, 370, 709, 337, 50596], "temperature": 0.0, "avg_logprob":
  -0.2606357607925147, "compression_ratio": 1.4573170731707317, "no_speech_prob":
  0.0056219808757305145}, {"id": 385, "seek": 412072, "start": 4126.320000000001,
  "end": 4130.88, "text": " introducing it from so multiple angles. We''ve enjoyed
  this conversation.", "tokens": [50644, 15424, 309, 490, 370, 3866, 14708, 13, 492,
  600, 4626, 341, 3761, 13, 50872], "temperature": 0.0, "avg_logprob": -0.2606357607925147,
  "compression_ratio": 1.4573170731707317, "no_speech_prob": 0.0056219808757305145},
  {"id": 386, "seek": 412072, "start": 4132.96, "end": 4145.68, "text": " Thank you
  Dimitriv again for this great opportunity. I hope the audience also enjoyed", "tokens":
  [50976, 1044, 291, 20975, 270, 470, 85, 797, 337, 341, 869, 2650, 13, 286, 1454,
  264, 4034, 611, 4626, 51612], "temperature": 0.0, "avg_logprob": -0.2606357607925147,
  "compression_ratio": 1.4573170731707317, "no_speech_prob": 0.0056219808757305145},
  {"id": 387, "seek": 414568, "start": 4146.56, "end": 4155.84, "text": " it as well
  and I hope it will be helpful for those who are interested in metric learning.",
  "tokens": [50408, 309, 382, 731, 293, 286, 1454, 309, 486, 312, 4961, 337, 729,
  567, 366, 3102, 294, 20678, 2539, 13, 50872], "temperature": 0.0, "avg_logprob":
  -0.19913691740769607, "compression_ratio": 1.608695652173913, "no_speech_prob":
  0.008144969120621681}, {"id": 388, "seek": 414568, "start": 4156.4800000000005,
  "end": 4162.96, "text": " Yeah, for sure. Thank you so much. I learned a ton and
  I hope I''ll also see you maybe doing", "tokens": [50904, 865, 11, 337, 988, 13,
  1044, 291, 370, 709, 13, 286, 3264, 257, 2952, 293, 286, 1454, 286, 603, 611, 536,
  291, 1310, 884, 51228], "temperature": 0.0, "avg_logprob": -0.19913691740769607,
  "compression_ratio": 1.608695652173913, "no_speech_prob": 0.008144969120621681},
  {"id": 389, "seek": 414568, "start": 4162.96, "end": 4168.240000000001, "text":
  " some presentations or reading your blogs to learn more about it. Thanks so much.",
  "tokens": [51228, 512, 18964, 420, 3760, 428, 31038, 281, 1466, 544, 466, 309, 13,
  2561, 370, 709, 13, 51492], "temperature": 0.0, "avg_logprob": -0.19913691740769607,
  "compression_ratio": 1.608695652173913, "no_speech_prob": 0.008144969120621681},
  {"id": 390, "seek": 414568, "start": 4169.6, "end": 4171.68, "text": " Thank you
  so much. Yeah, bye bye.", "tokens": [51560, 1044, 291, 370, 709, 13, 865, 11, 6543,
  6543, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19913691740769607, "compression_ratio":
  1.608695652173913, "no_speech_prob": 0.008144969120621681}]'
---

Hello, today we have a new episode of the Vector Podcast and today I'm super happy to have you, Suf Sangos, with me. He holds the role of AI Research Engineer at Quadrant.
It's a Vector Search Database Company and you might remember we had an episode with Tom Lackner, who is the user of Quadrant today. We have an episode and discussion with you, Suf, who works for Quadrant.
And one of the core topics today we're going to be discussing metric learning, but before that, hey, you Suf, how are you doing? I'm very excited to join you in this episode to discuss metric learning and thank you for having me. Yeah, thanks for coming up.
Really, I think this topic is something that has been crossing my area of focus and also some of the questions that users are asking, you know, okay, if I have this data set, how can I be sure that it will work with neural search, right? And I think metric learning seems to be one of the answers.
But before we start discussing this in deep, in depth, I was thinking, could you please introduce yourself to our audience? Yes, sure. Armist has told Suf's software developer and AI researcher with a background in linguistics at the university.
Actually, I've been developing software since my high school years. During my master's study, I combined my experience and my education to study machine translation.
After several years of experience in different roles and at different startups, I ended up with the multi model retrieval because I had a long experience in both computer vision and measured language processing. So for some time, my main focus is metric learning.
I was already a user of co-advent, even before joining co-advent and I thought it would be very cool to work for an open source project that I find valuable myself. Yeah, sounds awesome. Sounds cool. You just mentioned multi model.
So you mean like multi model search, right? And I think this field is still kind of in many ways shaping up and many people are still learning and kind of scratching their heads like what is multi model? Like maybe if you could give an example or a little bit explain what is multi model. Yes, sure.
Actually, as you just said, multi model is quite a new topic actually. Actually, it's resurrecting with developments in deep metric learning. One of the most famous applications is a clip by OpenAI, short for contrastive language image, the pre-training.
In the most basic term, they train a model to construct a unified vector space for both images and tests. Basically, they have two encoders, one for images and one for tests, support that you have a pair of images and its textual description.
When you see this image and that textual description to these encoders, you are supposed to get very similar vectors, vector output from these encoders. So you can search images with a textual query or Y-14.
So you sort of crossed the, so in a way with one modalities text or image is another modality, but in this case, we kind of like cross go across modalities. I think we can cross the border of modalities with this.
Yeah, which I think to many users will sound like a magic because you essentially, if you view an image like a set of pixels and if you query textual queries a set of words, now you sort of somehow magically search your words in pixels, but actually that's not exactly what's happening.
Of course, we do the embedding and so on, but in a nutshell, it kind of sounds like this magical cross model search there.
 Yes, I expected for newcomers is a little bit like magic, but from quite a long time, we have already been using vector search in the context of image search, but in that case, we search for images with a query which is image if that, but in this case, we make a connection between two modalities actually.
This is also how our human brain is functioning.
For the most of the time, we don't consume the information from a single modality actually when we try to understand our environment, we both take it as a visual input and also an audio input and we also talk to people around them for it gives us a better understanding of the environment.
So if we want to make our AI smarter, we also need to help them gain this ability as well. So beyond searching for images with a textual query, this also helps us to combine information from different sources.
So in this case, maybe we can also have AI better understand its environment by combining, for example, a stream from the camera and also maybe an output from a speech recognition and encoding them into a vector we can combine these two vectors to fit into that encoder.
So this also opens such new opportunities. Yeah, that's a great intro there also like how you gave analogy with how human brain functions, so like how we take so many signals into our decision making.
And specifically, like what you mentioned about clip, I like the fact that in practical settings, let's say if you have images, let's say of some goods and you want to make a search in those goods and you also have some metadata, let's say titles or descriptions, right?
It may be that some human decided what to put in that text, but they didn't put everything that there is on the image, right? And so I think clip helps us to find sort of semantics that's hidden inside the image itself, right? So I think that's kind of like has practical impact on what we built.
Yeah, exactly.
 Actually, in the traditional source, for example, let's get the product source as example, when you want to develop a product source for, for example, an e-commerce website, you need to enter different terms that can define that product to have a user's find that product with different wording, but this is not so practical because people use very different terms to refer to things.
And you in the current capacity of e-commerce websites, we have hundreds of thousands of products and they also need to be updated once you add new products and remove new products.
And also like myths acted at typos to this complexity, is actually explored to millions, maybe a tens of millions of possibilities. This is beyond the power of humans actually.
But once you make connections, make a connection between text and images, you don't need to enter such descriptive text, you only encode images into vectors and index time into a vector database.
Then in the inference time, all you need is just encode the textual input as well and create that pre-indexed database to get similar results.
Actually, this also buildings new opportunities, for example, people usually enter some pre-defined textual descriptors in this search engines, but some new products may have brand new features that people are not accustomed to.
So even in this case, our vector search based solution that combines images and text can be in that image as well. Yeah, that sounds cool. So it kind of opens up a lot of opportunities that didn't exist before when we modeled our object purely through textual representation.
Maybe somebody did attempt to also encode images of some other binary format, but I think maybe it wasn't as efficient or definitely not multi-model. So that sounds so cool.
And so how do you connect? Where do you start? Usually, let's say if you have a data set, right? And you want to implement neural search experience.
At one point of time, do you start thinking about what the metric is the best for my data set? And also, how do you approach it from which angle do you usually approach this? And this is something that really helps you to hear your theoretical as well as practical thoughts of this.
Yes, you're actually there are lots of very different techniques and methods and approaches to metric learning that can work for some specific types of problems.
But in my practical experience, I usually begin with with with an auto encoder, because it's already very easy to implement and easy to train. It can be applied to almost any data track. Basically, in auto encoders, we have two models and encoder and the encoders.
The encoders part encodes samples into an dimensional vector. This and should be much lower than the dimensionality of the input sample. And the decoder is supposed to reconstruct the input sample when this encoded vector is given to it. So this is a the self-provised method.
So it can be applied to any type of data set. You don't need labels. It usually gives a very good resource. After training such a model, you can visualize embedding. We call the output of the encoders, vectors embedding. So you can visualize such embedding with a tool.
This tool can be, for example, TensorFlow projectors and another tool by Yubach. I just couldn't show my word in there. Sorry. No worries. We can find those links later, I guess. Yeah, we can put a link in the description.
And this visualization tools have us see if our encoders really involve similar samples need to each closer to each other than the similar ones. If it is, we can use this encoders part.
We can just dispose the decoder part and we can simply keep the encoder part and use it to encode our samples and index them in the vector. And we can already start searching semantics. But we usually do buzzers than this one with only small set of labeled data.
And you actually need only a few with that one. Actually, we are preparing some publications to demonstrate this one. After you train and encoders with a considerable number of unlabeled data, all you need to do is just to find to in it with a small set of labeled data.
On the supervised site, there are really quite a number of very different approaches to matrix learning from more traditional margin-based approaches to newer categorization-based approaches. And actually, they deserve a long discussion of data. For sure. Yeah, that's awesome.
But just to unpack it a little bit, so in a natural metric learning process allows me to learn the optimal distance metric for my data. So it's kind of like a function of my dataset properties, inner properties. Yeah, actually, let's clarify this metric thing.
What does it mean in this context? In this context, a metric is a non-negative function with two inputs. Let's say X and Y. And it is used to measure what is called the distance between X and Y. When we feed such two inputs, it gives us a scaler's positive value.
If this value is closer to zero, then we can assume that those two inputs are more similar to each other with two inputs with a higher distance value. So our whole objective in metric learning is to train functions that can give this distance value.
On the practical site, we usually train a model that outputs a vector and a dimensional vector. And then we can apply different distance functions such as Euclidean and cosine distance to get a measurement of the distance value. There is also a term deep metric learning.
Actually, the traditional metric learning uses some linear transformations to project samples into an dimensional feature space to apply a metric function.
But this linear aspect of such transformations limits the use of traditional metric learning using time with more richers, data types, for example, images and texts.
So deep metric learning benefits from the methods of deep learning to learn non-linear transformations to project samples into a new and dimensional vector space.
But in this context, I usually use metric learning as an umbrella term to refer to both traditional metric learning and deep metric learning. Just like we do with machine learning to refer to both classical machine learning and deep learning. Yeah, that makes sense. Thank you.
And so essentially, in the lay main terms, deep learning allows us to vectorize data objects that previously we couldn't vectorize in a celly, so images or I don't know. And do it efficiently, because in images, you might have way too many pixels.
So if you just take the vector of all the pixels, it's way too big of an object to deal with. And so you vectorize, as you said, in the beginning, and you basically sort of project it in a lower dimensional space. So now you can actually efficiently operate on it. Exactly.
Let's get images as an example. Let's assume that we have images with a size of 200 times 200. And we also have a channel value of three. So we end up with 200 times 200 times three values for a single image. And also, let's actually, too many values also mean a great variance value.
So it's not so practical to make a measurement between two images, because those pixel values can include very surface, quite shallow surface features that do not make any sense in our semantics.
But once we encode those high dimensional inputs into a low dimensional vector space, for example, we usually have 500 to 12, 10 to the 12, 12, 12, 14 dimensional vectors. And this value is really low when compared to the original dimension of that sample.
So in this case, that model should learn, should learn a representation of high dimensional samples. Actually, we just throw the unnecessary part of those samples, and we only keep the part that matters for us. Yeah, yeah.
So kind of in some sense, you could say it's like signal compression, right?
So in some sense, like using the signal law, like the distribution, you could actually compress things, like I don't know if theoretically speaking in an image, you have like one object, and the rest is just the background of one color.
You really don't need to pass all these pixels independently, like you could just say, okay, it's a background I've learned that it's that color kind of semantically, I guess, and then what matters is the object somewhere there that we focus on when we look at this picture, right? Yeah, exactly.
Actually, in the original distribution case, for example, of images, we don't have any connection between the value of a pixel and the semantic counterpart of that pixel one. But once we transform it into a vector space, at least theoretically we can make conclusions.
For example, we have a 1024 dimensional vector as a representation of that image. In this case, if we examine this vector space, we can make conclusions of this value in the index zero, in cause the features of this feature of image.
For example, it can, in cause the size of a specific object or the colors value of a specific object or maybe some more abstract features of objects. This enables us to search it more efficiently instead of otherwise our values are actually distributed to a very wide range.
And we don't have such interpretations in that distribution space. Yeah, that makes sense. It's a very unique high variant and also in some senses, like waste of space because we are not communicating that much more information by sort of encoding all these pixels.
But we could actually extract some features and patterns in the image.
I think some early work on this was done using, if I remember, it was called a Godworth filter or some other ways of kind of smoothing your image and trying to learn what features you have, for instance, if you try to differentiate between spruce and widely trees.
So like for the purposes of keeping one tree and then maybe removing the others. But I think it wasn't as efficient perhaps as compared to deep learning because deep learning, as far as understanding, basically like learns without features in many ways.
It learns from the data and then you should have some target function that you're optimizing for so it can recreate the weights inside it. Exactly.
Actually, what is most differentiating feature of deep learning is deep learning is actually used to learn the parameters of complex functions instead of manually tuning them. Before deep learning, we already had most of the filters we currently have.
But the parameters of such filters were supposed to be manually tuned by experts in that domain. But in deep learning, we learn those parameters directly from data. And as you said, actually, the beginning of metric learning is also in dimensionality reduction.
We have most popular contrastive loss, for example.
 And the first introduction of contrastive loss is in 2005 and the original purpose of that function actually to reduce dimensionality of high dimensional inputs rather than vector source or anything F for actually another end just tried to reduce the dimensionality of high dimensional input to use lower dimensional input F features to other models.
Yeah, that sounds exciting. Actually, before you brought this up, I didn't think that way because I was experimenting in my team also with things like product quantization. So you do have all already the vectors computed by the neural network, but you could actually quantize them even further.
So you save space and maybe of course you introduce some overlaps that might decrease your precision, but slightly, but you're gonna save a ton of space and make your search more efficient.
So it's almost like you could think of dimensionality reduction in so many different levels and ways as you have the reason about your data, right? Yeah, exactly.
Actually, metric learning is itself a type of dimensionality reduction, but even after you apply metric learning and vector encoding to your data, you still have a high dimensional vector. You have, for example, 10, 10, 10, 10, 4, dimensional data times 32 bits for a single flaw.
So it's already a huge data when you have, for example, millions of samples. So you can still actually apply some quantization methods to get even smaller representations from that one.
And this can be also hierarchical, meaning that you can get several representations of the same sample at different levels of information encoded in that feature space. Yeah, that's fantastic.
 So I was also thinking like, if you could give like some practical example or setting where I could start thinking about deploying metric learning and also like, could you sort of point us in the direction of what tools are available so that I don't think we need to reinvent everything from scratch, but maybe there are some practices, also best practices available, you know, to structure this process.
Can you give some advice on that? Yeah, sure. For a starter example, actually, metric learning is best known for is used in face recognition, but personally, I don't support use of machine learning to process biometric information.
So I give an example from our everyday life, actually, we almost everyday use it, smart deploy. The feature found in, for example, GMA, LinkedIn, and other messaging apps. Actually, it is trained from a large collection of conversation histories in these platforms.
Basically, they just like the example we put in the beginning image and textual unified vector space, they construct a unified vector space for conversation histories and single sentences.
For any moment of conversation, you encode the history of that conversation to retrieve most relevant replies to that history. And you can show them as suggestions to the usage, and users can pitch one of them.
And what is exciting with this setup, you can also log the chosen reply, and you can continue improving your model from direct feedback from your actual users. So it's a really practical use case of metric learning.
And for practitioners who want to start experimenting with metric learning, actually, there are lots of tools to solve very few problems in metric learning.
So in the context of deep learning model development itself, we have several libraries, such as high-torch metric learning and transfer flow similarity.
There are other libraries as well, but I think these are the most mature libraries and most cultural, how should I say, virtual libraries to tackle with different data tasks.
On the other hand, for visualization, we have this transfer flow projector, is a browser-based tool for you can examine your embedding easily with that one.
There are also vector search databases, there are increasing in numbers, but of course, I am a fan of Quaddon because it's really doing a great job with an extensive filtering support for a variety of data tasks. And it's doing this very efficiently, very elegant in only 40 megawise.
So it opens up very important is to put your metric learning model into production and to combine vector search with super search as well. So you can just filter your data based on their payload information at the same time as vector search.
I think these are other than that, beyond beside my research and engineering practices, I'm also maintaining a repository called Automatic Learning and I'm regularly sharing new developments in the domain of metric learning with personal annotations.
So I think it might be also quite helpful for those who want to find their ways in this domain. That's awesome. Thank you.
I will certainly make sure to add all of these links in the description notes, in the notes to this podcast and usually all of these podcasts that I do, they have a lot of links that actually you almost can use as an educational material. And thanks so much for adding so much information here.
And I actually wanted to drill a little bit again into that example that brilliant example you gave about predicting sort of what snacks when I type.
Actually, I used this feature quite a lot and especially like when you're on the go and today I think I've used it somewhere with a Gmail, I was on the go and I had only one finger, right?
So just holding my phone as I go and there was a question and the answer was something, yes, it happened or yes, it did.
And maybe it wasn't the best sort of semantical choice or maybe not the most elegant choice linguistically, like maybe I would add more color, but because I was on the go, it was fine to save that, you know, few minutes and don't be distracted by the phone.
So I just pressed that button and off it goes. And so that's a fantastic feature. So I wanted to sort of open up the process a little bit of metric learning in this case. Basically, I imagine and please correct me if I'm wrong.
As an input, I would have, let's say, a pair of sentences that what was the input and what was the prediction and that prediction could be either curated by experts or we could have minded from the logs, whatever.
So let's say we have a corpus like this, right? So we can employ sequence to sequence model or some other model to actually train like our first first predictor.
So at which point would you start thinking and how exactly would you start thinking about metric learning? Like how can I change the behavior of my model? Like will I replace like last layer of my neural network with like different layer that I have learned from metric learning?
Can you a bit open up this kitchen for me? Thanks.
Actually, this smart supply has its own paper by Google as well and they are really doing a great job to describe the whole logic to whole design decisions behind this feature.
 As you already said, the suggested duplicates are not the best, the most specific replies that you can imagine, but this is actually are spied design because they do not generate those replies, but they have a large collection of such replies and they should be as flexible as possible to fit into different circumstances.
So they shouldn't have any specific references to a specific sentence in the conversation. So that should be a generic enough to apply almost any conversation.
For the training slide, yeah actually, they filter a large collection from the different platforms they are running Gmail and other platforms and they filter short replies and thematically more broad samples such as as you gave as an example. Yes, I did or no, I didn't does it have such examples.
And the actual training algorithm works like this. They actually come up with a very creative, very clever, lost function for just a terrain with this model. They have only a pair of two samples and there is no other label or information.
We only have one input and one ground truth, we have no other scoring, no other label or anything else. So we only get a batch of, for example, and samples and we encode those two and samples because we have two samples first page and we end up with two and samples.
And once we encode them with our encoder, we can compute a distance matrix between these all posts of the encoder. A distance matrix is a two-dimensional matrix to define every distance value between all possible pairs in a collection.
So we have a matrix of five and times and and we already have these samples as pairs. We already know that there is a company target samples for the sample, for the first sample at index zero, the company sample should also be at index zero for sample at index one.
The company sample sample should be at index one. So we can generate these target labels just based on this information. So it's like a categorical classification now.
So for the first sample in the pair at index zero, the categorical label should be zero and others all index values should be wrong.
So we can just encode this information as a one-heart encoding and we can simply use a Crohn's entropy loss once we encode this information as a one-heart encoding and we can train this model with this loss.
So it is called multiple negative ranking loss because in in some way we rank all possible replies in a batch with multiple negatives and only one positive sample. Yeah.
And so you would train this network with this with this loss function and so the output will be what? Like will it be like the optimal metric or optimal? Yeah.
In one, we train this model, we end up with a model that can encode a sentence in such a way that that vector can retrieve the most relevant vectors from a collection of possible replies. So after we train this model, we encode all possible replies and index them in a vector database.
And at the inference time, we encode the usage input again with this model and make a query, a vector source query to that pre-index database of possible replies and we can get, for example, a car, a chain, a nearest neighbor to that vector to suggest to use it. Yeah.
I mean, after you explain this, like to me, like the mental image that evokes is that we sort of like learn rather than learning the metric, we're actually learning the vectors themselves.
We're learning the best vector representation for our object to satisfy some goal, right? Let's say that for this sentence, the closest reply should be this in some sense. Yeah, exactly. Actually, the model learns a representation that satisfies the satisfies our purpose.
So in some way, we can fully pick any distance metric based on this intuition. Yeah. So the second part of your question, when we can think about metric learning.
Actually, metric learning can be applied to almost any domain of problems, but there are some particular cases where metric learning really shines over other alternatives.
These are actually data scarce regimes, especially for labeled, if you are short for labeled data, you can still do a pretty good job with, for example, auto encoder, as we already discussed previously. And also, if you have rapid-changing distributions, it's again, very helpful.
And if you have, for example, a very, very high number of classes, again, metric learning can do a good job. Finally, metric learning is one of the best way to be able to actually increase the performance of machine learning models, even after training.
In normal deep learning training, there is no way to increase the performance of a model after training is complete. But in metric learning, this is quite possible.
For example, instead of just a training classification model to make a probability distribution over set of classes, we can train a metric learning model and encode samples with that model to store somewhere.
And during the inference, we can query that store to get more similar chain nearest neighbors and decide on the predicted category based on the majority of those chain nearest neighbors. This is called chain uncostication, in fact.
And in the practical side, on the practical side, you can continue to add new samples to that store without any need to retrain the model. And once you add new samples to that store your model performance will also increase.
And also, there is another use case, for example, a more recent approach by DeepMind. Up until now, the only way to make AI smarter is usually train a bigger and bigger language model. But in the most recent study by DeepMind, they augment language models with retrieval capability.
This means actually they encode and store a large collection of corpus in a Rector's database. And during the inference, they query this database to get most relevant sentences, most relevant text to the user input. And they combine them to feed to the model.
And with this technique, they can achieve the same performance as GPT3 with 25x less parameters. So it's a very efficient way of AI. So I'm also quite happy to see the direction of AI towards a more efficient one with metric learning as well. Yeah, yeah, it's fantastic.
And I think it's like a good impact on the planning, because I don't think we want to spend too much electricity on all power and training neural networks. Yeah, exactly.
And it also enables democratization of Deep Learning, because not everyone has the same resources as this large companies as Google, Facebook, and OpenAI. So I think it's also important for that reason as well. Yeah, that's fantastic. I mean, you gave quite a lot of detail on metric learning.
Of course, there is a ton to learn. And I even, I've seen like a book cited on one of the metric learning pages that I found through your awesome metric learning resource. And now that we touched a bit on where the AI is also going and then how to make it more efficient.
I also like to ask a question of why sort of this magical question which drills into your motivation as to why at all you are in this space, let's say deep learning and quadrant vector search and also specifically metric learning.
Can you a bit elaborate on the philosophy that drives you here? Yes, sure. Actually, what motivates me to work with metric learning is it's potential to approach many different problems very efficiently.
Before metric learning actually, you need to train very different models to solve very different problems. But with metric learning, you can train a single model and you can use the very same model to solve very different problems.
And this is also another fight that makes metric learning efficient. Actually, metric learning has a great potential, but you also need a great tool to put it into production. For example, upon to now there was no way to combine vector search with paid-out information.
Even if you make a connection, it was not for practical because you lose some information because you do not, you could not filter the tool systems of information at the same time. Quadrant is doing a great job by combining vector search with filterable paid-out information.
So it opens up quite a few new opportunities. For that one, you can filter your information based on if geographic, geographic place, for example, or another sparse category, numeric value or anything else while at the same time doing a vector search. So I think it's really exciting.
One of the most common problems in AI, you actually do the research, but you don't have the required tooling to make it practical in the real world.
So I think it's quite important to have such tools as Quadrant to achieve very different, very difficult and challenging problems very alien than efficiently. Yeah, absolutely. That's quite deep. Thank you so much for sharing this.
It also resonates with me because in many ways, you know, deep learning on one hand, maybe some people feel like it's kind of overhyped and there is so much material on the web.
On the other hand, when you start doing it yourself, you might end up, you know, going into down the rabbit hole and you don't know all the tools as you said. You don't know all the best practices.
And also, like, before we had vector databases, you couldn't actually, well, apply this, like, okay, you could of course build some nice demo and, you know, throw a web page and just ask somebody, okay, type something here and my neural network will do something.
But now, like, you could kind of scale this further and index your embeddings and see the end result of what you're doing through the retrieval process. So I think that opens up a lot of opportunities. So that's super cool. Yeah, exactly.
Actually, once we have such tooling, the domain is also improving more rapidly and also the improvements in the domain also foster development of such tools.
So I think it's like too far and it will be a metric learning will be in a better place in the future with this rapid developments in the domain. Yeah, absolutely.
And I was thinking, like, there's like a ton of material, I'm sure we'll have to digest, at least I will have to digest a lot of it and see how I can apply this. And thankfully, you have, you know, you have this awesome metric learning resource on GitHub that we can check out.
We'll make sure to leave it in the notes. And if if some of us want to kind of work with you or interact with you, can you make like a little announcement where we can join forces and kind of learn more about metric learning and maybe contribute to this field together with you? Yes, you're right.
I have several announcements maybe. First, beyond my resource and engineering site, I'm also a community guide guide and we have a difficult server at Quadrant where we hold paper reading class. We had the first one about contrastive laws and we will also have another fashion about triplet laws.
And I also wrote a wrote an intuitional triplet law post. Our approach will be like, after I will write such intuitional post about papers and then we will hold Q&A sessions in our discourse servers.
So everyone who is curious about metric learning can join the discourse server to enjoy this discussion. Apart from that one, beside my professional life, I'm recognized as a Google developer expert on machine learning, on the volunteering site, community site.
And this year at Google's thunder off call, I will serve as a transfer flow mentor for the transfer flow, the library take Python package, if a package for metric learning in the transfer flow ecosystem.
So university students and fresh graduates can apply to Google's thunder off call if they want to work with me in this effort and contribute to the field. That's fantastic.
 I think Google's thunder off code is an exciting place to be and there are so many projects but it's great to learn that you are leading the metric learning exploration there and I'm sure there will be interest towards it and I will make sure to also leave the relevant link in the show notes on this.
Yeah, thanks so much. Use of this this was a pleasure to discuss with you. I feel like I dipped some of my fingers in the water of metric learning. I think there is still a ton to learn and thanks so much for introducing it from so multiple angles. We've enjoyed this conversation.
Thank you Dimitriv again for this great opportunity. I hope the audience also enjoyed it as well and I hope it will be helpful for those who are interested in metric learning. Yeah, for sure. Thank you so much.
I learned a ton and I hope I'll also see you maybe doing some presentations or reading your blogs to learn more about it. Thanks so much. Thank you so much. Yeah, bye bye.