---
description: '<p>This episode on YouTube: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw">https://www.youtube.com/watch?v=5fafSkzKpfw</a></p><p></p><p><a
  target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=0s">00:00</a>
  Intro</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=114s">01:54</a>
  Reflection on the past year in AI</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=488s">08:08</a>
  Reader LLM (and RAG)</p><p><a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=756s">12:36</a> Does it
  need fine-tuning to a domain?</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=860s">14:20</a>
  How LLMs can lie</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=1052s">17:32</a>
  What if data isn''t perfect</p><p><a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=1281s">21:21</a> SWIRL''s
  secret sauce with Reader LLM</p><p><a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=1435s">23:55</a> Feedback
  loop</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=1574s">26:14</a>
  Some surprising client perspective</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=1877s">31:17</a>
  How Gen AI can change communication interfaces</p><p><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://www.youtube.com/watch?v=5fafSkzKpfw&amp;t=2051s">34:11</a>
  Call-out to the Community</p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20240515_120505_ab56f7a7d7ebadfb6bbd3486a4d2e7ad.png
pub_date: Wed, 15 May 2024 12:57:55 GMT
title: Sid Probstein, part II - Bring AI to company data with SWIRL
url: https://rss.com/podcasts/vector-podcast/1480271
whisper_segments: '[{"id": 0, "seek": 0, "start": 0.0, "end": 20.96, "text": " Hello
  there, this is Vector Podcast Season 3 and I''m super excited to be talking to",
  "tokens": [50364, 2425, 456, 11, 341, 307, 691, 20814, 29972, 16465, 805, 293, 286,
  478, 1687, 2919, 281, 312, 1417, 281, 51412], "temperature": 0.0, "avg_logprob":
  -0.339648785798446, "compression_ratio": 1.0, "no_speech_prob": 0.0807822197675705},
  {"id": 1, "seek": 14096, "start": 140.96, "end": 147.28, "text": " companies with
  thousands and thousands of users and thousands and thousands of systems that it''s
  been a", "tokens": [50364, 3431, 365, 5383, 293, 5383, 295, 5022, 293, 5383, 293,
  5383, 295, 3652, 300, 309, 311, 668, 257, 50680], "temperature": 0.0, "avg_logprob":
  -0.16850528520407135, "compression_ratio": 1.6653061224489796, "no_speech_prob":
  0.5573660135269165}, {"id": 2, "seek": 14096, "start": 147.28, "end": 155.12, "text":
  " time of inspiration and a little bit of continued nervousness about what it all
  means. Last March on the", "tokens": [50680, 565, 295, 10249, 293, 257, 707, 857,
  295, 7014, 6296, 1287, 466, 437, 309, 439, 1355, 13, 5264, 6129, 322, 264, 51072],
  "temperature": 0.0, "avg_logprob": -0.16850528520407135, "compression_ratio": 1.6653061224489796,
  "no_speech_prob": 0.5573660135269165}, {"id": 3, "seek": 14096, "start": 155.12,
  "end": 163.04000000000002, "text": " 15th actually was the 14th was Pi Day and that
  was the one year anniversary of GPT-4. What I''ve learned", "tokens": [51072, 2119,
  392, 767, 390, 264, 3499, 392, 390, 17741, 5226, 293, 300, 390, 264, 472, 1064,
  12962, 295, 26039, 51, 12, 19, 13, 708, 286, 600, 3264, 51468], "temperature": 0.0,
  "avg_logprob": -0.16850528520407135, "compression_ratio": 1.6653061224489796, "no_speech_prob":
  0.5573660135269165}, {"id": 4, "seek": 14096, "start": 163.04000000000002, "end":
  169.12, "text": " is that those large enterprises were again looked at GPT-4 and
  said this is going to change our", "tokens": [51468, 307, 300, 729, 2416, 29034,
  645, 797, 2956, 412, 26039, 51, 12, 19, 293, 848, 341, 307, 516, 281, 1319, 527,
  51772], "temperature": 0.0, "avg_logprob": -0.16850528520407135, "compression_ratio":
  1.6653061224489796, "no_speech_prob": 0.5573660135269165}, {"id": 5, "seek": 16912,
  "start": 169.12, "end": 177.76, "text": " business. This can really help everybody
  be an efficient expert and just slice through the", "tokens": [50364, 1606, 13,
  639, 393, 534, 854, 2201, 312, 364, 7148, 5844, 293, 445, 13153, 807, 264, 50796],
  "temperature": 0.0, "avg_logprob": -0.15865017067302356, "compression_ratio": 1.583673469387755,
  "no_speech_prob": 0.002471005544066429}, {"id": 6, "seek": 16912, "start": 177.76,
  "end": 184.4, "text": " current problems of silo data and inconsistent systems.
  But at the same time there were a lot of", "tokens": [50796, 2190, 2740, 295, 3425,
  78, 1412, 293, 36891, 3652, 13, 583, 412, 264, 912, 565, 456, 645, 257, 688, 295,
  51128], "temperature": 0.0, "avg_logprob": -0.15865017067302356, "compression_ratio":
  1.583673469387755, "no_speech_prob": 0.002471005544066429}, {"id": 7, "seek": 16912,
  "start": 184.4, "end": 189.04000000000002, "text": " fear about well are we exposing
  invaluable internal data to AI''s that are then going to be trained on", "tokens":
  [51128, 4240, 466, 731, 366, 321, 33178, 40367, 6920, 1412, 281, 7318, 311, 300,
  366, 550, 516, 281, 312, 8895, 322, 51360], "temperature": 0.0, "avg_logprob": -0.15865017067302356,
  "compression_ratio": 1.583673469387755, "no_speech_prob": 0.002471005544066429},
  {"id": 8, "seek": 16912, "start": 189.04000000000002, "end": 194.64000000000001,
  "text": " it? Is this going to be exposed? Lost? There have been many many lawsuits.
  So ultimately the large", "tokens": [51360, 309, 30, 1119, 341, 516, 281, 312, 9495,
  30, 23422, 30, 821, 362, 668, 867, 867, 39493, 13, 407, 6284, 264, 2416, 51640],
  "temperature": 0.0, "avg_logprob": -0.15865017067302356, "compression_ratio": 1.583673469387755,
  "no_speech_prob": 0.002471005544066429}, {"id": 9, "seek": 19464, "start": 194.64,
  "end": 200.0, "text": " enterprises did what they always do which is engage with
  it on their own terms and many of them", "tokens": [50364, 29034, 630, 437, 436,
  1009, 360, 597, 307, 4683, 365, 309, 322, 641, 1065, 2115, 293, 867, 295, 552, 50632],
  "temperature": 0.0, "avg_logprob": -0.1540858849235203, "compression_ratio": 1.5934959349593496,
  "no_speech_prob": 0.0038459908682852983}, {"id": 10, "seek": 19464, "start": 200.0,
  "end": 207.51999999999998, "text": " purchased download installed AI, generative
  AI''s and LLMs in their private clouds and we''re working", "tokens": [50632, 14734,
  5484, 8899, 7318, 11, 1337, 1166, 7318, 311, 293, 441, 43, 26386, 294, 641, 4551,
  12193, 293, 321, 434, 1364, 51008], "temperature": 0.0, "avg_logprob": -0.1540858849235203,
  "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0038459908682852983},
  {"id": 11, "seek": 19464, "start": 207.51999999999998, "end": 214.32, "text": "
  with one large company that did that and trained it with a bunch of what they called
  safe data. So", "tokens": [51008, 365, 472, 2416, 2237, 300, 630, 300, 293, 8895,
  309, 365, 257, 3840, 295, 437, 436, 1219, 3273, 1412, 13, 407, 51348], "temperature":
  0.0, "avg_logprob": -0.1540858849235203, "compression_ratio": 1.5934959349593496,
  "no_speech_prob": 0.0038459908682852983}, {"id": 12, "seek": 19464, "start": 214.32,
  "end": 221.6, "text": " annual reports and you employ a handbook and it''s very
  interesting to talk to but it can''t really", "tokens": [51348, 9784, 7122, 293,
  291, 3188, 257, 1011, 2939, 293, 309, 311, 588, 1880, 281, 751, 281, 457, 309, 393,
  380, 534, 51712], "temperature": 0.0, "avg_logprob": -0.1540858849235203, "compression_ratio":
  1.5934959349593496, "no_speech_prob": 0.0038459908682852983}, {"id": 13, "seek":
  22160, "start": 221.6, "end": 228.16, "text": " help a business person or somebody
  trying to answer a question in the supply chain group or in", "tokens": [50364,
  854, 257, 1606, 954, 420, 2618, 1382, 281, 1867, 257, 1168, 294, 264, 5847, 5021,
  1594, 420, 294, 50692], "temperature": 0.0, "avg_logprob": -0.1555552811458193,
  "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.002812143648043275},
  {"id": 14, "seek": 22160, "start": 228.16, "end": 233.68, "text": " the R&D group
  or in HR because this doesn''t have access to those systems and in those places",
  "tokens": [50692, 264, 497, 5, 35, 1594, 420, 294, 19460, 570, 341, 1177, 380, 362,
  2105, 281, 729, 3652, 293, 294, 729, 3190, 50968], "temperature": 0.0, "avg_logprob":
  -0.1555552811458193, "compression_ratio": 1.6864111498257839, "no_speech_prob":
  0.002812143648043275}, {"id": 15, "seek": 22160, "start": 234.72, "end": 239.2,
  "text": " you''ve ever worked there. You know when you onboard the first thing they
  do is your manager does", "tokens": [51020, 291, 600, 1562, 2732, 456, 13, 509,
  458, 562, 291, 24033, 264, 700, 551, 436, 360, 307, 428, 6598, 775, 51244], "temperature":
  0.0, "avg_logprob": -0.1555552811458193, "compression_ratio": 1.6864111498257839,
  "no_speech_prob": 0.002812143648043275}, {"id": 16, "seek": 22160, "start": 239.2,
  "end": 243.84, "text": " right is they open a bunch of tickets so that you could
  have access to systems. That''s hard enough.", "tokens": [51244, 558, 307, 436,
  1269, 257, 3840, 295, 12628, 370, 300, 291, 727, 362, 2105, 281, 3652, 13, 663,
  311, 1152, 1547, 13, 51476], "temperature": 0.0, "avg_logprob": -0.1555552811458193,
  "compression_ratio": 1.6864111498257839, "no_speech_prob": 0.002812143648043275},
  {"id": 17, "seek": 22160, "start": 244.79999999999998, "end": 251.04, "text": "
  So the reason that there''s been so in a way so little progress right lots of installs
  of AI but not", "tokens": [51524, 407, 264, 1778, 300, 456, 311, 668, 370, 294,
  257, 636, 370, 707, 4205, 558, 3195, 295, 3625, 82, 295, 7318, 457, 406, 51836],
  "temperature": 0.0, "avg_logprob": -0.1555552811458193, "compression_ratio": 1.6864111498257839,
  "no_speech_prob": 0.002812143648043275}, {"id": 18, "seek": 25104, "start": 251.04,
  "end": 255.84, "text": " that much real I''d love to hear from you some of the use
  cases out there. People are still trying to", "tokens": [50364, 300, 709, 957, 286,
  1116, 959, 281, 1568, 490, 291, 512, 295, 264, 764, 3331, 484, 456, 13, 3432, 366,
  920, 1382, 281, 50604], "temperature": 0.0, "avg_logprob": -0.12538250724037925,
  "compression_ratio": 1.7644444444444445, "no_speech_prob": 0.0005569803761318326},
  {"id": 19, "seek": 25104, "start": 255.84, "end": 262.48, "text": " say we''re still
  trying to get the data to the AI so that it can provide the benefit and what ultimately",
  "tokens": [50604, 584, 321, 434, 920, 1382, 281, 483, 264, 1412, 281, 264, 7318,
  370, 300, 309, 393, 2893, 264, 5121, 293, 437, 6284, 50936], "temperature": 0.0,
  "avg_logprob": -0.12538250724037925, "compression_ratio": 1.7644444444444445, "no_speech_prob":
  0.0005569803761318326}, {"id": 20, "seek": 25104, "start": 262.48, "end": 270.0,
  "text": " what what happened is this they''ve got the AI''s installed the first
  generation of AI architecture", "tokens": [50936, 437, 437, 2011, 307, 341, 436,
  600, 658, 264, 7318, 311, 8899, 264, 700, 5125, 295, 7318, 9482, 51312], "temperature":
  0.0, "avg_logprob": -0.12538250724037925, "compression_ratio": 1.7644444444444445,
  "no_speech_prob": 0.0005569803761318326}, {"id": 21, "seek": 25104, "start": 270.0,
  "end": 276.15999999999997, "text": " solution architectures is what I will refer
  to as a vendor driven put the data in architecture", "tokens": [51312, 3827, 6331,
  1303, 307, 437, 286, 486, 2864, 281, 382, 257, 24321, 9555, 829, 264, 1412, 294,
  9482, 51620], "temperature": 0.0, "avg_logprob": -0.12538250724037925, "compression_ratio":
  1.7644444444444445, "no_speech_prob": 0.0005569803761318326}, {"id": 22, "seek":
  27616, "start": 276.16, "end": 282.48, "text": " literally every product out there
  I don''t want to name them but they all say the first step", "tokens": [50364, 3736,
  633, 1674, 484, 456, 286, 500, 380, 528, 281, 1315, 552, 457, 436, 439, 584, 264,
  700, 1823, 50680], "temperature": 0.0, "avg_logprob": -0.0800493066961115, "compression_ratio":
  1.6784452296819787, "no_speech_prob": 0.0043405089527368546}, {"id": 23, "seek":
  27616, "start": 282.48, "end": 288.56, "text": " is put the data in again like for
  some people for many applications from POVs for testing it out", "tokens": [50680,
  307, 829, 264, 1412, 294, 797, 411, 337, 512, 561, 337, 867, 5821, 490, 22299, 53,
  82, 337, 4997, 309, 484, 50984], "temperature": 0.0, "avg_logprob": -0.0800493066961115,
  "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0043405089527368546},
  {"id": 24, "seek": 27616, "start": 288.56, "end": 292.96000000000004, "text": "
  that''s great and I''ve who hasn''t done it with a few PDFs right and got some interesting
  results", "tokens": [50984, 300, 311, 869, 293, 286, 600, 567, 6132, 380, 1096,
  309, 365, 257, 1326, 17752, 82, 558, 293, 658, 512, 1880, 3542, 51204], "temperature":
  0.0, "avg_logprob": -0.0800493066961115, "compression_ratio": 1.6784452296819787,
  "no_speech_prob": 0.0043405089527368546}, {"id": 25, "seek": 27616, "start": 293.6,
  "end": 300.64000000000004, "text": " but you can''t just take a copy of a departmental
  database and hand it over to a centralized", "tokens": [51236, 457, 291, 393, 380,
  445, 747, 257, 5055, 295, 257, 5882, 304, 8149, 293, 1011, 309, 670, 281, 257, 32395,
  51588], "temperature": 0.0, "avg_logprob": -0.0800493066961115, "compression_ratio":
  1.6784452296819787, "no_speech_prob": 0.0043405089527368546}, {"id": 26, "seek":
  27616, "start": 300.64000000000004, "end": 305.92, "text": " corporate database
  for training like that their rules in place to prevent that even more difficult",
  "tokens": [51588, 10896, 8149, 337, 3097, 411, 300, 641, 4474, 294, 1081, 281, 4871,
  300, 754, 544, 2252, 51852], "temperature": 0.0, "avg_logprob": -0.0800493066961115,
  "compression_ratio": 1.6784452296819787, "no_speech_prob": 0.0043405089527368546},
  {"id": 27, "seek": 30592, "start": 305.92, "end": 311.12, "text": " is the idea
  that you would send it outside your perimeter into someone else''s cloud right at
  another", "tokens": [50364, 307, 264, 1558, 300, 291, 576, 2845, 309, 2380, 428,
  32404, 666, 1580, 1646, 311, 4588, 558, 412, 1071, 50624], "temperature": 0.0, "avg_logprob":
  -0.10807621479034424, "compression_ratio": 1.651877133105802, "no_speech_prob":
  0.0006289776647463441}, {"id": 28, "seek": 30592, "start": 311.12, "end": 316.16,
  "text": " big manufacturing firm they have a 24 month waiting list to onboard a
  new SaaS product right they''d", "tokens": [50624, 955, 11096, 6174, 436, 362, 257,
  4022, 1618, 3806, 1329, 281, 24033, 257, 777, 49733, 1674, 558, 436, 1116, 50876],
  "temperature": 0.0, "avg_logprob": -0.10807621479034424, "compression_ratio": 1.651877133105802,
  "no_speech_prob": 0.0006289776647463441}, {"id": 29, "seek": 30592, "start": 316.16,
  "end": 321.92, "text": " like we have to put our security team on it so I believe
  it''s a very interesting time and ultimately", "tokens": [50876, 411, 321, 362,
  281, 829, 527, 3825, 1469, 322, 309, 370, 286, 1697, 309, 311, 257, 588, 1880, 565,
  293, 6284, 51164], "temperature": 0.0, "avg_logprob": -0.10807621479034424, "compression_ratio":
  1.651877133105802, "no_speech_prob": 0.0006289776647463441}, {"id": 30, "seek":
  30592, "start": 321.92, "end": 327.36, "text": " what happened is Swirl thought
  differently about the problem as you said we thought about it from", "tokens": [51164,
  437, 2011, 307, 3926, 1648, 1194, 7614, 466, 264, 1154, 382, 291, 848, 321, 1194,
  466, 309, 490, 51436], "temperature": 0.0, "avg_logprob": -0.10807621479034424,
  "compression_ratio": 1.651877133105802, "no_speech_prob": 0.0006289776647463441},
  {"id": 31, "seek": 30592, "start": 327.36, "end": 333.92, "text": " the search technology
  perspective why would we move all of the data instead move the", "tokens": [51436,
  264, 3164, 2899, 4585, 983, 576, 321, 1286, 439, 295, 264, 1412, 2602, 1286, 264,
  51764], "temperature": 0.0, "avg_logprob": -0.10807621479034424, "compression_ratio":
  1.651877133105802, "no_speech_prob": 0.0006289776647463441}, {"id": 32, "seek":
  33392, "start": 334.88, "end": 340.88, "text": " essentially take only the data
  that you didn''t give it to the AI at that moment and what Swirl does", "tokens":
  [50412, 4476, 747, 787, 264, 1412, 300, 291, 994, 380, 976, 309, 281, 264, 7318,
  412, 300, 1623, 293, 437, 3926, 1648, 775, 50712], "temperature": 0.0, "avg_logprob":
  -0.10267083834757847, "compression_ratio": 1.7563636363636363, "no_speech_prob":
  0.0016980391228571534}, {"id": 33, "seek": 33392, "start": 340.88, "end": 346.16,
  "text": " first to do that is we create a single pane of glass well the next thing
  I''ll mention is Swirl is", "tokens": [50712, 700, 281, 360, 300, 307, 321, 1884,
  257, 2167, 32605, 295, 4276, 731, 264, 958, 551, 286, 603, 2152, 307, 3926, 1648,
  307, 50976], "temperature": 0.0, "avg_logprob": -0.10267083834757847, "compression_ratio":
  1.7563636363636363, "no_speech_prob": 0.0016980391228571534}, {"id": 34, "seek":
  33392, "start": 346.16, "end": 351.04, "text": " software we are a software company
  and our software is typically deployed in the customers private", "tokens": [50976,
  4722, 321, 366, 257, 4722, 2237, 293, 527, 4722, 307, 5850, 17826, 294, 264, 4581,
  4551, 51220], "temperature": 0.0, "avg_logprob": -0.10267083834757847, "compression_ratio":
  1.7563636363636363, "no_speech_prob": 0.0016980391228571534}, {"id": 35, "seek":
  33392, "start": 351.04, "end": 357.20000000000005, "text": " cloud there''s we are
  happy to do hosting for POVs and for various applications but for larger", "tokens":
  [51220, 4588, 456, 311, 321, 366, 2055, 281, 360, 16058, 337, 22299, 53, 82, 293,
  337, 3683, 5821, 457, 337, 4833, 51528], "temperature": 0.0, "avg_logprob": -0.10267083834757847,
  "compression_ratio": 1.7563636363636363, "no_speech_prob": 0.0016980391228571534},
  {"id": 36, "seek": 33392, "start": 357.20000000000005, "end": 361.92, "text": "
  enterprise we don''t expect that to be the case once you deploy Swirl it integrates
  with your", "tokens": [51528, 14132, 321, 500, 380, 2066, 300, 281, 312, 264, 1389,
  1564, 291, 7274, 3926, 1648, 309, 3572, 1024, 365, 428, 51764], "temperature": 0.0,
  "avg_logprob": -0.10267083834757847, "compression_ratio": 1.7563636363636363, "no_speech_prob":
  0.0016980391228571534}, {"id": 37, "seek": 36192, "start": 362.24, "end": 367.6,
  "text": " single sign on systems such as Microsoft or Octa or ping federate others
  you can have cast whatever", "tokens": [50380, 2167, 1465, 322, 3652, 1270, 382,
  8116, 420, 6788, 64, 420, 26151, 38024, 473, 2357, 291, 393, 362, 4193, 2035, 50648],
  "temperature": 0.0, "avg_logprob": -0.13063430786132812, "compression_ratio": 1.652542372881356,
  "no_speech_prob": 0.002351338043808937}, {"id": 38, "seek": 36192, "start": 367.6,
  "end": 375.52000000000004, "text": " in there once it''s configured you send a question
  prompt or query search query to Swirl it brokers", "tokens": [50648, 294, 456, 1564,
  309, 311, 30538, 291, 2845, 257, 1168, 12391, 420, 14581, 3164, 14581, 281, 3926,
  1648, 309, 47549, 51044], "temperature": 0.0, "avg_logprob": -0.13063430786132812,
  "compression_ratio": 1.652542372881356, "no_speech_prob": 0.002351338043808937},
  {"id": 39, "seek": 36192, "start": 375.52000000000004, "end": 381.44, "text": "
  that query to all the sources that it''s authorized to do so and it does so on behalf
  of that user", "tokens": [51044, 300, 14581, 281, 439, 264, 7139, 300, 309, 311,
  28312, 281, 360, 370, 293, 309, 775, 370, 322, 9490, 295, 300, 4195, 51340], "temperature":
  0.0, "avg_logprob": -0.13063430786132812, "compression_ratio": 1.652542372881356,
  "no_speech_prob": 0.002351338043808937}, {"id": 40, "seek": 36192, "start": 382.56,
  "end": 388.48, "text": " so it''s not only is it safe compliance search using existing
  infrastructure but it''s personal", "tokens": [51396, 370, 309, 311, 406, 787, 307,
  309, 3273, 15882, 3164, 1228, 6741, 6896, 457, 309, 311, 2973, 51692], "temperature":
  0.0, "avg_logprob": -0.13063430786132812, "compression_ratio": 1.652542372881356,
  "no_speech_prob": 0.002351338043808937}, {"id": 41, "seek": 38848, "start": 389.36,
  "end": 395.20000000000005, "text": " the data the user or caller gets back is based
  on what that user can see so I use it all the time", "tokens": [50408, 264, 1412,
  264, 4195, 420, 48324, 2170, 646, 307, 2361, 322, 437, 300, 4195, 393, 536, 370,
  286, 764, 309, 439, 264, 565, 50700], "temperature": 0.0, "avg_logprob": -0.09666124396367905,
  "compression_ratio": 1.78515625, "no_speech_prob": 0.00889927800744772}, {"id":
  42, "seek": 38848, "start": 395.76, "end": 400.8, "text": " and it''s my email my
  outlook my calendar my LinkedIn whatever right it''s my view", "tokens": [50728,
  293, 309, 311, 452, 3796, 452, 26650, 452, 12183, 452, 20657, 2035, 558, 309, 311,
  452, 1910, 50980], "temperature": 0.0, "avg_logprob": -0.09666124396367905, "compression_ratio":
  1.78515625, "no_speech_prob": 0.00889927800744772}, {"id": 43, "seek": 38848, "start":
  401.92, "end": 406.72, "text": " we actually love the idea that we should present
  the data to the user so you get that single", "tokens": [51036, 321, 767, 959, 264,
  1558, 300, 321, 820, 1974, 264, 1412, 281, 264, 4195, 370, 291, 483, 300, 2167,
  51276], "temperature": 0.0, "avg_logprob": -0.09666124396367905, "compression_ratio":
  1.78515625, "no_speech_prob": 0.00889927800744772}, {"id": 44, "seek": 38848, "start":
  406.72, "end": 410.16, "text": " pane of glass and actually you can decide what
  to do with it you can say I don''t want this", "tokens": [51276, 32605, 295, 4276,
  293, 767, 291, 393, 4536, 437, 281, 360, 365, 309, 291, 393, 584, 286, 500, 380,
  528, 341, 51448], "temperature": 0.0, "avg_logprob": -0.09666124396367905, "compression_ratio":
  1.78515625, "no_speech_prob": 0.00889927800744772}, {"id": 45, "seek": 38848, "start":
  410.16, "end": 415.68, "text": " source or whatever you can make adjustments but
  ultimately we then execute rag we have our own", "tokens": [51448, 4009, 420, 2035,
  291, 393, 652, 18624, 457, 6284, 321, 550, 14483, 17539, 321, 362, 527, 1065, 51724],
  "temperature": 0.0, "avg_logprob": -0.09666124396367905, "compression_ratio": 1.78515625,
  "no_speech_prob": 0.00889927800744772}, {"id": 46, "seek": 41568, "start": 416.40000000000003,
  "end": 422.16, "text": " excellent high quality rag better than many in particular
  it seeks highly relevant", "tokens": [50400, 7103, 1090, 3125, 17539, 1101, 813,
  867, 294, 1729, 309, 28840, 5405, 7340, 50688], "temperature": 0.0, "avg_logprob":
  -0.12839764887743657, "compression_ratio": 1.6867924528301887, "no_speech_prob":
  0.0017235697014257312}, {"id": 47, "seek": 41568, "start": 422.16, "end": 426.64,
  "text": " passages from all of the documents we can fetch the documents and authenticate
  on the fly", "tokens": [50688, 31589, 490, 439, 295, 264, 8512, 321, 393, 23673,
  264, 8512, 293, 9214, 8700, 322, 264, 3603, 50912], "temperature": 0.0, "avg_logprob":
  -0.12839764887743657, "compression_ratio": 1.6867924528301887, "no_speech_prob":
  0.0017235697014257312}, {"id": 48, "seek": 41568, "start": 426.64, "end": 431.28000000000003,
  "text": " as to do that um bind those to a prompt we have our own prompt engineering
  you can", "tokens": [50912, 382, 281, 360, 300, 1105, 14786, 729, 281, 257, 12391,
  321, 362, 527, 1065, 12391, 7043, 291, 393, 51144], "temperature": 0.0, "avg_logprob":
  -0.12839764887743657, "compression_ratio": 1.6867924528301887, "no_speech_prob":
  0.0017235697014257312}, {"id": 49, "seek": 41568, "start": 431.28000000000003, "end":
  437.36, "text": " uh override it and then do the rag against a huge list of AI providers
  actually we support more than", "tokens": [51144, 2232, 42321, 309, 293, 550, 360,
  264, 17539, 1970, 257, 2603, 1329, 295, 7318, 11330, 767, 321, 1406, 544, 813, 51448],
  "temperature": 0.0, "avg_logprob": -0.12839764887743657, "compression_ratio": 1.6867924528301887,
  "no_speech_prob": 0.0017235697014257312}, {"id": 50, "seek": 41568, "start": 437.36,
  "end": 442.72, "text": " 20 today including most of the ones we see out there open
  AI open AI and azure bedrock and", "tokens": [51448, 945, 965, 3009, 881, 295, 264,
  2306, 321, 536, 484, 456, 1269, 7318, 1269, 7318, 293, 7883, 540, 2901, 17799, 293,
  51716], "temperature": 0.0, "avg_logprob": -0.12839764887743657, "compression_ratio":
  1.6867924528301887, "no_speech_prob": 0.0017235697014257312}, {"id": 51, "seek":
  44272, "start": 442.72, "end": 451.04, "text": " prop at google mistral uh co here
  etc and in all cases no code should be required you configure", "tokens": [50364,
  2365, 412, 20742, 3544, 2155, 2232, 598, 510, 5183, 293, 294, 439, 3331, 572, 3089,
  820, 312, 4739, 291, 22162, 50780], "temperature": 0.0, "avg_logprob": -0.15557110526344992,
  "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.001988664735108614},
  {"id": 52, "seek": 44272, "start": 451.04, "end": 455.68, "text": " an existing
  connector more than likely you''re putting in just endpoint information and authentication",
  "tokens": [50780, 364, 6741, 19127, 544, 813, 3700, 291, 434, 3372, 294, 445, 35795,
  1589, 293, 26643, 51012], "temperature": 0.0, "avg_logprob": -0.15557110526344992,
  "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.001988664735108614},
  {"id": 53, "seek": 44272, "start": 455.68, "end": 461.28000000000003, "text": "
  tokens and then swirl again does that broker and creates that pane of glass and
  execute rag you can", "tokens": [51012, 22667, 293, 550, 30310, 797, 775, 300, 26502,
  293, 7829, 300, 32605, 295, 4276, 293, 14483, 17539, 291, 393, 51292], "temperature":
  0.0, "avg_logprob": -0.15557110526344992, "compression_ratio": 1.8432835820895523,
  "no_speech_prob": 0.001988664735108614}, {"id": 54, "seek": 44272, "start": 461.28000000000003,
  "end": 466.88000000000005, "text": " also use swirl just for the R if you have your
  own rag right you can get the result list and do", "tokens": [51292, 611, 764, 30310,
  445, 337, 264, 497, 498, 291, 362, 428, 1065, 17539, 558, 291, 393, 483, 264, 1874,
  1329, 293, 360, 51572], "temperature": 0.0, "avg_logprob": -0.15557110526344992,
  "compression_ratio": 1.8432835820895523, "no_speech_prob": 0.001988664735108614},
  {"id": 55, "seek": 44272, "start": 466.88000000000005, "end": 472.0, "text": " your
  fetching or you can hook after you''ve got the swirl has the fetched results and
  you can operate", "tokens": [51572, 428, 23673, 278, 420, 291, 393, 6328, 934, 291,
  600, 658, 264, 30310, 575, 264, 23673, 292, 3542, 293, 291, 393, 9651, 51828], "temperature":
  0.0, "avg_logprob": -0.15557110526344992, "compression_ratio": 1.8432835820895523,
  "no_speech_prob": 0.001988664735108614}, {"id": 56, "seek": 47200, "start": 472.0,
  "end": 478.64, "text": " that on just the full documents the key to this i love
  that you asked is the reader lllm we have been", "tokens": [50364, 300, 322, 445,
  264, 1577, 8512, 264, 2141, 281, 341, 741, 959, 300, 291, 2351, 307, 264, 15149,
  287, 285, 76, 321, 362, 668, 50696], "temperature": 0.0, "avg_logprob": -0.1298468278186156,
  "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0022196744102984667},
  {"id": 57, "seek": 47200, "start": 478.64, "end": 484.72, "text": " really heads
  down working on the reader llm um i''ve actually been asking people if they have
  heard", "tokens": [50696, 534, 8050, 760, 1364, 322, 264, 15149, 287, 75, 76, 1105,
  741, 600, 767, 668, 3365, 561, 498, 436, 362, 2198, 51000], "temperature": 0.0,
  "avg_logprob": -0.1298468278186156, "compression_ratio": 1.7731481481481481, "no_speech_prob":
  0.0022196744102984667}, {"id": 58, "seek": 47200, "start": 484.72, "end": 490.72,
  "text": " the term before and many haven''t uh i don''t know what what your take
  is on on reader llm these days", "tokens": [51000, 264, 1433, 949, 293, 867, 2378,
  380, 2232, 741, 500, 380, 458, 437, 437, 428, 747, 307, 322, 322, 15149, 287, 75,
  76, 613, 1708, 51300], "temperature": 0.0, "avg_logprob": -0.1298468278186156, "compression_ratio":
  1.7731481481481481, "no_speech_prob": 0.0022196744102984667}, {"id": 59, "seek":
  47200, "start": 492.0, "end": 496.96, "text": " oh yeah i''m still catching up really
  i mean the way i see it and i''m still kind of", "tokens": [51364, 1954, 1338, 741,
  478, 920, 16124, 493, 534, 741, 914, 264, 636, 741, 536, 309, 293, 741, 478, 920,
  733, 295, 51612], "temperature": 0.0, "avg_logprob": -0.1298468278186156, "compression_ratio":
  1.7731481481481481, "no_speech_prob": 0.0022196744102984667}, {"id": 60, "seek":
  49696, "start": 497.03999999999996, "end": 503.68, "text": " plowing through rag
  itself right so you you said what is my take on on how easy it is to", "tokens":
  [50368, 499, 9637, 807, 17539, 2564, 558, 370, 291, 291, 848, 437, 307, 452, 747,
  322, 322, 577, 1858, 309, 307, 281, 50700], "temperature": 0.0, "avg_logprob": -0.15840950277116564,
  "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.003570165252313018},
  {"id": 61, "seek": 49696, "start": 503.68, "end": 511.59999999999997, "text": "
  on board to the say i models and so on i i have a sense that people are aware of
  this because", "tokens": [50700, 322, 3150, 281, 264, 584, 741, 5245, 293, 370,
  322, 741, 741, 362, 257, 2020, 300, 561, 366, 3650, 295, 341, 570, 51096], "temperature":
  0.0, "avg_logprob": -0.15840950277116564, "compression_ratio": 1.6057142857142856,
  "no_speech_prob": 0.003570165252313018}, {"id": 62, "seek": 49696, "start": 511.59999999999997,
  "end": 519.1999999999999, "text": " it''s so easy to access through chat chat gpt
  and similar tools but then when it comes to deploying", "tokens": [51096, 309, 311,
  370, 1858, 281, 2105, 807, 5081, 5081, 290, 662, 293, 2531, 3873, 457, 550, 562,
  309, 1487, 281, 34198, 51476], "temperature": 0.0, "avg_logprob": -0.15840950277116564,
  "compression_ratio": 1.6057142857142856, "no_speech_prob": 0.003570165252313018},
  {"id": 63, "seek": 51920, "start": 519.2800000000001, "end": 527.44, "text": " these
  things i don''t think that it''s as easy right so because you you have to go through
  a list of", "tokens": [50368, 613, 721, 741, 500, 380, 519, 300, 309, 311, 382,
  1858, 558, 370, 570, 291, 291, 362, 281, 352, 807, 257, 1329, 295, 50776], "temperature":
  0.0, "avg_logprob": -0.13676402443333677, "compression_ratio": 1.7798165137614679,
  "no_speech_prob": 0.007207722403109074}, {"id": 64, "seek": 51920, "start": 527.44,
  "end": 532.32, "text": " models you need to figure out which one to pick and and
  and and hence you need to be a data scientist", "tokens": [50776, 5245, 291, 643,
  281, 2573, 484, 597, 472, 281, 1888, 293, 293, 293, 293, 16678, 291, 643, 281, 312,
  257, 1412, 12662, 51020], "temperature": 0.0, "avg_logprob": -0.13676402443333677,
  "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.007207722403109074},
  {"id": 65, "seek": 51920, "start": 532.32, "end": 539.0400000000001, "text": " right
  at that point or ml practitioner or whatever um and it''s not and it''s like the
  web is", "tokens": [51020, 558, 412, 300, 935, 420, 23271, 32125, 420, 2035, 1105,
  293, 309, 311, 406, 293, 309, 311, 411, 264, 3670, 307, 51356], "temperature": 0.0,
  "avg_logprob": -0.13676402443333677, "compression_ratio": 1.7798165137614679, "no_speech_prob":
  0.007207722403109074}, {"id": 66, "seek": 51920, "start": 539.6800000000001, "end":
  548.08, "text": " exploding with so many cheap advice you know use these use that
  but then as you go through that", "tokens": [51388, 35175, 365, 370, 867, 7084,
  5192, 291, 458, 764, 613, 764, 300, 457, 550, 382, 291, 352, 807, 300, 51808], "temperature":
  0.0, "avg_logprob": -0.13676402443333677, "compression_ratio": 1.7798165137614679,
  "no_speech_prob": 0.007207722403109074}, {"id": 67, "seek": 54808, "start": 548.08,
  "end": 553.36, "text": " process you realize that none of those models work and
  so you need to do something okay the", "tokens": [50364, 1399, 291, 4325, 300, 6022,
  295, 729, 5245, 589, 293, 370, 291, 643, 281, 360, 746, 1392, 264, 50628], "temperature":
  0.0, "avg_logprob": -0.15698103471235794, "compression_ratio": 1.7455357142857142,
  "no_speech_prob": 0.0012613519793376327}, {"id": 68, "seek": 54808, "start": 553.36,
  "end": 559.76, "text": " risk rag but setting up rag means that you need to bring
  in an effective database that you haven''t", "tokens": [50628, 3148, 17539, 457,
  3287, 493, 17539, 1355, 300, 291, 643, 281, 1565, 294, 364, 4942, 8149, 300, 291,
  2378, 380, 50948], "temperature": 0.0, "avg_logprob": -0.15698103471235794, "compression_ratio":
  1.7455357142857142, "no_speech_prob": 0.0012613519793376327}, {"id": 69, "seek":
  54808, "start": 559.76, "end": 568.64, "text": " seen before and things like this
  right so it''s yeah so i love that so just speaking of misinformation", "tokens":
  [50948, 1612, 949, 293, 721, 411, 341, 558, 370, 309, 311, 1338, 370, 741, 959,
  300, 370, 445, 4124, 295, 34238, 51392], "temperature": 0.0, "avg_logprob": -0.15698103471235794,
  "compression_ratio": 1.7455357142857142, "no_speech_prob": 0.0012613519793376327},
  {"id": 70, "seek": 54808, "start": 568.64, "end": 574.4000000000001, "text": " right
  i think you''re absolutely right there''s so much um confusing stuff out there you
  do not need", "tokens": [51392, 558, 741, 519, 291, 434, 3122, 558, 456, 311, 370,
  709, 1105, 13181, 1507, 484, 456, 291, 360, 406, 643, 51680], "temperature": 0.0,
  "avg_logprob": -0.15698103471235794, "compression_ratio": 1.7455357142857142, "no_speech_prob":
  0.0012613519793376327}, {"id": 71, "seek": 57440, "start": 574.4, "end": 579.52,
  "text": " a vector database to rag you never did it''s it''s a it''s a vendor thing
  that i totally understand", "tokens": [50364, 257, 8062, 8149, 281, 17539, 291,
  1128, 630, 309, 311, 309, 311, 257, 309, 311, 257, 24321, 551, 300, 741, 3879, 1223,
  50620], "temperature": 0.0, "avg_logprob": -0.15462010527310305, "compression_ratio":
  1.8513931888544892, "no_speech_prob": 0.003457391634583473}, {"id": 72, "seek":
  57440, "start": 579.52, "end": 583.6, "text": " they''re charging per gigabyte or
  whatever so they say you have to have it to rag uh there''s an excellent", "tokens":
  [50620, 436, 434, 11379, 680, 8741, 34529, 420, 2035, 370, 436, 584, 291, 362, 281,
  362, 309, 281, 17539, 2232, 456, 311, 364, 7103, 50824], "temperature": 0.0, "avg_logprob":
  -0.15462010527310305, "compression_ratio": 1.8513931888544892, "no_speech_prob":
  0.003457391634583473}, {"id": 73, "seek": 57440, "start": 583.6, "end": 588.48,
  "text": " study by zet hub and actually simpson garf ankles and advisor to swore
  all you may have heard that name", "tokens": [50824, 2979, 538, 710, 302, 11838,
  293, 767, 1034, 16962, 3691, 69, 40962, 293, 19161, 281, 1693, 418, 439, 291, 815,
  362, 2198, 300, 1315, 51068], "temperature": 0.0, "avg_logprob": -0.15462010527310305,
  "compression_ratio": 1.8513931888544892, "no_speech_prob": 0.003457391634583473},
  {"id": 74, "seek": 57440, "start": 588.48, "end": 593.36, "text": " incredible tech
  writer um he recently wrote a study a survey or a summary i should say of the zet",
  "tokens": [51068, 4651, 7553, 9936, 1105, 415, 3938, 4114, 257, 2979, 257, 8984,
  420, 257, 12691, 741, 820, 584, 295, 264, 710, 302, 51312], "temperature": 0.0,
  "avg_logprob": -0.15462010527310305, "compression_ratio": 1.8513931888544892, "no_speech_prob":
  0.003457391634583473}, {"id": 75, "seek": 57440, "start": 593.36, "end": 598.72,
  "text": " hub study the zet hub study shows that you do not need to vectorize your
  data to get high quality", "tokens": [51312, 11838, 2979, 264, 710, 302, 11838,
  2979, 3110, 300, 291, 360, 406, 643, 281, 8062, 1125, 428, 1412, 281, 483, 1090,
  3125, 51580], "temperature": 0.0, "avg_logprob": -0.15462010527310305, "compression_ratio":
  1.8513931888544892, "no_speech_prob": 0.003457391634583473}, {"id": 76, "seek":
  57440, "start": 598.72, "end": 604.16, "text": " results instead you just increase
  the number of results you get from a so-called naive nonvector", "tokens": [51580,
  3542, 2602, 291, 445, 3488, 264, 1230, 295, 3542, 291, 483, 490, 257, 370, 12, 11880,
  29052, 2107, 303, 1672, 51852], "temperature": 0.0, "avg_logprob": -0.15462010527310305,
  "compression_ratio": 1.8513931888544892, "no_speech_prob": 0.003457391634583473},
  {"id": 77, "seek": 60416, "start": 604.16, "end": 610.0, "text": " search engine
  or database and re-rank using vectors that''s exactly what swore all this we vectorize",
  "tokens": [50364, 3164, 2848, 420, 8149, 293, 319, 12, 20479, 1228, 18875, 300,
  311, 2293, 437, 1693, 418, 439, 341, 321, 8062, 1125, 50656], "temperature": 0.0,
  "avg_logprob": -0.15329949468628973, "compression_ratio": 1.8996138996138996, "no_speech_prob":
  0.0006137712625786662}, {"id": 78, "seek": 60416, "start": 610.0, "end": 616.56,
  "text": " the result set snippets we vectorize the full text of the documents we
  vectorize the query the prompt", "tokens": [50656, 264, 1874, 992, 35623, 1385,
  321, 8062, 1125, 264, 1577, 2487, 295, 264, 8512, 321, 8062, 1125, 264, 14581, 264,
  12391, 50984], "temperature": 0.0, "avg_logprob": -0.15329949468628973, "compression_ratio":
  1.8996138996138996, "no_speech_prob": 0.0006137712625786662}, {"id": 79, "seek":
  60416, "start": 616.56, "end": 621.68, "text": " whatever it is right and our reader
  llem is responsible for a complex similarity re-ranking", "tokens": [50984, 2035,
  309, 307, 558, 293, 527, 15149, 287, 10386, 307, 6250, 337, 257, 3997, 32194, 319,
  12, 20479, 278, 51240], "temperature": 0.0, "avg_logprob": -0.15329949468628973,
  "compression_ratio": 1.8996138996138996, "no_speech_prob": 0.0006137712625786662},
  {"id": 80, "seek": 60416, "start": 622.4, "end": 627.1999999999999, "text": " you
  can actually plug the a your choice of embeddings into our reader llem embeddings
  are actually", "tokens": [51276, 291, 393, 767, 5452, 264, 257, 428, 3922, 295,
  12240, 29432, 666, 527, 15149, 287, 10386, 12240, 29432, 366, 767, 51516], "temperature":
  0.0, "avg_logprob": -0.15329949468628973, "compression_ratio": 1.8996138996138996,
  "no_speech_prob": 0.0006137712625786662}, {"id": 81, "seek": 60416, "start": 627.1999999999999,
  "end": 632.88, "text": " just a feature one one of the many things that llem''s
  do so you can change that but the reader llem", "tokens": [51516, 445, 257, 4111,
  472, 472, 295, 264, 867, 721, 300, 287, 10386, 311, 360, 370, 291, 393, 1319, 300,
  457, 264, 15149, 287, 10386, 51800], "temperature": 0.0, "avg_logprob": -0.15329949468628973,
  "compression_ratio": 1.8996138996138996, "no_speech_prob": 0.0006137712625786662},
  {"id": 82, "seek": 63288, "start": 633.6, "end": 639.28, "text": " here''s really
  the core of it it''s the middle layers of the generative AI llem without the", "tokens":
  [50400, 510, 311, 534, 264, 4965, 295, 309, 309, 311, 264, 2808, 7914, 295, 264,
  1337, 1166, 7318, 287, 10386, 1553, 264, 50684], "temperature": 0.0, "avg_logprob":
  -0.11676945405847886, "compression_ratio": 1.7004405286343611, "no_speech_prob":
  0.002972096437588334}, {"id": 83, "seek": 63288, "start": 640.0, "end": 646.32,
  "text": " you know um text generation and text interpretation part that''s not there
  at all instead you use it", "tokens": [50720, 291, 458, 1105, 2487, 5125, 293, 2487,
  14174, 644, 300, 311, 406, 456, 412, 439, 2602, 291, 764, 309, 51036], "temperature":
  0.0, "avg_logprob": -0.11676945405847886, "compression_ratio": 1.7004405286343611,
  "no_speech_prob": 0.002972096437588334}, {"id": 84, "seek": 63288, "start": 646.32,
  "end": 651.76, "text": " to determine the similarity right cosine or they''re many
  different algorithms but ultimately you''re", "tokens": [51036, 281, 6997, 264,
  32194, 558, 23565, 420, 436, 434, 867, 819, 14642, 457, 6284, 291, 434, 51308],
  "temperature": 0.0, "avg_logprob": -0.11676945405847886, "compression_ratio": 1.7004405286343611,
  "no_speech_prob": 0.002972096437588334}, {"id": 85, "seek": 63288, "start": 651.76,
  "end": 656.64, "text": " taking some algorithm like that and you''re using embeddings
  plus the reader llem''s own knowledge", "tokens": [51308, 1940, 512, 9284, 411,
  300, 293, 291, 434, 1228, 12240, 29432, 1804, 264, 15149, 287, 10386, 311, 1065,
  3601, 51552], "temperature": 0.0, "avg_logprob": -0.11676945405847886, "compression_ratio":
  1.7004405286343611, "no_speech_prob": 0.002972096437588334}, {"id": 86, "seek":
  65664, "start": 657.28, "end": 663.6, "text": " to say how similar is the query
  or prompt or part of it to the response that i got or find the", "tokens": [50396,
  281, 584, 577, 2531, 307, 264, 14581, 420, 12391, 420, 644, 295, 309, 281, 264,
  4134, 300, 741, 658, 420, 915, 264, 50712], "temperature": 0.0, "avg_logprob": -0.13489681285816235,
  "compression_ratio": 1.673728813559322, "no_speech_prob": 0.003514697542414069},
  {"id": 87, "seek": 65664, "start": 663.6, "end": 668.88, "text": " most relevant
  passage in a document because you''re absolutely right there are tools like langshane",
  "tokens": [50712, 881, 7340, 11497, 294, 257, 4166, 570, 291, 434, 3122, 558, 456,
  366, 3873, 411, 2265, 2716, 1929, 50976], "temperature": 0.0, "avg_logprob": -0.13489681285816235,
  "compression_ratio": 1.673728813559322, "no_speech_prob": 0.003514697542414069},
  {"id": 88, "seek": 65664, "start": 668.88, "end": 673.76, "text": " out there as
  in one example which give you lots of interesting tooling right but it''s still
  on you", "tokens": [50976, 484, 456, 382, 294, 472, 1365, 597, 976, 291, 3195, 295,
  1880, 46593, 558, 457, 309, 311, 920, 322, 291, 51220], "temperature": 0.0, "avg_logprob":
  -0.13489681285816235, "compression_ratio": 1.673728813559322, "no_speech_prob":
  0.003514697542414069}, {"id": 89, "seek": 65664, "start": 673.76, "end": 681.6,
  "text": " the developer i actually had chat tpt generate me a pipeline just as a
  demo and the biggest problem is", "tokens": [51220, 264, 10754, 741, 767, 632, 5081,
  256, 662, 8460, 385, 257, 15517, 445, 382, 257, 10723, 293, 264, 3880, 1154, 307,
  51612], "temperature": 0.0, "avg_logprob": -0.13489681285816235, "compression_ratio":
  1.673728813559322, "no_speech_prob": 0.003514697542414069}, {"id": 90, "seek": 68160,
  "start": 681.6, "end": 687.36, "text": " it generated a function that i have to
  fill in which is called select documents that''s really hard", "tokens": [50364,
  309, 10833, 257, 2445, 300, 741, 362, 281, 2836, 294, 597, 307, 1219, 3048, 8512,
  300, 311, 534, 1152, 50652], "temperature": 0.0, "avg_logprob": -0.12007498467105558,
  "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00027063299785368145},
  {"id": 91, "seek": 68160, "start": 687.9200000000001, "end": 692.72, "text": " and
  ultimately like you''re basically just providing the pipeline to move the data once
  again", "tokens": [50680, 293, 6284, 411, 291, 434, 1936, 445, 6530, 264, 15517,
  281, 1286, 264, 1412, 1564, 797, 50920], "temperature": 0.0, "avg_logprob": -0.12007498467105558,
  "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.00027063299785368145},
  {"id": 92, "seek": 68160, "start": 693.36, "end": 700.32, "text": " but it''s the
  reader llem in swirl is all about re-ranking and finding the best passages so that
  you", "tokens": [50952, 457, 309, 311, 264, 15149, 287, 10386, 294, 30310, 307,
  439, 466, 319, 12, 20479, 278, 293, 5006, 264, 1151, 31589, 370, 300, 291, 51300],
  "temperature": 0.0, "avg_logprob": -0.12007498467105558, "compression_ratio": 1.7004405286343611,
  "no_speech_prob": 0.00027063299785368145}, {"id": 93, "seek": 68160, "start": 700.32,
  "end": 705.9200000000001, "text": " are not sending a hundred pdf of which one paragraph
  is relevant you are sending the paragraph", "tokens": [51300, 366, 406, 7750, 257,
  3262, 280, 45953, 295, 597, 472, 18865, 307, 7340, 291, 366, 7750, 264, 18865, 51580],
  "temperature": 0.0, "avg_logprob": -0.12007498467105558, "compression_ratio": 1.7004405286343611,
  "no_speech_prob": 0.00027063299785368145}, {"id": 94, "seek": 70592, "start": 706.56,
  "end": 711.92, "text": " that way you can put a lot more data and you can also not
  blow out your token limits right assuming", "tokens": [50396, 300, 636, 291, 393,
  829, 257, 688, 544, 1412, 293, 291, 393, 611, 406, 6327, 484, 428, 14862, 10406,
  558, 11926, 50664], "temperature": 0.0, "avg_logprob": -0.14703163023917906, "compression_ratio":
  1.7720588235294117, "no_speech_prob": 0.0033860597759485245}, {"id": 95, "seek":
  70592, "start": 711.92, "end": 716.56, "text": " you have such a thing if you''re
  on prem but that''s what that''s the reader lm i''ll say this", "tokens": [50664,
  291, 362, 1270, 257, 551, 498, 291, 434, 322, 5624, 457, 300, 311, 437, 300, 311,
  264, 15149, 287, 76, 741, 603, 584, 341, 50896], "temperature": 0.0, "avg_logprob":
  -0.14703163023917906, "compression_ratio": 1.7720588235294117, "no_speech_prob":
  0.0033860597759485245}, {"id": 96, "seek": 70592, "start": 716.56, "end": 723.52,
  "text": " their reader lm are the unsung heroes of especially search but also a
  rag when you''re looking at", "tokens": [50896, 641, 15149, 287, 76, 366, 264, 2693,
  1063, 12332, 295, 2318, 3164, 457, 611, 257, 17539, 562, 291, 434, 1237, 412, 51244],
  "temperature": 0.0, "avg_logprob": -0.14703163023917906, "compression_ratio": 1.7720588235294117,
  "no_speech_prob": 0.0033860597759485245}, {"id": 97, "seek": 70592, "start": 723.52,
  "end": 728.8, "text": " i would say bing or or chat gpt and you ask it a question
  and it goes and fetches documents from", "tokens": [51244, 741, 576, 584, 272, 278,
  420, 420, 5081, 290, 662, 293, 291, 1029, 309, 257, 1168, 293, 309, 1709, 293, 15136,
  3781, 8512, 490, 51508], "temperature": 0.0, "avg_logprob": -0.14703163023917906,
  "compression_ratio": 1.7720588235294117, "no_speech_prob": 0.0033860597759485245},
  {"id": 98, "seek": 70592, "start": 728.8, "end": 734.9599999999999, "text": " the
  web it''s almost certainly using a reader llm to determine which pages are best
  and to be fair", "tokens": [51508, 264, 3670, 309, 311, 1920, 3297, 1228, 257, 15149,
  287, 75, 76, 281, 6997, 597, 7183, 366, 1151, 293, 281, 312, 3143, 51816], "temperature":
  0.0, "avg_logprob": -0.14703163023917906, "compression_ratio": 1.7720588235294117,
  "no_speech_prob": 0.0033860597759485245}, {"id": 99, "seek": 73496, "start": 735.36,
  "end": 739.6, "text": " being in google have incredible knowledge of that already
  so it''s not like it''s that hard but then", "tokens": [50384, 885, 294, 20742,
  362, 4651, 3601, 295, 300, 1217, 370, 309, 311, 406, 411, 309, 311, 300, 1152, 457,
  550, 50596], "temperature": 0.0, "avg_logprob": -0.11065021861683239, "compression_ratio":
  1.8, "no_speech_prob": 0.00176139734685421}, {"id": 100, "seek": 73496, "start":
  739.6, "end": 743.76, "text": " they''re almost certainly reading the most relevant
  passages right they''re not just passing the whole web", "tokens": [50596, 436,
  434, 1920, 3297, 3760, 264, 881, 7340, 31589, 558, 436, 434, 406, 445, 8437, 264,
  1379, 3670, 50804], "temperature": 0.0, "avg_logprob": -0.11065021861683239, "compression_ratio":
  1.8, "no_speech_prob": 0.00176139734685421}, {"id": 101, "seek": 73496, "start":
  743.76, "end": 750.72, "text": " page in so reader lm''s are a thing they''re definitely
  becoming more and more prevalent and they", "tokens": [50804, 3028, 294, 370, 15149,
  287, 76, 311, 366, 257, 551, 436, 434, 2138, 5617, 544, 293, 544, 30652, 293, 436,
  51152], "temperature": 0.0, "avg_logprob": -0.11065021861683239, "compression_ratio":
  1.8, "no_speech_prob": 0.00176139734685421}, {"id": 102, "seek": 73496, "start":
  750.72, "end": 755.76, "text": " provide a critical non hallucinating step to help
  find the best results so the user doesn''t have to", "tokens": [51152, 2893, 257,
  4924, 2107, 35212, 8205, 1823, 281, 854, 915, 264, 1151, 3542, 370, 264, 4195, 1177,
  380, 362, 281, 51404], "temperature": 0.0, "avg_logprob": -0.11065021861683239,
  "compression_ratio": 1.8, "no_speech_prob": 0.00176139734685421}, {"id": 103, "seek":
  73496, "start": 756.64, "end": 764.88, "text": " and that''s very interesting and
  and how let''s say if you plug into a companies network right so", "tokens": [51448,
  293, 300, 311, 588, 1880, 293, 293, 577, 718, 311, 584, 498, 291, 5452, 666, 257,
  3431, 3209, 558, 370, 51860], "temperature": 0.0, "avg_logprob": -0.11065021861683239,
  "compression_ratio": 1.8, "no_speech_prob": 0.00176139734685421}, {"id": 104, "seek":
  76488, "start": 764.88, "end": 770.64, "text": " and they focus on something i don''t
  know healthcare banking what have you would you need to fine tune", "tokens": [50364,
  293, 436, 1879, 322, 746, 741, 500, 380, 458, 8884, 18261, 437, 362, 291, 576, 291,
  643, 281, 2489, 10864, 50652], "temperature": 0.0, "avg_logprob": -0.09680914324383404,
  "compression_ratio": 1.6936170212765957, "no_speech_prob": 0.00058299012016505},
  {"id": 105, "seek": 76488, "start": 770.64, "end": 778.24, "text": " reader lm in
  any way no i actually don''t recommend it i think there''s a lot of evidence that
  fine", "tokens": [50652, 15149, 287, 76, 294, 604, 636, 572, 741, 767, 500, 380,
  2748, 309, 741, 519, 456, 311, 257, 688, 295, 4467, 300, 2489, 51032], "temperature":
  0.0, "avg_logprob": -0.09680914324383404, "compression_ratio": 1.6936170212765957,
  "no_speech_prob": 0.00058299012016505}, {"id": 106, "seek": 76488, "start": 778.24,
  "end": 783.12, "text": " tuning because of its fundamentally lossy process right
  is somewhat responsible for hallucinations", "tokens": [51032, 15164, 570, 295,
  1080, 17879, 4470, 88, 1399, 558, 307, 8344, 6250, 337, 35212, 10325, 51276], "temperature":
  0.0, "avg_logprob": -0.09680914324383404, "compression_ratio": 1.6936170212765957,
  "no_speech_prob": 0.00058299012016505}, {"id": 107, "seek": 76488, "start": 783.12,
  "end": 788.0, "text": " there''s been quite a bit written about this and i think
  that ultimately the the winning combination", "tokens": [51276, 456, 311, 668, 1596,
  257, 857, 3720, 466, 341, 293, 741, 519, 300, 6284, 264, 264, 8224, 6562, 51520],
  "temperature": 0.0, "avg_logprob": -0.09680914324383404, "compression_ratio": 1.6936170212765957,
  "no_speech_prob": 0.00058299012016505}, {"id": 108, "seek": 78800, "start": 788.08,
  "end": 795.52, "text": " today is that you use a very well trained capable model
  that is generalist and you provide it with", "tokens": [50368, 965, 307, 300, 291,
  764, 257, 588, 731, 8895, 8189, 2316, 300, 307, 2674, 468, 293, 291, 2893, 309,
  365, 50740], "temperature": 0.0, "avg_logprob": -0.060065312044961114, "compression_ratio":
  1.9203187250996017, "no_speech_prob": 0.004245159216225147}, {"id": 109, "seek":
  78800, "start": 795.52, "end": 800.24, "text": " the data that you need to provide
  it with at the moment you need to for example swirls prompt", "tokens": [50740,
  264, 1412, 300, 291, 643, 281, 2893, 309, 365, 412, 264, 1623, 291, 643, 281, 337,
  1365, 30310, 82, 12391, 50976], "temperature": 0.0, "avg_logprob": -0.060065312044961114,
  "compression_ratio": 1.9203187250996017, "no_speech_prob": 0.004245159216225147},
  {"id": 110, "seek": 78800, "start": 800.24, "end": 805.2, "text": " engineering
  does a few things one we force it to only consider the rag data and not add its
  own", "tokens": [50976, 7043, 775, 257, 1326, 721, 472, 321, 3464, 309, 281, 787,
  1949, 264, 17539, 1412, 293, 406, 909, 1080, 1065, 51224], "temperature": 0.0, "avg_logprob":
  -0.060065312044961114, "compression_ratio": 1.9203187250996017, "no_speech_prob":
  0.004245159216225147}, {"id": 111, "seek": 78800, "start": 805.2, "end": 809.84,
  "text": " model thoughts right you can interpret but don''t say don''t create facts
  that aren''t presented to you", "tokens": [51224, 2316, 4598, 558, 291, 393, 7302,
  457, 500, 380, 584, 500, 380, 1884, 9130, 300, 3212, 380, 8212, 281, 291, 51456],
  "temperature": 0.0, "avg_logprob": -0.060065312044961114, "compression_ratio": 1.9203187250996017,
  "no_speech_prob": 0.004245159216225147}, {"id": 112, "seek": 78800, "start": 810.64,
  "end": 816.0, "text": " second force it to disambiguate this is one of the worst
  errors in prompt engineering is not", "tokens": [51496, 1150, 3464, 309, 281, 717,
  2173, 328, 10107, 341, 307, 472, 295, 264, 5855, 13603, 294, 12391, 7043, 307, 406,
  51764], "temperature": 0.0, "avg_logprob": -0.060065312044961114, "compression_ratio":
  1.9203187250996017, "no_speech_prob": 0.004245159216225147}, {"id": 113, "seek":
  81600, "start": 816.0, "end": 820.64, "text": " is just letting it go right up on
  past equating right two entities with the same name as if they''re", "tokens": [50364,
  307, 445, 8295, 309, 352, 558, 493, 322, 1791, 1267, 990, 558, 732, 16667, 365,
  264, 912, 1315, 382, 498, 436, 434, 50596], "temperature": 0.0, "avg_logprob": -0.12356334924697876,
  "compression_ratio": 1.8438661710037174, "no_speech_prob": 0.0021778661757707596},
  {"id": 114, "seek": 81600, "start": 820.64, "end": 826.08, "text": " the same thing
  so our default engineering says listen if you see and into two entities with the
  same", "tokens": [50596, 264, 912, 551, 370, 527, 7576, 7043, 1619, 2140, 498, 291,
  536, 293, 666, 732, 16667, 365, 264, 912, 50868], "temperature": 0.0, "avg_logprob":
  -0.12356334924697876, "compression_ratio": 1.8438661710037174, "no_speech_prob":
  0.0021778661757707596}, {"id": 115, "seek": 81600, "start": 826.08, "end": 831.84,
  "text": " name don''t you know essentially call that out and don''t just gloss over
  it the last one is especially", "tokens": [50868, 1315, 500, 380, 291, 458, 4476,
  818, 300, 484, 293, 500, 380, 445, 19574, 670, 309, 264, 1036, 472, 307, 2318, 51156],
  "temperature": 0.0, "avg_logprob": -0.12356334924697876, "compression_ratio": 1.8438661710037174,
  "no_speech_prob": 0.0021778661757707596}, {"id": 116, "seek": 81600, "start": 831.84,
  "end": 836.56, "text": " when you''re talking about multiple sources of data and
  enterprise data the user must be able to", "tokens": [51156, 562, 291, 434, 1417,
  466, 3866, 7139, 295, 1412, 293, 14132, 1412, 264, 4195, 1633, 312, 1075, 281, 51392],
  "temperature": 0.0, "avg_logprob": -0.12356334924697876, "compression_ratio": 1.8438661710037174,
  "no_speech_prob": 0.0021778661757707596}, {"id": 117, "seek": 81600, "start": 836.56,
  "end": 841.28, "text": " verify or nobody wants to make a career limiting move because
  they took chat gpt''s and answer and", "tokens": [51392, 16888, 420, 5079, 2738,
  281, 652, 257, 3988, 22083, 1286, 570, 436, 1890, 5081, 290, 662, 311, 293, 1867,
  293, 51628], "temperature": 0.0, "avg_logprob": -0.12356334924697876, "compression_ratio":
  1.8438661710037174, "no_speech_prob": 0.0021778661757707596}, {"id": 118, "seek":
  84128, "start": 841.28, "end": 846.24, "text": " said here it is here it is right
  put it up on the on the investor site not a good idea but", "tokens": [50364, 848,
  510, 309, 307, 510, 309, 307, 558, 829, 309, 493, 322, 264, 322, 264, 18479, 3621,
  406, 257, 665, 1558, 457, 50612], "temperature": 0.0, "avg_logprob": -0.11461341164328835,
  "compression_ratio": 1.7735849056603774, "no_speech_prob": 0.00804323423653841},
  {"id": 119, "seek": 84128, "start": 846.8, "end": 852.48, "text": " swirl also forces
  the AI to quote the sources that you use to cite them and of course you also have",
  "tokens": [50640, 30310, 611, 5874, 264, 7318, 281, 6513, 264, 7139, 300, 291, 764,
  281, 37771, 552, 293, 295, 1164, 291, 611, 362, 50924], "temperature": 0.0, "avg_logprob":
  -0.11461341164328835, "compression_ratio": 1.7735849056603774, "no_speech_prob":
  0.00804323423653841}, {"id": 120, "seek": 84128, "start": 852.48, "end": 856.4,
  "text": " access to the underlying search results right so you can verify that yes
  you have a million", "tokens": [50924, 2105, 281, 264, 14217, 3164, 3542, 558, 370,
  291, 393, 16888, 300, 2086, 291, 362, 257, 2459, 51120], "temperature": 0.0, "avg_logprob":
  -0.11461341164328835, "compression_ratio": 1.7735849056603774, "no_speech_prob":
  0.00804323423653841}, {"id": 121, "seek": 84128, "start": 856.4, "end": 862.0, "text":
  " dollars in insurance coverage and it covers x y and z that''s key yeah that''s
  amazing I was just", "tokens": [51120, 3808, 294, 7214, 9645, 293, 309, 10538, 2031,
  288, 293, 710, 300, 311, 2141, 1338, 300, 311, 2243, 286, 390, 445, 51400], "temperature":
  0.0, "avg_logprob": -0.11461341164328835, "compression_ratio": 1.7735849056603774,
  "no_speech_prob": 0.00804323423653841}, {"id": 122, "seek": 84128, "start": 862.0,
  "end": 867.1999999999999, "text": " you reminded me of when you said about hallucinations
  I was just listening to one interview", "tokens": [51400, 291, 15920, 385, 295,
  562, 291, 848, 466, 35212, 10325, 286, 390, 445, 4764, 281, 472, 4049, 51660], "temperature":
  0.0, "avg_logprob": -0.11461341164328835, "compression_ratio": 1.7735849056603774,
  "no_speech_prob": 0.00804323423653841}, {"id": 123, "seek": 86720, "start": 868.0,
  "end": 874.88, "text": " is not related to AI world attack world it''s political
  sciences and so she was asked the", "tokens": [50404, 307, 406, 4077, 281, 7318,
  1002, 2690, 1002, 309, 311, 3905, 17677, 293, 370, 750, 390, 2351, 264, 50748],
  "temperature": 0.0, "avg_logprob": -0.1185360116473699, "compression_ratio": 1.8588235294117648,
  "no_speech_prob": 0.014525174163281918}, {"id": 124, "seek": 86720, "start": 874.88,
  "end": 879.44, "text": " scientists she was asked you know are you using chat gpt
  at work and she said yes sometimes I do", "tokens": [50748, 7708, 750, 390, 2351,
  291, 458, 366, 291, 1228, 5081, 290, 662, 412, 589, 293, 750, 848, 2086, 2171, 286,
  360, 50976], "temperature": 0.0, "avg_logprob": -0.1185360116473699, "compression_ratio":
  1.8588235294117648, "no_speech_prob": 0.014525174163281918}, {"id": 125, "seek":
  86720, "start": 879.44, "end": 885.9200000000001, "text": " sometimes I use it as
  a co-writer so you know I I draft some things quickly and and I still see", "tokens":
  [50976, 2171, 286, 764, 309, 382, 257, 598, 12, 23681, 370, 291, 458, 286, 286,
  11206, 512, 721, 2661, 293, 293, 286, 920, 536, 51300], "temperature": 0.0, "avg_logprob":
  -0.1185360116473699, "compression_ratio": 1.8588235294117648, "no_speech_prob":
  0.014525174163281918}, {"id": 126, "seek": 86720, "start": 885.9200000000001, "end":
  890.1600000000001, "text": " that chat gpt is very crude you know in the way it
  approaches you know I can do it better but", "tokens": [51300, 300, 5081, 290, 662,
  307, 588, 30796, 291, 458, 294, 264, 636, 309, 11587, 291, 458, 286, 393, 360, 309,
  1101, 457, 51512], "temperature": 0.0, "avg_logprob": -0.1185360116473699, "compression_ratio":
  1.8588235294117648, "no_speech_prob": 0.014525174163281918}, {"id": 127, "seek":
  86720, "start": 890.1600000000001, "end": 896.88, "text": " sometimes I''m just
  you know lazy or tired okay let it do it but then the thing that struck her was",
  "tokens": [51512, 2171, 286, 478, 445, 291, 458, 14847, 420, 5868, 1392, 718, 309,
  360, 309, 457, 550, 264, 551, 300, 13159, 720, 390, 51848], "temperature": 0.0,
  "avg_logprob": -0.1185360116473699, "compression_ratio": 1.8588235294117648, "no_speech_prob":
  0.014525174163281918}, {"id": 128, "seek": 89688, "start": 896.88, "end": 903.04,
  "text": " that it actually hallucinates she was asking give me you know top five
  books in political science", "tokens": [50364, 300, 309, 767, 35212, 259, 1024,
  750, 390, 3365, 976, 385, 291, 458, 1192, 1732, 3642, 294, 3905, 3497, 50672], "temperature":
  0.0, "avg_logprob": -0.09582684491131757, "compression_ratio": 1.9467213114754098,
  "no_speech_prob": 0.0008806909318082035}, {"id": 129, "seek": 89688, "start": 903.04,
  "end": 908.88, "text": " you know in specific country and chat gpt was very confident
  and and said they said the five books", "tokens": [50672, 291, 458, 294, 2685, 1941,
  293, 5081, 290, 662, 390, 588, 6679, 293, 293, 848, 436, 848, 264, 1732, 3642, 50964],
  "temperature": 0.0, "avg_logprob": -0.09582684491131757, "compression_ratio": 1.9467213114754098,
  "no_speech_prob": 0.0008806909318082035}, {"id": 130, "seek": 89688, "start": 908.88,
  "end": 913.92, "text": " and the authors and when she googled them they don''t exist
  and and then she said they don''t exist", "tokens": [50964, 293, 264, 16552, 293,
  562, 750, 50061, 1493, 552, 436, 500, 380, 2514, 293, 293, 550, 750, 848, 436, 500,
  380, 2514, 51216], "temperature": 0.0, "avg_logprob": -0.09582684491131757, "compression_ratio":
  1.9467213114754098, "no_speech_prob": 0.0008806909318082035}, {"id": 131, "seek":
  89688, "start": 914.64, "end": 918.96, "text": " and then chat gpt responded okay
  here is only one book that you should read and that didn''t", "tokens": [51252,
  293, 550, 5081, 290, 662, 15806, 1392, 510, 307, 787, 472, 1446, 300, 291, 820,
  1401, 293, 300, 994, 380, 51468], "temperature": 0.0, "avg_logprob": -0.09582684491131757,
  "compression_ratio": 1.9467213114754098, "no_speech_prob": 0.0008806909318082035},
  {"id": 132, "seek": 89688, "start": 918.96, "end": 925.12, "text": " exist either
  so she was genuinely like baffled and she said okay you might say something", "tokens":
  [51468, 2514, 2139, 370, 750, 390, 17839, 411, 272, 2518, 1493, 293, 750, 848, 1392,
  291, 1062, 584, 746, 51776], "temperature": 0.0, "avg_logprob": -0.09582684491131757,
  "compression_ratio": 1.9467213114754098, "no_speech_prob": 0.0008806909318082035},
  {"id": 133, "seek": 92512, "start": 925.92, "end": 933.12, "text": " with less confidence
  but why lie why do you lie she doesn''t know what is hallucinations but she''s",
  "tokens": [50404, 365, 1570, 6687, 457, 983, 4544, 983, 360, 291, 4544, 750, 1177,
  380, 458, 437, 307, 35212, 10325, 457, 750, 311, 50764], "temperature": 0.0, "avg_logprob":
  -0.08857180408595763, "compression_ratio": 1.75, "no_speech_prob": 0.015747902914881706},
  {"id": 134, "seek": 92512, "start": 933.12, "end": 939.52, "text": " she looks at
  it as a user and it''s very disconcerting so believe it or not when I first started",
  "tokens": [50764, 750, 1542, 412, 309, 382, 257, 4195, 293, 309, 311, 588, 717,
  1671, 1776, 783, 370, 1697, 309, 420, 406, 562, 286, 700, 1409, 51084], "temperature":
  0.0, "avg_logprob": -0.08857180408595763, "compression_ratio": 1.75, "no_speech_prob":
  0.015747902914881706}, {"id": 135, "seek": 92512, "start": 939.52, "end": 944.96,
  "text": " using gpt 4 I got a hallucination that I thought was so real I wrote to
  the publisher and said why is", "tokens": [51084, 1228, 290, 662, 1017, 286, 658,
  257, 35212, 2486, 300, 286, 1194, 390, 370, 957, 286, 4114, 281, 264, 25088, 293,
  848, 983, 307, 51356], "temperature": 0.0, "avg_logprob": -0.08857180408595763,
  "compression_ratio": 1.75, "no_speech_prob": 0.015747902914881706}, {"id": 136,
  "seek": 92512, "start": 944.96, "end": 950.0, "text": " this article no longer online
  and the publisher wrote back and said there is no such article but", "tokens": [51356,
  341, 7222, 572, 2854, 2950, 293, 264, 25088, 4114, 646, 293, 848, 456, 307, 572,
  1270, 7222, 457, 51608], "temperature": 0.0, "avg_logprob": -0.08857180408595763,
  "compression_ratio": 1.75, "no_speech_prob": 0.015747902914881706}, {"id": 137,
  "seek": 95000, "start": 950.0, "end": 955.12, "text": " it could have been it was
  authored by someone they said gpt 4 said it was authored by another author", "tokens":
  [50364, 309, 727, 362, 668, 309, 390, 6979, 2769, 538, 1580, 436, 848, 290, 662,
  1017, 848, 309, 390, 6979, 2769, 538, 1071, 3793, 50620], "temperature": 0.0, "avg_logprob":
  -0.11615510490851674, "compression_ratio": 1.7544483985765125, "no_speech_prob":
  0.02273571863770485}, {"id": 138, "seek": 95000, "start": 955.12, "end": 960.0,
  "text": " who had posted on that site the url looked correct and the content looked
  I mean the snippet", "tokens": [50620, 567, 632, 9437, 322, 300, 3621, 264, 4038,
  75, 2956, 3006, 293, 264, 2701, 2956, 286, 914, 264, 35623, 302, 50864], "temperature":
  0.0, "avg_logprob": -0.11615510490851674, "compression_ratio": 1.7544483985765125,
  "no_speech_prob": 0.02273571863770485}, {"id": 139, "seek": 95000, "start": 960.0,
  "end": 966.16, "text": " it gave me looked absolutely real but again when they build
  these models a few you know 10 20 gigabyte", "tokens": [50864, 309, 2729, 385, 2956,
  3122, 957, 457, 797, 562, 436, 1322, 613, 5245, 257, 1326, 291, 458, 1266, 945,
  8741, 34529, 51172], "temperature": 0.0, "avg_logprob": -0.11615510490851674, "compression_ratio":
  1.7544483985765125, "no_speech_prob": 0.02273571863770485}, {"id": 140, "seek":
  95000, "start": 966.16, "end": 971.44, "text": " model right of gpt 4 or 35 or whatever
  it is petabytes and petabytes of data went into that so by", "tokens": [51172, 2316,
  558, 295, 290, 662, 1017, 420, 6976, 420, 2035, 309, 307, 3817, 24538, 293, 3817,
  24538, 295, 1412, 1437, 666, 300, 370, 538, 51436], "temperature": 0.0, "avg_logprob":
  -0.11615510490851674, "compression_ratio": 1.7544483985765125, "no_speech_prob":
  0.02273571863770485}, {"id": 141, "seek": 95000, "start": 971.44, "end": 977.52,
  "text": " definition it''s lossy but the way the lllm the generative part works
  is it must provide a response", "tokens": [51436, 7123, 309, 311, 4470, 88, 457,
  264, 636, 264, 287, 285, 76, 264, 1337, 1166, 644, 1985, 307, 309, 1633, 2893, 257,
  4134, 51740], "temperature": 0.0, "avg_logprob": -0.11615510490851674, "compression_ratio":
  1.7544483985765125, "no_speech_prob": 0.02273571863770485}, {"id": 142, "seek":
  97752, "start": 977.52, "end": 982.3199999999999, "text": " so you know how that
  is when you can''t quite remember the name of something and it''s essentially",
  "tokens": [50364, 370, 291, 458, 577, 300, 307, 562, 291, 393, 380, 1596, 1604,
  264, 1315, 295, 746, 293, 309, 311, 4476, 50604], "temperature": 0.0, "avg_logprob":
  -0.08851746532404534, "compression_ratio": 1.7740740740740741, "no_speech_prob":
  0.0024549805093556643}, {"id": 143, "seek": 97752, "start": 982.3199999999999, "end":
  986.24, "text": " doing the same thing so it knows it I saw an artifact that looked
  like that but I don''t have the", "tokens": [50604, 884, 264, 912, 551, 370, 309,
  3255, 309, 286, 1866, 364, 34806, 300, 2956, 411, 300, 457, 286, 500, 380, 362,
  264, 50800], "temperature": 0.0, "avg_logprob": -0.08851746532404534, "compression_ratio":
  1.7740740740740741, "no_speech_prob": 0.0024549805093556643}, {"id": 144, "seek":
  97752, "start": 986.24, "end": 993.1999999999999, "text": " artifact anymore so
  it generates something that is the consensus version of what it would have been",
  "tokens": [50800, 34806, 3602, 370, 309, 23815, 746, 300, 307, 264, 19115, 3037,
  295, 437, 309, 576, 362, 668, 51148], "temperature": 0.0, "avg_logprob": -0.08851746532404534,
  "compression_ratio": 1.7740740740740741, "no_speech_prob": 0.0024549805093556643},
  {"id": 145, "seek": 97752, "start": 993.1999999999999, "end": 999.52, "text": "
  had it existed and that''s why I don''t believe in fine tuning so much think if
  you have a high", "tokens": [51148, 632, 309, 13135, 293, 300, 311, 983, 286, 500,
  380, 1697, 294, 2489, 15164, 370, 709, 519, 498, 291, 362, 257, 1090, 51464], "temperature":
  0.0, "avg_logprob": -0.08851746532404534, "compression_ratio": 1.7740740740740741,
  "no_speech_prob": 0.0024549805093556643}, {"id": 146, "seek": 97752, "start": 999.52,
  "end": 1004.4, "text": " capable model with some reasoning and the ability to interpret
  text and follow instructions", "tokens": [51464, 8189, 2316, 365, 512, 21577, 293,
  264, 3485, 281, 7302, 2487, 293, 1524, 9415, 51708], "temperature": 0.0, "avg_logprob":
  -0.08851746532404534, "compression_ratio": 1.7740740740740741, "no_speech_prob":
  0.0024549805093556643}, {"id": 147, "seek": 100440, "start": 1004.4, "end": 1011.04,
  "text": " you provide it with your internal data and that is the beauty of reg because
  here''s the thing", "tokens": [50364, 291, 2893, 309, 365, 428, 6920, 1412, 293,
  300, 307, 264, 6643, 295, 1121, 570, 510, 311, 264, 551, 50696], "temperature":
  0.0, "avg_logprob": -0.13919814573515446, "compression_ratio": 1.7984790874524714,
  "no_speech_prob": 0.0036329131107777357}, {"id": 148, "seek": 100440, "start": 1011.68,
  "end": 1016.24, "text": " the reason it''s so good at things why does why does chat
  gpt 4 sound like a smart person on", "tokens": [50728, 264, 1778, 309, 311, 370,
  665, 412, 721, 983, 775, 983, 775, 5081, 290, 662, 1017, 1626, 411, 257, 4069, 954,
  322, 50956], "temperature": 0.0, "avg_logprob": -0.13919814573515446, "compression_ratio":
  1.7984790874524714, "no_speech_prob": 0.0036329131107777357}, {"id": 149, "seek":
  100440, "start": 1016.24, "end": 1020.24, "text": " you know reddit or or some or
  Facebook or something like that right and that''s because that''s", "tokens": [50956,
  291, 458, 2182, 17975, 420, 420, 512, 420, 4384, 420, 746, 411, 300, 558, 293, 300,
  311, 570, 300, 311, 51156], "temperature": 0.0, "avg_logprob": -0.13919814573515446,
  "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.0036329131107777357},
  {"id": 150, "seek": 100440, "start": 1020.24, "end": 1025.2, "text": " where it
  was trained from you''re internal and of course on something like reddit or whatever
  they", "tokens": [51156, 689, 309, 390, 8895, 490, 291, 434, 6920, 293, 295, 1164,
  322, 746, 411, 2182, 17975, 420, 2035, 436, 51404], "temperature": 0.0, "avg_logprob":
  -0.13919814573515446, "compression_ratio": 1.7984790874524714, "no_speech_prob":
  0.0036329131107777357}, {"id": 151, "seek": 100440, "start": 1025.2, "end": 1029.44,
  "text": " have a new the same conversation 10 million times right I mean how many
  discussions of whatever", "tokens": [51404, 362, 257, 777, 264, 912, 3761, 1266,
  2459, 1413, 558, 286, 914, 577, 867, 11088, 295, 2035, 51616], "temperature": 0.0,
  "avg_logprob": -0.13919814573515446, "compression_ratio": 1.7984790874524714, "no_speech_prob":
  0.0036329131107777357}, {"id": 152, "seek": 102944, "start": 1029.44, "end": 1034.88,
  "text": " twin peaks or battle star galactica you know are there there are a lot
  of them and so it learns the", "tokens": [50364, 18397, 26897, 420, 4635, 3543,
  7660, 578, 2262, 291, 458, 366, 456, 456, 366, 257, 688, 295, 552, 293, 370, 309,
  27152, 264, 50636], "temperature": 0.0, "avg_logprob": -0.10733833483287267, "compression_ratio":
  1.8544776119402986, "no_speech_prob": 0.00649447413161397}, {"id": 153, "seek":
  102944, "start": 1034.88, "end": 1039.6000000000001, "text": " core of these things
  right and can answer those questions but if you feed at your internal data like",
  "tokens": [50636, 4965, 295, 613, 721, 558, 293, 393, 1867, 729, 1651, 457, 498,
  291, 3154, 412, 428, 6920, 1412, 411, 50872], "temperature": 0.0, "avg_logprob":
  -0.10733833483287267, "compression_ratio": 1.8544776119402986, "no_speech_prob":
  0.00649447413161397}, {"id": 154, "seek": 102944, "start": 1039.6000000000001, "end":
  1046.56, "text": " it''s probably not so repetitive it''s probably much more conflicting
  than not and so you that''s", "tokens": [50872, 309, 311, 1391, 406, 370, 29404,
  309, 311, 1391, 709, 544, 43784, 813, 406, 293, 370, 291, 300, 311, 51220], "temperature":
  0.0, "avg_logprob": -0.10733833483287267, "compression_ratio": 1.8544776119402986,
  "no_speech_prob": 0.00649447413161397}, {"id": 155, "seek": 102944, "start": 1046.56,
  "end": 1051.28, "text": " why you produce more problems it''s much better give it
  the one thing that''s really relevant and let it", "tokens": [51220, 983, 291, 5258,
  544, 2740, 309, 311, 709, 1101, 976, 309, 264, 472, 551, 300, 311, 534, 7340, 293,
  718, 309, 51456], "temperature": 0.0, "avg_logprob": -0.10733833483287267, "compression_ratio":
  1.8544776119402986, "no_speech_prob": 0.00649447413161397}, {"id": 156, "seek":
  102944, "start": 1051.28, "end": 1058.4, "text": " reason yeah that sounds go and
  slight live right it''s something that can be updated throughout the", "tokens":
  [51456, 1778, 1338, 300, 3263, 352, 293, 4036, 1621, 558, 309, 311, 746, 300, 393,
  312, 10588, 3710, 264, 51812], "temperature": 0.0, "avg_logprob": -0.10733833483287267,
  "compression_ratio": 1.8544776119402986, "no_speech_prob": 0.00649447413161397},
  {"id": 157, "seek": 105840, "start": 1058.4, "end": 1063.2, "text": " lifecycle
  of your company or department whatever but there is one challenge they want to offer
  to you", "tokens": [50364, 45722, 295, 428, 2237, 420, 5882, 2035, 457, 456, 307,
  472, 3430, 436, 528, 281, 2626, 281, 291, 50604], "temperature": 0.0, "avg_logprob":
  -0.14367870850996536, "compression_ratio": 1.7012987012987013, "no_speech_prob":
  0.0036661839112639427}, {"id": 158, "seek": 105840, "start": 1063.2, "end": 1071.0400000000002,
  "text": " and came to me just today as I was thinking and preparing for this episode
  data is not gold you know", "tokens": [50604, 293, 1361, 281, 385, 445, 965, 382,
  286, 390, 1953, 293, 10075, 337, 341, 3500, 1412, 307, 406, 3821, 291, 458, 50996],
  "temperature": 0.0, "avg_logprob": -0.14367870850996536, "compression_ratio": 1.7012987012987013,
  "no_speech_prob": 0.0036661839112639427}, {"id": 159, "seek": 105840, "start": 1071.0400000000002,
  "end": 1076.72, "text": " sometimes it is gold because everyone talks about it la
  la la but like it also is very complex", "tokens": [50996, 2171, 309, 307, 3821,
  570, 1518, 6686, 466, 309, 635, 635, 635, 457, 411, 309, 611, 307, 588, 3997, 51280],
  "temperature": 0.0, "avg_logprob": -0.14367870850996536, "compression_ratio": 1.7012987012987013,
  "no_speech_prob": 0.0036661839112639427}, {"id": 160, "seek": 105840, "start": 1076.72,
  "end": 1082.8000000000002, "text": " machinery and it can have the stakes of it
  of its own you know misattribution misclassification", "tokens": [51280, 27302,
  293, 309, 393, 362, 264, 28429, 295, 309, 295, 1080, 1065, 291, 458, 3346, 1591,
  30783, 3346, 11665, 3774, 51584], "temperature": 0.0, "avg_logprob": -0.14367870850996536,
  "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.0036661839112639427},
  {"id": 161, "seek": 108280, "start": 1083.68, "end": 1091.52, "text": " and human
  error what have you how how would you say reader lllm or swirl gonna tackle this",
  "tokens": [50408, 293, 1952, 6713, 437, 362, 291, 577, 577, 576, 291, 584, 15149,
  287, 285, 76, 420, 30310, 799, 14896, 341, 50800], "temperature": 0.0, "avg_logprob":
  -0.18142213017107492, "compression_ratio": 1.6077586206896552, "no_speech_prob":
  0.004327643662691116}, {"id": 162, "seek": 108280, "start": 1091.52, "end": 1096.56,
  "text": " issue or is it just gonna transparently sort of like garbage in garbage
  out type of response", "tokens": [50800, 2734, 420, 307, 309, 445, 799, 7132, 6420,
  1333, 295, 411, 14150, 294, 14150, 484, 2010, 295, 4134, 51052], "temperature":
  0.0, "avg_logprob": -0.18142213017107492, "compression_ratio": 1.6077586206896552,
  "no_speech_prob": 0.004327643662691116}, {"id": 163, "seek": 108280, "start": 1098.48,
  "end": 1104.3999999999999, "text": " it''s a great question speaking of hallucinations
  in AI right we all have probably worked with", "tokens": [51148, 309, 311, 257,
  869, 1168, 4124, 295, 35212, 10325, 294, 7318, 558, 321, 439, 362, 1391, 2732, 365,
  51444], "temperature": 0.0, "avg_logprob": -0.18142213017107492, "compression_ratio":
  1.6077586206896552, "no_speech_prob": 0.004327643662691116}, {"id": 164, "seek":
  108280, "start": 1104.3999999999999, "end": 1109.04, "text": " somebody at one time
  another who made a mistake right or whatever didn''t understand the problem", "tokens":
  [51444, 2618, 412, 472, 565, 1071, 567, 1027, 257, 6146, 558, 420, 2035, 994, 380,
  1223, 264, 1154, 51676], "temperature": 0.0, "avg_logprob": -0.18142213017107492,
  "compression_ratio": 1.6077586206896552, "no_speech_prob": 0.004327643662691116},
  {"id": 165, "seek": 110904, "start": 1109.28, "end": 1113.84, "text": " enough and
  that stuff gets into teams and slack and you know documents are wrong like it''s",
  "tokens": [50376, 1547, 293, 300, 1507, 2170, 666, 5491, 293, 29767, 293, 291, 458,
  8512, 366, 2085, 411, 309, 311, 50604], "temperature": 0.0, "avg_logprob": -0.15550581014381265,
  "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.00762926647439599},
  {"id": 166, "seek": 110904, "start": 1113.84, "end": 1118.72, "text": " incredible
  you''re right it''s incredibly messy in the enterprise happy as anybody not worked
  at a firm", "tokens": [50604, 4651, 291, 434, 558, 309, 311, 6252, 16191, 294, 264,
  14132, 2055, 382, 4472, 406, 2732, 412, 257, 6174, 50848], "temperature": 0.0, "avg_logprob":
  -0.15550581014381265, "compression_ratio": 1.7655677655677655, "no_speech_prob":
  0.00762926647439599}, {"id": 167, "seek": 110904, "start": 1118.72, "end": 1124.8,
  "text": " where they had you know 500 versions of the same PowerPoint that is just
  evolved right so absolutely", "tokens": [50848, 689, 436, 632, 291, 458, 5923, 9606,
  295, 264, 912, 25584, 300, 307, 445, 14178, 558, 370, 3122, 51152], "temperature":
  0.0, "avg_logprob": -0.15550581014381265, "compression_ratio": 1.7655677655677655,
  "no_speech_prob": 0.00762926647439599}, {"id": 168, "seek": 110904, "start": 1124.8,
  "end": 1128.8799999999999, "text": " well these are things that ultimately are gonna
  have to be continued to be work on but here''s", "tokens": [51152, 731, 613, 366,
  721, 300, 6284, 366, 799, 362, 281, 312, 7014, 281, 312, 589, 322, 457, 510, 311,
  51356], "temperature": 0.0, "avg_logprob": -0.15550581014381265, "compression_ratio":
  1.7655677655677655, "no_speech_prob": 0.00762926647439599}, {"id": 169, "seek":
  110904, "start": 1128.8799999999999, "end": 1133.92, "text": " one point number
  one if you leave the system in the system data in the system of record you''re",
  "tokens": [51356, 472, 935, 1230, 472, 498, 291, 1856, 264, 1185, 294, 264, 1185,
  1412, 294, 264, 1185, 295, 2136, 291, 434, 51608], "temperature": 0.0, "avg_logprob":
  -0.15550581014381265, "compression_ratio": 1.7655677655677655, "no_speech_prob":
  0.00762926647439599}, {"id": 170, "seek": 113392, "start": 1133.92, "end": 1139.68,
  "text": " much less likely to introduce new problems especially like security problems
  and you leave it in", "tokens": [50364, 709, 1570, 3700, 281, 5366, 777, 2740, 2318,
  411, 3825, 2740, 293, 291, 1856, 309, 294, 50652], "temperature": 0.0, "avg_logprob":
  -0.10585203877201786, "compression_ratio": 1.772563176895307, "no_speech_prob":
  0.0005382790695875883}, {"id": 171, "seek": 113392, "start": 1139.68, "end": 1144.48,
  "text": " the system of record than any domain modeling lexicon''s ontologies text
  items you get the benefit of", "tokens": [50652, 264, 1185, 295, 2136, 813, 604,
  9274, 15983, 476, 87, 11911, 311, 6592, 6204, 2487, 4754, 291, 483, 264, 5121, 295,
  50892], "temperature": 0.0, "avg_logprob": -0.10585203877201786, "compression_ratio":
  1.772563176895307, "no_speech_prob": 0.0005382790695875883}, {"id": 172, "seek":
  113392, "start": 1144.48, "end": 1149.04, "text": " those if someone cared about
  that source they might very well have done some of that right so if", "tokens":
  [50892, 729, 498, 1580, 19779, 466, 300, 4009, 436, 1062, 588, 731, 362, 1096, 512,
  295, 300, 558, 370, 498, 51120], "temperature": 0.0, "avg_logprob": -0.10585203877201786,
  "compression_ratio": 1.772563176895307, "no_speech_prob": 0.0005382790695875883},
  {"id": 173, "seek": 113392, "start": 1149.04, "end": 1154.3200000000002, "text":
  " you pull it all out and put it in a vector database like what happened to all
  of that knowledge so", "tokens": [51120, 291, 2235, 309, 439, 484, 293, 829, 309,
  294, 257, 8062, 8149, 411, 437, 2011, 281, 439, 295, 300, 3601, 370, 51384], "temperature":
  0.0, "avg_logprob": -0.10585203877201786, "compression_ratio": 1.772563176895307,
  "no_speech_prob": 0.0005382790695875883}, {"id": 174, "seek": 113392, "start": 1154.3200000000002,
  "end": 1159.8400000000001, "text": " I would argue that the systems of record that
  are valuable have things in place to deal with that", "tokens": [51384, 286, 576,
  9695, 300, 264, 3652, 295, 2136, 300, 366, 8263, 362, 721, 294, 1081, 281, 2028,
  365, 300, 51660], "temperature": 0.0, "avg_logprob": -0.10585203877201786, "compression_ratio":
  1.772563176895307, "no_speech_prob": 0.0005382790695875883}, {"id": 175, "seek":
  115984, "start": 1160.48, "end": 1165.76, "text": " number two the reader lm does
  a couple things that to help this one it''s aware of certain", "tokens": [50396,
  1230, 732, 264, 15149, 287, 76, 775, 257, 1916, 721, 300, 281, 854, 341, 472, 309,
  311, 3650, 295, 1629, 50660], "temperature": 0.0, "avg_logprob": -0.11508199572563171,
  "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0027516174595803022},
  {"id": 176, "seek": 115984, "start": 1165.76, "end": 1172.08, "text": " commonly
  problematic formats email is the worst reply forward and signature content very
  very", "tokens": [50660, 12719, 19011, 25879, 3796, 307, 264, 5855, 16972, 2128,
  293, 13397, 2701, 588, 588, 50976], "temperature": 0.0, "avg_logprob": -0.11508199572563171,
  "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0027516174595803022},
  {"id": 177, "seek": 115984, "start": 1172.08, "end": 1177.04, "text": " problematic
  we have a solution for public data too so that you can get article content without",
  "tokens": [50976, 19011, 321, 362, 257, 3827, 337, 1908, 1412, 886, 370, 300, 291,
  393, 483, 7222, 2701, 1553, 51224], "temperature": 0.0, "avg_logprob": -0.11508199572563171,
  "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.0027516174595803022},
  {"id": 178, "seek": 115984, "start": 1177.04, "end": 1183.36, "text": " getting
  as an example at navigation advertisements cloked data stuff like that right so
  because", "tokens": [51224, 1242, 382, 364, 1365, 412, 17346, 42897, 596, 9511,
  1412, 1507, 411, 300, 558, 370, 570, 51540], "temperature": 0.0, "avg_logprob":
  -0.11508199572563171, "compression_ratio": 1.7622641509433963, "no_speech_prob":
  0.0027516174595803022}, {"id": 179, "seek": 115984, "start": 1183.36, "end": 1188.08,
  "text": " very often public data is relevant right to to large enterprise like they
  want to see policy", "tokens": [51540, 588, 2049, 1908, 1412, 307, 7340, 558, 281,
  281, 2416, 14132, 411, 436, 528, 281, 536, 3897, 51776], "temperature": 0.0, "avg_logprob":
  -0.11508199572563171, "compression_ratio": 1.7622641509433963, "no_speech_prob":
  0.0027516174595803022}, {"id": 180, "seek": 118808, "start": 1188.08, "end": 1193.84,
  "text": " changes regulatory changes online catalog changes right that that''s all
  relevant stuff then there''s", "tokens": [50364, 2962, 18260, 2962, 2950, 19746,
  2962, 558, 300, 300, 311, 439, 7340, 1507, 550, 456, 311, 50652], "temperature":
  0.0, "avg_logprob": -0.13668353469283492, "compression_ratio": 1.8446969696969697,
  "no_speech_prob": 0.0026266479399055243}, {"id": 181, "seek": 118808, "start": 1193.84,
  "end": 1198.96, "text": " the similarity problem right so one of the another thing
  the reader lm does it can do semantic", "tokens": [50652, 264, 32194, 1154, 558,
  370, 472, 295, 264, 1071, 551, 264, 15149, 287, 76, 775, 309, 393, 360, 47982, 50908],
  "temperature": 0.0, "avg_logprob": -0.13668353469283492, "compression_ratio": 1.8446969696969697,
  "no_speech_prob": 0.0026266479399055243}, {"id": 182, "seek": 118808, "start": 1198.96,
  "end": 1204.8799999999999, "text": " analysis to determine which is the latest version
  of the same document that''s a one of the great", "tokens": [50908, 5215, 281, 6997,
  597, 307, 264, 6792, 3037, 295, 264, 912, 4166, 300, 311, 257, 472, 295, 264, 869,
  51204], "temperature": 0.0, "avg_logprob": -0.13668353469283492, "compression_ratio":
  1.8446969696969697, "no_speech_prob": 0.0026266479399055243}, {"id": 183, "seek":
  118808, "start": 1204.8799999999999, "end": 1209.6799999999998, "text": " lm''s
  are amazing at that much better at old school like multi windows setups where you''re
  trying to", "tokens": [51204, 287, 76, 311, 366, 2243, 412, 300, 709, 1101, 412,
  1331, 1395, 411, 4825, 9309, 46832, 689, 291, 434, 1382, 281, 51444], "temperature":
  0.0, "avg_logprob": -0.13668353469283492, "compression_ratio": 1.8446969696969697,
  "no_speech_prob": 0.0026266479399055243}, {"id": 184, "seek": 118808, "start": 1210.3999999999999,
  "end": 1215.6799999999998, "text": " take out like a signature of the document and
  say well this could it''s very similar but lm does", "tokens": [51480, 747, 484,
  411, 257, 13397, 295, 264, 4166, 293, 584, 731, 341, 727, 309, 311, 588, 2531, 457,
  287, 76, 775, 51744], "temperature": 0.0, "avg_logprob": -0.13668353469283492, "compression_ratio":
  1.8446969696969697, "no_speech_prob": 0.0026266479399055243}, {"id": 185, "seek":
  121568, "start": 1215.68, "end": 1219.52, "text": " it much better right and you
  can quickly say now this is the latest version of that spreadsheet", "tokens": [50364,
  309, 709, 1101, 558, 293, 291, 393, 2661, 584, 586, 341, 307, 264, 6792, 3037, 295,
  300, 27733, 50556], "temperature": 0.0, "avg_logprob": -0.08372452022793057, "compression_ratio":
  1.8277153558052435, "no_speech_prob": 0.003425214672461152}, {"id": 186, "seek":
  121568, "start": 1220.48, "end": 1226.0800000000002, "text": " or you can let the
  user decide it''s another thing who doesn''t love shopping I love being able to",
  "tokens": [50604, 420, 291, 393, 718, 264, 4195, 4536, 309, 311, 1071, 551, 567,
  1177, 380, 959, 8688, 286, 959, 885, 1075, 281, 50884], "temperature": 0.0, "avg_logprob":
  -0.08372452022793057, "compression_ratio": 1.8277153558052435, "no_speech_prob":
  0.003425214672461152}, {"id": 187, "seek": 121568, "start": 1226.0800000000002,
  "end": 1231.2, "text": " look at my shopping cart full of swirl results and say
  you know this one I know isn''t really relevant", "tokens": [50884, 574, 412, 452,
  8688, 5467, 1577, 295, 30310, 3542, 293, 584, 291, 458, 341, 472, 286, 458, 1943,
  380, 534, 7340, 51140], "temperature": 0.0, "avg_logprob": -0.08372452022793057,
  "compression_ratio": 1.8277153558052435, "no_speech_prob": 0.003425214672461152},
  {"id": 188, "seek": 121568, "start": 1231.2, "end": 1235.8400000000001, "text":
  " these are the five I''ve risen or maybe this is the source or these are the sources
  that I want my", "tokens": [51140, 613, 366, 264, 1732, 286, 600, 28614, 420, 1310,
  341, 307, 264, 4009, 420, 613, 366, 264, 7139, 300, 286, 528, 452, 51372], "temperature":
  0.0, "avg_logprob": -0.08372452022793057, "compression_ratio": 1.8277153558052435,
  "no_speech_prob": 0.003425214672461152}, {"id": 189, "seek": 121568, "start": 1235.8400000000001,
  "end": 1241.3600000000001, "text": " data from today that''s another way of allowing
  the user to bring their expertise and experience", "tokens": [51372, 1412, 490,
  965, 300, 311, 1071, 636, 295, 8293, 264, 4195, 281, 1565, 641, 11769, 293, 1752,
  51648], "temperature": 0.0, "avg_logprob": -0.08372452022793057, "compression_ratio":
  1.8277153558052435, "no_speech_prob": 0.003425214672461152}, {"id": 190, "seek":
  124136, "start": 1241.36, "end": 1247.6, "text": " and knowledge and say no no no
  colibre not thoughts but snowflake not oracle whatever and I''m", "tokens": [50364,
  293, 3601, 293, 584, 572, 572, 572, 1173, 897, 265, 406, 4598, 457, 44124, 619,
  406, 420, 7041, 2035, 293, 286, 478, 50676], "temperature": 0.0, "avg_logprob":
  -0.10338307263558372, "compression_ratio": 1.8277153558052435, "no_speech_prob":
  0.0075044408440589905}, {"id": 191, "seek": 124136, "start": 1247.6, "end": 1252.9599999999998,
  "text": " not picking on anybody we all can say they''re all present they all have
  value the question is which", "tokens": [50676, 406, 8867, 322, 4472, 321, 439,
  393, 584, 436, 434, 439, 1974, 436, 439, 362, 2158, 264, 1168, 307, 597, 50944],
  "temperature": 0.0, "avg_logprob": -0.10338307263558372, "compression_ratio": 1.8277153558052435,
  "no_speech_prob": 0.0075044408440589905}, {"id": 192, "seek": 124136, "start": 1252.9599999999998,
  "end": 1258.24, "text": " one has the answer for me today well until the until they
  can write the query with the context that", "tokens": [50944, 472, 575, 264, 1867,
  337, 385, 965, 731, 1826, 264, 1826, 436, 393, 2464, 264, 14581, 365, 264, 4319,
  300, 51208], "temperature": 0.0, "avg_logprob": -0.10338307263558372, "compression_ratio":
  1.8277153558052435, "no_speech_prob": 0.0075044408440589905}, {"id": 193, "seek":
  124136, "start": 1258.24, "end": 1262.3999999999999, "text": " answers that you
  know I think the key is to keep the user in the loop make sure that there are",
  "tokens": [51208, 6338, 300, 291, 458, 286, 519, 264, 2141, 307, 281, 1066, 264,
  4195, 294, 264, 6367, 652, 988, 300, 456, 366, 51416], "temperature": 0.0, "avg_logprob":
  -0.10338307263558372, "compression_ratio": 1.8277153558052435, "no_speech_prob":
  0.0075044408440589905}, {"id": 194, "seek": 124136, "start": 1262.3999999999999,
  "end": 1267.76, "text": " citations and ultimately that in a year the systems will
  be smarter and many of these problems will", "tokens": [51416, 4814, 763, 293, 6284,
  300, 294, 257, 1064, 264, 3652, 486, 312, 20294, 293, 867, 295, 613, 2740, 486,
  51684], "temperature": 0.0, "avg_logprob": -0.10338307263558372, "compression_ratio":
  1.8277153558052435, "no_speech_prob": 0.0075044408440589905}, {"id": 195, "seek":
  126776, "start": 1267.76, "end": 1273.6, "text": " be solved after all almost all
  the naive search engines right that were BM25 or whatever", "tokens": [50364, 312,
  13041, 934, 439, 1920, 439, 264, 29052, 3164, 12982, 558, 300, 645, 15901, 6074,
  420, 2035, 50656], "temperature": 0.0, "avg_logprob": -0.21619083180147058, "compression_ratio":
  1.5261044176706828, "no_speech_prob": 0.0016703270375728607}, {"id": 196, "seek":
  126776, "start": 1273.6, "end": 1278.4, "text": " pretty much all have vector upgrades
  now only questions can you wait long enough to vectorize", "tokens": [50656, 1238,
  709, 439, 362, 8062, 24868, 586, 787, 1651, 393, 291, 1699, 938, 1547, 281, 8062,
  1125, 50896], "temperature": 0.0, "avg_logprob": -0.21619083180147058, "compression_ratio":
  1.5261044176706828, "no_speech_prob": 0.0016703270375728607}, {"id": 197, "seek":
  126776, "start": 1278.4, "end": 1286.0, "text": " at high-dimensional space a few
  million documents exactly yeah sometimes when I use chat GPT I don''t", "tokens":
  [50896, 412, 1090, 12, 18759, 1901, 257, 1326, 2459, 8512, 2293, 1338, 2171, 562,
  286, 764, 5081, 26039, 51, 286, 500, 380, 51276], "temperature": 0.0, "avg_logprob":
  -0.21619083180147058, "compression_ratio": 1.5261044176706828, "no_speech_prob":
  0.0016703270375728607}, {"id": 198, "seek": 126776, "start": 1286.0, "end": 1291.36,
  "text": " use it that often by the way for some reason maybe it says something about
  me maybe I should you", "tokens": [51276, 764, 309, 300, 2049, 538, 264, 636, 337,
  512, 1778, 1310, 309, 1619, 746, 466, 385, 1310, 286, 820, 291, 51544], "temperature":
  0.0, "avg_logprob": -0.21619083180147058, "compression_ratio": 1.5261044176706828,
  "no_speech_prob": 0.0016703270375728607}, {"id": 199, "seek": 129136, "start": 1291.84,
  "end": 1298.0, "text": " learn to do it but sometimes as you said you know it just
  generates something it seems a little", "tokens": [50388, 1466, 281, 360, 309, 457,
  2171, 382, 291, 848, 291, 458, 309, 445, 23815, 746, 309, 2544, 257, 707, 50696],
  "temperature": 0.0, "avg_logprob": -0.10082892120861617, "compression_ratio": 1.8549618320610688,
  "no_speech_prob": 0.009863103739917278}, {"id": 200, "seek": 129136, "start": 1298.0,
  "end": 1302.7199999999998, "text": " average you know it''s a code snippet or something
  like that you try it it doesn''t work at that point", "tokens": [50696, 4274, 291,
  458, 309, 311, 257, 3089, 35623, 302, 420, 746, 411, 300, 291, 853, 309, 309, 1177,
  380, 589, 412, 300, 935, 50932], "temperature": 0.0, "avg_logprob": -0.10082892120861617,
  "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.009863103739917278},
  {"id": 201, "seek": 129136, "start": 1303.4399999999998, "end": 1308.32, "text":
  " when I when I get frustrated a little bit I''m like can you show me the source
  maybe a link to", "tokens": [50968, 562, 286, 562, 286, 483, 15751, 257, 707, 857,
  286, 478, 411, 393, 291, 855, 385, 264, 4009, 1310, 257, 2113, 281, 51212], "temperature":
  0.0, "avg_logprob": -0.10082892120861617, "compression_ratio": 1.8549618320610688,
  "no_speech_prob": 0.009863103739917278}, {"id": 202, "seek": 129136, "start": 1308.32,
  "end": 1314.1599999999999, "text": " stack over flow so I can go and drill in for
  myself you know I don''t have to sort of keep pounding", "tokens": [51212, 8630,
  670, 3095, 370, 286, 393, 352, 293, 11392, 294, 337, 2059, 291, 458, 286, 500, 380,
  362, 281, 1333, 295, 1066, 40034, 51504], "temperature": 0.0, "avg_logprob": -0.10082892120861617,
  "compression_ratio": 1.8549618320610688, "no_speech_prob": 0.009863103739917278},
  {"id": 203, "seek": 129136, "start": 1314.1599999999999, "end": 1319.9199999999998,
  "text": " and you and I''m asking you know okay that didn''t work this didn''t work
  because I can do the same", "tokens": [51504, 293, 291, 293, 286, 478, 3365, 291,
  458, 1392, 300, 994, 380, 589, 341, 994, 380, 589, 570, 286, 393, 360, 264, 912,
  51792], "temperature": 0.0, "avg_logprob": -0.10082892120861617, "compression_ratio":
  1.8549618320610688, "no_speech_prob": 0.009863103739917278}, {"id": 204, "seek":
  131992, "start": 1319.92, "end": 1324.96, "text": " thing just staring at the stack
  over flow page right and maybe they have been already some updates", "tokens": [50364,
  551, 445, 18043, 412, 264, 8630, 670, 3095, 3028, 558, 293, 1310, 436, 362, 668,
  1217, 512, 9205, 50616], "temperature": 0.0, "avg_logprob": -0.14670597423206677,
  "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.0025897531304508448},
  {"id": 205, "seek": 131992, "start": 1324.96, "end": 1329.68, "text": " and someone
  said no that doesn''t work in all some times they see you see the selected answer",
  "tokens": [50616, 293, 1580, 848, 572, 300, 1177, 380, 589, 294, 439, 512, 1413,
  436, 536, 291, 536, 264, 8209, 1867, 50852], "temperature": 0.0, "avg_logprob":
  -0.14670597423206677, "compression_ratio": 1.7522522522522523, "no_speech_prob":
  0.0025897531304508448}, {"id": 206, "seek": 131992, "start": 1329.68, "end": 1335.76,
  "text": " but then there is another one which everyone says that works not the selected
  one so that''s just", "tokens": [50852, 457, 550, 456, 307, 1071, 472, 597, 1518,
  1619, 300, 1985, 406, 264, 8209, 472, 370, 300, 311, 445, 51156], "temperature":
  0.0, "avg_logprob": -0.14670597423206677, "compression_ratio": 1.7522522522522523,
  "no_speech_prob": 0.0025897531304508448}, {"id": 207, "seek": 131992, "start": 1335.76,
  "end": 1343.52, "text": " funny yeah that''s amazing so reader lulm like just to
  sort of bring it back to the ground especially", "tokens": [51156, 4074, 1338, 300,
  311, 2243, 370, 15149, 287, 425, 76, 411, 445, 281, 1333, 295, 1565, 309, 646, 281,
  264, 2727, 2318, 51544], "temperature": 0.0, "avg_logprob": -0.14670597423206677,
  "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.0025897531304508448},
  {"id": 208, "seek": 134352, "start": 1343.52, "end": 1350.72, "text": " for those
  who are sort of no vice like myself I still consider myself no vice you know have
  you", "tokens": [50364, 337, 729, 567, 366, 1333, 295, 572, 11964, 411, 2059, 286,
  920, 1949, 2059, 572, 11964, 291, 458, 362, 291, 50724], "temperature": 0.0, "avg_logprob":
  -0.14166661671229772, "compression_ratio": 1.8160377358490567, "no_speech_prob":
  0.009141028858721256}, {"id": 209, "seek": 134352, "start": 1350.72, "end": 1356.56,
  "text": " sort of taken a reader lulm off the shelf you sort of implemented someone''s
  paper or took it", "tokens": [50724, 1333, 295, 2726, 257, 15149, 287, 425, 76,
  766, 264, 15222, 291, 1333, 295, 12270, 1580, 311, 3035, 420, 1890, 309, 51016],
  "temperature": 0.0, "avg_logprob": -0.14166661671229772, "compression_ratio": 1.8160377358490567,
  "no_speech_prob": 0.009141028858721256}, {"id": 210, "seek": 134352, "start": 1356.56,
  "end": 1364.56, "text": " or did you did you have to train it how did you go about
  it we built it largely from it''s it''s been", "tokens": [51016, 420, 630, 291,
  630, 291, 362, 281, 3847, 309, 577, 630, 291, 352, 466, 309, 321, 3094, 309, 11611,
  490, 309, 311, 309, 311, 668, 51416], "temperature": 0.0, "avg_logprob": -0.14166661671229772,
  "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.009141028858721256},
  {"id": 211, "seek": 134352, "start": 1364.56, "end": 1369.76, "text": " an evolving
  thing but they''re they''re definitely our other reader lulm''s out there the key
  is to", "tokens": [51416, 364, 21085, 551, 457, 436, 434, 436, 434, 2138, 527, 661,
  15149, 287, 425, 76, 311, 484, 456, 264, 2141, 307, 281, 51676], "temperature":
  0.0, "avg_logprob": -0.14166661671229772, "compression_ratio": 1.8160377358490567,
  "no_speech_prob": 0.009141028858721256}, {"id": 212, "seek": 136976, "start": 1369.76,
  "end": 1374.4, "text": " preserve the structure right and the pieces the structure
  that allows you to do similarity we", "tokens": [50364, 15665, 264, 3877, 558, 293,
  264, 3755, 264, 3877, 300, 4045, 291, 281, 360, 32194, 321, 50596], "temperature":
  0.0, "avg_logprob": -0.10438672115928248, "compression_ratio": 1.8314176245210727,
  "no_speech_prob": 0.0012780092656612396}, {"id": 213, "seek": 136976, "start": 1374.4,
  "end": 1378.96, "text": " implemented our own similarity and other algos we also
  do things like named entity recognition and", "tokens": [50596, 12270, 527, 1065,
  32194, 293, 661, 3501, 329, 321, 611, 360, 721, 411, 4926, 13977, 11150, 293, 50824],
  "temperature": 0.0, "avg_logprob": -0.10438672115928248, "compression_ratio": 1.8314176245210727,
  "no_speech_prob": 0.0012780092656612396}, {"id": 214, "seek": 136976, "start": 1378.96,
  "end": 1383.84, "text": " sentiment analysis well those are great at that stuff
  it can do scoring for machine learning", "tokens": [50824, 16149, 5215, 731, 729,
  366, 869, 412, 300, 1507, 309, 393, 360, 22358, 337, 3479, 2539, 51068], "temperature":
  0.0, "avg_logprob": -0.10438672115928248, "compression_ratio": 1.8314176245210727,
  "no_speech_prob": 0.0012780092656612396}, {"id": 215, "seek": 136976, "start": 1383.84,
  "end": 1388.8799999999999, "text": " purposes right so we have a nice intention
  detection system now that will essentially based on", "tokens": [51068, 9932, 558,
  370, 321, 362, 257, 1481, 7789, 17784, 1185, 586, 300, 486, 4476, 2361, 322, 51320],
  "temperature": 0.0, "avg_logprob": -0.10438672115928248, "compression_ratio": 1.8314176245210727,
  "no_speech_prob": 0.0012780092656612396}, {"id": 216, "seek": 136976, "start": 1388.8799999999999,
  "end": 1393.76, "text": " the responses that you get to tell you which sources are
  most relevant right up front right based", "tokens": [51320, 264, 13019, 300, 291,
  483, 281, 980, 291, 597, 7139, 366, 881, 7340, 558, 493, 1868, 558, 2361, 51564],
  "temperature": 0.0, "avg_logprob": -0.10438672115928248, "compression_ratio": 1.8314176245210727,
  "no_speech_prob": 0.0012780092656612396}, {"id": 217, "seek": 139376, "start": 1393.76,
  "end": 1398.24, "text": " on the responses and also optionally ratings right if
  you want to bring bring that into the system", "tokens": [50364, 322, 264, 13019,
  293, 611, 3614, 379, 24603, 558, 498, 291, 528, 281, 1565, 1565, 300, 666, 264,
  1185, 50588], "temperature": 0.0, "avg_logprob": -0.10572098348742333, "compression_ratio":
  1.8314176245210727, "no_speech_prob": 0.001105372211895883}, {"id": 218, "seek":
  139376, "start": 1399.44, "end": 1404.72, "text": " passage detection is totally
  in response our reader lulm''s passage detection is totally in", "tokens": [50648,
  11497, 17784, 307, 3879, 294, 4134, 527, 15149, 287, 425, 76, 311, 11497, 17784,
  307, 3879, 294, 50912], "temperature": 0.0, "avg_logprob": -0.10572098348742333,
  "compression_ratio": 1.8314176245210727, "no_speech_prob": 0.001105372211895883},
  {"id": 219, "seek": 139376, "start": 1404.72, "end": 1410.96, "text": " response
  to the problem you described right which is the data is messy and we don''t necessarily
  want", "tokens": [50912, 4134, 281, 264, 1154, 291, 7619, 558, 597, 307, 264, 1412,
  307, 16191, 293, 321, 500, 380, 4725, 528, 51224], "temperature": 0.0, "avg_logprob":
  -0.10572098348742333, "compression_ratio": 1.8314176245210727, "no_speech_prob":
  0.001105372211895883}, {"id": 220, "seek": 139376, "start": 1410.96, "end": 1417.68,
  "text": " to ship a you know 500 100 page PDFs that have essentially the same data
  so there we it finds the", "tokens": [51224, 281, 5374, 257, 291, 458, 5923, 2319,
  3028, 17752, 82, 300, 362, 4476, 264, 912, 1412, 370, 456, 321, 309, 10704, 264,
  51560], "temperature": 0.0, "avg_logprob": -0.10572098348742333, "compression_ratio":
  1.8314176245210727, "no_speech_prob": 0.001105372211895883}, {"id": 221, "seek":
  139376, "start": 1417.68, "end": 1421.6, "text": " most relevant passage super quickly
  and truncates the document down to a window around it", "tokens": [51560, 881, 7340,
  11497, 1687, 2661, 293, 504, 409, 66, 1024, 264, 4166, 760, 281, 257, 4910, 926,
  309, 51756], "temperature": 0.0, "avg_logprob": -0.10572098348742333, "compression_ratio":
  1.8314176245210727, "no_speech_prob": 0.001105372211895883}, {"id": 222, "seek":
  142160, "start": 1422.56, "end": 1426.8, "text": " um those those are the things
  and it we''ve really implemented it ourselves it''s our it''s our own", "tokens":
  [50412, 1105, 729, 729, 366, 264, 721, 293, 309, 321, 600, 534, 12270, 309, 4175,
  309, 311, 527, 309, 311, 527, 1065, 50624], "temperature": 0.0, "avg_logprob": -0.1914636960593603,
  "compression_ratio": 1.8066037735849056, "no_speech_prob": 0.010378072038292885},
  {"id": 223, "seek": 142160, "start": 1426.8, "end": 1431.9199999999998, "text":
  " it''s our own creation oh that''s fantastic so that''s your secret source as well
  I mean that''s", "tokens": [50624, 309, 311, 527, 1065, 8016, 1954, 300, 311, 5456,
  370, 300, 311, 428, 4054, 4009, 382, 731, 286, 914, 300, 311, 50880], "temperature":
  0.0, "avg_logprob": -0.1914636960593603, "compression_ratio": 1.8066037735849056,
  "no_speech_prob": 0.010378072038292885}, {"id": 224, "seek": 142160, "start": 1431.9199999999998,
  "end": 1438.56, "text": " that''s something to be proud of and also I want to sort
  of close up that the sort of you know", "tokens": [50880, 300, 311, 746, 281, 312,
  4570, 295, 293, 611, 286, 528, 281, 1333, 295, 1998, 493, 300, 264, 1333, 295, 291,
  458, 51212], "temperature": 0.0, "avg_logprob": -0.1914636960593603, "compression_ratio":
  1.8066037735849056, "no_speech_prob": 0.010378072038292885}, {"id": 225, "seek":
  142160, "start": 1438.56, "end": 1446.8799999999999, "text": " description that
  he gave or maybe looking at the future does world have some way of feedback do you",
  "tokens": [51212, 3855, 300, 415, 2729, 420, 1310, 1237, 412, 264, 2027, 775, 1002,
  362, 512, 636, 295, 5824, 360, 291, 51628], "temperature": 0.0, "avg_logprob": -0.1914636960593603,
  "compression_ratio": 1.8066037735849056, "no_speech_prob": 0.010378072038292885},
  {"id": 226, "seek": 144688, "start": 1446.88, "end": 1453.5200000000002, "text":
  " plan to if not do you plan to implement do you think it''s reasonable to have
  a feedback loop you", "tokens": [50364, 1393, 281, 498, 406, 360, 291, 1393, 281,
  4445, 360, 291, 519, 309, 311, 10585, 281, 362, 257, 5824, 6367, 291, 50696], "temperature":
  0.0, "avg_logprob": -0.14271168856276678, "compression_ratio": 1.7092511013215859,
  "no_speech_prob": 0.01309046521782875}, {"id": 227, "seek": 144688, "start": 1453.5200000000002,
  "end": 1458.4, "text": " know like in chat jpd you can say thumbs up thumbs down
  you cannot say much you know you can say", "tokens": [50696, 458, 411, 294, 5081,
  361, 79, 67, 291, 393, 584, 8838, 493, 8838, 760, 291, 2644, 584, 709, 291, 458,
  291, 393, 584, 50940], "temperature": 0.0, "avg_logprob": -0.14271168856276678,
  "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.01309046521782875},
  {"id": 228, "seek": 144688, "start": 1458.4, "end": 1463.68, "text": " this was
  the answer I don''t know if that''s gonna go into the loop but whatever because
  it gives me", "tokens": [50940, 341, 390, 264, 1867, 286, 500, 380, 458, 498, 300,
  311, 799, 352, 666, 264, 6367, 457, 2035, 570, 309, 2709, 385, 51204], "temperature":
  0.0, "avg_logprob": -0.14271168856276678, "compression_ratio": 1.7092511013215859,
  "no_speech_prob": 0.01309046521782875}, {"id": 229, "seek": 144688, "start": 1463.68,
  "end": 1472.4, "text": " the the joy of sort of completing it um yeah oh yes so
  when swirl AI connect is deployed in the", "tokens": [51204, 264, 264, 6258, 295,
  1333, 295, 19472, 309, 1105, 1338, 1954, 2086, 370, 562, 30310, 7318, 1745, 307,
  17826, 294, 264, 51640], "temperature": 0.0, "avg_logprob": -0.14271168856276678,
  "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.01309046521782875},
  {"id": 230, "seek": 147240, "start": 1472.4, "end": 1478.16, "text": " enterprise
  where starters you get to connect the data and get rag and your choice of AI''s
  and by", "tokens": [50364, 14132, 689, 35131, 291, 483, 281, 1745, 264, 1412, 293,
  483, 17539, 293, 428, 3922, 295, 7318, 311, 293, 538, 50652], "temperature": 0.0,
  "avg_logprob": -0.13804718653361003, "compression_ratio": 1.9254901960784314, "no_speech_prob":
  0.0011625030310824513}, {"id": 231, "seek": 147240, "start": 1478.16, "end": 1482.0800000000002,
  "text": " the way it''s again configuration for the AI you put your keys in right
  check pick the model and you", "tokens": [50652, 264, 636, 309, 311, 797, 11694,
  337, 264, 7318, 291, 829, 428, 9317, 294, 558, 1520, 1888, 264, 2316, 293, 291,
  50848], "temperature": 0.0, "avg_logprob": -0.13804718653361003, "compression_ratio":
  1.9254901960784314, "no_speech_prob": 0.0011625030310824513}, {"id": 232, "seek":
  147240, "start": 1482.0800000000002, "end": 1487.0400000000002, "text": " can rag
  against it you can also choose the role you want to use different generative AI
  in rights you", "tokens": [50848, 393, 17539, 1970, 309, 291, 393, 611, 2826, 264,
  3090, 291, 528, 281, 764, 819, 1337, 1166, 7318, 294, 4601, 291, 51096], "temperature":
  0.0, "avg_logprob": -0.13804718653361003, "compression_ratio": 1.9254901960784314,
  "no_speech_prob": 0.0011625030310824513}, {"id": 233, "seek": 147240, "start": 1487.0400000000002,
  "end": 1491.1200000000001, "text": " can use it for query writing you can use it
  for direct answer you can use it for rag if it has", "tokens": [51096, 393, 764,
  309, 337, 14581, 3579, 291, 393, 764, 309, 337, 2047, 1867, 291, 393, 764, 309,
  337, 17539, 498, 309, 575, 51300], "temperature": 0.0, "avg_logprob": -0.13804718653361003,
  "compression_ratio": 1.9254901960784314, "no_speech_prob": 0.0011625030310824513},
  {"id": 234, "seek": 147240, "start": 1491.1200000000001, "end": 1496.8000000000002,
  "text": " embeddings you can use that to power the the reader L so just just to
  be clear there it''s uh it''s", "tokens": [51300, 12240, 29432, 291, 393, 764, 300,
  281, 1347, 264, 264, 15149, 441, 370, 445, 445, 281, 312, 1850, 456, 309, 311, 2232,
  309, 311, 51584], "temperature": 0.0, "avg_logprob": -0.13804718653361003, "compression_ratio":
  1.9254901960784314, "no_speech_prob": 0.0011625030310824513}, {"id": 235, "seek":
  149680, "start": 1496.8, "end": 1502.6399999999999, "text": " a bit more flexible
  yeah i was asking about feedback right so like do you plan do you have it and",
  "tokens": [50364, 257, 857, 544, 11358, 1338, 741, 390, 3365, 466, 5824, 558, 370,
  411, 360, 291, 1393, 360, 291, 362, 309, 293, 50656], "temperature": 0.0, "avg_logprob":
  -0.0796822767991286, "compression_ratio": 1.7419354838709677, "no_speech_prob":
  0.00028872390976175666}, {"id": 236, "seek": 149680, "start": 1502.6399999999999,
  "end": 1508.6399999999999, "text": " if not do you plan to think it''s reasonable
  to have it right absolutely so after you deploy AI", "tokens": [50656, 498, 406,
  360, 291, 1393, 281, 519, 309, 311, 10585, 281, 362, 309, 558, 3122, 370, 934, 291,
  7274, 7318, 50956], "temperature": 0.0, "avg_logprob": -0.0796822767991286, "compression_ratio":
  1.7419354838709677, "no_speech_prob": 0.00028872390976175666}, {"id": 237, "seek":
  149680, "start": 1508.6399999999999, "end": 1513.9199999999998, "text": " connect
  as mentioned you get those abilities then we have an analytics package which will
  give you", "tokens": [50956, 1745, 382, 2835, 291, 483, 729, 11582, 550, 321, 362,
  364, 15370, 7372, 597, 486, 976, 291, 51220], "temperature": 0.0, "avg_logprob":
  -0.0796822767991286, "compression_ratio": 1.7419354838709677, "no_speech_prob":
  0.00028872390976175666}, {"id": 238, "seek": 149680, "start": 1513.9199999999998,
  "end": 1520.6399999999999, "text": " insight as to which sources are providing the
  most relevant responses and rating and putting all", "tokens": [51220, 11269, 382,
  281, 597, 7139, 366, 6530, 264, 881, 7340, 13019, 293, 10990, 293, 3372, 439, 51556],
  "temperature": 0.0, "avg_logprob": -0.0796822767991286, "compression_ratio": 1.7419354838709677,
  "no_speech_prob": 0.00028872390976175666}, {"id": 239, "seek": 149680, "start":
  1520.6399999999999, "end": 1525.04, "text": " of that into dashboards understanding
  who are the number one users who write the best prompts who", "tokens": [51556,
  295, 300, 666, 8240, 17228, 3701, 567, 366, 264, 1230, 472, 5022, 567, 2464, 264,
  1151, 41095, 567, 51776], "temperature": 0.0, "avg_logprob": -0.0796822767991286,
  "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.00028872390976175666},
  {"id": 240, "seek": 152504, "start": 1525.04, "end": 1529.68, "text": " get which
  sources produce the best results which prompts you absolutely is all part of the",
  "tokens": [50364, 483, 597, 7139, 5258, 264, 1151, 3542, 597, 41095, 291, 3122,
  307, 439, 644, 295, 264, 50596], "temperature": 0.0, "avg_logprob": -0.14850575455995363,
  "compression_ratio": 1.743682310469314, "no_speech_prob": 0.00039235540316440165},
  {"id": 241, "seek": 152504, "start": 1530.3999999999999, "end": 1537.28, "text":
  " the offering and ultimately it''s part of what we tailor right for the for the
  deployment and again", "tokens": [50632, 264, 8745, 293, 6284, 309, 311, 644, 295,
  437, 321, 33068, 558, 337, 264, 337, 264, 19317, 293, 797, 50976], "temperature":
  0.0, "avg_logprob": -0.14850575455995363, "compression_ratio": 1.743682310469314,
  "no_speech_prob": 0.00039235540316440165}, {"id": 242, "seek": 152504, "start":
  1537.28, "end": 1543.04, "text": " that can be on premises but AI connect is the
  key because it''s collecting that data on again always", "tokens": [50976, 300,
  393, 312, 322, 34266, 457, 7318, 1745, 307, 264, 2141, 570, 309, 311, 12510, 300,
  1412, 322, 797, 1009, 51264], "temperature": 0.0, "avg_logprob": -0.14850575455995363,
  "compression_ratio": 1.743682310469314, "no_speech_prob": 0.00039235540316440165},
  {"id": 243, "seek": 152504, "start": 1543.04, "end": 1548.24, "text": " in the customers
  private cloud like we don''t see it we''re not sass but that data is absolutely",
  "tokens": [51264, 294, 264, 4581, 4551, 4588, 411, 321, 500, 380, 536, 309, 321,
  434, 406, 262, 640, 457, 300, 1412, 307, 3122, 51524], "temperature": 0.0, "avg_logprob":
  -0.14850575455995363, "compression_ratio": 1.743682310469314, "no_speech_prob":
  0.00039235540316440165}, {"id": 244, "seek": 152504, "start": 1548.24, "end": 1552.8799999999999,
  "text": " turnable into gold a variety of different gold things and so you can hopefully
  figure out which AI", "tokens": [51524, 1261, 712, 666, 3821, 257, 5673, 295, 819,
  3821, 721, 293, 370, 291, 393, 4696, 2573, 484, 597, 7318, 51756], "temperature":
  0.0, "avg_logprob": -0.14850575455995363, "compression_ratio": 1.743682310469314,
  "no_speech_prob": 0.00039235540316440165}, {"id": 245, "seek": 155288, "start":
  1552.88, "end": 1557.6000000000001, "text": " works best for which groups you can
  figure out which sources right are providing the best", "tokens": [50364, 1985,
  1151, 337, 597, 3935, 291, 393, 2573, 484, 597, 7139, 558, 366, 6530, 264, 1151,
  50600], "temperature": 0.0, "avg_logprob": -0.08563851375205844, "compression_ratio":
  1.7224334600760456, "no_speech_prob": 0.004041314125061035}, {"id": 246, "seek":
  155288, "start": 1557.6000000000001, "end": 1566.4, "text": " input for rag etc
  yeah that''s fantastic i love that you do have feedback uh i think it''s definitely",
  "tokens": [50600, 4846, 337, 17539, 5183, 1338, 300, 311, 5456, 741, 959, 300, 291,
  360, 362, 5824, 2232, 741, 519, 309, 311, 2138, 51040], "temperature": 0.0, "avg_logprob":
  -0.08563851375205844, "compression_ratio": 1.7224334600760456, "no_speech_prob":
  0.004041314125061035}, {"id": 247, "seek": 155288, "start": 1566.4, "end": 1570.5600000000002,
  "text": " gold it could could also be super messy and noisy and stuff but it''s
  better than", "tokens": [51040, 3821, 309, 727, 727, 611, 312, 1687, 16191, 293,
  24518, 293, 1507, 457, 309, 311, 1101, 813, 51248], "temperature": 0.0, "avg_logprob":
  -0.08563851375205844, "compression_ratio": 1.7224334600760456, "no_speech_prob":
  0.004041314125061035}, {"id": 248, "seek": 155288, "start": 1571.3600000000001,
  "end": 1577.92, "text": " absence of it um yeah that''s amazing maybe like in the
  past year so you''ve been deploying", "tokens": [51288, 17145, 295, 309, 1105, 1338,
  300, 311, 2243, 1310, 411, 294, 264, 1791, 1064, 370, 291, 600, 668, 34198, 51616],
  "temperature": 0.0, "avg_logprob": -0.08563851375205844, "compression_ratio": 1.7224334600760456,
  "no_speech_prob": 0.004041314125061035}, {"id": 249, "seek": 155288, "start": 1577.92,
  "end": 1582.5600000000002, "text": " this with clients obviously you don''t have
  to mention the names but was there something that", "tokens": [51616, 341, 365,
  6982, 2745, 291, 500, 380, 362, 281, 2152, 264, 5288, 457, 390, 456, 746, 300, 51848],
  "temperature": 0.0, "avg_logprob": -0.08563851375205844, "compression_ratio": 1.7224334600760456,
  "no_speech_prob": 0.004041314125061035}, {"id": 250, "seek": 158256, "start": 1582.6399999999999,
  "end": 1590.1599999999999, "text": " surprised you how clients you know sort of
  perceived uh swirl yeah i i would say that um", "tokens": [50368, 6100, 291, 577,
  6982, 291, 458, 1333, 295, 19049, 2232, 30310, 1338, 741, 741, 576, 584, 300, 1105,
  50744], "temperature": 0.0, "avg_logprob": -0.13787886229428378, "compression_ratio":
  1.6724890829694323, "no_speech_prob": 0.00022580412041861564}, {"id": 251, "seek":
  158256, "start": 1591.52, "end": 1599.36, "text": " people have not really been
  looking for search if i''m the AI the explosion of AI and the excitement", "tokens":
  [50812, 561, 362, 406, 534, 668, 1237, 337, 3164, 498, 741, 478, 264, 7318, 264,
  15673, 295, 7318, 293, 264, 14755, 51204], "temperature": 0.0, "avg_logprob": -0.13787886229428378,
  "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00022580412041861564},
  {"id": 252, "seek": 158256, "start": 1599.36, "end": 1605.52, "text": " around AI
  kind of crowded everything out round everything out so that''s why i think so many
  of these", "tokens": [51204, 926, 7318, 733, 295, 21634, 1203, 484, 3098, 1203,
  484, 370, 300, 311, 983, 741, 519, 370, 867, 295, 613, 51512], "temperature": 0.0,
  "avg_logprob": -0.13787886229428378, "compression_ratio": 1.6724890829694323, "no_speech_prob":
  0.00022580412041861564}, {"id": 253, "seek": 158256, "start": 1605.52, "end": 1611.36,
  "text": " copy in architectures were got so much momentum but and i by the way i
  think people are doing", "tokens": [51512, 5055, 294, 6331, 1303, 645, 658, 370,
  709, 11244, 457, 293, 741, 538, 264, 636, 741, 519, 561, 366, 884, 51804], "temperature":
  0.0, "avg_logprob": -0.13787886229428378, "compression_ratio": 1.6724890829694323,
  "no_speech_prob": 0.00022580412041861564}, {"id": 254, "seek": 161136, "start":
  1611.36, "end": 1615.04, "text": " incredible stuff with that so it''s not like
  those aren''t perfectly legitimate i mean every database", "tokens": [50364, 4651,
  1507, 365, 300, 370, 309, 311, 406, 411, 729, 3212, 380, 6239, 17956, 741, 914,
  633, 8149, 50548], "temperature": 0.0, "avg_logprob": -0.10960712271221613, "compression_ratio":
  1.8014440433212997, "no_speech_prob": 0.0005734601290896535}, {"id": 255, "seek":
  161136, "start": 1615.04, "end": 1620.08, "text": " ever starts that way right you
  put the data in and you get inside of it''s just that there''s a bit more", "tokens":
  [50548, 1562, 3719, 300, 636, 558, 291, 829, 264, 1412, 294, 293, 291, 483, 1854,
  295, 309, 311, 445, 300, 456, 311, 257, 857, 544, 50800], "temperature": 0.0, "avg_logprob":
  -0.10960712271221613, "compression_ratio": 1.8014440433212997, "no_speech_prob":
  0.0005734601290896535}, {"id": 256, "seek": 161136, "start": 1620.08, "end": 1624.3999999999999,
  "text": " to the story right there''s a whole other world of well there''s a lot
  of these and i just moved them", "tokens": [50800, 281, 264, 1657, 558, 456, 311,
  257, 1379, 661, 1002, 295, 731, 456, 311, 257, 688, 295, 613, 293, 741, 445, 4259,
  552, 51016], "temperature": 0.0, "avg_logprob": -0.10960712271221613, "compression_ratio":
  1.8014440433212997, "no_speech_prob": 0.0005734601290896535}, {"id": 257, "seek":
  161136, "start": 1624.3999999999999, "end": 1629.84, "text": " to cloud and i can''t
  necessarily do it but people weren''t thinking search i''m not sure what they",
  "tokens": [51016, 281, 4588, 293, 741, 393, 380, 4725, 360, 309, 457, 561, 4999,
  380, 1953, 3164, 741, 478, 406, 988, 437, 436, 51288], "temperature": 0.0, "avg_logprob":
  -0.10960712271221613, "compression_ratio": 1.8014440433212997, "no_speech_prob":
  0.0005734601290896535}, {"id": 258, "seek": 161136, "start": 1629.84, "end": 1634.8,
  "text": " believed the answer would be but uh there were some excellent posts there
  was one on linkedin by um", "tokens": [51288, 7847, 264, 1867, 576, 312, 457, 2232,
  456, 645, 512, 7103, 12300, 456, 390, 472, 322, 9408, 259, 538, 1105, 51536], "temperature":
  0.0, "avg_logprob": -0.10960712271221613, "compression_ratio": 1.8014440433212997,
  "no_speech_prob": 0.0005734601290896535}, {"id": 259, "seek": 163480, "start": 1635.76,
  "end": 1640.48, "text": " vector ventures i think or um i should probably get the
  name right but in any event they", "tokens": [50412, 8062, 6931, 1303, 741, 519,
  420, 1105, 741, 820, 1391, 483, 264, 1315, 558, 457, 294, 604, 2280, 436, 50648],
  "temperature": 0.0, "avg_logprob": -0.13112862604968953, "compression_ratio": 1.7410071942446044,
  "no_speech_prob": 0.0007252498180605471}, {"id": 260, "seek": 163480, "start": 1641.44,
  "end": 1647.28, "text": " published an excellent piece about how search is probably
  the answer to making AI work in a lot", "tokens": [50696, 6572, 364, 7103, 2522,
  466, 577, 3164, 307, 1391, 264, 1867, 281, 1455, 7318, 589, 294, 257, 688, 50988],
  "temperature": 0.0, "avg_logprob": -0.13112862604968953, "compression_ratio": 1.7410071942446044,
  "no_speech_prob": 0.0007252498180605471}, {"id": 261, "seek": 163480, "start": 1647.28,
  "end": 1651.76, "text": " of these cases and uh you know they also point out there''s
  not that many people who have come at it", "tokens": [50988, 295, 613, 3331, 293,
  2232, 291, 458, 436, 611, 935, 484, 456, 311, 406, 300, 867, 561, 567, 362, 808,
  412, 309, 51212], "temperature": 0.0, "avg_logprob": -0.13112862604968953, "compression_ratio":
  1.7410071942446044, "no_speech_prob": 0.0007252498180605471}, {"id": 262, "seek":
  163480, "start": 1651.76, "end": 1657.04, "text": " from the search perspective
  so that was a bit surprising to me because the large enterprise has", "tokens":
  [51212, 490, 264, 3164, 4585, 370, 300, 390, 257, 857, 8830, 281, 385, 570, 264,
  2416, 14132, 575, 51476], "temperature": 0.0, "avg_logprob": -0.13112862604968953,
  "compression_ratio": 1.7410071942446044, "no_speech_prob": 0.0007252498180605471},
  {"id": 263, "seek": 163480, "start": 1657.84, "end": 1664.48, "text": " always loved
  search always uh because that that''s how knowledge workers and people get stuff
  done right", "tokens": [51516, 1009, 4333, 3164, 1009, 2232, 570, 300, 300, 311,
  577, 3601, 5600, 293, 561, 483, 1507, 1096, 558, 51848], "temperature": 0.0, "avg_logprob":
  -0.13112862604968953, "compression_ratio": 1.7410071942446044, "no_speech_prob":
  0.0007252498180605471}, {"id": 264, "seek": 166448, "start": 1664.48, "end": 1668.56,
  "text": " it''s yes you have business intelligence and dashboards and reporting
  and we like those things but", "tokens": [50364, 309, 311, 2086, 291, 362, 1606,
  7599, 293, 8240, 17228, 293, 10031, 293, 321, 411, 729, 721, 457, 50568], "temperature":
  0.0, "avg_logprob": -0.12428855895996094, "compression_ratio": 1.6888888888888889,
  "no_speech_prob": 0.00013345517800189555}, {"id": 265, "seek": 166448, "start":
  1669.28, "end": 1674.0, "text": " so much of the qualitative why did things happen
  explain it to me how do i solve this", "tokens": [50604, 370, 709, 295, 264, 31312,
  983, 630, 721, 1051, 2903, 309, 281, 385, 577, 360, 741, 5039, 341, 50840], "temperature":
  0.0, "avg_logprob": -0.12428855895996094, "compression_ratio": 1.6888888888888889,
  "no_speech_prob": 0.00013345517800189555}, {"id": 266, "seek": 166448, "start":
  1674.56, "end": 1680.8, "text": " that''s been something that search did a good
  job of um ultimately it''s a technology right and the", "tokens": [50868, 300, 311,
  668, 746, 300, 3164, 630, 257, 665, 1691, 295, 1105, 6284, 309, 311, 257, 2899,
  558, 293, 264, 51180], "temperature": 0.0, "avg_logprob": -0.12428855895996094,
  "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00013345517800189555},
  {"id": 267, "seek": 166448, "start": 1680.8, "end": 1690.32, "text": " marriage
  of search with llm that seems to be the unlocker if you will in the enterprise and
  that''s", "tokens": [51180, 7194, 295, 3164, 365, 287, 75, 76, 300, 2544, 281, 312,
  264, 11634, 260, 498, 291, 486, 294, 264, 14132, 293, 300, 311, 51656], "temperature":
  0.0, "avg_logprob": -0.12428855895996094, "compression_ratio": 1.6888888888888889,
  "no_speech_prob": 0.00013345517800189555}, {"id": 268, "seek": 169032, "start":
  1690.3999999999999, "end": 1694.8, "text": " that was surprising to me right i i
  thought that there would be much more of a search first approach", "tokens": [50368,
  300, 390, 8830, 281, 385, 558, 741, 741, 1194, 300, 456, 576, 312, 709, 544, 295,
  257, 3164, 700, 3109, 50588], "temperature": 0.0, "avg_logprob": -0.10382959577772352,
  "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.001205646782182157},
  {"id": 269, "seek": 169032, "start": 1694.8, "end": 1699.2, "text": " and i think
  everybody had to get through the understanding of what it means to adopt AI and
  how", "tokens": [50588, 293, 741, 519, 2201, 632, 281, 483, 807, 264, 3701, 295,
  437, 309, 1355, 281, 6878, 7318, 293, 577, 50808], "temperature": 0.0, "avg_logprob":
  -0.10382959577772352, "compression_ratio": 1.7741935483870968, "no_speech_prob":
  0.001205646782182157}, {"id": 270, "seek": 169032, "start": 1699.2, "end": 1706.48,
  "text": " the first generation works and now i think people are recognizing real
  time real time architecture using", "tokens": [50808, 264, 700, 5125, 1985, 293,
  586, 741, 519, 561, 366, 18538, 957, 565, 957, 565, 9482, 1228, 51172], "temperature":
  0.0, "avg_logprob": -0.10382959577772352, "compression_ratio": 1.7741935483870968,
  "no_speech_prob": 0.001205646782182157}, {"id": 271, "seek": 169032, "start": 1706.48,
  "end": 1713.04, "text": " systems of record with re-ranking and and then just stay
  keeping up right with the incredible", "tokens": [51172, 3652, 295, 2136, 365, 319,
  12, 20479, 278, 293, 293, 550, 445, 1754, 5145, 493, 558, 365, 264, 4651, 51500],
  "temperature": 0.0, "avg_logprob": -0.10382959577772352, "compression_ratio": 1.7741935483870968,
  "no_speech_prob": 0.001205646782182157}, {"id": 272, "seek": 169032, "start": 1713.04,
  "end": 1718.3999999999999, "text": " innovation in it in let''s call it generative
  AI right that''s that''s the interesting thing but again", "tokens": [51500, 8504,
  294, 309, 294, 718, 311, 818, 309, 1337, 1166, 7318, 558, 300, 311, 300, 311, 264,
  1880, 551, 457, 797, 51768], "temperature": 0.0, "avg_logprob": -0.10382959577772352,
  "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.001205646782182157},
  {"id": 273, "seek": 171840, "start": 1718.5600000000002, "end": 1722.72, "text":
  " i come back to what i said at the beginning there''s going to be a many many incredible
  generative", "tokens": [50372, 741, 808, 646, 281, 437, 741, 848, 412, 264, 2863,
  456, 311, 516, 281, 312, 257, 867, 867, 4651, 1337, 1166, 50580], "temperature":
  0.0, "avg_logprob": -0.14686921161154043, "compression_ratio": 1.862962962962963,
  "no_speech_prob": 0.0010927822440862656}, {"id": 274, "seek": 171840, "start": 1722.72,
  "end": 1727.76, "text": " AI''s they''re going to do different things that we haven''t
  even seen i don''t think the most extraordinary", "tokens": [50580, 7318, 311, 436,
  434, 516, 281, 360, 819, 721, 300, 321, 2378, 380, 754, 1612, 741, 500, 380, 519,
  264, 881, 10581, 50832], "temperature": 0.0, "avg_logprob": -0.14686921161154043,
  "compression_ratio": 1.862962962962963, "no_speech_prob": 0.0010927822440862656},
  {"id": 275, "seek": 171840, "start": 1727.76, "end": 1732.16, "text": " ones they''ll
  be from the big science publishers they''re going to build incredible life sciences
  gai", "tokens": [50832, 2306, 436, 603, 312, 490, 264, 955, 3497, 30421, 436, 434,
  516, 281, 1322, 4651, 993, 17677, 290, 1301, 51052], "temperature": 0.0, "avg_logprob":
  -0.14686921161154043, "compression_ratio": 1.862962962962963, "no_speech_prob":
  0.0010927822440862656}, {"id": 276, "seek": 171840, "start": 1732.8000000000002,
  "end": 1739.2800000000002, "text": " i''m sure people like bloomberg ft you''re
  going to build incredible um financial ones and that''s great", "tokens": [51084,
  741, 478, 988, 561, 411, 26899, 6873, 283, 83, 291, 434, 516, 281, 1322, 4651, 1105,
  4669, 2306, 293, 300, 311, 869, 51408], "temperature": 0.0, "avg_logprob": -0.14686921161154043,
  "compression_ratio": 1.862962962962963, "no_speech_prob": 0.0010927822440862656},
  {"id": 277, "seek": 171840, "start": 1739.92, "end": 1747.52, "text": " but all
  of those still need your data to operate on your environment and give you answers
  that are", "tokens": [51440, 457, 439, 295, 729, 920, 643, 428, 1412, 281, 9651,
  322, 428, 2823, 293, 976, 291, 6338, 300, 366, 51820], "temperature": 0.0, "avg_logprob":
  -0.14686921161154043, "compression_ratio": 1.862962962962963, "no_speech_prob":
  0.0010927822440862656}, {"id": 278, "seek": 174752, "start": 1747.52, "end": 1753.68,
  "text": " meaningful and that problem that problem is the problem that''s world
  solves so just to understand", "tokens": [50364, 10995, 293, 300, 1154, 300, 1154,
  307, 264, 1154, 300, 311, 1002, 39890, 370, 445, 281, 1223, 50672], "temperature":
  0.0, "avg_logprob": -0.1668013466729058, "compression_ratio": 1.813953488372093,
  "no_speech_prob": 0.0017082623671740294}, {"id": 279, "seek": 174752, "start": 1753.68,
  "end": 1761.44, "text": " what were they expecting was it like chat interface or
  you said they didn''t expect search no they", "tokens": [50672, 437, 645, 436, 9650,
  390, 309, 411, 5081, 9226, 420, 291, 848, 436, 994, 380, 2066, 3164, 572, 436, 51060],
  "temperature": 0.0, "avg_logprob": -0.1668013466729058, "compression_ratio": 1.813953488372093,
  "no_speech_prob": 0.0017082623671740294}, {"id": 280, "seek": 174752, "start": 1761.44,
  "end": 1768.0, "text": " thought they would ask the question oh yeah so they thought
  like a chat right yeah that everybody", "tokens": [51060, 1194, 436, 576, 1029,
  264, 1168, 1954, 1338, 370, 436, 1194, 411, 257, 5081, 558, 1338, 300, 2201, 51388],
  "temperature": 0.0, "avg_logprob": -0.1668013466729058, "compression_ratio": 1.813953488372093,
  "no_speech_prob": 0.0017082623671740294}, {"id": 281, "seek": 174752, "start": 1768.0,
  "end": 1772.96, "text": " wants kind of a conversational interface you know another
  thing i learned actually you''re really", "tokens": [51388, 2738, 733, 295, 257,
  2615, 1478, 9226, 291, 458, 1071, 551, 741, 3264, 767, 291, 434, 534, 51636], "temperature":
  0.0, "avg_logprob": -0.1668013466729058, "compression_ratio": 1.813953488372093,
  "no_speech_prob": 0.0017082623671740294}, {"id": 282, "seek": 177296, "start": 1772.96,
  "end": 1780.24, "text": " reminded me people are not so interested in the idea that
  there is an AI place that you go i think", "tokens": [50364, 15920, 385, 561, 366,
  406, 370, 3102, 294, 264, 1558, 300, 456, 307, 364, 7318, 1081, 300, 291, 352, 741,
  519, 50728], "temperature": 0.0, "avg_logprob": -0.0865283693586077, "compression_ratio":
  1.6926406926406927, "no_speech_prob": 0.0011206247145310044}, {"id": 283, "seek":
  177296, "start": 1780.24, "end": 1785.52, "text": " another very logical step is
  business folks knowledge workers they would like to use the channels", "tokens":
  [50728, 1071, 588, 14978, 1823, 307, 1606, 4024, 3601, 5600, 436, 576, 411, 281,
  764, 264, 9235, 50992], "temperature": 0.0, "avg_logprob": -0.0865283693586077,
  "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.0011206247145310044},
  {"id": 284, "seek": 177296, "start": 1785.52, "end": 1790.32, "text": " they use
  today so rather than i have to have a new place to go why can''t i talk to it on
  teens", "tokens": [50992, 436, 764, 965, 370, 2831, 813, 741, 362, 281, 362, 257,
  777, 1081, 281, 352, 983, 393, 380, 741, 751, 281, 309, 322, 24849, 51232], "temperature":
  0.0, "avg_logprob": -0.0865283693586077, "compression_ratio": 1.6926406926406927,
  "no_speech_prob": 0.0011206247145310044}, {"id": 285, "seek": 177296, "start": 1791.04,
  "end": 1799.52, "text": " or in slack or on my whatsapp why can''t i text it um
  if i need to get a visual i could always go to", "tokens": [51268, 420, 294, 29767,
  420, 322, 452, 29625, 1746, 983, 393, 380, 741, 2487, 309, 1105, 498, 741, 643,
  281, 483, 257, 5056, 741, 727, 1009, 352, 281, 51692], "temperature": 0.0, "avg_logprob":
  -0.0865283693586077, "compression_ratio": 1.6926406926406927, "no_speech_prob":
  0.0011206247145310044}, {"id": 286, "seek": 179952, "start": 1799.52, "end": 1804.8799999999999,
  "text": " a screen right and then i could have it show me the underlying data show
  me the graph show me the", "tokens": [50364, 257, 2568, 558, 293, 550, 741, 727,
  362, 309, 855, 385, 264, 14217, 1412, 855, 385, 264, 4295, 855, 385, 264, 50632],
  "temperature": 0.0, "avg_logprob": -0.1014098403274372, "compression_ratio": 1.7946428571428572,
  "no_speech_prob": 0.0011538842227309942}, {"id": 287, "seek": 179952, "start": 1804.8799999999999,
  "end": 1812.8799999999999, "text": " chart but for the the the future is not applications
  that are destinations the future is an ongoing", "tokens": [50632, 6927, 457, 337,
  264, 264, 264, 2027, 307, 406, 5821, 300, 366, 37787, 264, 2027, 307, 364, 10452,
  51032], "temperature": 0.0, "avg_logprob": -0.1014098403274372, "compression_ratio":
  1.7946428571428572, "no_speech_prob": 0.0011538842227309942}, {"id": 288, "seek":
  179952, "start": 1812.8799999999999, "end": 1818.48, "text": " that i log with the
  AI that understands your business your world has access to your data and becomes",
  "tokens": [51032, 300, 741, 3565, 365, 264, 7318, 300, 15146, 428, 1606, 428, 1002,
  575, 2105, 281, 428, 1412, 293, 3643, 51312], "temperature": 0.0, "avg_logprob":
  -0.1014098403274372, "compression_ratio": 1.7946428571428572, "no_speech_prob":
  0.0011538842227309942}, {"id": 289, "seek": 179952, "start": 1818.48, "end": 1824.0,
  "text": " your uh trusted advisor and agent i don''t want to use the word co-pilot
  because i think that''s a little", "tokens": [51312, 428, 2232, 16034, 19161, 293,
  9461, 741, 500, 380, 528, 281, 764, 264, 1349, 598, 12, 79, 31516, 570, 741, 519,
  300, 311, 257, 707, 51588], "temperature": 0.0, "avg_logprob": -0.1014098403274372,
  "compression_ratio": 1.7946428571428572, "no_speech_prob": 0.0011538842227309942},
  {"id": 290, "seek": 182400, "start": 1824.96, "end": 1829.68, "text": " it''s much
  more your confidant it''s much more your agent it''s going to tell you stuff like
  hey", "tokens": [50412, 309, 311, 709, 544, 428, 1497, 327, 394, 309, 311, 709,
  544, 428, 9461, 309, 311, 516, 281, 980, 291, 1507, 411, 4177, 50648], "temperature":
  0.0, "avg_logprob": -0.12206665674845378, "compression_ratio": 1.946843853820598,
  "no_speech_prob": 0.004025470931082964}, {"id": 291, "seek": 182400, "start": 1829.92,
  "end": 1834.08, "text": " that question you asked last month there''s a very different
  answer this month that''s a pretty", "tokens": [50660, 300, 1168, 291, 2351, 1036,
  1618, 456, 311, 257, 588, 819, 1867, 341, 1618, 300, 311, 257, 1238, 50868], "temperature":
  0.0, "avg_logprob": -0.12206665674845378, "compression_ratio": 1.946843853820598,
  "no_speech_prob": 0.004025470931082964}, {"id": 292, "seek": 182400, "start": 1834.08,
  "end": 1838.4, "text": " interesting thing or it''s going to let you explore so
  you know tell me about our customer service", "tokens": [50868, 1880, 551, 420,
  309, 311, 516, 281, 718, 291, 6839, 370, 291, 458, 980, 385, 466, 527, 5474, 2643,
  51084], "temperature": 0.0, "avg_logprob": -0.12206665674845378, "compression_ratio":
  1.946843853820598, "no_speech_prob": 0.004025470931082964}, {"id": 293, "seek":
  182400, "start": 1838.4, "end": 1843.36, "text": " ratings well which region right
  disambiguation which was previously something you know that you would", "tokens":
  [51084, 24603, 731, 597, 4458, 558, 717, 2173, 328, 16073, 597, 390, 8046, 746,
  291, 458, 300, 291, 576, 51332], "temperature": 0.0, "avg_logprob": -0.12206665674845378,
  "compression_ratio": 1.946843853820598, "no_speech_prob": 0.004025470931082964},
  {"id": 294, "seek": 182400, "start": 1843.36, "end": 1848.8, "text": " do like through
  facets in search right that''s kind of thing that should become more dialogue oriented",
  "tokens": [51332, 360, 411, 807, 49752, 294, 3164, 558, 300, 311, 733, 295, 551,
  300, 820, 1813, 544, 10221, 21841, 51604], "temperature": 0.0, "avg_logprob": -0.12206665674845378,
  "compression_ratio": 1.946843853820598, "no_speech_prob": 0.004025470931082964},
  {"id": 295, "seek": 182400, "start": 1848.8, "end": 1853.04, "text": " but those
  things that''s going to take some time because in order to know how to disambiguate
  you", "tokens": [51604, 457, 729, 721, 300, 311, 516, 281, 747, 512, 565, 570, 294,
  1668, 281, 458, 577, 281, 717, 2173, 328, 10107, 291, 51816], "temperature": 0.0,
  "avg_logprob": -0.12206665674845378, "compression_ratio": 1.946843853820598, "no_speech_prob":
  0.004025470931082964}, {"id": 296, "seek": 185304, "start": 1853.04, "end": 1857.6,
  "text": " still have to know what data is relevant right so so that''s been surprising
  but i think we''re going", "tokens": [50364, 920, 362, 281, 458, 437, 1412, 307,
  7340, 558, 370, 370, 300, 311, 668, 8830, 457, 741, 519, 321, 434, 516, 50592],
  "temperature": 0.0, "avg_logprob": -0.1127456318248402, "compression_ratio": 1.7863636363636364,
  "no_speech_prob": 0.0001136185455834493}, {"id": 297, "seek": 185304, "start": 1857.6,
  "end": 1864.6399999999999, "text": " to see a wave of search driven innovation and
  i''m excited about it i think the more people shift away", "tokens": [50592, 281,
  536, 257, 5772, 295, 3164, 9555, 8504, 293, 741, 478, 2919, 466, 309, 741, 519,
  264, 544, 561, 5513, 1314, 50944], "temperature": 0.0, "avg_logprob": -0.1127456318248402,
  "compression_ratio": 1.7863636363636364, "no_speech_prob": 0.0001136185455834493},
  {"id": 298, "seek": 185304, "start": 1864.6399999999999, "end": 1870.56, "text":
  " from innovating in a repository to innovating across repositories we''ll see you
  know another", "tokens": [50944, 490, 5083, 990, 294, 257, 25841, 281, 5083, 990,
  2108, 22283, 2083, 321, 603, 536, 291, 458, 1071, 51240], "temperature": 0.0, "avg_logprob":
  -0.1127456318248402, "compression_ratio": 1.7863636363636364, "no_speech_prob":
  0.0001136185455834493}, {"id": 299, "seek": 185304, "start": 1870.56, "end": 1876.48,
  "text": " another layer of innovation and and even more productivity left right
  for for the people to use it", "tokens": [51240, 1071, 4583, 295, 8504, 293, 293,
  754, 544, 15604, 1411, 558, 337, 337, 264, 561, 281, 764, 309, 51536], "temperature":
  0.0, "avg_logprob": -0.1127456318248402, "compression_ratio": 1.7863636363636364,
  "no_speech_prob": 0.0001136185455834493}, {"id": 300, "seek": 187648, "start": 1877.1200000000001,
  "end": 1882.72, "text": " oh yeah that''s fantastic the way i put it and i''m glad
  to hear this because there''s the search", "tokens": [50396, 1954, 1338, 300, 311,
  5456, 264, 636, 741, 829, 309, 293, 741, 478, 5404, 281, 1568, 341, 570, 456, 311,
  264, 3164, 50676], "temperature": 0.0, "avg_logprob": -0.15686762068006727, "compression_ratio":
  1.7219730941704037, "no_speech_prob": 0.00398709811270237}, {"id": 301, "seek":
  187648, "start": 1882.72, "end": 1889.44, "text": " professional you know now a
  product manager i love the fact that the powerhouse of you know the", "tokens":
  [50676, 4843, 291, 458, 586, 257, 1674, 6598, 741, 959, 264, 1186, 300, 264, 1347,
  6410, 295, 291, 458, 264, 51012], "temperature": 0.0, "avg_logprob": -0.15686762068006727,
  "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00398709811270237},
  {"id": 302, "seek": 187648, "start": 1889.44, "end": 1896.16, "text": " future of
  AI still continues to be searched right at the core and i think search it also says
  that", "tokens": [51012, 2027, 295, 7318, 920, 6515, 281, 312, 22961, 558, 412,
  264, 4965, 293, 741, 519, 3164, 309, 611, 1619, 300, 51348], "temperature": 0.0,
  "avg_logprob": -0.15686762068006727, "compression_ratio": 1.7219730941704037, "no_speech_prob":
  0.00398709811270237}, {"id": 303, "seek": 187648, "start": 1896.16, "end": 1902.8,
  "text": " search isn''t solved and maybe this is another iteration that we will
  approach it but it''s like", "tokens": [51348, 3164, 1943, 380, 13041, 293, 1310,
  341, 307, 1071, 24784, 300, 321, 486, 3109, 309, 457, 309, 311, 411, 51680], "temperature":
  0.0, "avg_logprob": -0.15686762068006727, "compression_ratio": 1.7219730941704037,
  "no_speech_prob": 0.00398709811270237}, {"id": 304, "seek": 190280, "start": 1903.36,
  "end": 1909.04, "text": " because search is also perception right it''s also how
  you express yourself how you perceive what you", "tokens": [50392, 570, 3164, 307,
  611, 12860, 558, 309, 311, 611, 577, 291, 5109, 1803, 577, 291, 20281, 437, 291,
  50676], "temperature": 0.0, "avg_logprob": -0.10426116270177505, "compression_ratio":
  1.8130841121495327, "no_speech_prob": 0.004224587697535753}, {"id": 305, "seek":
  190280, "start": 1909.04, "end": 1916.32, "text": " see maybe the interfaces will
  change right so sometimes i do want to you know that that product", "tokens": [50676,
  536, 1310, 264, 28416, 486, 1319, 558, 370, 2171, 741, 360, 528, 281, 291, 458,
  300, 300, 1674, 51040], "temperature": 0.0, "avg_logprob": -0.10426116270177505,
  "compression_ratio": 1.8130841121495327, "no_speech_prob": 0.004224587697535753},
  {"id": 306, "seek": 190280, "start": 1916.32, "end": 1922.48, "text": " that google
  had google glass sometimes i want to have glass on me to take a picture or you know",
  "tokens": [51040, 300, 20742, 632, 20742, 4276, 2171, 741, 528, 281, 362, 4276,
  322, 385, 281, 747, 257, 3036, 420, 291, 458, 51348], "temperature": 0.0, "avg_logprob":
  -0.10426116270177505, "compression_ratio": 1.8130841121495327, "no_speech_prob":
  0.004224587697535753}, {"id": 307, "seek": 190280, "start": 1922.48, "end": 1928.6399999999999,
  "text": " not to be as distracted by going and fetching my phone or something right
  because today i still", "tokens": [51348, 406, 281, 312, 382, 21658, 538, 516, 293,
  23673, 278, 452, 2593, 420, 746, 558, 570, 965, 741, 920, 51656], "temperature":
  0.0, "avg_logprob": -0.10426116270177505, "compression_ratio": 1.8130841121495327,
  "no_speech_prob": 0.004224587697535753}, {"id": 308, "seek": 192864, "start": 1928.72,
  "end": 1934.8000000000002, "text": " have to do that it''s not as immersive experience
  and also i''ve noticed working with engineers now", "tokens": [50368, 362, 281,
  360, 300, 309, 311, 406, 382, 35409, 1752, 293, 611, 741, 600, 5694, 1364, 365,
  11955, 586, 50672], "temperature": 0.0, "avg_logprob": -0.09696457121107313, "compression_ratio":
  1.7454545454545454, "no_speech_prob": 0.012357759289443493}, {"id": 309, "seek":
  192864, "start": 1934.8000000000002, "end": 1940.0800000000002, "text": " when i
  flipped on this side of you know the process i''m a product manager so i keep thinking
  about", "tokens": [50672, 562, 741, 26273, 322, 341, 1252, 295, 291, 458, 264, 1399,
  741, 478, 257, 1674, 6598, 370, 741, 1066, 1953, 466, 50936], "temperature": 0.0,
  "avg_logprob": -0.09696457121107313, "compression_ratio": 1.7454545454545454, "no_speech_prob":
  0.012357759289443493}, {"id": 310, "seek": 192864, "start": 1940.0800000000002,
  "end": 1945.76, "text": " things and they keep coding and sometimes i''ve noticed
  that they don''t even go back on slack", "tokens": [50936, 721, 293, 436, 1066,
  17720, 293, 2171, 741, 600, 5694, 300, 436, 500, 380, 754, 352, 646, 322, 29767,
  51220], "temperature": 0.0, "avg_logprob": -0.09696457121107313, "compression_ratio":
  1.7454545454545454, "no_speech_prob": 0.012357759289443493}, {"id": 311, "seek":
  192864, "start": 1945.76, "end": 1952.4, "text": " from like a couple hours when
  i when i ask something because they don''t want to be uh you know", "tokens": [51220,
  490, 411, 257, 1916, 2496, 562, 741, 562, 741, 1029, 746, 570, 436, 500, 380, 528,
  281, 312, 2232, 291, 458, 51552], "temperature": 0.0, "avg_logprob": -0.09696457121107313,
  "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.012357759289443493},
  {"id": 312, "seek": 195240, "start": 1952.96, "end": 1960.88, "text": " distracted
  from their ideas so maybe there could be a way for me or agent or whoever to sort
  of", "tokens": [50392, 21658, 490, 641, 3487, 370, 1310, 456, 727, 312, 257, 636,
  337, 385, 420, 9461, 420, 11387, 281, 1333, 295, 50788], "temperature": 0.0, "avg_logprob":
  -0.1186936212622601, "compression_ratio": 1.7692307692307692, "no_speech_prob":
  0.0033342058304697275}, {"id": 313, "seek": 195240, "start": 1960.88, "end": 1967.2800000000002,
  "text": " sneak into their idea ask a question talk to them right that would be
  fantastic maybe it sounds", "tokens": [50788, 13164, 666, 641, 1558, 1029, 257,
  1168, 751, 281, 552, 558, 300, 576, 312, 5456, 1310, 309, 3263, 51108], "temperature":
  0.0, "avg_logprob": -0.1186936212622601, "compression_ratio": 1.7692307692307692,
  "no_speech_prob": 0.0033342058304697275}, {"id": 314, "seek": 195240, "start": 1967.2800000000002,
  "end": 1974.0, "text": " also a little crazy like you you still want to have privacy
  and sort of you know uh flow but at the", "tokens": [51108, 611, 257, 707, 3219,
  411, 291, 291, 920, 528, 281, 362, 11427, 293, 1333, 295, 291, 458, 2232, 3095,
  457, 412, 264, 51444], "temperature": 0.0, "avg_logprob": -0.1186936212622601, "compression_ratio":
  1.7692307692307692, "no_speech_prob": 0.0033342058304697275}, {"id": 315, "seek":
  195240, "start": 1974.0, "end": 1980.24, "text": " same time there is reality of
  your job right you you do need to go back to your email to your slack", "tokens":
  [51444, 912, 565, 456, 307, 4103, 295, 428, 1691, 558, 291, 291, 360, 643, 281,
  352, 646, 281, 428, 3796, 281, 428, 29767, 51756], "temperature": 0.0, "avg_logprob":
  -0.1186936212622601, "compression_ratio": 1.7692307692307692, "no_speech_prob":
  0.0033342058304697275}, {"id": 316, "seek": 198024, "start": 1980.32, "end": 1986.08,
  "text": " or whatever you''re using teams and get distracted and then you forget
  what is it that you''ve been", "tokens": [50368, 420, 2035, 291, 434, 1228, 5491,
  293, 483, 21658, 293, 550, 291, 2870, 437, 307, 309, 300, 291, 600, 668, 50656],
  "temperature": 0.0, "avg_logprob": -0.08900578429059285, "compression_ratio": 1.6822033898305084,
  "no_speech_prob": 0.008829892612993717}, {"id": 317, "seek": 198024, "start": 1986.08,
  "end": 1994.72, "text": " onto when you come back to your your motive execution
  absolutely you know in a way applications", "tokens": [50656, 3911, 562, 291, 808,
  646, 281, 428, 428, 28827, 15058, 3122, 291, 458, 294, 257, 636, 5821, 51088], "temperature":
  0.0, "avg_logprob": -0.08900578429059285, "compression_ratio": 1.6822033898305084,
  "no_speech_prob": 0.008829892612993717}, {"id": 318, "seek": 198024, "start": 1994.72,
  "end": 1999.2, "text": " are distracting i think there was a really good study recently
  that showed the danger of interrupting", "tokens": [51088, 366, 36689, 741, 519,
  456, 390, 257, 534, 665, 2979, 3938, 300, 4712, 264, 4330, 295, 49455, 51312], "temperature":
  0.0, "avg_logprob": -0.08900578429059285, "compression_ratio": 1.6822033898305084,
  "no_speech_prob": 0.008829892612993717}, {"id": 319, "seek": 198024, "start": 1999.2,
  "end": 2004.96, "text": " engineers right because of the context switch um it''s
  definitely the same for business people they''re", "tokens": [51312, 11955, 558,
  570, 295, 264, 4319, 3679, 1105, 309, 311, 2138, 264, 912, 337, 1606, 561, 436,
  434, 51600], "temperature": 0.0, "avg_logprob": -0.08900578429059285, "compression_ratio":
  1.6822033898305084, "no_speech_prob": 0.008829892612993717}, {"id": 320, "seek":
  200496, "start": 2005.04, "end": 2010.88, "text": " just like everybody else right
  context matters and it can be hard to switch um i think that''s the real", "tokens":
  [50368, 445, 411, 2201, 1646, 558, 4319, 7001, 293, 309, 393, 312, 1152, 281, 3679,
  1105, 741, 519, 300, 311, 264, 957, 50660], "temperature": 0.0, "avg_logprob": -0.0938236220129605,
  "compression_ratio": 1.8507462686567164, "no_speech_prob": 0.0005857805954292417},
  {"id": 321, "seek": 200496, "start": 2010.88, "end": 2015.92, "text": " promise
  of a i look at chat gpt you go to chat gpt they''re gonna have search soon web search
  you can", "tokens": [50660, 6228, 295, 257, 741, 574, 412, 5081, 290, 662, 291,
  352, 281, 5081, 290, 662, 436, 434, 799, 362, 3164, 2321, 3670, 3164, 291, 393,
  50912], "temperature": 0.0, "avg_logprob": -0.0938236220129605, "compression_ratio":
  1.8507462686567164, "no_speech_prob": 0.0005857805954292417}, {"id": 322, "seek":
  200496, "start": 2015.92, "end": 2020.96, "text": " ask it questions you don''t
  have to go to google and being in five other places right to get it", "tokens":
  [50912, 1029, 309, 1651, 291, 500, 380, 362, 281, 352, 281, 20742, 293, 885, 294,
  1732, 661, 3190, 558, 281, 483, 309, 51164], "temperature": 0.0, "avg_logprob":
  -0.0938236220129605, "compression_ratio": 1.8507462686567164, "no_speech_prob":
  0.0005857805954292417}, {"id": 323, "seek": 200496, "start": 2020.96, "end": 2027.76,
  "text": " and that is the real possibility that you would choose the way you want
  to interact with it and", "tokens": [51164, 293, 300, 307, 264, 957, 7959, 300,
  291, 576, 2826, 264, 636, 291, 528, 281, 4648, 365, 309, 293, 51504], "temperature":
  0.0, "avg_logprob": -0.0938236220129605, "compression_ratio": 1.8507462686567164,
  "no_speech_prob": 0.0005857805954292417}, {"id": 324, "seek": 200496, "start": 2027.76,
  "end": 2034.56, "text": " that thing in theory that single point that single pane
  of glass or single conversational agent right", "tokens": [51504, 300, 551, 294,
  5261, 300, 2167, 935, 300, 2167, 32605, 295, 4276, 420, 2167, 2615, 1478, 9461,
  558, 51844], "temperature": 0.0, "avg_logprob": -0.0938236220129605, "compression_ratio":
  1.8507462686567164, "no_speech_prob": 0.0005857805954292417}, {"id": 325, "seek":
  203456, "start": 2034.96, "end": 2040.8, "text": " that could potentially be in
  front of many many sources of data and that i think that''s what", "tokens": [50384,
  300, 727, 7263, 312, 294, 1868, 295, 867, 867, 7139, 295, 1412, 293, 300, 741, 519,
  300, 311, 437, 50676], "temperature": 0.0, "avg_logprob": -0.11139005144065786,
  "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.002264224225655198},
  {"id": 326, "seek": 203456, "start": 2040.8, "end": 2044.96, "text": " what people
  realize it''s hard to say how does what does it really look like in five years",
  "tokens": [50676, 437, 561, 4325, 309, 311, 1152, 281, 584, 577, 775, 437, 775,
  309, 534, 574, 411, 294, 1732, 924, 50884], "temperature": 0.0, "avg_logprob": -0.11139005144065786,
  "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.002264224225655198},
  {"id": 327, "seek": 203456, "start": 2044.96, "end": 2049.2, "text": " if a i really
  continues along the path of sun the answer is it''s the end of applications", "tokens":
  [50884, 498, 257, 741, 534, 6515, 2051, 264, 3100, 295, 3295, 264, 1867, 307, 309,
  311, 264, 917, 295, 5821, 51096], "temperature": 0.0, "avg_logprob": -0.11139005144065786,
  "compression_ratio": 1.8097165991902835, "no_speech_prob": 0.002264224225655198},
  {"id": 328, "seek": 203456, "start": 2050.72, "end": 2055.7599999999998, "text":
  " yeah yeah exactly and going back to being immersive and sort of feeling that",
  "tokens": [51172, 1338, 1338, 2293, 293, 516, 646, 281, 885, 35409, 293, 1333, 295,
  2633, 300, 51424], "temperature": 0.0, "avg_logprob": -0.11139005144065786, "compression_ratio":
  1.8097165991902835, "no_speech_prob": 0.002264224225655198}, {"id": 329, "seek":
  203456, "start": 2056.72, "end": 2063.2799999999997, "text": " i''m myself and i
  am in control and not like vice versa when today i don''t feel like i''m in control",
  "tokens": [51472, 741, 478, 2059, 293, 741, 669, 294, 1969, 293, 406, 411, 11964,
  25650, 562, 965, 741, 500, 380, 841, 411, 741, 478, 294, 1969, 51800], "temperature":
  0.0, "avg_logprob": -0.11139005144065786, "compression_ratio": 1.8097165991902835,
  "no_speech_prob": 0.002264224225655198}, {"id": 330, "seek": 206328, "start": 2063.6000000000004,
  "end": 2070.1600000000003, "text": " applications update by themselves i phone restarts
  i have no idea what''s in that update i will not", "tokens": [50380, 5821, 5623,
  538, 2969, 741, 2593, 1472, 11814, 741, 362, 572, 1558, 437, 311, 294, 300, 5623,
  741, 486, 406, 50708], "temperature": 0.0, "avg_logprob": -0.12655060941522772,
  "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031326808966696262},
  {"id": 331, "seek": 206328, "start": 2070.1600000000003, "end": 2076.88, "text":
  " be able to ever understand what they do but they do it so sometimes i feel like
  whatever i have bought", "tokens": [50708, 312, 1075, 281, 1562, 1223, 437, 436,
  360, 457, 436, 360, 309, 370, 2171, 741, 841, 411, 2035, 741, 362, 4243, 51044],
  "temperature": 0.0, "avg_logprob": -0.12655060941522772, "compression_ratio": 1.7636363636363637,
  "no_speech_prob": 0.0031326808966696262}, {"id": 332, "seek": 206328, "start": 2076.88,
  "end": 2082.5600000000004, "text": " belongs to someone else but probably this will
  change and i think this should change", "tokens": [51044, 12953, 281, 1580, 1646,
  457, 1391, 341, 486, 1319, 293, 741, 519, 341, 820, 1319, 51328], "temperature":
  0.0, "avg_logprob": -0.12655060941522772, "compression_ratio": 1.7636363636363637,
  "no_speech_prob": 0.0031326808966696262}, {"id": 333, "seek": 206328, "start": 2083.84,
  "end": 2089.92, "text": " as we wrap up i was thinking is there something you want
  to sort of call out to the community and say", "tokens": [51392, 382, 321, 7019,
  493, 741, 390, 1953, 307, 456, 746, 291, 528, 281, 1333, 295, 818, 484, 281, 264,
  1768, 293, 584, 51696], "temperature": 0.0, "avg_logprob": -0.12655060941522772,
  "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0031326808966696262},
  {"id": 334, "seek": 208992, "start": 2090.0, "end": 2094.56, "text": " you know
  by now swirl obviously has progressed you guys open source i love it", "tokens":
  [50368, 291, 458, 538, 586, 30310, 2745, 575, 36789, 291, 1074, 1269, 4009, 741,
  959, 309, 50596], "temperature": 0.0, "avg_logprob": -0.14063210487365724, "compression_ratio":
  1.7019230769230769, "no_speech_prob": 0.0029575617518275976}, {"id": 335, "seek":
  208992, "start": 2095.28, "end": 2100.08, "text": " you you have a bunch of contributors
  probably that you trust and you work with but", "tokens": [50632, 291, 291, 362,
  257, 3840, 295, 45627, 1391, 300, 291, 3361, 293, 291, 589, 365, 457, 50872], "temperature":
  0.0, "avg_logprob": -0.14063210487365724, "compression_ratio": 1.7019230769230769,
  "no_speech_prob": 0.0029575617518275976}, {"id": 336, "seek": 208992, "start": 2100.08,
  "end": 2105.6, "text": " is there anything that you would you know benefit from
  calling out the the larger community", "tokens": [50872, 307, 456, 1340, 300, 291,
  576, 291, 458, 5121, 490, 5141, 484, 264, 264, 4833, 1768, 51148], "temperature":
  0.0, "avg_logprob": -0.14063210487365724, "compression_ratio": 1.7019230769230769,
  "no_speech_prob": 0.0029575617518275976}, {"id": 337, "seek": 208992, "start": 2107.76,
  "end": 2114.7200000000003, "text": " i think i''m very happy to see the the folks
  shift and focus towards search i think the thing i''d call", "tokens": [51256, 741,
  519, 741, 478, 588, 2055, 281, 536, 264, 264, 4024, 5513, 293, 1879, 3030, 3164,
  741, 519, 264, 551, 741, 1116, 818, 51604], "temperature": 0.0, "avg_logprob": -0.14063210487365724,
  "compression_ratio": 1.7019230769230769, "no_speech_prob": 0.0029575617518275976},
  {"id": 338, "seek": 211472, "start": 2114.7999999999997, "end": 2121.68, "text":
  " out is to say you know there are many different user communities that want to
  consume AI they will", "tokens": [50368, 484, 307, 281, 584, 291, 458, 456, 366,
  867, 819, 4195, 4456, 300, 528, 281, 14732, 7318, 436, 486, 50712], "temperature":
  0.0, "avg_logprob": -0.12618076408302392, "compression_ratio": 1.6359832635983265,
  "no_speech_prob": 0.0013017432065680623}, {"id": 339, "seek": 211472, "start": 2121.68,
  "end": 2130.08, "text": " benefit from it and i think the key is not to go too far
  on the hype cycle right and because honestly", "tokens": [50712, 5121, 490, 309,
  293, 741, 519, 264, 2141, 307, 406, 281, 352, 886, 1400, 322, 264, 24144, 6586,
  558, 293, 570, 6095, 51132], "temperature": 0.0, "avg_logprob": -0.12618076408302392,
  "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0013017432065680623},
  {"id": 340, "seek": 211472, "start": 2131.12, "end": 2137.8399999999997, "text":
  " another thing i learned is not everybody is into the details of how AI works right
  and like", "tokens": [51184, 1071, 551, 741, 3264, 307, 406, 2201, 307, 666, 264,
  4365, 295, 577, 7318, 1985, 558, 293, 411, 51520], "temperature": 0.0, "avg_logprob":
  -0.12618076408302392, "compression_ratio": 1.6359832635983265, "no_speech_prob":
  0.0013017432065680623}, {"id": 341, "seek": 211472, "start": 2137.8399999999997,
  "end": 2143.2, "text": " fine tuning is an example it''s a very deep discussion
  at some level i''m no expert right i can tell", "tokens": [51520, 2489, 15164, 307,
  364, 1365, 309, 311, 257, 588, 2452, 5017, 412, 512, 1496, 741, 478, 572, 5844,
  558, 741, 393, 980, 51788], "temperature": 0.0, "avg_logprob": -0.12618076408302392,
  "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0013017432065680623},
  {"id": 342, "seek": 214320, "start": 2143.4399999999996, "end": 2147.6, "text":
  " a lot about it but i think there''s people who have done much more on it than
  i will ever do", "tokens": [50376, 257, 688, 466, 309, 457, 741, 519, 456, 311,
  561, 567, 362, 1096, 709, 544, 322, 309, 813, 741, 486, 1562, 360, 50584], "temperature":
  0.0, "avg_logprob": -0.10820113198231843, "compression_ratio": 1.8409090909090908,
  "no_speech_prob": 0.0027906633913517}, {"id": 343, "seek": 214320, "start": 2148.24,
  "end": 2154.56, "text": " but the end of the day the user that''s way way way far
  from the user''s head right what they''re what", "tokens": [50616, 457, 264, 917,
  295, 264, 786, 264, 4195, 300, 311, 636, 636, 636, 1400, 490, 264, 4195, 311, 1378,
  558, 437, 436, 434, 437, 50932], "temperature": 0.0, "avg_logprob": -0.10820113198231843,
  "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0027906633913517},
  {"id": 344, "seek": 214320, "start": 2154.56, "end": 2157.52, "text": " they''re
  trying to understand and the people are making decisions about bringing these things
  in", "tokens": [50932, 436, 434, 1382, 281, 1223, 293, 264, 561, 366, 1455, 5327,
  466, 5062, 613, 721, 294, 51080], "temperature": 0.0, "avg_logprob": -0.10820113198231843,
  "compression_ratio": 1.8409090909090908, "no_speech_prob": 0.0027906633913517},
  {"id": 345, "seek": 214320, "start": 2157.52, "end": 2164.0, "text": " are is it
  safe how can i trust it how do i get it to provide a benefit so i think the honest
  thing is", "tokens": [51080, 366, 307, 309, 3273, 577, 393, 741, 3361, 309, 577,
  360, 741, 483, 309, 281, 2893, 257, 5121, 370, 741, 519, 264, 3245, 551, 307, 51404],
  "temperature": 0.0, "avg_logprob": -0.10820113198231843, "compression_ratio": 1.8409090909090908,
  "no_speech_prob": 0.0027906633913517}, {"id": 346, "seek": 214320, "start": 2164.0,
  "end": 2170.56, "text": " rather than focusing on like we''ve got a few more tokens
  than somebody else talk about use cases", "tokens": [51404, 2831, 813, 8416, 322,
  411, 321, 600, 658, 257, 1326, 544, 22667, 813, 2618, 1646, 751, 466, 764, 3331,
  51732], "temperature": 0.0, "avg_logprob": -0.10820113198231843, "compression_ratio":
  1.8409090909090908, "no_speech_prob": 0.0027906633913517}, {"id": 347, "seek": 217056,
  "start": 2170.64, "end": 2174.48, "text": " focus on the user i think that''s where
  that''s what''s world did from the beginning because if you''re", "tokens": [50368,
  1879, 322, 264, 4195, 741, 519, 300, 311, 689, 300, 311, 437, 311, 1002, 630, 490,
  264, 2863, 570, 498, 291, 434, 50560], "temperature": 0.0, "avg_logprob": -0.17847571165665335,
  "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.0029498040676116943},
  {"id": 348, "seek": 217056, "start": 2174.48, "end": 2180.16, "text": " in search
  like there is nothing but the user right the user''s intent is everything right
  and from", "tokens": [50560, 294, 3164, 411, 456, 307, 1825, 457, 264, 4195, 558,
  264, 4195, 311, 8446, 307, 1203, 558, 293, 490, 50844], "temperature": 0.0, "avg_logprob":
  -0.17847571165665335, "compression_ratio": 2.0280701754385966, "no_speech_prob":
  0.0029498040676116943}, {"id": 349, "seek": 217056, "start": 2180.72, "end": 2184.72,
  "text": " like we can go back to lots of lots of great writing about that from biaires
  e8s to tinkle and", "tokens": [50872, 411, 321, 393, 352, 646, 281, 3195, 295, 3195,
  295, 869, 3579, 466, 300, 490, 272, 654, 3145, 308, 23, 82, 281, 256, 14095, 293,
  51072], "temperature": 0.0, "avg_logprob": -0.17847571165665335, "compression_ratio":
  2.0280701754385966, "no_speech_prob": 0.0029498040676116943}, {"id": 350, "seek":
  217056, "start": 2184.72, "end": 2190.4, "text": " and all points in between but
  the user''s intent''s important and that''s the thing to focus on what", "tokens":
  [51072, 293, 439, 2793, 294, 1296, 457, 264, 4195, 311, 8446, 311, 1021, 293, 300,
  311, 264, 551, 281, 1879, 322, 437, 51356], "temperature": 0.0, "avg_logprob": -0.17847571165665335,
  "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.0029498040676116943},
  {"id": 351, "seek": 217056, "start": 2190.4, "end": 2195.44, "text": " are they
  trying to accomplish and build great use cases that ultimately you know allow people
  to", "tokens": [51356, 366, 436, 1382, 281, 9021, 293, 1322, 869, 764, 3331, 300,
  6284, 291, 458, 2089, 561, 281, 51608], "temperature": 0.0, "avg_logprob": -0.17847571165665335,
  "compression_ratio": 2.0280701754385966, "no_speech_prob": 0.0029498040676116943},
  {"id": 352, "seek": 217056, "start": 2195.44, "end": 2200.08, "text": " focus on
  the things they''d rather focus on instead of you know the minutia and the time
  of", "tokens": [51608, 1879, 322, 264, 721, 436, 1116, 2831, 1879, 322, 2602, 295,
  291, 458, 264, 13951, 654, 293, 264, 565, 295, 51840], "temperature": 0.0, "avg_logprob":
  -0.17847571165665335, "compression_ratio": 2.0280701754385966, "no_speech_prob":
  0.0029498040676116943}, {"id": 353, "seek": 220008, "start": 2200.08, "end": 2205.2799999999997,
  "text": " collecting all these different data points yeah that''s i think that''s
  i think what you do as a", "tokens": [50364, 12510, 439, 613, 819, 1412, 2793, 1338,
  300, 311, 741, 519, 300, 311, 741, 519, 437, 291, 360, 382, 257, 50624], "temperature":
  0.0, "avg_logprob": -0.15746388965182834, "compression_ratio": 1.730593607305936,
  "no_speech_prob": 0.003096443135291338}, {"id": 354, "seek": 220008, "start": 2205.2799999999997,
  "end": 2211.04, "text": " as a tech industry responding to the user demand for AI
  my two cents oh that''s amazing i don''t", "tokens": [50624, 382, 257, 7553, 3518,
  16670, 281, 264, 4195, 4733, 337, 7318, 452, 732, 14941, 1954, 300, 311, 2243, 741,
  500, 380, 50912], "temperature": 0.0, "avg_logprob": -0.15746388965182834, "compression_ratio":
  1.730593607305936, "no_speech_prob": 0.003096443135291338}, {"id": 355, "seek":
  220008, "start": 2211.04, "end": 2218.7999999999997, "text": " don''t try to outsmart
  the users and uh make things that you produce explainable and so they", "tokens":
  [50912, 500, 380, 853, 281, 484, 10817, 446, 264, 5022, 293, 2232, 652, 721, 300,
  291, 5258, 2903, 712, 293, 370, 436, 51300], "temperature": 0.0, "avg_logprob":
  -0.15746388965182834, "compression_ratio": 1.730593607305936, "no_speech_prob":
  0.003096443135291338}, {"id": 356, "seek": 220008, "start": 2218.7999999999997,
  "end": 2226.88, "text": " probably will adopt them weaker uh that''s amazing i also
  see uh super uh maybe provocative really", "tokens": [51300, 1391, 486, 6878, 552,
  24286, 2232, 300, 311, 2243, 741, 611, 536, 2232, 1687, 2232, 1310, 47663, 534,
  51704], "temperature": 0.0, "avg_logprob": -0.15746388965182834, "compression_ratio":
  1.730593607305936, "no_speech_prob": 0.003096443135291338}, {"id": 357, "seek":
  222688, "start": 2226.96, "end": 2233.28, "text": " nice one from your side where
  you say that you outshine google i will link it in the show notes and", "tokens":
  [50368, 1481, 472, 490, 428, 1252, 689, 291, 584, 300, 291, 484, 19686, 20742, 741,
  486, 2113, 309, 294, 264, 855, 5570, 293, 50684], "temperature": 0.0, "avg_logprob":
  -0.10509832612760775, "compression_ratio": 1.735159817351598, "no_speech_prob":
  0.00680195540189743}, {"id": 358, "seek": 222688, "start": 2233.28, "end": 2239.2000000000003,
  "text": " maybe we''ll discuss it at some point as well something about ranking
  looks it i really", "tokens": [50684, 1310, 321, 603, 2248, 309, 412, 512, 935,
  382, 731, 746, 466, 17833, 1542, 309, 741, 534, 50980], "temperature": 0.0, "avg_logprob":
  -0.10509832612760775, "compression_ratio": 1.735159817351598, "no_speech_prob":
  0.00680195540189743}, {"id": 359, "seek": 222688, "start": 2239.92, "end": 2246.7200000000003,
  "text": " enjoyed chatting to you today i''m sure there will be someone you know
  in the community reaching out", "tokens": [51016, 4626, 24654, 281, 291, 965, 741,
  478, 988, 456, 486, 312, 1580, 291, 458, 294, 264, 1768, 9906, 484, 51356], "temperature":
  0.0, "avg_logprob": -0.10509832612760775, "compression_ratio": 1.735159817351598,
  "no_speech_prob": 0.00680195540189743}, {"id": 360, "seek": 222688, "start": 2246.7200000000003,
  "end": 2252.0, "text": " and maybe trying out swirl to be honest it''s itching for
  me to try it out right when i when i", "tokens": [51356, 293, 1310, 1382, 484, 30310,
  281, 312, 3245, 309, 311, 309, 17354, 337, 385, 281, 853, 309, 484, 558, 562, 741,
  562, 741, 51620], "temperature": 0.0, "avg_logprob": -0.10509832612760775, "compression_ratio":
  1.735159817351598, "no_speech_prob": 0.00680195540189743}, {"id": 361, "seek": 225200,
  "start": 2252.0, "end": 2258.24, "text": " said it when i mentioned in my company
  and someone said i would love to have that single keyword box", "tokens": [50364,
  848, 309, 562, 741, 2835, 294, 452, 2237, 293, 1580, 848, 741, 576, 959, 281, 362,
  300, 2167, 20428, 2424, 50676], "temperature": 0.0, "avg_logprob": -0.11457275789837505,
  "compression_ratio": 1.727699530516432, "no_speech_prob": 0.009190120734274387},
  {"id": 362, "seek": 225200, "start": 2258.24, "end": 2264.56, "text": " so that
  i can search slack and conference and email and everything um that''s amazing that''s",
  "tokens": [50676, 370, 300, 741, 393, 3164, 29767, 293, 7586, 293, 3796, 293, 1203,
  1105, 300, 311, 2243, 300, 311, 50992], "temperature": 0.0, "avg_logprob": -0.11457275789837505,
  "compression_ratio": 1.727699530516432, "no_speech_prob": 0.009190120734274387},
  {"id": 363, "seek": 225200, "start": 2264.56, "end": 2270.88, "text": " fantastic
  and also amazing that you guys do it uh in the open so everyone can try it um all
  the best", "tokens": [50992, 5456, 293, 611, 2243, 300, 291, 1074, 360, 309, 2232,
  294, 264, 1269, 370, 1518, 393, 853, 309, 1105, 439, 264, 1151, 51308], "temperature":
  0.0, "avg_logprob": -0.11457275789837505, "compression_ratio": 1.727699530516432,
  "no_speech_prob": 0.009190120734274387}, {"id": 364, "seek": 225200, "start": 2270.88,
  "end": 2276.16, "text": " to you good luck in in whatever you''re building in your
  uh next big things", "tokens": [51308, 281, 291, 665, 3668, 294, 294, 2035, 291,
  434, 2390, 294, 428, 2232, 958, 955, 721, 51572], "temperature": 0.0, "avg_logprob":
  -0.11457275789837505, "compression_ratio": 1.727699530516432, "no_speech_prob":
  0.009190120734274387}, {"id": 365, "seek": 227616, "start": 2276.48, "end": 2281.2,
  "text": " thanks to me tree thank you so much it was great to talk to you and uh
  when you want to", "tokens": [50380, 3231, 281, 385, 4230, 1309, 291, 370, 709,
  309, 390, 869, 281, 751, 281, 291, 293, 2232, 562, 291, 528, 281, 50616], "temperature":
  0.0, "avg_logprob": -0.2952918677494444, "compression_ratio": 1.6122448979591837,
  "no_speech_prob": 0.015635771676898003}, {"id": 366, "seek": 227616, "start": 2281.2,
  "end": 2285.92, "text": " ready to try a swirl you got the open source version enjoy
  our slack and we''ll be happy to help", "tokens": [50616, 1919, 281, 853, 257, 30310,
  291, 658, 264, 1269, 4009, 3037, 2103, 527, 29767, 293, 321, 603, 312, 2055, 281,
  854, 50852], "temperature": 0.0, "avg_logprob": -0.2952918677494444, "compression_ratio":
  1.6122448979591837, "no_speech_prob": 0.015635771676898003}, {"id": 367, "seek":
  227616, "start": 2285.92, "end": 2296.0, "text": " absolutely thank you very much
  enjoy your day youtube", "tokens": [50852, 3122, 1309, 291, 588, 709, 2103, 428,
  786, 12487, 51356], "temperature": 0.0, "avg_logprob": -0.2952918677494444, "compression_ratio":
  1.6122448979591837, "no_speech_prob": 0.015635771676898003}]'
---

Hello there, this is Vector Podcast Season 3 and I'm super excited to be talking to companies with thousands and thousands of users and thousands and thousands of systems that it's been a time of inspiration and a little bit of continued nervousness about what it all means.
Last March on the 15th actually was the 14th was Pi Day and that was the one year anniversary of GPT-4. What I've learned is that those large enterprises were again looked at GPT-4 and said this is going to change our business.
This can really help everybody be an efficient expert and just slice through the current problems of silo data and inconsistent systems.
But at the same time there were a lot of fear about well are we exposing invaluable internal data to AI's that are then going to be trained on it? Is this going to be exposed? Lost? There have been many many lawsuits.
 So ultimately the large enterprises did what they always do which is engage with it on their own terms and many of them purchased download installed AI, generative AI's and LLMs in their private clouds and we're working with one large company that did that and trained it with a bunch of what they called safe data.
 So annual reports and you employ a handbook and it's very interesting to talk to but it can't really help a business person or somebody trying to answer a question in the supply chain group or in the R&D group or in HR because this doesn't have access to those systems and in those places you've ever worked there.
You know when you onboard the first thing they do is your manager does right is they open a bunch of tickets so that you could have access to systems. That's hard enough.
So the reason that there's been so in a way so little progress right lots of installs of AI but not that much real I'd love to hear from you some of the use cases out there.
 People are still trying to say we're still trying to get the data to the AI so that it can provide the benefit and what ultimately what what happened is this they've got the AI's installed the first generation of AI architecture solution architectures is what I will refer to as a vendor driven put the data in architecture literally every product out there I don't want to name them but they all say the first step is put the data in again like for some people for many applications from POVs for testing it out that's great and I've who hasn't done it with a few PDFs right and got some interesting results but you can't just take a copy of a departmental database and hand it over to a centralized corporate database for training like that their rules in place to prevent that even more difficult is the idea that you would send it outside your perimeter into someone else's cloud right at another big manufacturing firm they have a 24 month waiting list to onboard a new SaaS product right they'd like we have to put our security team on it so I believe it's a very interesting time and ultimately what happened is Swirl thought differently about the problem as you said we thought about it from the search technology perspective why would we move all of the data instead move the essentially take only the data that you didn't give it to the AI at that moment and what Swirl does first to do that is we create a single pane of glass well the next thing I'll mention is Swirl is software we are a software company and our software is typically deployed in the customers private cloud there's we are happy to do hosting for POVs and for various applications but for larger enterprise we don't expect that to be the case once you deploy Swirl it integrates with your single sign on systems such as Microsoft or Octa or ping federate others you can have cast whatever in there once it's configured you send a question prompt or query search query to Swirl it brokers that query to all the sources that it's authorized to do so and it does so on behalf of that user so it's not only is it safe compliance search using existing infrastructure but it's personal the data the user or caller gets back is based on what that user can see so I use it all the time and it's my email my outlook my calendar my LinkedIn whatever right it's my view we actually love the idea that we should present the data to the user so you get that single pane of glass and actually you can decide what to do with it you can say I don't want this source or whatever you can make adjustments but ultimately we then execute rag we have our own excellent high quality rag better than many in particular it seeks highly relevant passages from all of the documents we can fetch the documents and authenticate on the fly as to do that um bind those to a prompt we have our own prompt engineering you can uh override it and then do the rag against a huge list of AI providers actually we support more than 20 today including most of the ones we see out there open AI open AI and azure bedrock and prop at google mistral uh co here etc and in all cases no code should be required you configure an existing connector more than likely you're putting in just endpoint information and authentication tokens and then swirl again does that broker and creates that pane of glass and execute rag you can also use swirl just for the R if you have your own rag right you can get the result list and do your fetching or you can hook after you've got the swirl has the fetched results and you can operate that on just the full documents the key to this i love that you asked is the reader lllm we have been really heads down working on the reader llm um i've actually been asking people if they have heard the term before and many haven't uh i don't know what what your take is on on reader llm these days oh yeah i'm still catching up really i mean the way i see it and i'm still kind of plowing through rag itself right so you you said what is my take on on how easy it is to on board to the say i models and so on i i have a sense that people are aware of this because it's so easy to access through chat chat gpt and similar tools but then when it comes to deploying these things i don't think that it's as easy right so because you you have to go through a list of models you need to figure out which one to pick and and and and hence you need to be a data scientist right at that point or ml practitioner or whatever um and it's not and it's like the web is exploding with so many cheap advice you know use these use that but then as you go through that process you realize that none of those models work and so you need to do something okay the risk rag but setting up rag means that you need to bring in an effective database that you haven't seen before and things like this right so it's yeah so i love that so just speaking of misinformation right i think you're absolutely right there's so much um confusing stuff out there you do not need a vector database to rag you never did it's it's a it's a vendor thing that i totally understand they're charging per gigabyte or whatever so they say you have to have it to rag uh there's an excellent study by zet hub and actually simpson garf ankles and advisor to swore all you may have heard that name incredible tech writer um he recently wrote a study a survey or a summary i should say of the zet hub study the zet hub study shows that you do not need to vectorize your data to get high quality results instead you just increase the number of results you get from a so-called naive nonvector search engine or database and re-rank using vectors that's exactly what swore all this we vectorize the result set snippets we vectorize the full text of the documents we vectorize the query the prompt whatever it is right and our reader llem is responsible for a complex similarity re-ranking you can actually plug the a your choice of embeddings into our reader llem embeddings are actually just a feature one one of the many things that llem's do so you can change that but the reader llem here's really the core of it it's the middle layers of the generative AI llem without the you know um text generation and text interpretation part that's not there at all instead you use it to determine the similarity right cosine or they're many different algorithms but ultimately you're taking some algorithm like that and you're using embeddings plus the reader llem's own knowledge to say how similar is the query or prompt or part of it to the response that i got or find the most relevant passage in a document because you're absolutely right there are tools like langshane out there as in one example which give you lots of interesting tooling right but it's still on you the developer i actually had chat tpt generate me a pipeline just as a demo and the biggest problem is it generated a function that i have to fill in which is called select documents that's really hard and ultimately like you're basically just providing the pipeline to move the data once again but it's the reader llem in swirl is all about re-ranking and finding the best passages so that you are not sending a hundred pdf of which one paragraph is relevant you are sending the paragraph that way you can put a lot more data and you can also not blow out your token limits right assuming you have such a thing if you're on prem but that's what that's the reader lm i'll say this their reader lm are the unsung heroes of especially search but also a rag when you're looking at i would say bing or or chat gpt and you ask it a question and it goes and fetches documents from the web it's almost certainly using a reader llm to determine which pages are best and to be fair being in google have incredible knowledge of that already so it's not like it's that hard but then they're almost certainly reading the most relevant passages right they're not just passing the whole web page in so reader lm's are a thing they're definitely becoming more and more prevalent and they provide a critical non hallucinating step to help find the best results so the user doesn't have to and that's very interesting and and how let's say if you plug into a companies network right so and they focus on something i don't know healthcare banking what have you would you need to fine tune reader lm in any way no i actually don't recommend it i think there's a lot of evidence that fine tuning because of its fundamentally lossy process right is somewhat responsible for hallucinations there's been quite a bit written about this and i think that ultimately the the winning combination today is that you use a very well trained capable model that is generalist and you provide it with the data that you need to provide it with at the moment you need to for example swirls prompt engineering does a few things one we force it to only consider the rag data and not add its own model thoughts right you can interpret but don't say don't create facts that aren't presented to you second force it to disambiguate this is one of the worst errors in prompt engineering is not is just letting it go right up on past equating right two entities with the same name as if they're the same thing so our default engineering says listen if you see and into two entities with the same name don't you know essentially call that out and don't just gloss over it the last one is especially when you're talking about multiple sources of data and enterprise data the user must be able to verify or nobody wants to make a career limiting move because they took chat gpt's and answer and said here it is here it is right put it up on the on the investor site not a good idea but swirl also forces the AI to quote the sources that you use to cite them and of course you also have access to the underlying search results right so you can verify that yes you have a million dollars in insurance coverage and it covers x y and z that's key yeah that's amazing I was just you reminded me of when you said about hallucinations I was just listening to one interview is not related to AI world attack world it's political sciences and so she was asked the scientists she was asked you know are you using chat gpt at work and she said yes sometimes I do sometimes I use it as a co-writer so you know I I draft some things quickly and and I still see that chat gpt is very crude you know in the way it approaches you know I can do it better but sometimes I'm just you know lazy or tired okay let it do it but then the thing that struck her was that it actually hallucinates she was asking give me you know top five books in political science you know in specific country and chat gpt was very confident and and said they said the five books and the authors and when she googled them they don't exist and and then she said they don't exist and then chat gpt responded okay here is only one book that you should read and that didn't exist either so she was genuinely like baffled and she said okay you might say something with less confidence but why lie why do you lie she doesn't know what is hallucinations but she's she looks at it as a user and it's very disconcerting so believe it or not when I first started using gpt 4 I got a hallucination that I thought was so real I wrote to the publisher and said why is this article no longer online and the publisher wrote back and said there is no such article but it could have been it was authored by someone they said gpt 4 said it was authored by another author who had posted on that site the url looked correct and the content looked I mean the snippet it gave me looked absolutely real but again when they build these models a few you know 10 20 gigabyte model right of gpt 4 or 35 or whatever it is petabytes and petabytes of data went into that so by definition it's lossy but the way the lllm the generative part works is it must provide a response so you know how that is when you can't quite remember the name of something and it's essentially doing the same thing so it knows it I saw an artifact that looked like that but I don't have the artifact anymore so it generates something that is the consensus version of what it would have been had it existed and that's why I don't believe in fine tuning so much think if you have a high capable model with some reasoning and the ability to interpret text and follow instructions you provide it with your internal data and that is the beauty of reg because here's the thing the reason it's so good at things why does why does chat gpt 4 sound like a smart person on you know reddit or or some or Facebook or something like that right and that's because that's where it was trained from you're internal and of course on something like reddit or whatever they have a new the same conversation 10 million times right I mean how many discussions of whatever twin peaks or battle star galactica you know are there there are a lot of them and so it learns the core of these things right and can answer those questions but if you feed at your internal data like it's probably not so repetitive it's probably much more conflicting than not and so you that's why you produce more problems it's much better give it the one thing that's really relevant and let it reason yeah that sounds go and slight live right it's something that can be updated throughout the lifecycle of your company or department whatever but there is one challenge they want to offer to you and came to me just today as I was thinking and preparing for this episode data is not gold you know sometimes it is gold because everyone talks about it la la la but like it also is very complex machinery and it can have the stakes of it of its own you know misattribution misclassification and human error what have you how how would you say reader lllm or swirl gonna tackle this issue or is it just gonna transparently sort of like garbage in garbage out type of response it's a great question speaking of hallucinations in AI right we all have probably worked with somebody at one time another who made a mistake right or whatever didn't understand the problem enough and that stuff gets into teams and slack and you know documents are wrong like it's incredible you're right it's incredibly messy in the enterprise happy as anybody not worked at a firm where they had you know 500 versions of the same PowerPoint that is just evolved right so absolutely well these are things that ultimately are gonna have to be continued to be work on but here's one point number one if you leave the system in the system data in the system of record you're much less likely to introduce new problems especially like security problems and you leave it in the system of record than any domain modeling lexicon's ontologies text items you get the benefit of those if someone cared about that source they might very well have done some of that right so if you pull it all out and put it in a vector database like what happened to all of that knowledge so I would argue that the systems of record that are valuable have things in place to deal with that number two the reader lm does a couple things that to help this one it's aware of certain commonly problematic formats email is the worst reply forward and signature content very very problematic we have a solution for public data too so that you can get article content without getting as an example at navigation advertisements cloked data stuff like that right so because very often public data is relevant right to to large enterprise like they want to see policy changes regulatory changes online catalog changes right that that's all relevant stuff then there's the similarity problem right so one of the another thing the reader lm does it can do semantic analysis to determine which is the latest version of the same document that's a one of the great lm's are amazing at that much better at old school like multi windows setups where you're trying to take out like a signature of the document and say well this could it's very similar but lm does it much better right and you can quickly say now this is the latest version of that spreadsheet or you can let the user decide it's another thing who doesn't love shopping I love being able to look at my shopping cart full of swirl results and say you know this one I know isn't really relevant these are the five I've risen or maybe this is the source or these are the sources that I want my data from today that's another way of allowing the user to bring their expertise and experience and knowledge and say no no no colibre not thoughts but snowflake not oracle whatever and I'm not picking on anybody we all can say they're all present they all have value the question is which one has the answer for me today well until the until they can write the query with the context that answers that you know I think the key is to keep the user in the loop make sure that there are citations and ultimately that in a year the systems will be smarter and many of these problems will be solved after all almost all the naive search engines right that were BM25 or whatever pretty much all have vector upgrades now only questions can you wait long enough to vectorize at high-dimensional space a few million documents exactly yeah sometimes when I use chat GPT I don't use it that often by the way for some reason maybe it says something about me maybe I should you learn to do it but sometimes as you said you know it just generates something it seems a little average you know it's a code snippet or something like that you try it it doesn't work at that point when I when I get frustrated a little bit I'm like can you show me the source maybe a link to stack over flow so I can go and drill in for myself you know I don't have to sort of keep pounding and you and I'm asking you know okay that didn't work this didn't work because I can do the same thing just staring at the stack over flow page right and maybe they have been already some updates and someone said no that doesn't work in all some times they see you see the selected answer but then there is another one which everyone says that works not the selected one so that's just funny yeah that's amazing so reader lulm like just to sort of bring it back to the ground especially for those who are sort of no vice like myself I still consider myself no vice you know have you sort of taken a reader lulm off the shelf you sort of implemented someone's paper or took it or did you did you have to train it how did you go about it we built it largely from it's it's been an evolving thing but they're they're definitely our other reader lulm's out there the key is to preserve the structure right and the pieces the structure that allows you to do similarity we implemented our own similarity and other algos we also do things like named entity recognition and sentiment analysis well those are great at that stuff it can do scoring for machine learning purposes right so we have a nice intention detection system now that will essentially based on the responses that you get to tell you which sources are most relevant right up front right based on the responses and also optionally ratings right if you want to bring bring that into the system passage detection is totally in response our reader lulm's passage detection is totally in response to the problem you described right which is the data is messy and we don't necessarily want to ship a you know 500 100 page PDFs that have essentially the same data so there we it finds the most relevant passage super quickly and truncates the document down to a window around it um those those are the things and it we've really implemented it ourselves it's our it's our own it's our own creation oh that's fantastic so that's your secret source as well I mean that's that's something to be proud of and also I want to sort of close up that the sort of you know description that he gave or maybe looking at the future does world have some way of feedback do you plan to if not do you plan to implement do you think it's reasonable to have a feedback loop you know like in chat jpd you can say thumbs up thumbs down you cannot say much you know you can say this was the answer I don't know if that's gonna go into the loop but whatever because it gives me the the joy of sort of completing it um yeah oh yes so when swirl AI connect is deployed in the enterprise where starters you get to connect the data and get rag and your choice of AI's and by the way it's again configuration for the AI you put your keys in right check pick the model and you can rag against it you can also choose the role you want to use different generative AI in rights you can use it for query writing you can use it for direct answer you can use it for rag if it has embeddings you can use that to power the the reader L so just just to be clear there it's uh it's a bit more flexible yeah i was asking about feedback right so like do you plan do you have it and if not do you plan to think it's reasonable to have it right absolutely so after you deploy AI connect as mentioned you get those abilities then we have an analytics package which will give you insight as to which sources are providing the most relevant responses and rating and putting all of that into dashboards understanding who are the number one users who write the best prompts who get which sources produce the best results which prompts you absolutely is all part of the the offering and ultimately it's part of what we tailor right for the for the deployment and again that can be on premises but AI connect is the key because it's collecting that data on again always in the customers private cloud like we don't see it we're not sass but that data is absolutely turnable into gold a variety of different gold things and so you can hopefully figure out which AI works best for which groups you can figure out which sources right are providing the best input for rag etc yeah that's fantastic i love that you do have feedback uh i think it's definitely gold it could could also be super messy and noisy and stuff but it's better than absence of it um yeah that's amazing maybe like in the past year so you've been deploying this with clients obviously you don't have to mention the names but was there something that surprised you how clients you know sort of perceived uh swirl yeah i i would say that um people have not really been looking for search if i'm the AI the explosion of AI and the excitement around AI kind of crowded everything out round everything out so that's why i think so many of these copy in architectures were got so much momentum but and i by the way i think people are doing incredible stuff with that so it's not like those aren't perfectly legitimate i mean every database ever starts that way right you put the data in and you get inside of it's just that there's a bit more to the story right there's a whole other world of well there's a lot of these and i just moved them to cloud and i can't necessarily do it but people weren't thinking search i'm not sure what they believed the answer would be but uh there were some excellent posts there was one on linkedin by um vector ventures i think or um i should probably get the name right but in any event they published an excellent piece about how search is probably the answer to making AI work in a lot of these cases and uh you know they also point out there's not that many people who have come at it from the search perspective so that was a bit surprising to me because the large enterprise has always loved search always uh because that that's how knowledge workers and people get stuff done right it's yes you have business intelligence and dashboards and reporting and we like those things but so much of the qualitative why did things happen explain it to me how do i solve this that's been something that search did a good job of um ultimately it's a technology right and the marriage of search with llm that seems to be the unlocker if you will in the enterprise and that's that was surprising to me right i i thought that there would be much more of a search first approach and i think everybody had to get through the understanding of what it means to adopt AI and how the first generation works and now i think people are recognizing real time real time architecture using systems of record with re-ranking and and then just stay keeping up right with the incredible innovation in it in let's call it generative AI right that's that's the interesting thing but again i come back to what i said at the beginning there's going to be a many many incredible generative AI's they're going to do different things that we haven't even seen i don't think the most extraordinary ones they'll be from the big science publishers they're going to build incredible life sciences gai i'm sure people like bloomberg ft you're going to build incredible um financial ones and that's great but all of those still need your data to operate on your environment and give you answers that are meaningful and that problem that problem is the problem that's world solves so just to understand what were they expecting was it like chat interface or you said they didn't expect search no they thought they would ask the question oh yeah so they thought like a chat right yeah that everybody wants kind of a conversational interface you know another thing i learned actually you're really reminded me people are not so interested in the idea that there is an AI place that you go i think another very logical step is business folks knowledge workers they would like to use the channels they use today so rather than i have to have a new place to go why can't i talk to it on teens or in slack or on my whatsapp why can't i text it um if i need to get a visual i could always go to a screen right and then i could have it show me the underlying data show me the graph show me the chart but for the the the future is not applications that are destinations the future is an ongoing that i log with the AI that understands your business your world has access to your data and becomes your uh trusted advisor and agent i don't want to use the word co-pilot because i think that's a little it's much more your confidant it's much more your agent it's going to tell you stuff like hey that question you asked last month there's a very different answer this month that's a pretty interesting thing or it's going to let you explore so you know tell me about our customer service ratings well which region right disambiguation which was previously something you know that you would do like through facets in search right that's kind of thing that should become more dialogue oriented but those things that's going to take some time because in order to know how to disambiguate you still have to know what data is relevant right so so that's been surprising but i think we're going to see a wave of search driven innovation and i'm excited about it i think the more people shift away from innovating in a repository to innovating across repositories we'll see you know another another layer of innovation and and even more productivity left right for for the people to use it oh yeah that's fantastic the way i put it and i'm glad to hear this because there's the search professional you know now a product manager i love the fact that the powerhouse of you know the future of AI still continues to be searched right at the core and i think search it also says that search isn't solved and maybe this is another iteration that we will approach it but it's like because search is also perception right it's also how you express yourself how you perceive what you see maybe the interfaces will change right so sometimes i do want to you know that that product that google had google glass sometimes i want to have glass on me to take a picture or you know not to be as distracted by going and fetching my phone or something right because today i still have to do that it's not as immersive experience and also i've noticed working with engineers now when i flipped on this side of you know the process i'm a product manager so i keep thinking about things and they keep coding and sometimes i've noticed that they don't even go back on slack from like a couple hours when i when i ask something because they don't want to be uh you know distracted from their ideas so maybe there could be a way for me or agent or whoever to sort of sneak into their idea ask a question talk to them right that would be fantastic maybe it sounds also a little crazy like you you still want to have privacy and sort of you know uh flow but at the same time there is reality of your job right you you do need to go back to your email to your slack or whatever you're using teams and get distracted and then you forget what is it that you've been onto when you come back to your your motive execution absolutely you know in a way applications are distracting i think there was a really good study recently that showed the danger of interrupting engineers right because of the context switch um it's definitely the same for business people they're just like everybody else right context matters and it can be hard to switch um i think that's the real promise of a i look at chat gpt you go to chat gpt they're gonna have search soon web search you can ask it questions you don't have to go to google and being in five other places right to get it and that is the real possibility that you would choose the way you want to interact with it and that thing in theory that single point that single pane of glass or single conversational agent right that could potentially be in front of many many sources of data and that i think that's what what people realize it's hard to say how does what does it really look like in five years if a i really continues along the path of sun the answer is it's the end of applications yeah yeah exactly and going back to being immersive and sort of feeling that i'm myself and i am in control and not like vice versa when today i don't feel like i'm in control applications update by themselves i phone restarts i have no idea what's in that update i will not be able to ever understand what they do but they do it so sometimes i feel like whatever i have bought belongs to someone else but probably this will change and i think this should change as we wrap up i was thinking is there something you want to sort of call out to the community and say you know by now swirl obviously has progressed you guys open source i love it you you have a bunch of contributors probably that you trust and you work with but is there anything that you would you know benefit from calling out the the larger community i think i'm very happy to see the the folks shift and focus towards search i think the thing i'd call out is to say you know there are many different user communities that want to consume AI they will benefit from it and i think the key is not to go too far on the hype cycle right and because honestly another thing i learned is not everybody is into the details of how AI works right and like fine tuning is an example it's a very deep discussion at some level i'm no expert right i can tell a lot about it but i think there's people who have done much more on it than i will ever do but the end of the day the user that's way way way far from the user's head right what they're what they're trying to understand and the people are making decisions about bringing these things in are is it safe how can i trust it how do i get it to provide a benefit so i think the honest thing is rather than focusing on like we've got a few more tokens than somebody else talk about use cases focus on the user i think that's where that's what's world did from the beginning because if you're in search like there is nothing but the user right the user's intent is everything right and from like we can go back to lots of lots of great writing about that from biaires e8s to tinkle and and all points in between but the user's intent's important and that's the thing to focus on what are they trying to accomplish and build great use cases that ultimately you know allow people to focus on the things they'd rather focus on instead of you know the minutia and the time of collecting all these different data points yeah that's i think that's i think what you do as a as a tech industry responding to the user demand for AI my two cents oh that's amazing i don't don't try to outsmart the users and uh make things that you produce explainable and so they probably will adopt them weaker uh that's amazing i also see uh super uh maybe provocative really nice one from your side where you say that you outshine google i will link it in the show notes and maybe we'll discuss it at some point as well something about ranking looks it i really enjoyed chatting to you today i'm sure there will be someone you know in the community reaching out and maybe trying out swirl to be honest it's itching for me to try it out right when i when i said it when i mentioned in my company and someone said i would love to have that single keyword box so that i can search slack and conference and email and everything um that's amazing that's fantastic and also amazing that you guys do it uh in the open so everyone can try it um all the best to you good luck in in whatever you're building in your uh next big things thanks to me tree thank you so much it was great to talk to you and uh when you want to ready to try a swirl you got the open source version enjoy our slack and we'll be happy to help absolutely thank you very much enjoy your day youtube