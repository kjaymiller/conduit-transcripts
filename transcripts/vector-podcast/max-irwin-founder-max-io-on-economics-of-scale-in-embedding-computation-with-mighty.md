---
description: '<p>00:00 Introduction</p><p>01:10 Max''s deep experience in search and
  how he transitioned from structured data</p><p>08:28 Query-term dependence problem
  and Max''s perception of the Vector Search field</p><p>12:46 Is vector search a
  solution looking for a problem?</p><p>20:16 How to move embeddings computation from
  GPU to CPU and retain GPU latency?</p><p>27:51 Plug-in neural model into Java? Example
  with a Hugging Face model</p><p>33:02 Web-server Mighty and its philosophy</p><p>35:33
  How Mighty compares to in-DB embedding layer, like Weavite or Vespa</p><p>39:40
  The importance of fault-tolerance in search backends</p><p>43:31 Unit economics
  of Mighty</p><p>50:18 Mighty distribution and supported operating systems</p><p>54:57
  The secret sauce behind Mighty''s insane fast-ness</p><p>59:48 What a customer is
  paying for when buying Mighty</p><p>1:01:45 How will Max track the usage of Mighty:
  is it commercial or research use?</p><p>1:04:39 Role of Open Source Community to
  grow business</p><p>1:10:58 Max''s vision for Mighty connectors to popular vector
  databases</p><p>1:18:09 What tooling is missing beyond Mighty in vector search pipelines</p><p>1:22:34
  Fine-tuning models, metric learning and Max''s call for partnerships</p><p>1:26:37
  MLOps perspective of neural pipelines and Mighty''s role in it</p><p>1:30:04 Mighty
  vs AWS Inferentia vs Hugging Face Infinity</p><p>1:35:50 What''s left in ML for
  those who are not into Python</p><p>1:40:50 The philosophical (and magical) question
  of WHY</p><p>1:48:15 Announcements from Max</p><p>25% discount for the first year
  of using Mighty in your great product / project with promo code VECTOR:</p><p><a
  href="https://bit.ly/3QekTWE">https://bit.ly/3QekTWE</a></p><p>Show notes:</p><p>-
  Max''s blog about BERT and search relevance: <a href="https://opensourceconnections.com/blog/2019/11/05/understanding-bert-and-search-relevance/">https://opensourceconnections.com/blog/2019/11/05/understanding-bert-and-search-relevance/</a></p><p></p><p>-
  Case study and unit economics of Mighty: <a href="https://max.io/blog/encoding-the-federal-register.html">https://max.io/blog/encoding-the-federal-register.html</a></p><p></p><p>-
  Not All Vector Databases Are Made Equal: <a href="https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696">https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696</a></p><p></p><p>Watch
  on YouTube: <a href="https://youtu.be/LnF4hbl1cE4">https://youtu.be/LnF4hbl1cE4</a></p>'
image_url: https://media.rss.com/vector-podcast/20220616_060650_51fed3f5cf98ff1ddb61cc17e11e43be.jpg
pub_date: Thu, 16 Jun 2022 18:27:50 GMT
title: Max Irwin - Founder, MAX.IO - On economics of scale in embedding computation
  with Mighty
url: https://rss.com/podcasts/vector-podcast/522301
---

Hello, vector podcast is here. And today I'm going to be talking to Max Irwin. He's this star in the search engine business in search engine world. He has been doubling also in NLP a lot. I don't know 20 years. It's huge amount of time.
And I mean, he has been consulting in this space, also building products. And now he's focusing on building his new product. And he's the founder of company called max.io, which is also a website. You can go check it out. And he's building a mighty inference server.
And the number of other tools that I'm sure Max will talk about today. Hey, Max, how are you doing? I'm doing great. How are you? I'm great. And thanks so much for joining me today. I'm very happy to be talking to you today. I'm very happy to be talking to you today.
I'm very happy to be talking to you today. I'm very happy to be talking to you today. And I'm learning about my tea and all the things that you're cooking there. But I think as a tradition, could you start with introducing yourself first? Sure. Yeah. Hi. I was a younger in my language course.
I was a younger in my language course. I was more of a mathematics computer nerd. So I had to kind of relearn language and improve my language skills to be able to be dangerous and in NLP.
But I started NLP probably around 2014, 2013, depending on when I really first started hearing about it getting interested. But I earnestly really started in 2015, 2016 with actual product development around NLP. With search, I've been doing search since about 2010, 2011.
Again, it's fuzzy when I actually first started, but. I think the first real serious thing I did with search was when I went to take my first solar training course, which was one of the when lucid works still had solar training and they had contractors coming to give training.
So that was, that was in 2012, but I'd been messing around with engines before that. I started on an engine called DT search, which was the C++. Closed source engine, but you could buy the code for like a thousand dollars a year.
So the company I was working for many rakes, we actually bought the code. And I was, I was the newbie with search. I mean, we had guys working with it for a while. And they built a whole platform around DT search. I was starting to show its age. So we started shifting over to solar.
But yeah, since I started that, but well, before that, I did a little bunch of computer programs. So like the 20 years, 22 years, ish stuff that's in my bio, like I've been. I graduated university in the year 2000 and I've been, you know, working professionally software ever since.
But with search, I, I really got interested in search. Around 2012 is when I really said, wow, this is amazing. This is so much different from what I've been doing before. So that's when I really do have had first into into the problem space in the domain.
Yeah, and some people say that many of us ended up in search field by accident. As well as actually NLP, I've been talking to one professor here in the University of Helsinki has built machine translation team very, very strong one. And, and he has built the system called opus.
And, and he actually said that he ended up in NLP also by accident because it was just an offer from a professor and he decided to take it and he turned out to be quite good at it, you know.
And he also had another option just to go and work in in Germany, he's from Germany to work in Germany in some company database company. And, and, and likely he didn't take that path. How was it for you? How do you feel about yourself and then ending up in the in the in this space.
That's a great question. It's interesting. I feel like a ending it up. I, it was definitely somewhat accidental. I, I found, I, I had the pleasure of meeting so many people in search through my different positions that I was working with. And the varying degrees of expertise.
I found that a lot of people. Who got involved with machine learning found out about search. Because TFI DF and all that stuff is like an algorithm and it's like, oh, there's this whole language problem behind search so we have to figure out.
And then search people get involved in machine learning because oh, this language problem is horrible. How do we solve it with automation and learning. So I, I accidentally stumbled on it because I took it was a, it was a role that was in like healthcare compliance.
And I was interested in that domain specifically and search just happened to be a really important problem at space. So that's how I kind of got into the technical domain of search.
And it just was so much more fascinating than like the stuff that I was used to with crud, you know, just create read update delete and just workflow applications, which I've been doing for, you know, 10 to 12 years at that point. Yeah.
Yeah, I mean, for me, for me, like searching all like, I think I started 2002 2003 academically. But then it was like seven years past and I still couldn't find a niche or a job for myself because there haven't been many search companies in Finland actually at that point.
And then I found a company which I joined in 2010, AlfaSense and it was a patchy solar, you see in everything new.
But it was still somehow inviting and I think the first time when I, when I've built the backend and I was like, okay, somebody is going to use this somebody is going to type the queries and will try to find information.
So I also tried it out and kind of like maybe work maybe didn't I wasn't the, the user of this system, I didn't know what to type. So I was just grabbing some phrases from the documents and see, okay, does it find or not, you know.
So is this something that also like attracted you like, okay, find the ability, right, like discovery or maybe discovery is the next stage, but even the find ability itself.
Yeah, I guess search was really my first step towards working with real complex data that wasn't so unstructured unstructured data, right. You kind of, you kind of reach a limit with structured data at some point of getting stuff into databases, getting it out and things like that.
And you can, you can spend a lifetime in that work. But I felt like I'd been doing it for a while. And with, with search, it was like this, this weird world where it's like all this unknown stuff and you don't know what to do. So it's an unsolved problem.
I felt like databases and things like that were like this solved problem where search search wasn't a solved problem and still isn't. Now with the work, if I had been doing the same database work, that's all no code right now, you can just create the same stuff I was doing no code tools.
You don't even have to be a programmer if you don't want to. At the level that we were doing it, you know, in the mid 2000s. So, yeah, now it is. And it's still it's still unsolved. Even when we start talking, you know, we're going to talk about vectors, of course, but vector search.
But that's still an unsolved problem. It's like another tool, but you still have all these all these issues that you have to take into account. Yeah, so endless exploration. Yeah, it's like infinite quest in many ways. There is like a limitless amount of tasks to solve.
But then, so somehow in your career, there was a turn that you decided to get closer to this vector search field. I just wanted to hear your kind of first reaction.
Like what did you think about it? When did you hear about it? And also what attracted you? I'd say the first thing that really attracted me towards vector search was the birth paper. That was written in 2018, but I didn't I didn't come across until 2019.
And Google had written a blog about how they were using it for their for their web search. And, you know, you could download some Python and get this stuff to work. But the reason why I was so fascinated by that is because of working in search already six years. No, let's do some math.
So, you know, eight years at that point, I had been stumbling along with the vocabulary problem, the query term dependence problem, as we call it, where, okay, well, to solve this, you have to create a bunch of synonyms.
And you get to a certain level of advancement and then you create a taxonomy and then, you know, you created a knowledge graph.
And, you know, before before before bird, we'd started playing around with word to veck and saying, oh, can, you know, can these type of embeddings be used to solve this whackable problem with synonyms and knowledge graph vocabulary expansion. The answer turned out to be no word to veck.
It didn't work as well as we'd hoped it helped with some things, but not, but it harmed with others. So it produced a lot of noise and, you know, maybe we didn't give it a good enough chance, but we saw, okay, we can train this thing pretty quick and we can get this model from our content.
But there's still this problem.
 So when I started to play around with some of the Python tools that were available for, for bird and large language networks, which actually use word to veck as the preprocessing step to get the first, to get the first encodings and then, first embeddings and then use those identifiers to go forward.
And then we saw something there, I saw actual similarity where I didn't, I just saw kind of co occurrence with with word to veck before.
Yeah, these things are you see them in the same context, but with actual linguistic similarity, the first time I saw that was with bird and that's where all the hype came from. And then the next step with bird is like, okay, I have these vectors. Now, what do I do with them.
And then I said, okay, well, I have to use a dot product, right to use a cosine similarity. Okay, let me just do that. And then I say, oh, you can't just do that across every vector. It's impossible. You have to do something else. And then you go, you go on this learning path, right.
So that's, that's where I, that's where I ended up. And I had actually written a blog post in 2019. You know, about, and I think that post was, you know, widely accepted by the community, it's still in the open source connections blog.
And it was really, it was really showing like, hey, this is, this is a change, you know, it's not just Google that's going to be doing this like this is really interesting. And a lot of people agreed and there's, there was like this movement that kind of happened after that.
And a lot of other people were coming to the same conclusions, but there were a lot of challenges. So, with vector search and approximate nearest neighbor search. You know, that's, it would, that's just the tool to solve the problem. It's like, you know, you start with this problem over here.
And then you go like 10 steps over here. And finally, you get to vector searching. Okay, this is, this is a potential solution. Right. This is the core of the potential solution with all this stuff in the middle. Yeah.
But have you felt that I should read this blog and will definitely link it in the show notes. But sometimes when I look at vector search, let's say demos or applications algorithms. I get a feeling that you might just think, okay, I have a solution. Let me find a problem.
Because it's, it's all semitical. I mean, it's so sexy, right. Do you, do you think this is one of the sort of misconceptions, you know, in this field, or do you think that it's well passed that already. That's a great question. I don't know if. I don't think it's a solution looking for a problem.
I don't think that's true. I think there it actually does solve some problems. But I do agree that it gets, you know, there's a lot of gray area. And how do you arrive at that from I need to find things as a person.
You know, and all the things that you have to go through until vector search actually means something that's a solution. I think there's, there's a lot of people who picked it up and say, okay, we could just use this and it's going to solve solve these problems. But it doesn't do that. Right.
Because search is not just about similarity, you know, you can express a query similarity with a document using TF IDF PM 25, you know, the sentence transformer, you know, cosine distance, whatever. But that's only the similarity. There's also like the need that the person has to what they have.
So it's, it's a bunch of candidate documents that are similar, but what's the actual document you need. So that's where a lot of other things come into play.
 It's just one piece in a much larger search or recommendations platform, you know, you still have to take on all the other signals and, you know, common now in the, in the more mature platforms is, you know, you have some learning to rank algorithm that takes, you know, you have to see similarity is one is one feature in in a learning to rank model.
Along with, you know, PM 25 with the title, the end 25 of the body, you know, the number of clicks, the date, all this other stuff. And it's, it's a piece.
But the thing that the piece solves is that query term dependence problem, whereas like I don't have to, and sometimes, you know, I don't have to go in and craft synonyms by hand, and I don't have this endless task of doing that.
You just, you kind of have all these other tasks that you still have to do, but that one maybe has kept it bay a little bit. Yeah, yeah, absolutely. I mean, maybe I can a little bit like, I can restate my question or sort of like, clarify what I mean.
I guess when you read, I think when you read a paper like Bert, a similar papers, they also say, hey, we, we ran down, this one down stream task like sentiment analysis, we also did question answering, we did recommendation, all these other things, and it works great.
pushes you to think in the direction that is this a universal language model or approach that I can now take and apply to everywhere every task and the answer is actually now because. Hey, I mean, if you are in healthcare and they trained on news, it's not going to work.
So the vocabulary still was not excluded from this journey. So if it is mismatch, it's mismatch. But the model itself, of course, is a clever piece of, you know, tech, which you can then take and kind of apply fine tune, maybe or train. On your data. So I think that that's.
That's one way to look at it, right? It is, but I think that we, we see a huge. Still a huge gap in the domain, right? I think there are a lot of organizations that can just make use of retrained models and fine tuning them.
But, you know, we, I know that there are still domains that you can't do that. Like, when if you go up and you try, you know, something that's fine tuned, like law, right? Law is like its own language. I wouldn't even, like law written in English. I wouldn't even call that English.
I'd call that, you know, legal English, right? Because just the structure, the vocabulary, the grammar, all this stuff is so different than what's in like a Wikipedia article or news or something like that, right?
So when you try to do a fine tuning on a pre-trained model that's trained on, you know, let's say like on to notice five, which is a bunch of collections of like, you know, news, Wikipedia, like general knowledge that most people use.
When you find to it, I, there's still a gap. There's, there's something missing, right? Because the original trained model was lacking this context. And that's, that's only for the content. Also, that's just, that's just the content.
And when people search and they type in terms, you know, you can imagine like this, this Venn diagram of like, well, here's, here's all of the content over here that you've trained on.
And then here's all the terms that your people that the users know, right? And you try to like bring these closer together somehow, right? If the model was trained on content that is like up here, then you're going to have trouble like kind of putting it together.
I don't know if you can do a good job in my hands showing this, but no, you're doing perfect out there. So I think that one of the one of the big existing problems is pre-training still costs like a ridiculous amount of money and is out of the reach of those teams.
Yeah, I've read, I've read papers, you know, one of them was by Microsoft showing like if you, you know, the bird vocabulary is like 30,000 words or something like that. If you increase the vocabulary size to like 100,000 words, then the model generalizes much better.
And you, of course, you expand the content and the domains that are involved in that training. So I think you, I think we're going to see some more of that. The world is still stuck on this 30,000 terms in the pre-trained space of things like onto notes.
Because it's just so expensive, it's train models and Google and Microsoft and Facebook and these companies that train models, they're not going to bother open sourcing those maybe that maybe they will at some point.
But I think we're going to need to see big companies that are specific in that domain, train those models and then open source them. But if you spend millions of dollars to train a model and you're a big private company.
Are you going to open source the model weights probably not you're going to keep it for yourself because that's huge value, it's huge value for your product. I guess you open source the idea sort of you publish okay here is the bird model here is the mom model or whatever.
But then go train it yourself. Yeah, if you have a couple million dollars lying around.
Yeah, and then I was I was also talking to in another episode, I mean, Ahmad who used to work at Google and he said that entire teams would be dedicated on a quarterly basis to do the expensive fine tuning work with burrito similar models.
And can you imagine that it's like a team's effort and this people some of them invented the model some of them didn't but you know with all the resources that Google has to fine tune them for three months.
So I don't think this is out of reach of startups and I mean there are other things that are out of reach like and this is where you saw the gap with might you want to get closer to the might now.
 So there is you know every time I install a vector database i'm not going to name one and it says hey you know it will be faster if you use GPUs and i'm like okay I must start up I don't have GPUs you know so this is I think one of the gaps that you saw with mighty but are there other gaps that you saw that you are addressing with mighty server.
Yes, so the NLP world right now in the vector worlds right now they all they talk about is Python Python Python Python everything is in Python when you get to production you use something else but it's Python Python. So I wanted to.
I came from a non Python background I started with see Pascal when I was really young and that my seed programming is terrible for sure then I then I discovered you know.
Intermediate intermediately compiled languages Java C sharp things like that that was like early 2000s for me and I kind of went I was in the Microsoft world so I was doing C sharp for a while.
And then I found and all the while I've been doing JavaScript because of you know I was involved in the web in the mid 90s.
And that's how I got involved with content and content data and all this stuff there's just all web stuff and then you got a no JavaScript if you do anything with the web so it was like see see sharpen JavaScript for me for a while.
So I know that there's a gap if you go and if you go and you go into the JavaScript world the node or you know type script or those things do know now there's there there's nothing you want to do nLP learn Python. That's pretty much the suggestion.
 Same with C sharp you know okay well there's there's some libraries out there for their clunky nobody really you know Microsoft probably uses them right because they're Microsoft and they built C sharp and everybody's doing Microsoft stuff but you know outside of outside of Microsoft's like who's using C sharp for for natural language for us to train models.
Nobody and to host models you know okay well to do it you have to jump through all these hoops and it's really hard.
So unless you want to like put Python in your stack which is basically a non starter for a lot of teams a lot of teams that they they work in languages like node JavaScript C sharp Java Ruby go like there's so many huge languages out there that just can't touch.
So I wanted something that kind of broke out of this shell is Python is Python like enclosure of like how do you get this stuff into the hands of other people just want to build web applications they don't want to go and you know go into the Python family.
So that was that was one of the one of the starting catalysts from from mighty infrancerber.
 I there are there is one tool that I have to use that is Python because it has to you have to convert a model and I convert the model to on X which is most people know about on X if you're in the NLP world by now which stand it's oh n and X that's for open neural network exchange and is this intermediary format that can be used generically it's like an open model format.
Now there are runtimes that you can take on X and on X models and and run them so the biggest the biggest one is on X runtime and that's developed by Microsoft it's open source and I see licensed and that's written in C++.
 But there are bindings for other languages and community contributes by next so you can use on X runtime and Python if you want to you can and you'll get like for those Python people who want to host models in Python just convert your model to on X and hosted in a Python on X runtime it'll double the speed of the model inference like out of the box you don't have to do anything you press like a button you don't you you clone the repo you press a button and twice twice as fast.
But for others you know there's binding for C sharp there's bindings for Java there's there might be bindings for Ruby I haven't looked probably bindings for go and even if Microsoft doesn't support them the community builds them.
 So you can do this but there's this other problem that you have the other problem is that well those are just the model weights and if you're talking about and hosting the runtime for the model weights you so you put in inputs and you get outputs but where do you get the inputs from well you have to tokenize text and you have to do all the stuff to prepare it to pre process it and then when you tokenize and you do pre processing.
Then you can pass in those the tokenized data is inputs.
 But all the tokenizers are written in Python so now you have to now you have that problem so I actually used rust for mighty inferno server because hugging face based their tokenizer they're fast tokenizers on rust they wrote it in rust and they offer bindings for Python so if you if you install fast tokenizer in Python you're actually using rustbox for the first time you can do it.
 So I wrote a web server that wraps the rust tokenizers and on extra on time and I wrote a whole bunch of code for pipeline specific stuff like question answering sentence transformers sequence classification which is like sentiment analysis token classifications that's like entity and any recognition and I'm working on others also but that thing is really good.
It's so much faster it's so much faster than Python like it's not even close.
It's probably like three or four times as fast without any fine tuning of it and I've gone through fine tuning so by compared I haven't compared it to Python in a long time but I might be like five times as fast as Python right now on CPU.
You can also use GPU if you want and it's you maintain the same the same speed it's just as fast.
Yeah well it's just as fast as the the ratio of speed is like the you know if you took the model in Python and you put it in a GPU versus you take the model and on extra on time you put it in the GPU you get it's far faster.
And you say like when you said bindings in other languages you know like Java C sharp so if my stack is in Java I can take this model and kind of like it into my Java code.
You can take a let's take a hugging face model for example like just let's say brick based encased you know most people know that one. For based encased you can export that to on X with hugging face code in Python and you have now you have an on X model.
Now you can in Java you can stand up a class that wraps the on X runtime and you and you load the model into memory with on X into on X runtime in your class and then you can create methods around that class right and then you can call you can call in you can say I'm going to pass this.
 Say i'm going to pass in the inputs and i'm going to get out and that's all in Java now well with the C++ wrapper for on X runtime of course but to connect but to wrap that C++ runtime there have to be bindings between the language so Java has to have some application interface to talk to C++ which is GNI right Java native interface I think yeah I think so Java yeah.
So that part like having Java talk to on X runtime is taken care of already you still have to write all the other stuff around it like to you to leverage it but that's you know what programmers used to that sort of thing you know Java you can you can do that and I think.
 I don't know if it's I don't know how much we've seen it but Jeff Semarek who works at open source connections I know that is like he was working on a project where you know he could try to load an on X runtime and in open and LP which is a Java program so trying to get an on X model in open and LP and I think he succeeded I don't I haven't seen code for that.
 Yeah yeah that's awesome so I mean the reason I'm asking is because I witnessed this tectonic shift in in my previous company where we had the entire stack in Java even though we started with Pearl but we had to read right everything into Java it's just didn't scale on Pearl and yeah and I mean we had Apache solar and we had a lot of things like that.
 So I had a big number on one end as the open source search engine also written in Java and you know when we would customize it we would write plugins in Java and so on but then when we wanted to to introduce a into the pipeline of course everything was in Python we hired people who could only do Python nothing else fresh grats.
 With this new architecture okay you have Python as one step in the pipeline how do you call it how do you handle errors how do you scale this thing right and we were also moving to Kubernetes to add to this crazy mix and and what we ended up doing is that we would have a synchronous processor plugged in in every place where you have Python to abstract Python away from Java right so you would come.
And I kind of like just say send this message to an sqsq and on the other end there is somebody consuming it can you imagine how scalable this can be.
It works it works you can also like scale it locally but as the whole architecture I don't think it's it's a very kind of smooth in a way solution like not to mention that the performance element of it is just not taken care of.
And what you say now essentially like with on X binding in Java we could just train that model and and and then export it in on X format and then use it directly in Java. You can yes yeah. But you still have to get the inputs to the model so if.
If it's like an image or something like that it's usually pretty easy but if it's text then you have to tokenize first and you have to use the right tokenizer and you have to do you have to kind of jump through a bunch of hoops to get it to work correctly so it's probably.
A month's worth of work to get a tokenizer working in Java the way the way you needed to work. Yeah and maybe you could in principle share this tokenizer between tasks right so it's for sentiment or for entity recognition in principle you could use the same tokenizer yeah.
Right so the tokenizer is so the tokenizer relies on the vocabulary and the configuration which is bound to the model so the model is dependent on on these things so if you have a generalized way to load.
If a vocabulary in the configuration then yes you could just take the take the thing and your new.
New stack but having said all this with mighty you took a different you know approach like the philosophy behind mighty you offer it as a web server right and can you can can you tell me more about it i mean i'm sure you can open a lot of detail.
Yeah the reason I went that route is because when you when you want to do model inference. You want to give it as much compute as possible right.
And you kind of want it to be its own thing so I went the microservice route i'm not i'm not saying microservices are the way of the future and they're better than model it's and all this stuff but the idea of coupling like this.
And you know this model inference is part of like your regular application code.
Maybe you don't want to do that you know you want to have this other service that can and this is part of like the bigger m alops question which is well how often should I update models what are the things that I just know about.
So you know drift and all these things that are like what about logging and all this stuff it's like well okay you need a way to do this and if you embed model inference in your own code now you're also responsible for all this stuff right.
So as a as a microservice you can evolve that microservice and say all right this thing is responsible for model inference and that's it right.
So you can evolve that in all the side effects around that of like okay well you need a new model but if you have to a b test models what if you want to do logging but if you want to do all these other things.
You can evolve that in its own way and it's in the separation of concerns makes much more sense.
 And then it kind of gets you out of the it gets you out of the problem of like okay well am I going to build a mighty for Ruby am I going to build mighty for node am I going to build ready mighty for go like I don't have to do that I can just build mighty inference server as a web server or a grpc which own you know it's on it's on the road map I don't know how long that's going to take but.
So you have this thing and then I just have the right client libraries and the API is always the same client libraries for HTTP are super easy so. Yeah.
And if you compare this let's say we take a database like V8 of SBAR they have inference inside them right so like if you already bought into that solution in principle you could you could do this the only caveat I think is that if you have your custom model you'll have to go.
An extra mile to actually integrate it inside this database right and at that point with V8 I think you will have to master go with best but you'll have to master the C++ or Java i'm not sure i'm not an expert in that but there is a podcast with Joe Bergum that you can check out but yes.
So how would you kind of like on product side how would you compare my to that approach. So best but uses on exprontime best but perhaps on exprontime I believe it's on a current time I know the use on exprontimes don't know our percent of it's on exprontime.
So you still have to go through the step of doing that with with we be it it's a little bit different with we be it it's you have these things called modules and then the modules are typically like docker containers with.
With API six suppose and then there's logic written is in the module code for for we be eight that will wrap that API and it's easier if you just copy and paste a model and then change stuff to match the API of the thing that you have a doctor container.
So it's not that much work you still have to know go to do it. And yeah I think the other problem that I have with that approach and i'm not i'm not saying it's wrong but from my perspective.
So if you if you look at the documentation actually for a couple of vector search engines i'm not sure if this but I think we veaid and maybe another will say okay well it's better to use a GPU for inference and then CPU for the vector search right because.
You know you want to provide as many workers to the to the search algorithm as possible and you don't want the you don't want the inference the model inference and the in the vector search fighting for resources.
Because both are very expensive right so they say hey if you have GPU then all your model inferences and GPU and your vector search is all CPU and you get this one perfect box and everything just works but okay well what if you want to scale beyond that.
You can only send so many documents into into a GPU at a time what if I need what if I need 12 machines when I need 12 machines that are all hosting we veate and they're all hosting mighty all or whatever your inference solution is all at once right.
So this goes back to the separation of concerns problems like well what if I what if I have a lot of documents that I need to process and it doesn't take that long to to get them into the vector search by the vectors but processing those documents takes a long time so I have to pre process.
Well now you've kind of got like this situation where you might need another solution to do this batch pre processing right in another in another place.
And then you bypass the module when you when you integrate into into we be a you just want to send the director directly to we be so you don't you don't have any inference sending the director to direct.
So again it's like this i'm not saying it's wrong I think I think it's a great idea because you know you can just install something that will just work right you don't have to install like three different things and try to figure it all out.
So I think that getting getting up to speed on that is probably quick but in the long term like the scalability overall I think that you now have this coupling and it's a bit of a challenge so I don't know how that gets resolved.
Yeah that's actually a good point because you reminded me of I don't remember precisely what we were sort of balancing between but like with solar and a Java pipeline in front of it so the pipeline would process documents as they come in and you know what I mean.
You know chunk them classify them run sentiment analysis on them and so on.
 We were thinking okay some of these things could be computed inside solar like we could write some clever plugin which actually does i mean solar has a lot of things there you know like before it indexes a document you can run like a ton of things I think open and it is one one example right you could plug in and run something there.
And I remember that my manager like who was a VP of engineering he came and said hey what if we lose solar so we computed everything inside solar stored it and lost it then what.
Like now you need to bring it up back really quickly and usually what you want to do is probably like replicate some shard and off you go right but if you if you don't have that data you need to recompute it now so you don't have any intermediate storage storage.
Solar is not the storage solar is the database and so we backtracked and we said okay we will compute everything and store it in s3 you know in file storage and if in the event of losing solar we will restore it and reindex everything on the fly.
 So I mean that kind of also like you know resurrected that that that situation that also be deviator quadrant or any other database if you lose the fact if you lose the database you lose the vectors so if you have computed them inside the database now bringing it back and then turning it on and say hey please compute my vectors again please please please you know just too much time.
 You're exactly right and this is a this is a lesson that I learned I didn't learn this lesson the hard way thankfully but this is just a lesson I learned picking stuff up when I was at when I was at Walter's Clure which is a huge publishing firm and you have you have your content which is like editorial content primary source content and it's it's written in such a way where it's it's pretty raw from a machine perspective you know and then it goes through a series of things and it's really hard to get it.
 So I'm going to add the topics and then I'm going to save that state that's now on this somewhere back to okay well now I have to you know add this other thing you know do any recognition or something that's also said right so you have all these intermediate steps so if you lose anything it's really easy you don't have to rerun the entire you have to rerun the entire pipeline it takes you months to do that.
Not just days but like literally months to start from scratch with content so that's like a disastrous scenario. So this lesson that you learn is, okay, well, yeah, you don't do everything all in one place, because if you lose it, then it's all gone. You start from scratch.
So yeah, separating concerns in that way. And then the idea of, well, you can plug this thing in anywhere along the chain now. You know, you have this, you have a microservice, you can put it in, can put it anywhere.
And then you can, you don't even have to just take the vectors and then stick them in the search engine, right? Well, what if you want, what if you need the vectors and you want to do something else?
What if you have like a recommendations platform and you have this other system over here, and you want to do this other stuff.
It's like, well, you have to think about routing and all these other things. But if you just have an easy way to get vectors, you know, plug it anywhere along the stack, then that's up to you. You know, there's no, there's no prescribed way of doing things. It's a Lego.
You put the Lego wherever you want. Yeah, that's a great point because we also implemented like an algorithm, which was it computing some topics, I think. And we used fast text and work to back vectors.
But we didn't need the vectors in the end in the downstream system, we just computed them, cluster, run some magic algorithm, you know, produced. And then you store the topics. So you store actual words in some database, so index them in the search engine. So yeah, you're absolutely right.
Like sometimes you don't need the vectors, but they are still the medium to get to your, you know, to your target. So, and, and so, but you've, I've seen the blog post, which will also link you've published on marks that I owe.
And I think sort of almost like a unit unit economy of this thing, like if I have mighty gazillion amount of servers, how it will play out, you know, how much. Separation of concern and also resource separation all these things and how economical it is in the end.
And it is something that you are proposing. So let's say if somebody takes mighty and wants to scale it, you know, like all of a sudden you get, instead of 10,000 documents, you get 10 million documents to process, right.
Because somebody changed somewhere in the pipeline and now we need to rerun the whole thing. So how would you, what is your recommendation also on the economy side, how do you see mighty playing a role in making this huge thing more economical.
So the first thing, the first thing that I see is that you can, you can calculate the cost ahead of time, because it's absolutely linearly scalable, right. So, you can take, so mighty itself sits on one CPU, right.
It sits on one thread, I'll even say a thread because these days you have cores and CPUs and threads and it gets messed up.
You can, you can tell mighty to use multiple threads in certain situations if you want to, but the example for bash cross fixing that I use, which I actually learned from the best fatigue because they wrote an amazing block.
And I think it was early January, they released a blog post talking about this exact problem of, you know, do you have one process across multiple threads, do you have multiple processes.
So if you go with a multiple processes route, let's say I take, I take a bunch of documents and I pass them in and I, you know, I have some level of consistency in the document size.
So, you know, I actually do, pass them in and it takes you, it takes you, it takes you, X to get all of your documents, interest, right. So you have that number, you know how long it took and you know how much, how much content you process in terms of fights.
So if I, if I add, if I add another process now and I'm doing this purely paralyzeable so half of my documents go here, half of my documents go there, it's what I said exactly is linearly scalable, I add a CPU, it has the time, right, it has the time that it takes to do this.
So if I have, if I have a situation where I've said, okay, I did 10,000 documents, it took me X, now I have to do a million documents.
How long do I want it to take, you can actually write down the calculation and say, I need, I need this exact infrastructure, which is a huge problem right now, a lot of people don't know that.
Okay, let's just add a lot of GPUs and see what happens, you know, you can, you can spend the time to go through and do that calculation. But it's not so straightforward. And you'd have to do it like you'd, you'd have to cost it yourself.
I haven't released it, but I want to have a calculator that says, how many bytes do you have and, you know, how long do you want to spend, and I can say, well, it'll cost you this in Amazon or whatever. So that's, that's one, that's one thing.
I also want it so we, I mentioned GPUs is like, this is, I built it so it works on CPU.
If you are a company that's getting into this stuff, and it's this, this idea of unit economy, like how long does it take to process something? What's, what's the cost and, you know, how do I scale it, but the, the billion documents.
I'm coming into this ecosystem and content processing, and I'm used to working in Java, or it, you know, C sharp or something like that.
Now you're telling me I need to buy GPUs, like I have to run GPUs, and then I go check the prices and I'm like, well, that's not how much we spend on infrastructure. That's not in our budget. I'm sorry to tell you. So maybe we can't even do this.
So I wanted to have a way where you could get around that problem, where you could just use CPU, and it's a straightforward understanding of the cost that you have to put in.
I haven't checked Amazon, I haven't checked Amazon prices in a while, but I, might as well be posted online, which is, which is another cloud platform.
I just, the pricing is better, and I just like them, they were actually recently purchased by a huge content management system, stress on the name, forget the name, whatever, anyway, I use line note, and it's, it's cheap for CPUs, like it's great.
You want to run a GPU, it's like $500 a month or $1,000 a month. And that's a lot of money for one machine, and most teams are not willing to spend that. If you want to do fractional, you know, on AWS is probably fractional GPUs, I think. But it's still expensive.
And now you're now, it's like this cost that never goes away. Like once you once you do it, it's like, well, it's there, it's there for a long time, you know, CPUs are a commodity, GPUs, you have to fight with the, with the cryptocurrency crowd for the cost, all this stuff. So, yes.
I can imagine that GPUs can be used during the model training or fine tuning, but during serving that sounds way too expensive. Right. Yeah, yeah, that makes a lot of sense.
 And, and so now when you offer my, how exactly you offer it, it's, it's a binary package, right, that I can install and basically run on my, on my system, and I can decide whether it will be like a standalone kind of script or it will be a port in Kubernetes or Docker image and some other known Kubernetes.
So is that right? That's right. It's, it's a very small executable. It's, it's so Linux is a first class citizen.
Windows is, it'll run on Windows, it'll run on Mac, but I've heard people running it on Mac M1, but they had to like do a lot of stuff to like fix dependencies and it wasn't really working that well.
And I think what's it called Rosetta or something, I think it's still using that like to, to do the X86 like bridge, like the translation visualization. So Mac and one, it's not, I wouldn't consider it working. I've also seen some other problems on Mac that I'm trying to resolve.
It works fine works on my machine, right, that type of thing. But really it's meant to be running links. You can run it in Docker. It's really easy to get started in Docker. So you can download the executable and run it on your back.
Or you can just download the Docker and use that, which is probably a little bit more straightforward. And you don't have to worry about other dependencies. With Linux, I don't, if you're running it on on Linux machines, you can use the Docker if you're doing like Kubernetes and that stuff. Great.
Run it in Docker. Just make sure that you sort out like in your pod or whatever, like how much compute you're actually giving it. Because model inference doesn't, it's not just a mighty, it's like all model inference is really, really heavy. It's really expensive.
It wants a lot of, wants a lot of compute. Not so much memory, but compute. So just be sure to give it enough to satisfy your, since you, and you time I haven't done computer any tests myself. But I like to run, I'm old school, like this whole Docker thing.
Yeah, okay, I'll, I'll make a Docker file. Sure, you can use it in Docker. It's on the Docker hub. But I like to just install stuff the old fashioned way. In Ubuntu, I just, you know, download the download the thing. It's a tarball. And you, you know, you end up the tarball and you're good to go.
And the way you started is actually it's a, it's a rest program with a, with a library dependency, which is on its runtime. It's dynamically linked. It's not statically linked. But to start it, you can either start one core, you specify the model.
Or there's a thing that says it's called mighty cluster is just a back script back script. And it'll look and check how many chords you have on the machine. And it'll start a process for every core you have. So it does this for you.
And it takes like less than half a second for each quarter startup is, I actually put that in on purpose. That's a limit I put in to slow it down a little bit. So it didn't let go off the rails. But you could probably take that little bit off.
You could just go and modify the bash script and see how quickly it will start up. So I, so that blog post that you mentioned before. Like I rented on 128 course. So it would take like I actually took the rails off and let it start up really quickly.
But it can take, it can take a moment to start it up on every single core. And yeah, you could do it in Docker. You could do it bare metal. If there's any people out there using windows, I'd love to hear from you. I'm back from Mac and Linux, but I haven't gotten any windows feedback.
So I don't even know if it's worth building it for windows these days. Maybe not. Yeah, I think it depends if I don't know what should be the scenario is like a your developer on windows. And for some reason, you don't go on your server side to.
You want to develop everything locally, right? So you want to bring up I saw such guys actually in my team, they wanted to bring every single server service on their laptop. Yeah. And that's how they developed they didn't want to depend on any external connection.
Right, even even Docker is like a pain in windows these days sometimes, right. So I know that I know the windows ecosystem because I used to I used to be in in 2000s. But that's the mindset. It's like I'm just going to run everything natively in windows. Yeah.
And like when I tried mighty on on Mac, I think it took like some seconds to boot, but the moment it booted, I was like shooting some queries and like to compute the vectors and was insanely fast.
Is it only on a secret sauce and in this insane fastness? If you're if you're used to running models and Python, it'll seem insanely fast. A lot of it is on that's they get most of the credit there, yes.
But there's a lot of other stuff that goes into it, which is like the tokenization and the pre processing and the post processing is just it's fast is I've been using rust for it and it's just a rust is a really interesting language. It's it's gotten me back into systems programming.
I'm not here to say that like rust is like the most amazing thing ever. There are things I love about it, the things that are like, I don't know if I would do that way, but you're supposed to do things a certain way because the compiler understands that it'll super optimize it for you.
It's hard to it's hard to wrap your brain around it. If you're if you're from a dynamic type of language like Python or JavaScript, it's hard to get a handle on note. I compile background like you know type, type programming language is compile ahead of time.
I was used to that from my previous life. So I was able to pick it up again and I read the I read I just read the rest book. There's a free book out there. I actually bought the I bought a paper back because I like paperbacks and I like hard covers like actual paper these days.
So I was reading it like that. And just going through the examples took me a couple weeks to get a handle on rust. That gets a lot of the credit as well. The rust language. It's just it optimizes and you know you have to learn this field that I'm in now with model inference.
It's like the super niche field of like you have to understand the hardware and you have to understand the machine learning and there are those two fields are like so different. There are very few people out there that are really good in both. So I know that there's a word vectorization.
So vectorized on the CPU is like well, if I have to do a calculation with eight with a bite and you know I have a register at 64 bits. But I have an eight an eight bit bite like well I can vectorize and I can do eight calculations because it's with SIMD.
So I can do the same instruction multiple data right. So that so rust if you turn on certain compiler flags it'll do that for you automatically.
So you get that speed up so I turn I turn those knobs all the way up said use all the use AVX one and two if the process supports it and most processes do these days for your on x86 arm has a different set.
So I'm going to run into the arm world that I have to get an emblem back and I'm going to start messing around with all that but if you know that stuff and you know how to turn it on, rust does the rest for you.
You kind of have to write your code a certain way so that you know rest will do the optimization certain way you can't think like old school you have to kind of think in rest world a little bit.
And you get all this extra all this extra speed from pretty much nothing just from writing your code a certain way turning on a couple of compiled flags. That's why it was so fast.
Yeah, but you still needed to figure all of these out and I remember you were you were saying that you had a bunch of weeks you know coding on stuff.
 You get things done because I know and many of us probably know here in the audience that if you are a programmer, you might say yeah I can do it but you cannot actually estimate when you will be done so you get into the weeds and like oh my god it just like you to have for something else doesn't work or like i'm sending a requested fails whatever what's going on and you spend so much time or if you're doing an algorithm that's another story.
That's like an internet journey there like debugging all these states and and I mean i'm just trying to say that even though you you make it sound so easy to to master rust and you know to to go through all these maze and make it the way compiler wants it.
It's still time it's a lot of time it's skill and so you master it and that's why and in the end you know the end result was not given you you earned it right so why not turn this into the business so now on the business side i'm thinking.
 How do you offer us like so how do you offer excuse me mighty so you have you have the the binary you have the like the model will be shipped separately somehow outside of binary right but what as a customer i'm paying for and yeah and also kind of ahead of time a question, can you give a discount code for our audience to try it.
Oh that's a great question. um. Yes, so my business model is is again old school because i've been doing software for a long time so it's license software right you pay a license you get to use the software i'm still trying to figure out the exact price point.
Some people say some people say it's too cheap, which is interesting because I didn't think so. Some people think I say I should charge more money for it.
It's $99 a month right now when this podcast is published and after that it may change if you get it I don't have it's light up to strike I can go and create a discount code for folks I don't have a code right now but if you if you email me and you say I heard about you on the vector podcast.
Follow the link in the description like follow the link in the notes and email will will set something up so you can get a discount. That's the way it works, but that's that's for commercial so if you're using it commercially and you're making money from it.
Then you know I ask you pay a license please. If you are a nonprofit charity or just using it you're a student or you just have a side project you're messing around you just want to get some vectors go install it you know don't worry about it.
But if you put in a production is and you're charging money for your product please please buy a list. Yeah yeah. To have questions sign how will you track who is using it for commercial and who is using it for a hobbyist project. That's a great question and I don't I don't track that.
I'm also I'm really into a privacy and safety on the web. So I don't like the idea of like putting in a whole bunch of tracking into lemon tree. I think that's a terrible way to run a product these days.
I the only thing it does is I have it ask when it first starts up it just asks the server for what the latest version is and it'll tell you if there's a new version. So with that I see I see that okay that you know somebody asked for a new version.
And I anonymize all the IP addresses so I don't even know who like there's nothing there's no user information at all so I just use that to kind of track. How often to start and it's I see like maybe maybe five downloads a day. Right now.
That's what I do so if you're running it you're for pirating it. I can't stop you spending my time trying to stop you. It's not it's not worth my energy. I'd much rather work with teams who really want to gain something.
So if you do buy a license I'll work with you on setting it up and telling you how to use it and working it and working on it with you. It's not advertised but around model inference itself. I'm happy to offer services to get your model up and running and making sure that it's running.
And so that's not really. Even doing a model conversion with you setting you that setting you up with that stuff. But that's that's not advertised. It does say like I'll spend an hour with you if you buy a subscription to get you set up but if you need more help than that, you know, let me know.
Now there's another tier which is like if you're Amazon Amazon would never buy money they have their own world. So if you're like a cloud provider or if you want to offer it as an API. That it doesn't count because it's it's it's per. It's per product that I sell the license for.
So if you are selling it as a cloud provider as an API and you've got like a thousand clients that are now using money. Well, I actually count all of those clients as a mighty user.
I don't have a price published but if you have that situation, I'm not going to charge you 99 dollars a month for each client that's that that you know if you're running that's ever business. Contact me and we'll work will work something out. Yeah, that's perfect. I mean, sounds like a solid model.
I mean, for the start for sure. And another like favorite question I have and I've been asking this question also to open source players like V8. And I think quadrant.
So basically, I have these thought, you know, one way of kind of building that connection that may yield a business case for you is what you just explained right. So somebody buys a license and then you scale with them you explain how to make it better.
How to tune it maybe implement some features another route is to open a Slack channel or discord, whatever.
And you know, invite users there and then start talking to them and maybe you'll have some open source components as well at some point, you know, I don't know a tool that helps me to convert my model into representation that might you can read.
Have you considered also taking that open source route as one way of building that community of some of which will be your users and paying customers. Great question. I don't have a. I don't have a slack. I don't have a slack myself. I'm a member in many other slacks. I could set up a discord.
I'm on discord. Mostly just for the ML ops community and discord. But I could just start like a thread or a channel and that I don't know if mighty itself needs its own slack by itself. I think it's more of a community. It would be part of another community.
One of the one of the annoying things for me is I have to go and join like 12 million slacks because everybody has their own slack and it doesn't work with one another discord does that way better. Slack. We got to have words. You got to make it easier.
I have like four email addresses or five email addresses across like 12 different slacks right now. I can't keep track of them. I can't keep track of them. But in terms of open source, I already have a bunch of open source projects. So there is. Max dot IO but spelled out. M A X D O T I on GitHub.
Somebody already took max dash IO. We can't have dots and good. That's fine. You know, names or names. So there's mighty convert, which I actually am working on updating that because it's based on optimum, which is a hugging face repository that does model conversion.
It's a very light wrapper around optimum. It basically just converts the model for you. Bundles the tokenizer and a configuration. That's it. It's pretty straightforward. You can do that yourself. You don't have to use that. So that, but that's open source.
There's also a mighty batch, which is a node program, which is a way to do concurrent batch processing of documents, into vectors, pointing it at a mighty server.
That's best described in the blog post that I wrote and how that works about the blog post being converting the code of federal regulations. That's on it's on the homepage of max.io. And there's also a bunch of other open source projects that I haven't talked about yet.
So now node mighty, which is basically just an API client for node that will talk to mighty does connection pooling. So if you have like four mighty for mighty cores running, it'll talk to all the little negotiate which core to use when it makes a call.
So that's really easy to use and like an express server. I also wrote to other node modules while I was at it that aren't from mighty, but I wrote node quadrant. So now there's a node client for the quadrant vector database.
And I told, I told the guys that quadrant that this exists and trying to work on a blog post out of an answer, I guess this is the announcement. I'll publish something. There's going to be a demo. I also wrote node Pinecone. So well, it's Pinecone dash I owe.
So now there's a node JS integration into Pinecone. So you can talk to Pinecone from from node from a kind of express server or something. The guys of Pinecone don't know that I wrote that yet because it wasn't I didn't I just put it out there. It's on npm.
So I got a I got a I got to work that out and they might want it. If you guys if you want this, you know, I just wanted something that I could use. But it's your name. So please take the take the package from me. If you don't get upset that I used your name.
I just wanted a tool that I could use for my own node JS testing. But then this stuff integrates with with mighty really easily. So I have all these node clients now. And I'm fucking focusing focusing on JavaScript for it. So all this stuff is going to be released.
It's already it's already up there. It's on npm. It's on my my GitHub. It's free to use. Maybe needs a little bit more polish. I haven't fully mapped out the APIs. I just mapped out the core stuff that I needed to do.
So it doesn't do things like the scroll command, you know, where you can scroll through all the points on quadrant. But I don't know how much of a use for that is it's really easy to add that I just didn't have the time. So yeah, there's there's a bunch of open source work that I've been doing.
I also want to mention I'm working on starter applications. So I have. Right now. Basically, it's like a it's like a starter app that works with node and node mighty and quadrant. And also node mighty and Pinecone.
I have two starter apps that aren't released yet that I'm working on polishing up and getting out there where it's where it's really easy. So if you're a job script person to just take documents, convert them into vectors, load them into a vector database and have a search app running using them.
That's fantastic. I mean, so much turned back and I think this could be one of the like a we're witnessing a community written software for a close software company.
I mean, Pinecone is a close software company, right? And we have an episode with Greg Kogan, who is a chief marketing marketing officer with Pinecone. We can connect you to and you can discuss the future. I talk to Greg. We're working on some stuff.
So what my question is what made you write those connectors like did you think that this would also pave the way to using mighty, you know, plugging in mighty in the pipeline. Let's say if I'm a Pinecone user and I can have a node Pinecone connector at the same time as mighty.
I'd say half half that, you know, there is, you know, I do want to promote it, of course. But again, I want to bring these tools outside of the Python ecosystem.
If you look at the vector databases right now with it with the exception of with we be eight, we be eight does a great job of having different clients for different different languages and stacks. Well, but both, both quadrant and Pinecone right now, it's all Python.
Quadrant quadrant is written in rust, but their client right now is their first class client is in Python. Which they did that because obviously everybody who has to get vectors has to use Python anyway. Or they used to. But that's why they chose Python, at least that's that's what I speculate.
And Pinecone as well, all their examples are in notebook form and Jupyter notebook form you go in and you want to do a semantic search example, that's a Python notebook. I'm not crazy about Python notebooks.
I think Python notebooks are good for illustrating like ideas and sketches for papers, but it's really hard for me to look at a Python notebook and say, here's how I make this into a working application. It doesn't translate well because the architecture isn't there.
It's a bunch of cells that you run an order. That's not how, you know, real world applications work.
So the idea is to just get these tools and get these ideas and capabilities out into the hands of a lot of other people who want to be able to use this stuff and are not familiar with Python, they're not familiar with NLP. I want to be able to use this.
This new technology because they might have a business problem to try to solve. So you're thinking actually about engineers who are day to day productizing their code and thinking, okay, yeah, I need a embedding layer, but I don't care about notebooks. I'm not a Pythonista or whatever.
So, you know, just give me the tool. Exactly.
 I mean, and by the tools, you also like disclose something like ahead of time with me that to me that you are like one of the overarching goals for you is to develop as many tools for the vector search community as possible and like some of the tools you mentioned, like go beyond, you know, pure engineering components like connectors, you said, maybe like fine tuning a model or something of that sort, which at that point, I think you are stepping on the ground of, you know, other start.
Like, you know, other startups like Gina and, you know, Deepset and so on. Do you feel that way or do you not concern yourself with those and you are just thinking, okay, what's missing in the field, I'm going to add it, I'm going to open source it. Yeah, same.
So, Deepset is like it's all Python again, Gina, I think is a lot of Python, right. I'm not as familiar with Gina. Yeah, they are Python mostly.
Yeah, it's, there's a huge opportunity to make these tools available to non Python stacks and I don't, I, before I started working in machine learning, I've never even considered Python as, as an application framework, you know, people are using like Django, like, flash and stuff like that.
But for me, it was like, it's not that it didn't take it seriously, I just felt it wasn't, it wasn't something that I would have chosen to use aside from, you know, a lot of other, a lot of other stacks.
So there are so many other teams out there that want to be able to use these things, but now they have to, oh, Python, Python, Python, it's nonstop. So we got to break out of that somehow. And I'm starting with node because the JavaScript ecosystem is just absolutely enormous.
I think people underestimate the size of the JavaScript ecosystem. If you're in machine learning, and you're listening to this podcast right now, like there, there are like, maybe a hundred people for every one of you who's using JavaScript in, for applications, like that's how big it is.
So I'm starting there. I just know it's just an enormous community. And not only for front end development, you know, we need to emphasize this because you also have server side JavaScript, like node. Yes, and others. And it's huge.
And a lot of software, which is kind of the middleware between your super cool search engine or your vector database. And you have a lot of middleware written in node because it's so much easier. Oh, well, not easy. I don't know.
Is it easier? But I think it's just the pervasive, you know, nature of JavaScript. Yeah, I don't know if I'd say not as easier than Python. I think it's, you know, I think they're similar actually in a lot of ways. The syntax is a little bit different. Curly braces versus tabs.
But I think that node, we're getting away from vectors right now, but nodes started because JavaScript was the language of the web. And people didn't want to learn another language to also write back end code.
You know, you were using pearl, right? So a lot of the, there was a lot of time where it was like pearl PHP plus JavaScript, right? There was that whole world out there. So that's where node came from. It came from the web, the web front end. So that's, web front end is enormous.
And they all and a lot of them just adopted note and then node had its own hype cycle like 2010 through 2014 was like maybe nodes heyday where it just was like. Through the roof, everything was no JS. It was going crazy. Now it's all now it's all.
You know machine learning and AI a lot of people got involved in this in this world, but there's still a huge, a huge section of the world that's written on top of node from applications that started and, you know, the early 2010s and have evolved ever since.
Yeah, but back to tools like so you said in the early notes you shared like you also want to address some of the problem solved problems like a model fine tuning or some other like pipeline steps that that may be precede the embedding layer that you have now addressed with my team.
So what are your thoughts there, what do you think is missing? I don't, yeah, I don't know if I'm going to get into actual model tuning. I think that first of all, I'm not, I'm not as good as I'm not as good training models as other people. There are other people that are suited to train models.
But I do think there's a lot of other information that is lacking in model in the ML ops world and vector and vector search. One of them is just like well, how similar are these things right what's the distribution of similarities.
I think we V8 said they do support some of that and vest bus what some of that and logging. But I don't know about Pinecone. I'm pretty sure Russ, I'm sure pretty sure quadrant does not so it's like what do I mean by this it's like if I if I have a vector and I get a lot of data.
And I get and I do a query against a quadrant, for example, I get back a list of documents that are nearest neighbors and the similarities.
Well, where does that fit like if I get it back and the first document is like point four or nine similar right is that good is that bad like what are the what's what's real what's real good similar maybe maybe the best similarities are like point eight range.
So now I know that well in terms of my entire corpus and how people usually query this result is actually not that great. There's a lot of questions to be answered around that stuff. So I think that's lacking in a lot of ways. I don't know if that's the right fit for mighty though.
I think there's just external tools that you know I'm kicking around all that stuff would be open source. So I'm very interested in in mighty being the area of the business and then all the other stuff is open source to make things easier for people to use these things.
But yeah, there's a lot there's a lot of stuff in terms of. So in terms of the options, there's like model drift it's like well I used you know if let's say I have like 100 100 sentences right. And I vectorize these against you know model 1.2.
3 right and I got back I got back a list of vectors now I've upgraded my model I model 1.3.8 right. Now I now I run my test vectors in my test sentences through and I get different vectors. Like how how much has changed what's the difference there.
So there's this whole world around measuring model drift and there's some there's some interesting open source tools on this already. But they're written in Python. Now you'd have to use by thons and do all this stuff. So I'm trying to understand what what the tools.
And what tools could be written that are not in Python land. That could expose these statistics and this important information to people who. You know, you don't want to marry themselves to Python. Yeah, yeah, absolutely.
This sounds like you touched also on this very important topic which I think is known as a metric learning where. And one hand you do want to know what is the optimal distance and maybe you need to find junior model or maybe your data is not good fit for this model and so on.
But you do need the tools maybe something like keep it for you know ranking evaluation and tuning you would also have some tool which is keep it like maybe even with the UI where you can load this vectors visualize them and see OK.
So if you do the field together what's missing and so on and then have the stats on it right so you can actually run the statistics and you know. I want to let Eric write that tool. I love Quepid. Quepid is so great. Eric go right, keep it for vector search.
Yeah, I think we can pay it up on that maybe all of us contribute make it open source. Yeah, but yeah, I think this is one way to look at it right and I think quadrant.
Developers, they push the metric learning quite heavily forward by the time this podcast is this episode is out there will be another episode with a developer from quadrant who is actually very big on this idea of metric learning.
 And he open sources of course everything and I mean he offers tools and also like papers that you can read and educate yourself on this space I think this is something that barely is scratched at the moment by the community by by even the end users you know they don't know OK I take clip model I have the images plug them in together works fine I'm done what if it doesn't work.
What if you have some images you never find them for any query but you do want to find them because it's an image of some product that was recently released and you do want to showcase it right and you're not using keyword search there it's an image.
 And using vectors to retrieve it right so it thinks like this I mean it's kind of like there's a bunch of topics there one another one favorite that I like is the robustness right so if I have an aircraft I rotated a little bit and all of a sudden I find kittens instead of the aircrafts and this is what corner short and showed yesterday on on on the gena meetup and was amazing I mean robustness you just change slightly your input and you just yeah doesn't work.
So I think there is a lot of things missing but like you like from what I sense in your answer like it feels like you do still want to keep your focus on mighty and push that as further along as possible right.
Yes, and I want to what I really want is I love that people download and install it and use it and do whatever they want to get vectors with my that's awesome i'm really trying to find partners i'm really trying to find partners who. Who want to just really make it super easy.
To do inference model inference at scale.
So for example I haven't gotten any replies i've been like spamming not spamming i've been emailing and trying to get in touch with like cloud cloud providers right to say serverless inference if you could offer serverless inference right through lambdas or whatever.
That's like so many people are asking for that you know you can't do that with Python tools these days you can do it it's just going to it would take forever and it would be really expensive really slow. But there's such an opportunity for cloud providers to make it super easy so you can have.
You know you want to get content from from point A into into your recommendation engine or your vector database or whatever you know.
Do you want to stand up like the big GPU server in the middle to get this no you don't want to do that if you can avoid it so how about something that's that serverless and people can just run so i'm trying to find partners there i'm trying to find partners who who have.
Search platforms and and other and other platforms or just see this is a Lego and their stack and things is going to make it easier and they don't want to you know hire a team and spend months building this thing and trying to figure it out.
You can do that of course go go do that but you know you can save yourself a lot of time and pay and buy it. By working with stuff that's already there. Yeah that makes sense i mean probably companies like the likes of algolia or. Right exactly but potentially elastic you know because they.
Both of these want to get closer to the neural search even though maybe they were not wired up originally to be vector search databases but they do have the components like elastic based on leasing and algolia.
Probably based also leasing i'm not sure but i'm sure that they're looking at this field so i mean for them and now we are getting a little bit into amelops and vision that you also shared a little bit ahead of time that. My tea could be one of the components in the amelops ecosystem right.
Yeah absolutely not just a stand long kind of script which I download and then i'm thinking okay where do I plug it in. Right I mean if it was if it was are you thinking in that direction as well yourself like okay.
Identifying the tools and systems where might it could kind of like play a long a role of the embedding software. Yeah absolutely. It's.
I have to if the other thing I want to figure out is does it make sense as it is right now as as a web server like that for every case probably not there's probably situations grpc was one request. But yeah it's it's meant to be flexible for you stick in a model that your model.
You know and you run it how you want the other thing that I found was that i'm I met a lot of people who are.
Like scratching their heads saying like which model should I use also right is my my first model or whatever and I just want to start playing around with this so that's the other thing I did is I is I have like default models that I that I chose that I know work well because.
You know especially like news rumors he's amazing and he's done amazing.
Community development around around expert and the models that he's trained and the documentation is published around why certain models are good and others are bad so other people they don't know of of this stuff so it's just like well you don't have to go off and learn and understand.
 Right away why why I should choose one model versus another it's a hard decision to make so there's some there's some defaults that I chose so it's really easy to get started so there's a lot of people that I don't know of this stuff so it's just like well you don't have to go off and learn and understand.
So it's just one model versus another it's a hard decision to make so there's some there's some defaults that I chose so it's really easy to get started so the so the vectors themselves right off the bat or if you do question answering it'll be it'll be pretty good like for.
For regular regular English not to make specific you you still have to do fine tuning for most cases but you're not going to start fine tuning before you even know how this thing performs like.
 So in the beginning right you want to try a model and see what how close it is so there's some starting starting work there I know Algolia is getting to the vector search stuff so I don't know maybe they maybe they don't know how to choose a model so you guys you can use my default model if you want it's just.
 Yeah absolutely I mean so far what I hear from you is that mighty has the qualities let's say it can run on pure CPU which is a win on cost it scales which is also a win on cost in the long term right and it also is insanely fast which is a win on product it's a win on go to market like I have this document how quickly it travels through the.
 Travels through the pipeline and is searchable right so I mean it's important use case in some cases like paramount you know like financial space you know a document came out I wanted to be indexed right away like a second after I don't want to wait five minutes I it will be way too late for me to make a decision so.
I mean is there something else like you and maybe if you if you could compare now or point us to the blog post you know with other vendors like Amazon has in French you know hugging face has infinity infinity right.
 And then and video I think they also had some layer I forgot its name but like those are probably fairly expensive they probably I'm not $90 per piece so what what what is your thinking there so like you I think you also are vocal in this space or like in that direction that mighty is much more economical.
Then these more expensive solutions but they probably offer something else as well but like you have an issue for sure yeah.
I think that so the interesting thing with if you if you get involved with like if you if you get into Amazon like in French yet and all this stuff they crafted like their entire like they build their own hardware they have their neural core. That all the stuff is based around and that's like.
It's locking it's big time locking right. This is just a web API you can just use it I think that.
 I've considered also like hosting an API like hugging face hugging faces like one of the most amazing software companies ever it's like that's like the real community driven open source stuff that they do such amazing work so I don't want to I don't want to say anything bad about hugging this because I really have nothing bad to say at all.
But you know infinity definitely has a fit for the market which is like you know if you are like Walmart and you need a solution okay. Hugging face infinity is in your budget go pay for it you know that's the type of thing that Walmart should use.
But if you are just like if you're a five person developer team or like even if you work at a company does like you know 300 people. And it is like really really expensive.
So there is a there is a market segmentation there there's a difference between okay well how much can you afford and who can you hire and what's the level of internal support that you have to put around this thing and how does it all fit.
 And the teams that are just starting off that need to use something that that works really fast easy to use them that's that's where my fits it I don't think mighty can compete with infinity because honestly I you know hey Walmart if you want me as a customer if you want if you want to buy lighty sure go ahead you know let's talk or you can pay the 99 bucks a month but you know that's not that's not one target i'm trying to make it super easy for everybody else.
I rank recently connected with me a LinkedIn I think some kind of VP of engineering hey if you're looking into embeddings contact max. Really.
So we understand infinity a little bit better because I didn't try this at all is this like some kind of web service that you basically buy subscription for like sauce kind of thing. No it's like a dark container I think infinity is a dark container.
I don't know it might be it might even be written in rust i'm not sure consider tokenizers are written in rust they may have done I may have done something infinity came out before mighty so they may have done something. Perfect competitor for for mighty in that sense.
I mean no time pricing but I mean only the package itself right so basically it's like darker darker anyway. Yeah and I think I think mighty well I think infinity encourages GPU like you they want you to use GPU for it but that's like.
I think infinity fits well if you have like you know a million requests an hour something like that scale you know. If you have like 20,000 requests a day or a thousand requests a day you know that that range 100,000 you know.
I think by these perfect for that you know it's not you don't have to have like this huge scale it can get bigger you can just you know spend more money on hardware and scale it up as much as you want you can support.
You can support you know a million requests a day if you want to or 10 million you just have to put more hardware behind it. So I think i'm just competing in a different market I don't think I don't think infinity and I are targeting the same the same businesses.
Yeah yeah and I mean you do have the edge on the fact that you want to address the community beyond Python so like I think it's a big it's a big message to send. And in some ways through you you channeled this this feeling that hey this guy is in no j as a Java probably feel like left out.
From this thing but it's probably not true I mean I know also there is this deep learning for J and blah blah blah but like it's like an island in the ocean probably comparing it with.
 I think it's an offer just didn't get the adoption that Python got I remember going through these internal pains myself right when I was when it was like 2015 2016 and I started and I started getting deep learning training and I took of course air courses and rings courses on machine learning and stuff.
 So I was kind of tough with Octav which is an open source is a mathematical or whatever it's a new Octav but it's like it's its own language right so it's mathematics just just code but then like the next courses where all Python and I was like oh no I have to learn Python I don't know Python I have to knew new new new language to use this stuff okay fine i'll do it right so I went down that and I learned Python and I got pretty good at it.
So a lot of people just don't want to take that step you know they want to they want to ship code in their stack so it's a big ask to say if you want to use these awesome tools you got to use you got to convert you got to convert your language.
yeah yeah exactly and if you're you know if you're not into data science or machine learning then why would you enter Python at all like it has no no like single winning point well maybe simplicity but hey is that it you know.
 And it's like lose deposition of course you can make it more strict with typing and blah blah but like still but like it took me I think it took me actually good three years to learn Python properly because it's like not like okay I understand how to do the for loop I understand the indentation and blah blah but like to actually master it right like you know avoid stupid loading of the model multiple times in the unicorn.
So so I think the all like sys and I didn't enter thysand the sys and world likely. But but even just writing normal soft when Python takes a lot of time productizing it takes a lot of time so.
So why would you enter it if you are not after the tasty machine learning and data science so why would you consider even converting your software stack into this.
 So it should be the other way around and I think you're doing a great job there with mighty basically offered as a service offered as maybe in the future as some kind of library or some kind of environment I mean Microsoft has been doing a bunch of these things I don't know if you remember the CLR common language runtime.
 So like you you you bring up the the visual studio and you can say okay my project will be in Pearl compiled and run for Java I don't remember it was crazy I was just experimenting with it and I was like I barely knew any of these languages as a student but I was fascinated by the idea it didn't fly I think but it was it was amazing.
 Yeah absolutely and you know I would I did I did play around with the idea of well what if you don't even have to download mighty what if I was playing around with this idea from the mpm perspective like what if you just installed it from mpm command and I thought that's a little bit heavy weight do I want to bring in this thing from you know I could I don't know if that's if I should do that.
 I also don't want to set false expectations to and maybe this is just because I'm not great at marketing but I don't want to set the expectation of you just do mpm install mighty server and then you have a perfectly running thing because it's more than that you have you have to scale it properly you have to give it its own compute.
You have to choose the appropriate model you have to you have to do certain things to really get the most out of it.
So I don't want to set false expectations where somebody deploys it and and it's like it doesn't work well at all because they just did mpm install mighty server which doesn't exist by the way don't try that and then it didn't and then it didn't work.
So I you know there is a little bit of knowledge that you do have to attain so I want to pass you know you do have to familiarize yourself with some concepts that doesn't mean learning an entirely new language and stack.
Yeah it's more like probably like I'm a lobster dev ops somebody can pick it up and I mean learning that way is much faster than actually you know figuring out how the hell will I plug it into my Java code or C++ code or whatever so yeah of course.
I think we like I've really enjoyed this conversation max we went deep into all these aspects maybe not we can record another episode you know going in another direction i'm sure there is like me and me and me and me.
So we can Pauline and directions to take but i enjoy asking this question of philosophical why if you can still spare you few minutes, like, why why why why why why so why how are you fascinating by this field of vector search what brought you into it and i remember i will also.
mentioned this that we did form a team with you and you responded positively to my inquiry to compete in in billion scale and in competition. And you basically single handedly almost driven the idea of body PQ.
Of course, we also have Alexander Simarov who was helping you and all of us been brainstorming with you.
But so that was kind of like maybe academic fascination with it, right? But are there other facets that keep you going also giving your background in search, which was prevector search?
Yeah, I'd say just my endless curiosity into things, you know, I think a lot of us have that as, you know, if you're listening to this podcast, the audience, there's probably a lot of you are very curious about just check technology in general and the limitations of technology and what's positive.
And getting to that new magical thing. And trying something for the first time and saying, oh my god, this is incredible. I can't believe this actually worked that I did this. So it's that. I mean, I, you know, I'm in my 40s now.
So I've gone through that cycle a lot of times where I've tried something and it was amazing. I do really feel that there's a lot of practicality to it though, you know, in my wisdom now.
I see that, yeah, just because something looks cool doesn't mean it's the best thing in the world and it should be used everywhere. So I see the practical, the practical use and need for vector search.
Whether or not it turns out to be like the end all be all with search, you know, that debate is open, right? But I don't think it is. I think it's just one piece in the puzzle. But it does solve this whole class of problems that were unsolvable before if you go back 10 years.
When I first started in search, the types of things that I'm doing right now. And I'll give you an example and I actually, you know, I set this to somebody the other day.
It's like, you know, the first time I installed solar, this is even, you know, maybe elastic search was around at that time, but maybe it was compass search. It wasn't even elastic yet. The first time I installed solar and I put in some documents, I was like, wow, this is amazing.
Like I can do a search. This is so much better than that crappy index that I was using on SQL Server. So it was just really, it was like that type of amazement.
But then you, you know, you work with it over time, you see the limitations and it's like, oh, this got it out of these synonyms and all these other problems and all this stuff.
I'll say that, you know, when you, when you first start off and like the relevance of solar, like out of the box, you take their example, schema XML, and you, and you add some documents to it and you get back stuff and you're like, okay, this is cool.
If you take that feeling and then you, and I'll just use quadrant for an example because quadrant is in my opinion, like super easy to use, like you just docker pull quadrant and use throw some stuff in there. Especially now with this node thing.
So when I did that, the first time I used quadrant and I wrote this node wrapper and I just chucked in a whole bunch of documents, I saw that like just the out of the box relevance. And I'm not saying this is fine tuned, like this isn't something production worthy.
But just the out of the box relevance, I was like, this is better and I would spend, in my opinion, less time worrying about this than I would with an inverted index, you know, just because, well, yeah, maybe the results aren't like super precise all the time, and things like that.
But if I'm on a team and it's like, I got this search bar and I got this content and I don't want to worry about it, right? I don't want to worry about it. I just want it to work. I want it to surface stuff that's like reasonably accurate. It doesn't have to be the best search in the world.
But it's a cost for me. It's a cost for me as its team. I don't make money from search, but it's something I have to support.
I think vector search offers are really, really good solution there because it's not like you have to chase that endless bug of this doesn't even have anything to do with my search.
You just I searched for, you know, what is the best hiking boot or something like that, you know? And all of the documents they matched what 10 times, but there's no semblance of hiking boot or anything in my document. This is terrible. You know, you don't get anything like that in vector search.
And that's I think the appeal. You know, when you get into like real, real production, like highly tuned search, it's just one piece. But just for the teams that's like out of the box, I want it to work and I don't want to deal with it.
I think it's a better, I think it's a better solution than elasticer solar. You end up spending a lot less time and headache. Yeah, that's amazing. That's that's so deep. And in what you said, speaks speaks and sings a practitioner, but I think also speaks and sings a dreamer.
I think you dream of of better ways of searching things, right?
And like you you went through it practically, but also, you know, when you when you get so deep into practical elements, you get stuck into it and you're like you're thinking in the in the framework of the given system or the given language even, right?
Sometimes the paradigms that you read through the docs and you keep thinking about them, it's hard to unstick yourself from them.
And I mean, the fact that vector search field was born is magical in many ways. So I feel like you you feel the same.
And I mean, the fact that you also ventured with me and others into building a new algorithm for vector search also says that that you wanted to go as deep as implementing an algorithm, right? So which what could be sexier than implementing an algorithm? I mean, I don't know.
Of course, all other things are also sexy, but I'm just saying that it's very complex. It's very like demanding intellectually demanding work. So that that's amazing. Thanks so much for this depth.
And is there something you would like to share or announce with, you know, on mighty or maybe on something you're going to be presenting on Berlin buzzwords, I know as well, but is there something that you would like to share? Yeah, so I am presenting a Berlin buzzwords.
I am putting together a charity event to hack on vector search. And that's going to be on May 5th. I don't know when this podcast will be published, but on May 5th, I want to get and I want it to be it's just going to be an all day learning session. And I'm not charging money for this.
This is like free. I just want to show people how to use these tools if you're not in the Python world. If you're part of the Python world, I want to join amazing. Great.
But I want to do an all day like hackathon, where I'll show you how to get these things up and running hack away at it by the end you'll have a working example on your own.
And all the money, all the time we're going to raise money for charity, specifically around refugees and displaced people, because of the horrible things that are happening in the Ukraine and other parts of the world as well.
Getting some learning happening and also raising money for charity seems like a great way to spend time. So I plan to host that on May 5th.
It's probably going to be on Twitch, because I want to just to be an open drop in drop out format, you can come, you can go, it's not going to be like a controlled zoom where you, you know, it's like that. It's going to be on Twitch with chat and stuff like that. So I'm going to get it all set up.
Details are going to come out shortly. By the time this is published, maybe the details will be available already. We'll drop a link. Yeah, awesome.
This sounds amazing that you also keep thinking about this sensitive topics or like what's happening in the world and you are also contributing with your skills into a good course here. Thanks so much. I will try to publish this podcast before May 5th.
So make sure that somebody can join and get chappin, of course. We can do the most media, social media push. This is amazing. Thanks so much, Max. I've enjoyed this conversation thoroughly. We went into depth and with and everything all dimensions. It's a multi-dimensional conversation.
So thanks so much and keep it up. And I'm curious to hear news about Mighty and the tooling around it and also looking forward to your building buzzwords presentation. Yeah, thank you so much, Dima. It's great to chat. Yeah, thank you, Max. Cheers. Cheers. Take care. Bye-bye.