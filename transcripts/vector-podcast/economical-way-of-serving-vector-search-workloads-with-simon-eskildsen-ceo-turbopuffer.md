---
description: '<p>Turbopuffer search engine supports such products as Cursor, Notion,
  Linear, Superhuman and Readwise.</p><p></p><p>This episode on YouTube: <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://youtu.be/I8Ztqajighg">https://youtu.be/I8Ztqajighg</a></p><p>Medium:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://dmitry-kan.medium.com/vector-podcast-simon-eskildsen-turbopuffer-69e456da8df3">https://dmitry-kan.medium.com/vector-podcast-simon-eskildsen-turbopuffer-69e456da8df3</a></p><p>Dev:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://dev.to/vectorpodcast/vector-podcast-simon-eskildsen-turbopuffer-cfa">https://dev.to/vectorpodcast/vector-podcast-simon-eskildsen-turbopuffer-cfa</a></p><p></p><p>If
  you are on Lucene / OpenSearch stack, you can go managed by signing up here: <a
  target="_blank" rel="noopener noreferrer nofollow" href="https://console.aiven.io/signup?utm_source=youtube&amp;utm_medium=&amp;&amp;utm_content=vectorpodcast">https://console.aiven.io/signup?utm_source=youtube&amp;utm_medium=&amp;&amp;utm_content=vectorpodcast</a></p><p></p><p>Time
  codes:</p><p>00:00 Intro</p><p>00:15 Napkin Problem 4: Throughput of Redis</p><p>01:35
  Episode intro</p><p>02:45 Simon''s background, including implementation of Turbopuffer</p><p>09:23
  How Cursor became an early client</p><p>11:25 How to test pre-launch</p><p>14:38
  Why a new vector DB deserves to exist?</p><p>20:39 Latency aspect</p><p>26:27 Implementation
  language for Turbopuffer</p><p>28:11 Impact of LLM coding tools on programmer craft</p><p>30:02
  Engineer 2 CEO transition</p><p>35:10 Architecture of Turbopuffer</p><p>43:25 Disk
  vs S3 latency, NVMe disks, DRAM</p><p>48:27 Multitenancy</p><p>50:29 Recall@N benchmarking</p><p>59:38
  filtered ANN and Big-ANN Benchmarks</p><p>1:00:54 What users care about more (than
  Recall@N benchmarking)</p><p>1:01:28 Spicy question about benchmarking in competition</p><p>1:06:01
  Interesting challenges ahead to tackle</p><p>1:10:13 Simon''s announcement</p><p></p><p>Show
  notes:</p><p>- Turbopuffer in Cursor: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=oFfVt3S51T4&amp;t=5223s">https://www.youtube.com/watch?v=oFfVt3S51T4&amp;t=5223s</a></p><p>transcript:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://lexfridman.com/cursor-team-transcript">https://lexfridman.com/cursor-team-transcript</a></p><p>-
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://turbopuffer.com/">https://turbopuffer.com/</a></p><p>-
  Napkin Math: <a target="_blank" rel="noopener noreferrer nofollow" href="https://sirupsen.com/napkin">https://sirupsen.com/napkin</a></p><p>-
  Follow Simon on X: <a target="_blank" rel="noopener noreferrer nofollow" href="https://x.com/Sirupsen">https://x.com/Sirupsen</a></p><p>-
  Not All Vector Databases Are Made Equal: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696/">https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696/</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20250919_060954_7832a7c20742f9493a19a27a0c5d8947.png
pub_date: Fri, 19 Sep 2025 06:09:39 GMT
title: Economical way of serving vector search workloads with Simon Eskildsen, CEO
  Turbopuffer
url: https://rss.com/podcasts/vector-podcast/2222846
---

Now, let's get started. MAPKIN Problem 4 Today, as you are preparing your organic, high mountain type needs along in the kitchen net.
One of your lovely co-workers mentioned that they were looking at adding more radices because it was maxing out at 10,000 commands per second, which they were trending aggressively towards. You asked them how they were using it.
Were they writing some obscure order and command? They would BPF probes to determine that it was all get key and set key value. They also confirmed all the values were about less than 64 bytes. For those unfamiliar with radices, it's single threaded in memory key value store written in C.
And faced, after this encounter, you walk to the window. You look out and see if your high mountain type needs along. As you stare at yet another condominium building being built, it hits you. 10,000 commands per second. 10,000.
Isn't that a bit low? Shouldn't something that's fundamentally just doing random memory reads and writes over an established TCP session be able to do more? Hello there. Vector podcast is back. Season 4.
And we are kicking off with an exciting topic and guest assignment, Eskulls and CEO of TurboPuffer. I've been watching you guys from almost from the start, just following each other on Twitter, like virtual friends.
And it's funny that before this episode, you're the CEO of the company and this for this episode, you try to sell TurboPuffer to me and say, hey, why don't you use it? Did you make a compound tom should pass? Yeah, should pass, for sure. But tell me, hey, welcome, first of all, welcome.
And thank you very much for having with me. It's a tradition to usually start with the background. If you could speak in your own words about yourself, your journey.
I know that you've worked at Shopify at some point, also scaling databases, I guess, right? But I've also been following your napkin math newsletter. I was reading maybe I'll quote some text today from there, just to amuse an exciting audience. But tell me about yourself.
Yeah, I can give a very brief overview and if we can dig into anything, if there's anything that stands out. I started programming when I was a teenager. Similar to you, English is not my first language.
So at some point, I exhausted the Danish web and then like divulged into video game addiction for three years as a teenager to learn enough English to sort of, you know, get my own chat CBT moment and take off point.
And then I spent a lot of time in high school being not very good at competitive programming, but good enough to qualify for the small country of Denmark. And then I spent almost a decade working at Shopify doing mainly infrastructure work.
So when I joined infrastructure Shopify and the infrastructure team, we were doing, I mean, it was not even an infrastructure team like DevOps was just becoming a thing. And we were driving just a couple hundred requests per second. And by the time I left, we saw peaks of more than a million.
And I more or less worked on all of the stateful systems that power that because they generally tend to be the bottleneck just playing whack-a-mole every single year for every Black Friday for many years. And I spent the majority of those years on one of the last resort pages for Shopify as well.
One of those pages were the pages are very scary in the middle of the night and where a lot of GME of course runs through Shopify. So very high responsibility on that. I left in 2021 and kind of jumped around at my friends' companies helping them with various things.
And I spent almost my entire career at one company. So I wanted to dabble and just go and basically help my friends with any infrastructure challenges that they had. And in 2023 when Chatschy BT launched and the APIs launched, I was working with my friends at this company called Readwise.
They have a product similar to a pocket and others for reading articles later from an Amal product. And they asked me to build a recommendation feature for articles. And I was like, well, it's perfect, right? LLMs or embedding models are basically just LLMs with their heads chopped off.
And they're trained on exactly this data. So we built something and it actually worked pretty well for just recommending articles. But then I ran the back of the envelope math on what it would cost to do this for the entire article catalog, right? It had hundreds of millions of articles.
And it would have cost more than 30 grand a month to do. And for a large company that's not a big deal for an experiment. But this was a company that was spending three grand a month on a Postgres instance that prior to working on this, I tuned.
And spending 10 times that on just recommendations and possibly search was just untenable. So it sort of lost it. It was lost in its track. And it was a bit sad.
And it's sort of ended up in that bucket that a lot of companies have of like, okay, we're going to work on this when it becomes cheaper and then we'll ship this feature. But it was a bit sad because I was excited about this feature. It's a user of the product as well.
And I could not stop thinking about that. Why was it so expensive? And the vector databases at the time were storing everything in memory. And DRAM on a cloud cost somewhere between two to five dollars per gigabyte. And this just economics of this didn't line up.
It wasn't that this vector database was doing anything, you know, malicious in their pricing. They're just trying to earn them on its margin on memory pricing. But memory pricing was just too high and it stopped its feature and it's tracks.
And what I couldn't stop thinking about is, why can't we do all of this on top of Obick storage, right? Like we just put it on an Obick storage. That's the source of truth. And then we actually need to some piece of data and we put it in memory or even on this if we can.
And I did it to Mac and Nathan. And I was like, I think that's about a hundred times cheaper. And of course, that would have been no brainer for read wise.
We would have just bought it and started using it and tried it out, right? And maybe put way more data in and maybe worked our way up to that 30 grand a month bill. But with a different workload. And so yeah, I couldn't stop thinking about it.
And then eventually started writing the first version over the summer of 2023. Just me alone in the wills of Canada and then launched it in October of 2023, which is probably where you saw it. I didn't really tell anyone about it. I was just I was just hacking away.
Launched it did a lot of our deal over that summer insights that some of them still are in the product and a lot of them we've since faced out. But the most important thing was that it launched.
And the first version of trouble puffer didn't have I was just looking at the website the other day for an unrelated reason. It didn't have mutable indexes. So you just wrote to it and then you called an index endpoint and then you're logged in like that's it. And it didn't have any SDKs.
It was just a big pure HTML website. But it was enough to ship it and it caught the attention at the time of the cursor team back in 2023. And of course, this was this was this was early on for cursor. It was early on for us.
And they are they're a vector database built did not line up with their per user economics and how they wanted to use rag in their in cursor. And so they they wanted to try to work together.
And we exchanged a bunch of emails of bullet points and it was very clear that they thought that this architecture was exactly now knowing the team are now they were just sat down at the dining table, done to napkin math over there and then thought why hasn't anyone built it like this.
 And so we worked we worked I went to San Francisco and spent some time with them and came up with a bunch of features that they would need and called the best engineer that I knew would Shopify my co founder Justin and asked if they'd come on board because I think I think maybe there's something here.
 And yeah, we launched it cursor cursor moved over and their bill was reduced by 95% and of course the additional storage architect today were on before didn't make sense for the cursor economics but our storage architecture really did because you put all the all the code based embeddings on s3 and then the ones that are actively being used we can use in grammar or have in disk.
I'll stop there but that would be what led up to to this moment. Oh, that's amazing journey.
 A lot to ask of course a lot of questions but just on that cursor thing as I told you before we started recording you know I knew about you launching this working on this and then I've released it to the Lex Friedman podcast episode with the cursor team and they didn't mention turbo pop for sort of like in passing but you know I think that also probably created a lot of attention to you guys but I'm just curious like how did you get together how did you know cursor team somehow someone on the cursor team that you could like partner early on and essentially help they kind of like helped you to pioneer it right in some sense becoming the first client or maybe future client right how did you approach them.
They did I mean they were a design partner in every sense of the word right we had a slack channel and I feel like they treated us as part of their team and we treated them as part of our team.
 They came inbound they sent an email based on the website and they said hey we would need mutable indexes and glob and a couple of other things so it's like well that's a very reasonable request right and I think they had the conviction that this was the right architecture and like if we could prove in their trust and then be able to be in a good place so it was really just a it was just an oneness conversation just the way that the website is today a very honest description of what are the trade-offs what can it do what can it not do what is the latency profile what are the guarantees and that's exactly the kind of bullet point discussion that we engaged in over email before I met the team in person yeah and they of course they were a small team at the time right it was and they needed help with the with with parts of their infrastructure and working very very closely with teams that they could trust with the right economics and the right the right reliability.
 Yeah for sure but I guess that honesty which I also value a lot you know and in my work as I became a product manager you know three years ago and I think it applies to any discipline be honest but but you know like that honesty probably lies on the fact that you you've done your napkin math and you knew where this will scale where how this can go right how did you go about doing that pre-launch right before having any client is that the company of your friends that helped you to kind of like figure out the economics and sort of the the throughput and all of these rigorous questions that you ask you know as problem statements on napkin math.
 I think that should almost bring up the internet archive version of it the the first version of TurboPuffer I had not thought about the business at all I didn't have any launch playbook I had I had one of course all the economics of what it would cost me to operate and spend a decent amount of time on the pricing because that felt like an important thing to spend time on at the time but there was really not much more than that of course the the Readwise team was very interested but at the time I could barely do a you know I could just do around 10 million factors which is not enough for their use case.
 I can screen share the website with you right here of what it looked like at the time and then we can get for the for the for the listening audience we can get your reaction but it was it was very simple I wouldn't I wouldn't put in any sophistication and it was honestly I was exhausted I've been working on this like completely alone not telling anyone about it no interested customers for like four months extremely focused like every single day and I couldn't like you ask my wife she would say I was very distracted and she's just like well how are you working so hard on this like there's no one on the team you don't have any customer line up and I'm just like someone has to do this and I I just launched it and I launched it I mean now I feel some verising would be did launch it just couldn't do that launch it was pretty slow I spent a bunch of time actually trying to make it work in wasm and on the edge but it was too hard to make it fast and a bunch of other false starch like that on different types of a and end indexing structures we could talk about that as well and would be settled on but there was no real sophistication in the go to market it was really just here it is here's the outcome math here's what it does let's see how the world takes it but I think when when when you sit on a well you didn't sit on it yet but you had a cool like technology ID and mind right you knew you know it may play out but also of course it required a lot of hard work like you said but after that after you see it fly like on some small scale or whatever scale I think that brings you like that excitement to bring it to the world right so yeah I see you're sharing the screen of the of the web archive page yeah that's it very simple yeah yeah that's awesome but yeah that's actually a good segue to you know you probably know I've been at the emergence of the field of vector database field I've been I think I was the first probably to write just a simple block post with like you know these crump snippets of what each vector database did and how they stand out and so on turbo buffer wasn't there because turbo buffer was still in your mind I think but but the segue here is I don't have it covered in that block post but in your mind why were you not happy with the vector date is like at large did you try all of them did you try some of them why did you think that a new vector database deserves to exist yeah I think I think it really just came back to the read wise example right there's I there's they look like great products I really like the API of many of them they had lots of features that have taken me a long time to build that even features that we don't have today although we have a lot of features today compared to when we launched it came out of the cost piece that it felt that there was a lot of latent demand built up in the market of people who wanted to use these things but it just didn't make sense with the economics it's very difficult to earn a return on search I mean I remember the search clusters that Shopify were very expensive but ecommerce is a lot about search and so it was okay right but for a lot of companies search is a an important feature but is not the feature right and so the per user economics just have to make sense it's not that everyone just wants it in the cheapest possible way is that if you invest in infrastructure you have to get a return on that investment and it felt that I knew that I'd read wise they could get a return on that investment but it wasn't on 30 grand a month it was maybe close at a 3 grand or 5 grand a month that they would feel that they could earn a return on that feature and gender conversion engagement and whatever so it was really about the storage architecture and I think that when I think about databases now this was not as coherent to me at the time at the time I was driven by the Nipkin Math Naptkin Math not the not the market nothing else it was based on one qualitative experience and an Naptkin Math there was nothing else in it and speak about it in a more sophisticated way now being you know having learned a lot about go-to-market sense but the that those were that's really all that was at the time it was an insight on those two things the best ideas right are simultaneous inventions right someone else would have done it six months later probably other people were doing it at the time that launched a later right we were the first to launch with this particular architecture but it was out there for the grappling right the idea was in the air like s3 had the the dpites now finally so the way that I think about this to really boil this down is that if you want to create a generational database company I think you need two things you need a new workload the new workload here is that we have almost every company on earth sits on their treasure troll of data and they want to connect that to LLAMs especially all the unstructured data that it's always been very difficult to do we did this for structured data into 2010s the new workload was that we wanted to do analytics on billions tens of billions trillions of rows of structured data but now with LLAMs we're entering into that with the unstructured data that's the first thing we needed new workload because that's when people go out shopping for a new database the second thing that you need is a new storage architecture if you don't have a new storage architecture that is fundamentally a better tradeoff for the particular workload then there's no reason why tacking on a secondary index to your relational database to your OLAB to your existing search engine they would eat it I would have made that decision in the shoes that Shopify right it's like well this database like has a really good vector index but it doesn't bring anything new in terms of the storage architecture so we're just going to invest in the mySQL extension right it's what we really want to Shopify or the uh Lucine Lucine workload right these are great databases they've stood the test of time and when you're on call you become very conservative in what you adopt for new workloads but you cannot ignore a new storage architecture that is an order of magnitude cheaper than the previous one when you store a gigabyte of data in a traditional storage engine you have to replicate that to three disks maybe two if you have a little bit or if you have more risk tolerance but likely three a gigabyte of disk with from the cloud vendors cost about 10 cents you run it at 50% utilization otherwise it's too scary to be on call 20 cents per gigabyte times three for all the replicas 60 cents per gigabyte obi storage is two cents per gigabyte right so it's it's it's 30 times cheaper if it's all cold now by the time you have some of it in SSD and you have it in memory then the blended cost ends up being different but it tracks the actual value to the customer even if you have all of that in disk well you only need one copy right and that disk you can run it at 100% utilization meaning the blended cost is now 12 cents per gigabyte right so the 10 cents 100% utilization plus the two cents per gigabyte for obi storage so now you have the ingredients of a new actual database you have a new workload right which is which makes means that people are out there trying to look for ways to connect their data to LLMs and then you have the second ingredient which is a new storage architecture that allows them to do it in order of magnitude easier and cheaper than what they can do when they're existing architectures and this matters because vectors are so big right a kilobyte of text easily turns into tens of kilobytes of vector data yeah yeah it's absolutely true one other thing that I kept keep hearing or kept hearing about you know whether or not to introduce a vector search in the mix for some really heavy workloads is that it will bring certain latency on top that we cannot tolerate right for example if you run a hybrid search like you guys have implemented as well you know one of these will be slowest and therefore you will have to wait for that slowest component and so if it adds I don't know a few hundred milliseconds on top of your original you know retrieval mechanism then it's going to be an off-line what's your take on that have you thought obviously you have thought about that what's the edge that turbo buffer brings in this space over maybe pure databases yeah I think I think there's we see two types of ways that people adopt vector databases or turbo buffer we don't consider turbo buffer a pure play vector database we consider it a search engine we actually consider it a full database because there's a full generic LSM underneath all of that and we consider that the actual acid of turbo puffer is an LSM that's obnox storage native and doesn't rely on any state we just think that the vector index and the search engine index is what the market needed the most so let's speak about latency there's no real fundamental latency trade off with this architecture the only thing is that once in a while you will hit that cold query but the entire databases optimize the round minimizing the amount of round trips that you do to SS3 S3 you can max out a a network card right so you can get on a gcp or AWS box you can get 50 to 100 gigabytes per second of network bandwidth you give it per second of network bandwidth so this is similar to this band with the latency actually even better in the clouds often than disks even with SSDs even with N and NVME SSD so the network is phenomenal you can drive you know say you can drive all of that data you can drive gigabytes of data per second in a single round trip so you can get great throughput but the latency is high the p90 might be around 200 milliseconds to s3 for every round trip someone regardless of how much data that you transfer assuming you're saturating the box we've decide almost everything interval buffer around minimizing the number of round trips to 3 to 4 that doesn't just help for s3 it also helps for modern disk which the same thing you can drive enormous amounts of bandwidth but the round trip time is is long right it's like a hundreds of microseconds versus hundreds of milliseconds but still still substantial compared to dm so the latency tradeoff is not a fundamental tradeoff with this architecture by the time that it makes it into the memory cache it's just as fast as everyone else we have found that people don't care if it's like a millisecond or five milliseconds as long as it's reliably less than around 50 milliseconds they're good right and I think that a lot of the traditional storage architectures especially because of the sharding structure with multiple nodes you're already in a worse position than going to two systems where if you write a query on some of the traditional search engine generally you touch five ten maybe more nodes depending on depending on this because the shard size is very very small you go into more depth on that so you already have this problem what we see is that there's two types of ways that people adopt it so the first one is you have an existing lexical search engine you are having a hard time running it because of the traditional like very stateful architecture and they're reputed for just being difficult to run and you're like already a little bit add your threshold for the amount of money that you're spending on this cluster and if you put the vector data in it's often 10 to 20 times larger than the text data it is just it's a project that stops in its tracks similar to the read wise case that I mentioned before so for those players we often see that they have something that's really well tuned for the lexical and they adopt a vector store and then they do two queries in parallel the vector store should not be slower than the lexical right so these are just two futures that you merge together in use and in general we see that our customers are actually quite happy to move some of the ranking and the final like second stage ranking out of the search engine and into a search.
py instead of a big search.
json which can be very difficult to maintain many of these companies express a lot of desire to move more and more of their lexical work also onto turbo buffer and we have a full tech search engine we don't have every feature of blue scene yet but we're working very very actively on bringing this up what we also see is that a lot of our customers don't need all of the features of blue scene anymore because the vectors are so good that a lot of the you know Ph.
D.
 level efforts we did before to turn strings into things is not as much of an issue anymore and really what we use strings for more is that when you search for DM you get the metri right like like for a prefix match whereas an embedding model might think that that's a direct message those kinds of things are important and we still need string matching for that lots of applications needed but there's a lot of things that we do in leucine with synonyms with stemming with all these kinds of things the team models are frankly just a lot better at so we find that this is an adoption curve that is there a lot of the newer companies just start with embedding models and simple full-text search and and they get it up and running on turbo puffer and they like that they just pay for what they need they don't think about it and they could pump a petabyte of data and if they want it and it would be extremely competitive on pricing and they don't have to think about it oh that's awesome that's awesome actually I forgot to mention I forgot to ask you which language did you choose to implement to revolver on yeah we we um well it was just me at the time but I chose I chose Rust and I think I spent the majority of my career writing Ruby at Shopify and and then a lot of go as well for some of the infrastructure components and then mainly debugging C which all the databases that we were using were we're doing and reading C I I really like go and I like go alongside Ruby at Shopify because go was one of those things as when leading teams I didn't have to worry about whether someone knew go or not because the adoption to learn it is two weeks um the adoption to learn Rust and being proficient in it is months right and somehow that's written Rust for two years it's a lot more productive than someone who's written it for two months in the language um and that's just not the case for go like someone who's spent two years in it is just not that much more productive and so and I think that's an amazing feature of the language but from from my own point of view and from the napkin web math point of you I just I was always so hungry having been in time inside of runtimes in the Ruby MRI runtime and then inside of the go runtime I was just hungry to just get directly connected to the metal of the machine and so and for a database in particular that was very important right we need to vectorize everything we need full control over that and I think that I think that that full control as remarkable now as Go is which would I think it would be would have been okay that raw access to the machine has been has is needed for for writing something like TurboPover.
 Yeah for sure I still remember coding the times when I was learning and coded industrially in CNC++ like you like you really need it to be very very careful but in return you can get a lot of like performance gains you know and some of your ideas really fly but yeah today I guess I'm coding more in Python or should I even say that I called in Python when I use cursor more and more which is by the way scary you know the the that feeling when some some other entity writes code and you are just reading it right it's it's a little bit scary and I'm still grappling with it but the amount of productivity that I get is enormous and it's like you know I can shoot daily like features and just see them being used that's amazing.
 I think what I love about it is that I still love to sit there and write the additional code by hand you know maybe at some point we will we will we will mark TurboPover as an a seasonally written database because we don't use a ton of AI for the very key parts because I mean we're at the edge of what the LLMs could know but I think that for me in a position where I'm in and out of meetings all day these days but I can actually get a lot done in a 30 minute window when I have something that's prompting and writing the tests right and you follow it off at the beginning of the meeting you check in and they're like you know 15 30 minutes you have in between blocks and this allows me to actually contribute a lot more code than I was otherwise going to be able to not into the core engine you know I don't get led led into led into a lot of that anymore because I don't have the the time and focus that it takes to fully think something through there but for the website the API to initial features all of that it's just been wonderful yeah that's amazing I also wanted to go a bit on the tangent like you essentially you've been you could say mathematician engineer but you took a leap towards becoming a CEO right and I think you know as you said you go to meetings you do lots of you know probably sales and and and and product and all of that stuff was it a natural transition for you like what what have you learned in this in this journey and what what maybe do you miss from from your previous career when you when you were like you know hands on and sit down and write a bunch of code I think I have a I have a couple of angles to answer the question not necessarily a directing answer I think one one angle is that fundamentally I'm like a growth junkie for better or worse and I think that entrepreneurship is the ultimate path for a growth junkie it was never really something that I assumed that I was going to do I've never before even when I was working on the project it's never about becoming a founder it's just about creating the database right and at some point becoming the founder of the company becomes a means to an end of creating the database and getting it into the hands of our users and making sure they have a great time that's always what like that's what drove me right was the read why I should have this right our customers should have this this you have a great experience and that's always what's driven me and to me the the founder and all of the other things have been a mean towards an end there I think that one of the things that is maybe both a controversial but also feels like a true statement is that at some point I feel a bit numb to what work that I enjoy and what I don't enjoy anymore because what I enjoy the most is making this company successful and making the database successful for our customers that's what I care the most about and I'm yeah I honestly I love sales I love marketing I love the engineering I love hiring people for the team I love all of these things but it's not a simplistic answer to oh I've been coding my whole life I think it's more that that is my idle activity if there is that one to two hour and there's nothing urgent on then I'm going to go spend some time in the code it's like oh how did how did Nathan implement this new this new query planning of query plan or heuristic that is a natural that's where my idle activity and I always like to also an interviewing people try to understand especially if they're in a more hybrid world what's your idle activity what's the thing did you do when you have one to two hours and nothing else comes up do you gravitate towards the code do you start looking at just start writing an article do you start playing with the product what is that idle activity and it is code for me that's what everything is grounded in and I think it I think it has a deep influence on how I can how I can lead the company but I don't think it's been I think I often think about something that tell them said you know the author of anti fragile and a bunch of other books is that you the best authors of books are not the ones that sit down and like you know read a bunch of papers then write a page then read another paper write a page the best books are written by people who just you know go to the cabin sit down write 500 pages and and hit publish of course that's not what actually happens but if you read it to let books it's probably pretty close to what actually happened and he just has the citations in his head and I think about that often when building this company that it has felt like I've worked or this my whole life without knowing for it and I feel every every morning that I wake up that this is exactly what it has led up to so it's very naturally even if it wasn't go onto itself that it makes sense with the experience I've had to do exactly this and I tremendously enjoy it but it's not a simplistic answer to do I miss coding no I want to make this company incredibly successful but sometimes I will do it as a recreational activity yeah I mean definitely like when I look at you like on twitter for example you come across as a very technical person and you are for sure right even though you know that to grow your business you need to do a lot of other activities but at the same time I mean yeah I don't mean to ask it in a way that hey you regret now that you do sales you regret not doing more coding which which is not true you still do that and I think that all of all of the engineers will become better engineers if they learn the mastery of actually presenting what they do right and then they will not need a middle layer or someone else who will go and talk to that product manager or whoever else needs to talk to right so they can actually represent themselves but also I also love how you put it really eloquently that what is your idle activity right what do you what's your affinity what you gravitate to and I actually can it resonates a lot with me because my idle activity that I'm really nervous that I do nothing especially on vacations I start coding you know I just go and just okay let's just let's just hypothesize about something but let's let's dial back for for the into the architecture like when I look at the architecture page of turbo buffer it's very simple it's like client connecting over you know TCP to a database instance and it has just two components they are memory or SSD cache and the object storage tell me a bit more so I think our listeners and I mostly know what object storage is but tell me a bit more about that memory component like what algorithm design went into that maybe trade-offs and you know how frequently you need to do the round trips to the object storage versus when we actually don't do that yeah I think it would be easiest to do this by speaking about the lifetime of a request as the cache worms so we we'll actually start with the right path and when when you do a right into turbo buffer it's as simple as you can imagine it I mean at this point we've optimized parts of it that it's not this simple but this is the the best way to explain it when you when you do a right to turbo buffer that right basically goes into a file in a directory called the right ahead log so when you write to a namespace you can imagine that on s3 it's like slash namespace slash you know right ahead log the right ahead log is basically just a sequence of all the rights in order the raw rights so you do your right and it might be okay I'm inserting a document with text the metri and one with text Simon and those are the two documents you can in the simplest way you can imagine that this file is called zero job JSON and the next one is called one dot JSON three dot JSON that's a database right that's just the right ahead log and if you want to satisfy a query you just scan through all the JSON documents and you satisfy the query that's actually respectable database and it's not even that far from the first version of turbo buffer but of course you have to index that data as well so basically as you can imagine once many megabytes of data come in asynchronously an indexing node will pick it up and put it into the inverted index for full text search put it into an a and an index for vector search and put it into a attribute or filtering index for other attributes and there will be other indexing types in the future when when that happens it will put it into slash namespace slash index and just start putting files in there right and then the query layer can then consult those files right instead of scanning through every single document to find a metri you can just plop in and look at the metri in the inverted index find the document and return it that's how right works when a right happens it will go through one of the query nodes and the right will be also written into the into the cache right so both the memory cache and the disk cache and when so when you do a query you will go to that same query node right there's a consistent hashing so if there's three it's sort of like the same namespace will end up on node one all the time if it hashes that I know when you've satisfied when you when you do a query it will first take the cat check the caches if you just did the right well it's already there because we just wrote all the rights into the cache to have this you know the right through cache and and we will satisfy the query mainly from the cache if for whatever reason this namespace is not maybe you did the right a month ago and so it's following that a cache and you do the read well then we'll read through cache by going directly to opix storage with its few round trips as possible to get the data to satisfy the query both from the index and from the wall will do range reads directly on s3 right the old like hcp range header to get exactly the bytes we need to satisfy the query and then start hydrating the cache on the on the query node so the subsequent queries get faster and faster and we can do that a gigabyte per second we can hydrate the cache even for very very very large namespaces so that's the general architecture of turbo puffer on a completely cold query it takes hundreds of milliseconds and on a warm query it can take as little as 10 milliseconds to to satisfy the query the the last detail I'll point out and then we can go into a particular aspect of this is that turbo puffer has chosen to do consistent reads by default this is an unusual choice for search engines we've seen doesn't do this unless you turn it on explicitly I think they've done more work now for real time indexing which to me is the gold standard which is why I keep referring back to it's phenomenal piece of software and turbo power requests consistent reads by default meaning that if you do it right and then you read immediately afterwards that right will be visible and in order to satisfy that we can't just rely on the cache on that node being out of date that node could have died it could have you know the hashed ring could have moved because we scaled up so every single um query we go to op storage and see what is the latest entry in the wall and do we have that entry right is it at 3.
json or is that 5.
json and do I have that so we have a little pointer file that we can look and we can download and look at right and that round trip is basically our p50 like our spans are basically you know often like one to two milliseconds of actual search and then on gcs about depending on the region 12 to 16 milliseconds waiting for that consistency check on s3 the small obnoxiously it's a little bit better so it's eight milliseconds but you can turn this off and you will still get up to you you can get eventual consistency that's very normal for these databases like could be up to one minute out of date and then you can see often less than a millisecond or a millisecond latency to a turbo buffer by turning off that check but we find that this is a very safe default and I think that database should ship with very safe and unsurprising defaults yeah for sure for sure um so in that cache but you also have the you also have the let's focus only back to search part for now you also have the a and n index is that also stored on s3 and then is it do you also keep kind of like a replica of it in memory to to quick access and how do you sort of it's true how do you sort of synchronize the two both the right-ahead log and the index are everything is stored on s3 if you killed all of the compute nodes of turbo buffer in all of our clusters we would not lose any data there is no data on the compute notes that matter it's only transient caching but we cache everything yeah if you're accessing the index will cache the index if you're just accessing the right-ahead log files because it's so small or there's parts of the data that hasn't been indexed then and that's also on s3 and goes into the same cache with everything else right prioritized by the workloads to try to get the best performance possible yeah it's quite smart so effectively you like I remember like at some previous companies when I was running Apache Solar one of the problems was always that all of these charts are super cold because they're never used right we still pay for them but then when the query hits you incur so much latency that's super painful and so I was always coming up with these ideas what if I run some you know post indexing warm-up script that will go and shoot a bunch of queries to all of the charts just to keep them you know up and running and and warm or just cat all the indices on Linux into memory we've done that too that was like 10 years ago or so that was very strange feeling like why do I need to mess with that level of detail it never actually paid off I think what pays off is a most smart way to organize your index and how you read data backwards like essentially when you users really only need fresh data first like on Twitter for example everyone is really after the recent tweets and not some archive and that was very similar case for us but it's very interesting like you go into so much detail there to to make the database effectively like a living organism adjusting to the usage but you also you also have multi-tenancy right so meaning that the same the same turbo buffer deployed across the data centers is going to be used by multiple companies at the same time unless they demand an isolation how do you think about that when they use the same effectively in the same instance compute and index I'd love to go into the solar example for just one second before we go into multi-tenancy how slow were those queries because when you say it cold you mean that it's not in memory when I say cold I mean that it's on S3 what kind of latency were you seeing that you had to do this work on it was very slow first of all the it has to do also with the domain specificity you know the the queries were Boolean and very long and so they they would take sometimes just a query itself would take a minute to execute on now like a regional index design and that was like just super crazy right but it was also very accurate because it was like sentence level search and then I had to design a new system new architecture where we could retain the accuracy of that engine but not have to spend so much money on on on indexing individual sentences so we indexed one complete document right so I had to change the algorithms slightly and so it went to sub-second it was still I think it's still slow right but it was much faster and and users started like like we could scale the company effectively after that right with one minute and 75% of infrastructure costs were like you know shaving off so but that's that's that was part of the Lucine you know munging with the algorithm and changing how it scans the document it had nothing to do with the level that you go into you know with turbo buffer you know like effectively controlling the whole the whole process there got it yeah I think the the point I the the point there is that I think we do see that some customers are concerned with with this cash because they've gone bit and by basically the the way that I would think about it is in in some of the traditional engines the way that they do IO if something is on disk it feels like it's bad like if it's on disk it's slow and it really has to be in memory and so you sort of have you know the pufferfish is either you know the pufferfish is sort of because when it's fully inflated it's a DRAM right it's a deflated it's in s3 well it only had two settings right either it's in disk which is quite slow and frankly in some of the traditional storage engine I've seen the latency on disk being similar to our latency on s3 yeah and so then you have to load it into DRAM and what a lot of these traditional databases they have to do a full copy into DRAM they can't just like zero copy off of disk and in the disk are also quite slow these old network disks right the NVME disks are so fast right they are they can drive bandwidth that's within you know a very low multiple of DRAM right tens of gigabytes per second but their cost is almost and two orders of magnitude lower so this completely changes the economics but you you can't take advantage of these very easily you can't just put as some software on it and just it's going to be like 10 times faster than an original disk even if it's fundamentally capable of it because what we found for example is that we had to remove the Linux page cache because the Linux page cache cannot keep up with these disks so you have to do direct dial but when you do direct dial you don't get coalescing you don't get all these other things now you have to write your own IO driver right and so you just databases have not been built to take advantage of it because they're also like they're not built to try to do an IO depth like basically so many outside standing IO requests they can they can drive there's a lot of throughput so there's just a lot of barriers of entry there so what we find is that when again speaking in generic terms here of like you know millions of vectors query that once when something is in disk it's maybe high tens of milliseconds mid you know 50 70 milliseconds when it's fully on disk maybe lower depending on the query the machinality whatever and when it's in memory it's closer to 10 to 20 milliseconds right so it's like these are not this is not bad like the user is barely going to notice it and but of course you're going to get more throughput that way and then means it's on s3 it's maybe more like five to six hundred milliseconds it's sort of user would notice but a lot of our customers like notion for example when you open the q and a dialog and these different dialogues that will query turbo puffer they will send a request to tell turbo puffer hey can you start warming up the cash here in a way that makes sense and by cash we just mean putting it into disk and starting with sort of the upper layers of the an index and other things to reduce the time as much as possible so there's a lot of things that can be done here that are very very simple that means that the there's there's there's barely a trade-off yeah but we let's go back into multi-tenancy unless you had a follow up let's do that yeah let's do that like how do you use a multi-tenancy part so so turbo puffer can run in three different ways it can run yeah multi-tenancy clusters that's what I mean that's what cursor does that's what that's what linear does and many of our customers so in multi-tenancy you share you share the compute we can do this so cheaply right because we can share the caching can share the we can share all of this infrastructure it's very easy for us to run this way so that's the default mode the cash is of course segregated off in in in in in in different ways but is also like shared in ways where you have a big burst the traffic rate you get more of the cash than others so that's what we so it's a very great way of running multi-tenancy the other thing we do for multi-tenancy to keep it very secure is that because all the data at rest is in the bucket you can pass an encryption key to turbo puffer that we don't have access to unless it's audit logged on your side where we can encrypt and decrypt the object which is logically and from a security point standpoint equivalent to you having all the data in your bucket so this is a very nice primitive that for example linear it takes advantage of because they have full control over their data they can see when turbo puffer is accessing it they can shut it down at any point in time and they can even pass that on to any other customers where turbo puffer can encrypt data for linear customers on behalf of the customer with the customer's key it this is like really really I think groundbreaking and underrated in this architecture you can of course do single tenancy with turbo puffer as well with the computers only for you you can do b y o c where we run to recover inside of your cloud in a way that's like very compliant we can never see customer data but we find it in multi-tenancy with the encryption which can be done per namespities satisfies the security requirements of even some of the biggest companies in the world yeah that sounds awesome I also wanted to pick one topic which usually used to I don't know if any more I don't see that as much pick up a lot of flame discussions what is your recall at n and when I go to the docs of turbo puffer it says recall at n is 100% recall at 10 excuse me but vector search bar so does that not 100% we said 9200 right no I think it says what wait wait wait I'll need to what was the page where you do that oh here the limits oh I see observed in production yeah it should say up to 100% that's a bug in the docs that I shipped last night I'm gonna I'm gonna fix that after this awesome but what it says in the in the limits is 90 to 100% but let's talk about recall I'd love to get into recall so I think recall is incredibly important it's the equivalent of your database you have to trust your database to do it in the same way that you have to trust your database do f sync and you have to trust your database that when we say that hey we don't return a success to you unless it's committed to s3 you have to trust that recall is similar right if you are working on search and you're working on connecting data to llem's then you don't want to worry in your e-vails on whether your vector database is giving you low recall it's actually a very sophisticated problem to evaluate whether this is the cause so you have to trust your vendor this is an underrated problem and I love that you're asking about it and very few people ask about it unless they're quite sophisticated so let's go into it let's go into a long answer here for your audience because I think this is paramount most databases that have a vector index are trained on or not trained on but they're benchmarked against for these different A&N open source projects so there's sift and others problem with these data is that they do not represent what we've seen in the real world a lot of them are very low dimensionality like when we do benchmarking on a billion that we're working on right now the biggest data sets we can find are like 64 dimensions this is not what people are doing in production they're doing at least 512 often generally I'd say the average is around 768 dimensions these are not representative data sets and the distributions in the academic benchmarks are also completely different for what we see in real data sets right in real data sets we see millions of copy of duplicates right we see filtering all these chaotic environments that do not present themselves in the academic thench works so if if you're using a vector index that's only been tested on academic benchmarks it's it's I mean it's like the LLMs right it's like you don't you don't really trust it just based on discording it's sort of you it's all the vibes right it's all the qualitative thing right outside the benchmark was that everyone was dreaming on them that it will work for your domain right like the LLM that's right like early on very very early on in in in interval puffer's history in the first month I was mainly iterating against the SIFT data set right just like 128 dimensional data set I didn't know anything about an end at the time so it's like okay this is pretty good we can tune some risks on this and then I can do go wider but I have the feedback loop and the observation I had at the time was that I found that one I so I got something that worked really well great heristics on SIFT and then when I went it on the other data sets it just completely did not work well or generalized to the other data sets and I think that taught me an early lesson that the these academic data sets are just not enough and the only way to know what your recall is going to be is to measure it in production this is what TurboPuffer does for a percentage of queries it depends on the number of queries that you do but let's say around 1% of queries TurboPuffer will run an exhaustive search against the A&N index on a separate worker fleet we will then emit a metric to data dog that is the recall number right for this query right like which is basically okay this is the top 10 we know is accurate and this is the you know heristic A&N index is what's the overlap and we will average that over time I have a graph in data dog that shows all the different organizations that have more than 100 queries in the in the past in the past hour or whatever and then we have the recall for all of them we have the recall at what they asked for to recall a 10 the p10 recall the p90 recall and we try to our best to make sure that this is green at all times and we consider green anything above 90% is generally quite good it well 90% is is quite good for some queries but for simple queries often it's closer to 100% many of our customers have have 99.
5% recall so this is the only way that we know to do this and it's fun you ask this question today because last night I was I was hacking on putting this into the dashboard so literally putting the recall that we observe from this from this monitoring system into the dashboard of the user because we think it's that important and it's very difficult to get right we have spent thousands of engineering hours to make sure that the recall is high now recall on academic benchmarks easy recall on raw ann search is especially on academic benchmarks very easy raw recall on production data sets I'd say medium to medium hard high recall on ann queries with filters with mixed selectivity and incremental indexing absolute hard mode this is what the like you just slap a secondary vector index onto an existing database this is what they can't do they can't sustain them like a thousand writes per second with high recall in the face of very difficult filter queries so let's talk about filters recall for a second there is barely any academic data sets on this yet it's all the production workloads what a filtered in an index means is that let's say that for example you have you have an ecommerce and you're searching for you're searching for I don't know yellow right and you want to only get things that ship to Canada that cuts the clusters in different weird ways that might end up with a selectivity of 50% and so if you just visit the the closest whatever vectors with some horrific you have you're not going to get the true ann because you actually have to search maybe twice as many maybe three times as many vectors to get the right recall the query planner the thing in the database that decides where to go on disk and figure out the data and aggregate it altogether and return into the user needs to be aware of the selectivity of the filter and plan that into the ann index again if a database is not really serious about their vector offering they're not doing this they're not measuring in a production they're not they're not willing to show their users and they're not they don't have a full infrastructure in place to measure the recall so I'd say we take this extremely seriously and we don't want our users to have to get to get to guest this and it's a it's sometimes a thankless job because because many many many many emails that we see against some of the other vector indexes have very low recall and and how are users supposed to know because running these running these tests is extremely difficult it is and it's like it's as you said like you need to trust there right trust your vendor and it's basically like the like in some documentation pages you say the floor or like the bottom line right like the needs of each it just doesn't make sense right if the quality isn't there then why are you even running this it's a difference between you know finding that product with those constraints when it exists and actually not finding it right therefore not buying it and so on and so on and so forth it's right and I think yeah I you can never guarantee a recall you can observe what you are trying to make it be here on every data set but if you send a billion completely random vectors with 3000 dimensions and try to query them in turbo buffer with query vectors and there is no natural clustering because they're random vectors you're not going to get 100% recall when you send that with a 10% selectivity filters that just like completely breaks every heuristic that's made right but all data in production real data that people want to search has some natural clustering to it so that's not a real benchmark that you can evaluate recall on right and so we always take this seriously and the in the in the in POCs and with the monitoring we do we're looking at these numbers all the time but there are like absolute edge cases that can be very very difficult and what you have to do too as a database vendor is like it's a tug is a tug of war between we're going to look at more data to try to get high recall and we're going to try to improve the clustering of the data so that we have to search less data and so you're always trying to improve the clustering and you're always trying to improve the performance of the database so we can look at more data to get high recall yeah for sure I know that you mentioned about filters you know challenges vegan and I don't know if you aware of those the reason an end-end benchmarks right but there is also a big end-end benchmarks that they happen to pleasure to participate in they have one of the datasets one of the tasks they have is the filtered search I have not participated in that one but again as you said it's kind of like academic but some of the datasets are quite logical like beaten points dimensions and not that huge it's like that's the thing there are 156 yeah it's not like yeah there are hundreds of 200 dimensions these are not real data sets like no there are real data sets but they are real data sets from the past generation of vectors right the the pre the pre current modern embedding error right which are just scores so much higher than these so we just don't see people use these yeah yeah exactly it's still fun to participate in this benchmark by the way because the data is there and you know the some of the guarantees that you need to hit really high you know like thousands of tens of thousands of queries per second so if you can create a toy index that works just a proud moment I guess that's right but I would say that people don't care about these bench like now people care about the benchmarks like their fun competitions but I think it can ruin your company if all you're trying to do is maximize these benchmarks because how many companies on in the world are trying to do 10,000 QPS on a billion vectors right not that many but there's a lot of companies that have a billion vectors lying around that they want to search and they just don't want the pricing to be offensive right we're a turbo buffer can you do this depending on the dimensionality for like a thousand dollars in month that's what people really seem to care about yeah sure maybe if I ask you like a spicy question if I may why do you think some of the vector database players like in Dalgium cells into that game of showing the benchmark and telling we are the best and then you know someone else cuts them over and says no you made a mistake in the benchmark why do you think this is happening like like publicly if you're comfortable talking about this yeah we we don't we don't publish benchmarks against anyone else in in fact it's it's it's usually against the terms of service to do that for almost every vendor including the big vendors like the hyper scalers probably shouldn't be it probably should be prohibited for the hyper scalers for like any competitive reasons or but anyway for the peers I think it's it's like a low blow because everyone can sort of p-hack their way to something where they they go better and becomes like month throwing and it's very distracting activity um we benchmark ourselves in ways that we find that our customers are actually using the database so we're not doing it at 10,000 qps because it's just not what we see to a single namespace um so we benchmark against ourselves we benchmark against first principles and we're always considering what is the gap between what turbo puffer does and first principles there's that's what I've learned that's why I do napkin math is because the fundamental thing you should be benchmarking against our first principles there's a gap between what the DRAM or disband with it is multiplied by how much if it you need and your database is not doing that well then you either have a gap and you're understanding or you have a you've found room for improvement that's what matters and of course it also matters what other people are doing but what matters the most is what your customers are trying to do and they'll they'll pull you in that direction so we think that this is a this easily becomes one of these metrics where you know if you give people a metric they'll optimize for it um and benchmarks of how many qps you can do at some number recall it's just not what people care about um they care about it working they care about enormous ride throughput they care about costs they care about other things necessarily that are much harder to put in such a benchmark um I think benchmarks are important like we need to give people a sense of what they should expect and they should hold us truth at that and what I would love to have is like more absurd ability in the turbo puffer product of like what kind of like performance are you seeing um we're working on you know explaining um our exposing query plans from turbo puffer right so you can see um well what's what's causing the indexing uh sorry what's causing the the query latency to be what it is so yeah I don't think the mud throwing is great um I think that at some point someone's going to publish a benchmark with turbo puffer and um and and again something else and and then we'll have to deal with that as it comes right um by um but it's certainly not an activity that we plan to engage in yeah I love your answer because it also resonates with me like in a different dimension you know I found myself in a situation when at some point in the past when um we've been copy copycatted I can't be say in that way so there was a company that really literally copied the whole interface and it like how how the product looks and we felt threatened but what they couldn't copy is essentially the internal IP right all the algorithms everything where we've spent hard hard uh working time on you know they couldn't copy that and and effectively that doesn't fly by itself right so so basically what I'm trying to say is that you know even though it felt threatening still thinking about what you need to solve right by the laws of physics you really need to focus just on that and if you solve that you become the leader of the market and that's what happened to the company actually it the story was that it actually acquired this copycat right and that's it uh I mean it doesn't mean that's the bad thing bad outcome for either of them but what I'm trying to say is that just focus on that on that thing that you're trying to solve and don't try to indulge into these you know games of like you said you know not throwing and stuff I like that really well said yeah that's I think I think so we focus on on customer studies we focus on we focus on on first principles we focus on benchmarking and we focus on on what are customers telling you telling us that they need and I think that um I think those are the right things to focus on for our company for sure and and and just looking at the clientele right the the ones that you shared just just knowing those names cursor and and notion that everyone is pretty much using every day that's like a testament to what you've done um I also wanted to ask you before we close I wanted to ask you about what are the maybe technical or business or some other challenges that you see ahead of yourself or maybe that's what already is happening and you see that it's important especially in this space of LLAMS where LLAMS can bring value um what is it it feels like you you have wildly successful as a business and as a technology but is there something that you see is still unsolved and and is ahead of you and worth solving I'll go back to a you know I spent a long time at Shopify and so part of growing up at that company from when I was very young was being taught a bit in the school of Toby Lukay the CEO and something that he often said is that you know you about himself is like you have to grow to keep up with the business and that's that's what it is for me as well right I um first had to grow as an engineer to put out the first version um then I had to build an engineering team to take it much further than I ever could alone and I think we have and just an absolutely like 99% college engineering team now then I turned my focus to sales and learning that and now I turn now I have to turn it towards marketing towards legal towards all these different things to build the company I we spoke a little bit about this about this before um about I think that one of the beliefs that we have is just the town density of the team and I don't think there's a lot of I think that a lot of people talk a lot about the town density and I think that there is um now a generation of companies that's really trying to do it um I think that um the tools that we now have available to us and especially the kind of tool that we work on every day um the floor for productivity has been raised but the ceiling has been raised far more and so what really matters to us is having a team of individuals that where everyone is is a player right we see these teams as a symbol of almost more like sports teams today um then how companies were originally uh originally built um and I think that we we hold that as a as a strong belief in how we we um we are building the company but it demands a lot from everyone to work in this way but it's very fun um and I think that the the growth that that embodies on everyone including myself is important and I have to keep up with that I have to keep up with the demand of of of how our customers and our team internally and everything grows and that's that's the biggest challenge is just the amount of new that has to be learned um so that we can become a successful company which is is important for me for our customers and for everyone who's chosen to come along for the ride and join the company.
 Oh that's awesome yeah this field changes so quickly um it it felt much slower when I was coding myself you know Java you seen all that stuff you you had like solar elastic search that's it for like long time and then a lot of new engines popped up especially when vector search appeared on the scene but now with the LEM advancements and all of that it just feels so crazy so yeah it's very interesting challenge for sure you know personally and business wise and and team wise and yeah and and keeping balance is another one.
 I think the pace of the pace that we see everyone running at now in the successful companies is beyond anything that I've seen before um it reminds me of of just the the the months leading up to black friday at Shopify but it's all the time and I love it I'm addicted to that pace and I think that we have created a team of people who seek intensity and that's exactly what we think that we need to create the right product at a pace that makes sense for also our customers so the day are never bottled next on us and that is what keeps me up at night.
Oh that's awesome that's great cause. We usually end with some sort of announcement anything you want to say to the audience especially now that you said that you want to go deeper into marketing it beats your chance anything that you want to share all call for.
 I think that we we we we've refrained from doing any large releases and we try to just ship as rapidly as we possibly can if I look at the change log this month it's um I mean we we launched to we launched the region the Singapore Canada we've added the float tight uh we've we've recently added clients for Java Go Ruby um one of the things that I think is is really exciting is our conditional rights and this is where turbo puffer is not just a bunch of files on s3 it's not even just a search engine it could do conditional rights where you can say hey I only want to replace this document if it's newer than the old version right these are real database features and things like patch right where you do a partial update but we don't we we just we just launched and then we put it on put it on x and we move on um so I don't have any big announcement we went GA a couple a couple months ago it would have luck to have done that but we just tried to ship an announcement just get it out there as soon as possible move on and ship the next thing.
 Yeah congratulations on Jay on your original launch but on Jay I think it's a big milestone as well and as you said you're probably not as sort of transactional anymore you just keep shipping and you follow what the customers need and but sometimes some of these things may go and notice unless you know people follow exactly what you do and so in that sense I feel like there is a room or stage for saying hey guys go go use it it's GA right run your benchmark.
 I think now do they think about it been more I think one announcement might be that early and unintervaled puffer's history we were very focused on doing many namespaces that were small but we are getting very good at large namespaces now and we have customers that are searching billions of vectors at once and we have customers that want to search hundreds of billions of vectors all at once and we are working with them on that and this is not particularly scary anymore you know exactly what we need to get there so if you have use cases of that caliber you may have passed by turbo puffer before but we're getting ready and we are ready for hundreds of million and billions at once the only limitation there is the is really just the size of a single machine and then we shard over them but I think back to the sharding we had before you need to make every shard as large as possible to get the best economics and the best performance and that's been one of the issues with some of the traditional story tangents.
Yeah for sure.
 Yeah I really enjoyed the convo I know we could have gone and so many topics like I really wanted to ask you also about an an algorithms and stuff but I also I feel like we could talk more later as well you know down the road as you guys are progressing hopefully you'll be open to that but I've learned a ton and it's very interesting designed that you have and and the whole journey of you pushing for four months you know and interrupted I hope you now regain some of the balance back to your life now that you have the team supporting you but I really enjoyed this conversation Simon thank you so much for your time thank you Dmitry