---
description: '<p>Show notes:</p><p>- The ML Test Score: A Rubric for ML Production
  Readiness and Technical Debt Reduction <a href="https://research.google/pubs/pub46555/"
  rel="noopener noreferrer nofollow">https://research.google/pubs/pub46555/</a></p><p>-
  IEEE MLOps Standard for Ethical AI <a href="https://docs.google.com/document/d/1x..."
  rel="noopener noreferrer nofollow">https://docs.google.com/document/d/1x...</a></p><p>-
  Qdrant: <a href="https://qdrant.tech/" rel="noopener noreferrer nofollow">https://qdrant.tech/</a></p><p></p><p>-
  Elixir connector for Qdrant by Tom: <a href="https://github.com/tlack/exqdr" rel="noopener
  noreferrer nofollow">https://github.com/tlack/exqdr</a></p><p></p><p>- Other 6 vector
  databases: <a href="https://towardsdatascience.com/milvus..." rel="noopener noreferrer
  nofollow">https://towardsdatascience.com/milvus...</a></p><p></p><p>- ByT5: Towards
  a token-free future with pre-trained byte-to-byte models <a href="https://arxiv.org/abs/2105.13626"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2105.13626</a></p><p></p><p>-
  Tantivy: <a href="https://github.com/quickwit-inc/tantivy" rel="noopener noreferrer
  nofollow">https://github.com/quickwit-inc/tantivy</a></p><p></p><p>- Papers with
  code: <a href="https://paperswithcode.com/" rel="noopener noreferrer nofollow">https://paperswithcode.com/</a></p>'
image_url: https://media.rss.com/vector-podcast/20211223_041259_de64d1b728c612795842622095155ffc.jpg
pub_date: Thu, 23 Dec 2021 16:01:59 GMT
title: Tom Lackner - VP Engineering - Classic.com - on Qdrant, NFT, challenges and
  joys of ML engineering
url: https://rss.com/podcasts/vector-podcast/347538
---

Hi, everyone. Bector Podcast is here. And today we have Tom Lackner, Vice President of Technology at the company called Classic. And I'm sure Tom will talk more about it. And he's also the founder and sole developer of Lookpop, which I'm sure Tom will talk more about as well today.
And what's really cool is that Tom has been using vector database called Quadrant in his development. And so today we have a user of a vector database, not a maker. And that's amazing to hear firsthand how it goes with vector database. Hey Tom. Hey, what's going on? So great that you joined today.
And I just wanted to start as usual, like if you could please introduce yourself and give you a little bit like color to your background. Sure. My name is Tom Lackner. I'm a software developer living in Miami, Florida, a very warm place.
I've been developing stuff on the web for about 20 years now since the early days of it. And I really, really love vector databases these days and doing stuff with embeddings. Yeah, fantastic, fantastic.
And can you tell more about classic? So I know that it's about classic cars, but yeah, what's this website is about and what's the community maybe around it and so on. So I'm the VP of technology for a site called classic.com, the tracks classic car values.
So what we basically do is we go out on the web and we grab all the car sales that are occurring that are happening in a way that's easily understood. So if anything is sold with a price on it, we record that information.
And then we cross reference all these vehicles broken down into what we call markets.
So if a vehicle came in two different trims, two different levels of options, we break those out separately and we can give the user a really good estimate of value with very specific and granular understanding of what a car is really worth.
So it's basically like a big data for cars type project, I guess you could say. Yeah, and I mean, I checked the website and I mean, the cars look so great and some of them are kind of like on on high end in terms of pricing.
So it also defines the audience, right? Yeah, it's classic car values have really gone up in the past five years, especially considering COVID and a couple of factors in the United States. So it's more important than ever to do really intelligent, like savvy shopping before you make a purchase.
So that's where we're coming from. Oh, yeah. Awesome. And like, is it so that the kind of the user experience is mostly kind of managed on the website or you have also some offline part of the operations? So most of our operations are online on the website.
We also have an iPhone app, but what's really important is our backend crawlers. So we have a huge amount of software and resources attached to the idea of brightened crawlers that can understand different auction websites really, really well.
That's like a critical part of the infrastructure that's sort of behind the scenes, but ends up doing becoming, you know, a key part of what we're doing. Yeah. And I knew obviously I have a search bar there.
So what happens when I type something like in, you know, on classic, we use a combination of Postgres for the actual like OLTP data, like, you know, the actual ground truth. And then we feed that into a last search to do the full text search.
What we're actually trying to do there is transition that as well to using a text embedding. We I find that text embeddings are easier to use in the long run. But what's actually challenging there is developing a good understanding of typos. Right.
So we could probably go into one detail later, but the most of the text embeddings that you encounter aren't really typo tolerant. So in our case, that search box needs to really understand, like, let's say, Ferrari or Lamborghini. Those words are often spelled incorrectly for obvious reasons.
So one of the things that's holding us up there is developing a typo, is a system level of embedding.
 Yeah, it sounds also some similarities to web search, for example, you know, like where users are using like colloquial language, or if they if they talk to their microphone instead of typing, then you have these typical problems from ASR, like automatic speech recognition systems, and you need to tolerate that.
So so it means that I don't know, like we've been thinking about data augmentation techniques. Have you have you thought about that as well? So what I've tried to do is to retrain the retrain the model using basically our input data, but with certain transformations applied certain permutations.
At this point, I'm not I am not the point where I have a usable model coming out of that, but I'm still doing some research and I it should work in theory.
Yeah, and there are so many models like on hugging face that they guess you can also kind of tap in, right? And that's actually one of the hard parts is to evaluate all those models.
So I have been taking a couple days to write a script that just downloaded every single one and tried them to determine which to best at our tasks. Yeah, exactly. And also like choosing kind of the quality metrics is another, another direction like how do you evaluate? Yeah, absolutely.
Yeah, we're kind of a new territory for a lot of this. So I mean, that's exciting on one hand, but on the other hand, like sometimes you just don't know the answer to a problem, which is like, yeah, yeah, for sure.
So in that sense, classic is kind of well, it's it's funny that there is a coincidence classic and then classic search like in a way that you're using TF idea for BM25, right? Well, of course, you will add an embedding layer at some point to make it more semantic, right?
Then you said that you have look pop, which where you are now experimenting with vector search.
Can you tell me more what is look pop? And then how do you implement vector search there? For the last couple of years, I've been really interested in search engines and how search engines work.
I feel like Google has sort of done us a disservice in certain ways over the past, you know, a couple generations of its development. So I've been interested in like, you know, developing better web search tools. Lookpop.co is my effort to make a NFT search tool.
So NFTs are digital artworks that you can buy in the past year. The NFT market has exploded. I think something like $6 billion has been exchanged this year in NFTs.
But the problem with NFTs is that coming from the world and the language of cryptocurrency, a lot of the websites related to NFTs are about the price, the up, the down, the this, that that, you know, what's hot?
What's not blah, blah, blah, who's flipping, you know, like I personally don't care for that.
So I was looking for an NFT search engine that could actually help me understand the meaning of NFTs and find visually similar ones.
If I find something I like, it would be cool like the up to see stuff that's kind of in that same vein without having to manually search around on OpenC, for instance, which is the number one NFT market. You can only search by the name of the creators, right, which is so weird to me.
I wanted to be able to search by themes, by visual styles. And when I came across clip, the text embedding system or the imaging embedding system, it really, like it provided all those features in a pretty easy to use way. So I'm really excited about that functionality. Yeah.
And clip is basically the embedding model developed by OpenAI. I think it's also available as a hiking face model. So you can kind of plug it in much easier in the code.
And so you have what is your experience with clip so far? So one of the great things about embedding is that when they work, it's like a sort of like magic, right? Like it's like, it's amazing that this was even possible.
The problem is though, if you actually look at the results set as a whole, it's only 80% accurate, right? Like you'll find 20% of those in there are just what the hell is this?
So as a sort of imperative programmer coming to it, or a guy who's my experience is based in the world of traditional programming, to see that it's like, okay, this is a bug, but it's not.
It's one of the switchovers you need to make is to accept the fact that you're going to get a lot of great results for very little effort, but you're not going to get every 100% results.
It's more about identifying the results that are bad, flagging them and trying to retrain to get them out of the loop in the future. Yeah, exactly.
So like building that pipeline, it's essentially like MLOPS pipeline, right? Machine learning operations where you need to switch your mind a little bit into building this pipeline way.
You can like detect problems and then feedback to the process of building a next version of your model, right? So it's not as easy as opening your debugger and then, okay, here is the bug. It's logical, fix it, done. Yeah, the pipeline to develop these models is long term.
It's very different than a piece of software and you need continuous monitoring and you need to continuously be able to sort of have signals feeding in to make that make that model better next time. It's actually pretty difficult.
I know there's a lot of startups around like MLOPS now, which makes total sense to me, but it's almost like I feel like developers myself included need to build the mindset and to know, and to, and to mentally know like, okay, these are the different components that I need to put into the system.
Yeah, absolutely. And there is like, there are white papers published, for example, there is one by Google, I will try to also, you know, link it in the show notes and share with you.
But it's fairly long document and it goes so high level that you might get, get, get a slip, you know, while reading it. So you need like real tools, right? And you need some kind of understanding, okay, I stitched this together and I just achieved my goal.
I don't want to like build the fully blown MLOPS pipeline, right? And it's also expensive, like retraining these models is very slow, which means you're going to want to use the best hardware you can.
And if you're doing that every day, which is crazy, but let's say you do it every week or every month, it's still a significant amount of like fixed resources. You have to allocate to it and like mental resources to understand it to debug it. Yes. Yes. Yeah. You're right.
Yeah, I think there is still a long way to go in this in this direction, but at the same time, you like you as a developer, you want to focus on the task and on the result, right? Like not on figuring out what's the bug and that framework or whatever.
So yeah, I think there are tools already available. So maybe one of them that I've been using is determined AI or kind of doing some early stage moves there. It's completely open source and it's and it claims that basically it utilizes your GPUs to the maximum because GPU is super expensive.
And yeah, so basically at the abstracts GPU kind of allocation away from you, but it has some limitations as well. So the team is working on or resulting them, but like PyTorch and TensorFlow are supported. So like you can run some fine tuning or training or evaluation and hyperparameter search.
So yeah, I mean, it gives you a sense of control in a way, but of course that comes with some rigidity built in, but eventually I really hope that they will make it and it will be more kind of widespread. Yeah. Awesome. Awesome.
So today if I go to look up, can I buy an NFT already? Okay, then just find it. You can just find it and click through the open see the actual process of you getting the deliverable and the token and all that stuff is actually pretty complicated. So I'm going to let them do it.
I really want to on look up, hopefully be indexing tons of different NFT markets. Open see is the biggest one, but there's quite a few other small ones. So I didn't want to time myself too closely to one particular blockchain or one particular form of operation.
I do think that this is developing so quickly. NFTs weren't even a thing until about two years ago. So I feel like it's a little early to sort of like get in bed with just one of the vendors or just one of the vendors. Yeah. Yeah.
And I've been also like when I joined Quadrant Telegram channel, I saw like you've been so active like you are sending some advice or commentary almost every single day. So I love Quadrant and I love Telegram.
Yeah, I was just thinking like you are the developer of Quadrant or what, but you are the user, is that right?
Yeah, I mean, I think I'm doing like informal tech support in the opposite time zone because they're all on CET like you are in a, you know, for some reason, although a lot of people wind up in, you know, American time in there.
I was looking for a long time for a vector database. I tried FAASS face, I think they call it in Python and I tried a couple others and I really didn't find anything.
I guess you could say like intuitive that intuitively like scratch, scratch my itch, you know, like I don't like software to complicated things that are sort of like isolated and independent and easy to install and use.
Quadrant just sort of like ticked all those boxes for me like it's a small download, it's a dockerized so it's very easy to install. The API just makes sense. I had evaluated other vector databases we can talk about that if you want.
I found that Quadrant was the best mix of all those different factors. So, you know, when I embrace an open source project to try to do my best to help them out. So I built the first elixir connector to use Quadrant for me, Lixir. I'm trying to still develop other little pieces of the puzzle.
So actually I'm interested, I'm quite interested because you know, like I published a blog on seven vector databases.
It was actually six and then the founder of Quadrant knocks on my door, virtual door and he said hey, please add our database as well because we are the new kid on the block and you just probably didn't see us.
And then I opened their website and I was like kind of a little bit blown away because, you know, the documentation looks interesting, very good and also like the way they position it. Yeah. They talk about like metric deep learning, some things that I didn't even hear before.
And then I also discovered the developer team, like what they do and also they customized like H&SW algorithm algorithm as well. They do graph algorithm.
And can you like a little bit walk me through the options that you considered, like which are the databases you have taken a look at, how deep you went before you decided to go with Quadrant and what was the ticking moment like, okay, I like this. Right.
So I think the main one that I think I studied for a while, I think a lot of people look at is Milvis. Milvis has like a lot of really exciting energy going on. I think they go to have a good replication story as well.
But the problem where they seem like they wanted me to use their Python data science toolkit to sort of interact with it. They were, their API was very abstract and focused on, obviously just not what I was really doing.
I needed an API that was oriented around data operations and working not so much analysis. So that kind of slowed me down there. With Quadrant, I felt that as soon as I got to the web page, I knew how to use it. Right.
Which, you know, for some reason to me, you know, if a software system abstracts as specific of how it operates and how you use it too much.
I like, I like to say, like, I like a language where you go to the site and you just see a bit of code there on the home page and it's a button you can run it, you know, like, I don't want to be too removed from the actual tasks of what I'm doing.
So Quadrant just seemed to like understand that and get that. And then going to the channel, I like the fact that they had a good, a good, a deep technical understanding of how it works, but they weren't trying to beat you over the head with the specifics.
It was kind of like abstract at the right level. So, and you know, it's really fast. I tried tossing millions of records at it and it was almost if perceptibly slower. Like you almost couldn't tell that you were adding so many records. So I thought that was, that was really fair.
A lot of these vector databases now, I feel like the more like platforms, you know, I didn't want that.
I wanted almost like a redis of vector databases, you know, that kind of, by platform, do you mean like this database is trying to lock you in in a way, kind of like give you so many features you don't need? Exactly. Okay.
So I think it's all well meaning, but I just, I just feel like I can't, I can't trust one vendor for a lot of what I'm doing. I need to sort of spread my, spread my risk, you know, over different parts. So I try not to embrace any parts of the system that are too large, too monolithic.
You know, yeah. And I guess at this point, you're wearing your VP hat, right? VP of the genuine hat that you, you don't just kind of like, oh, this is a sexy platform or tool. Let's use it.
But you want to see long term, what are the implications, right? And you need to, you need to adopt technology that has the right sort of surface shape that you know, it's going to slide in easily.
You know, I, I, I, with, for instance, with Python face or whatever, I knew that that was going to be a nightmare to wrap to make connectors for for different systems. And I also knew that I wasn't going to program my entire thing in Python.
And I also knew that, you know, I would need to have a long term component running, running a web server that was independent of Python back and restarts. So with all those factors together, I think that quadrant was kind of like the obvious choice.
And plus just looking through the code, it seemed, it seemed short. I, for some reason, I've been having really good, really good results with stuff right and rest lately. I've, like, a lot of rest software come across as really reliable and performant. Yes.
So like, that's what I was about to ask, like, when you, then when you're choosing, you were choosing the platform or the database, did you pay attention to the programming language that was implemented in as well? Yeah.
For some reason, I know it's unfair, but I've definitely observed certain patterns in all, let's say, all Java based server applications, like, let's say, elastic is a great example. They always want to consume many, many, many CPUs and have low RAM limitations.
And of course, they're still that confusing garbage collection cycles and Java. And like every time I run a Java based service, I need to end up doing tweaking on the JVM Yeah, which is like a garbage collector, right? You don't want to do that black magic.
Yeah, I want, I want the thing that's doing the running to sort of be self operating. I don't want to have to be tweaking that all the time as my application needs grow and change. So that kind of like disqualifies all Java based software for me.
And I believe that one of the major vector databases is Java, right? I think it's the, the, the, the, the, the, the, the, the, the, the, it is written in go, actually. So, but if you mean Vespa, Vespa is written in Java and some other part. Yeah, Vespa, yeah. Yeah.
But by the way, did you consider Vespa or VEV8 as you've been studying different databases? I believe I checked out the Vespa site, like you said, Java. What was the other one you said? VEV8. VEV8. Written and go. Yeah. It's also open source and also first year cloud.
By the way, I have episodes with both Milbus and VEV8 for those of us that would like to kind of listen in what, what are the building blocks and architectures behind this? Yeah. And features. It's actually awesome. I have to listen to that. Yeah.
I'm actually very curious about the different implementation choices. Yeah. Yeah. Because go is also a high performance language, right? Compared to Python or Java. Of course, in Java, it also depends how you do it.
You know, if you take elastic search or solar, both of them are using Apache Ducin, which is a search library inside. And Apache Ducin has been optimized for, well, close to 20 years or even more. So I mean, it's close to see in some sense, but of course, it is not see.
So like when you, when you load more and more data, eventually it will, you will run into situations that you just explained, right? That you start tweaking the garbage collector.
There is like a dedicated channel or like even a person, I feel like Sean Hasey in the commuter side, who has a lot of wisdom in how specifically you need to tune which parameters in which garbage collector.
Do you want GC or whatever you have? Depending on the Java version, right? Because different Java versions have different GCs and like it's almost like opening a whole can of worms when you don't want to actually. You want to solve that task and move on to the next one.
So yeah, so far, I haven't had that issue with the rest based stuff that I'm integrating into my work so far. But you know, I would imagine the people using the first wave of Java based server software didn't find any problems either.
So maybe as time goes on, we'll discover that, you know, you can't do large allocations in a roster or something like that.
Yeah, it's also cool that actually Rust has been picked up by many teams developing search engines and not necessarily the vector databases, but like traditional, you know, inverted index databases like Tantivi is one of them, which is using Rust.
And they have a nice blog as well explaining some of the performance bottlenecks they were able to resolve. And Tantivi is way faster than you've seen. So there is another presentation by the Tantivi main. I'm looking for a good free tech system with BM 25 and all that kind of stuff.
Unfortunately, quadrant isn't going to add that. I don't think it's kind of off base for them, but that's such an important part. You know, there's a reason that so many of these startups have a team of people just doing search result quality, you know, search results are critical.
Yeah, now that you mentioned this important topic, I also wanted to kind of a little bit pick your brain on that, you know, like you have the traditional search engine, let's say on your classic dot com site, right, where I type text and you use inverted index to find the results.
And then you want to bring in bird model or some other model to deal with more semantic search, right. So have you thought about how you would combine them? Let's say inverted index versus vector search.
So I actually, okay, so we say search, right, but there's actually kind of different sub tasks inside of search. One of them is when you search for something, we want to show you something that's similar. So you don't necessarily want to get the exact same term.
So that requires like one piece of data or one mechanism, right, which is more like a recommendation type system. You also want to handle things with direct keyword matches, of course, but you also want to handle typos, right.
So typos requires like a second layer and structure of databases or the way you implement it as to work a certain way. Okay.
And I feel like the best way to kind of do this is to have the search piece do multiple different attempts at solving the query and then combine them with an intelligent strategy. So, like for instance, on classic, now we're building a better auto-suggest component.
And it's actually doing three different types of queries.
And I think that if you really, really, if you start recording what users are doing and you start looking at every single, every single search and saying, what did we do wrong here? How could we not service? I think you'll see that it's actually not just one type of query.
When people see a type of search box, they'll just start plugging things in. They don't know if they're going to do English language queries, which is something that an embedding would handle, right, because an embedding can understand any sort of information or any sort of intention in that query.
But sometimes they're just searching for a specific model number or something like that. In my experience, a lot of text embedding models, if you use a term that's outside of the domain, something that was outside the keyword list it was trained with, you're going to get really bad results.
So that's another thing I have to sort of be thinking about.
So unfortunately, right now I think that the best way to set these things up is to do multiple last-use search queries, maybe a postgres query, and maybe a quadrant query, and then correlate all those results and display them intelligently. Yeah, exactly.
So basically, you almost need some smart runker or re-runker, which combines these results, and it doesn't care, which algorithm was used to bring them in. But what it cares is the, you know, to optimize the KPI, let's say, flicks through rate or whatever it is.
Because in some applications, like, I've been talking to one company building maps, and they said that, for example, when you sit in a car and you start typing some, like, few letters, like two or three, you don't have much time as a driver, and you just need to hit the road going, right?
 So if this company is doing bad at predicting the intent, and by the way, what they do is that they don't limit the search only to the radius around you, because they believe that you might be going to the airport from where you will fly out to, I don't know, Washington, DC, and then you are looking for that specific street while you are sitting in the car in Europe.
And so they search the whole database of points of interest, and, and you know, like, first of all, it's scale. It's going to be a problem. And the other thing is, you need to actually rank the results in such a way that they get it, right? So it's extremely difficult problem to handle.
So in that case, I would, yeah, in that case, they're probably predicting from where you are now. If you're here now, what's the most likely thing that you want to go to? It's kind of an interesting problem.
And actually, that's like, you actually kind of bring up a good point, is that a lot of startups don't have enough data to make those intelligent associations.
So it becomes a game of sort of, this, of finding an open data set that you can use, or something you do have, and like sort of abstracting from it, or extending it in a certain way that you can make these intelligent inferences. But it's very, very difficult.
And until you have a lot of users, you don't have any data coming back to you, telling you whether or not you're doing a good job. So it's not easy.
And I think that's one of the reasons that we see that some of these big startups, these platforms become very entrenched with their data learning tools, or their machine learning tools, their data sets, there you have, it becomes hard to, hard to unseat them.
Because all the activity and that space is happening on their property, you know? Yeah, yeah, exactly. So, and one thing I wanted to also mention that you said you want to handle typos.
Did you know, or did you look into bite level embedding models? So basically, instead of, you know, segmenting the word, let's say, letter by letter, whatever, which could be also expensive. They go into bite level. I think that paper was published by Google.
I will try to look it up and also link it in the show notes. But have you, have, did you know about this? Or did you consider such models? That's news to me.
What I've been trying to do is just retrain an existing model with a bunch of permutations and things like that, obviously, think of that were like common typos like dashes and stuff like that. But that's a very interesting idea.
So basically, they're working on a character by character level, right? So the embedding itself is composed of, it's even bite because the language could be something they like, okay, you don't want to apply some linguistic component, which is language dependent.
Let's say in Chinese, you need to segment the string, right? You need to know where is that space, which is not there geometrically. And then in some other languages, let's say, Russian, you will have like rich morphology.
So a lot of endings and prefixes of the word, right? So instead of, yeah, like surface forms, instead of applying that surface form lematizer, you will go and just look at the bites. And then you ask the neural networks to pay attention to the bites instead of the characters. Yeah.
That's actually a brilliant idea. And no, I haven't heard that, but I would love to apply it. So please send me the link. I will, I will for sure. It would be cool if you can apply and take a look at it.
And hopefully, there is a model that you can take off the shelf and not like spend weeks or months researching it. So the amount of effort going into training these models now is really, really absurd. It's ludicrous, yes.
And I mean, the models are getting bigger and bigger, but it's funny that they not necessarily becoming more smart in a way. And I will try to open it. And I'm actually now editing an episode. So by the time this one is published, that one should also be published.
And yeah, so basically, yeah, how you train everything, I don't know, it's like, you don't want like you as a developer, you don't have time researching things, right?
Now with what options do you have, you will probably need to go to some company, which will offer you the service for money, or you need to go and scout on the hugging face side and hope for the best, right? How do you feel about that? Yeah.
I haven't spent so much time just sort of getting my brain around certain things that are like, you know, there's no real jumping off point for a lot of the stuff.
There's no single place you can go to people, I see people on the web on these sites, like, what book should I read about this? Book, you kidding me? What book should you write about this? There are no books about any of this, you know?
It's changing so quickly that I feel like you have to be part of numerous, like internet subcultures and very specific, like research websites, even I understand what's going on.
But thank God that people like hugging face are putting so many resources into just making these tools available, like their pipelines package is like such a time saver. I can't believe I was ever implementing all these things from scratch or from, you know, separate tool.
Yeah, yeah, there is another site that I wanted to mention, which also picks up is papers with code because when you read a paper and you're like, okay, how do I do it? I need to spend a few weekends to implement it. Some of us have the skills, some of us don't. So people are lucky. Yeah.
And they probably belong to the communities so well that, you know, they know their way through. Yeah, I think that, you know, papers without code are kind of like, like to me, a little scandalous now.
I feel like it's very difficult to measure someone's results and to really evaluate what people are doing if they're not releasing the code. And even if they explain the algorithm and the paper a lot of times, the specific training process for the model is what's really critical.
So certain decisions they made about what's included in the day set, what isn't included in the day set. And just sort of like the training loop engineering, like I feel like that's super important.
So I think what the success of OpenAI has had with Clip, I think goes to show that someone with a great idea and a model that's released on time and early is just it's going to really be a game changer for the industry.
Yeah, I remember like in the Telegram channel of Quadrant, two people including you, I think, said that without any fine tuning, you got really great results with Clip. And I think you guys applied it to different domains.
And that was amazing because especially the cross-domain application, you know, it's such a big pain for these models.
There is a paper as well where they take the off-the-shelf models and they apply them to different like search tasks because a search task could be, let's say, it looks like a question answering system or it operates in financial domain, right? So it goes in specific domain.
And then they basically what they did is that they applied no fine tuning whatsoever. So they took the models and they applied what they call zero short learning, right? So you just, you need to predict it right away. And they showed that, ah, man, they're not all equal at all.
And sometimes they miserably fail. But they actually found out that the specific category of algorithms based on, I think based on dense retrievers, if I remember correctly. So they perform better than others.
But if you compare the dense retrievers to BM25 method, BM25 was still fairly close to them. And it's way less expensive. So that's really interesting. Yeah. It also depends a lot on the very, very specific use cases of your users.
Like what you're saying with BM25, if something is very, if they're searching for a lot of sort of jargon and industry specific stuff, BM25 is definitely going to kill it compared to even models that are tolerant of the terms outside of their, outside of the keyword space.
Like, I really feel like what we need is a more natural way to blend these two kinds of techniques.
And I think as we see more and more advanced vector-based search engines coming out, we're going to see systems that are able to sort of store the full text, store the vector embedding, compare them both and rank them in a uniform way, which is like so critical.
And one of the, I think something you mentioned that is super interesting is to the systems that are, they're retraining them using these simple keyword question answering tasks. And result comes out much, much better, the accuracy and stuff. I think that's so interesting.
And I believe that if you could take a model and take specifics about your use case and blend them together very quickly and easily, I think that we would end up seeing embeddings that produce a much better result in the field. Yeah.
And I think you are tapping also in the in the part where I hope that at some point, we also get a confidence level. Let's say from BM25, you can also get a confidence estimate how well it worked. And the same goes to dense retrieving, you know, the vector retrieving.
Wait, could also say, hey, I'm kind of like 60% confident that I found you a good result, right? Then the question is how you build it, but like that's the goal. Let's say I'm just pushing this out to the universe. So hey, everyone who works on search engines, maybe you can consider this.
Or maybe you already are considering this. But I think that would be so much easier because, for example, in e-commerce, right, one of the problems is zero heat search. And probably for your search as well, right? Like somebody typed something that you couldn't handle.
Now, what do you show? A blank screen or you show the most popular NFTs, right? And that's one of the hard things about, I guess, a traditional imperative based search engine. You can never show the user an empty page. You can never just say, ah, nothing, sorry, try again.
You have to always be like feeding them next steps that they can go to that make that make a lot of sense. And that's definitely one of the challenges with old style database search approaches, just finding, finding results that are relevant, but not quite right.
You know, SQL doesn't really do that. So that's another great thing about Quadrant. At least a lot of times you're getting a score metric back that is a good and continuous, value. It's not bullying. Yes, this match, no, that didn't match. Yeah, yeah, exactly.
I remember like when I was entering this field slowly, I had a friend who was majoring in information retrieval systems as an academic.
And and I asked him, hey, so if I, how do search engine work, you know, like if I type something, what happens next? And I knew nothing about inverted indexes, nothing at all. And he said, yeah, there is like an inverted index. So we break that documents down into this kind of vector of terms.
And then it points each, each term points to the posting list with the OKDs and so on. And then you apply Boolean, Boolean like logic on top of those. And you make it efficient. But then I was still not satisfied.
I said, hey, so it means that if I need to find something, let's say I'm in discovery mode, I don't know what type. So what should I do? And and he said, yeah, the IR is not there yet. Like there is no discovery.
You literally need to type at least something, right? And then I said, OK, when I type something, like how does the search engine know what I'm looking for?
And he said, well, that inverted model, which is a vector space model from the 60s or 70s, it basically builds some kind of understanding of the document.
And I said, how exactly does it understand the document? And he said, basically, it's a bag of words. And I said, how can it, how can it make sure that it understands the meaning when it's just bag of words? Well, he said, there is also IDF component and NTF component. And these two play together.
And hopefully the ideas that you will find some unique document, which which uniquely explains what you're looking for. But if I'm not looking for a term, if I'm looking for to be or not to be, each of these words is a stop word.
Now, how does it know what I'm looking for? And then he said, OK, Google actually pays more attention to the title. So like if these words occur in the title, they will rank the document higher. And at that point, I was like, this is like magic. So it doesn't understand anything. I'm searching.
It's just tuning it, right? It's layers of hacks upon hacks upon hacks to achieve certain goals. It's very interesting. And in the case of Google, it's amazing. And it works as well as it does. The scope of documents that they have in that index is ridiculous.
And to be able to sort of fulfill realistic queries, especially if you consider doing an exact magic query for long terms across a huge index of documents, like how the hell, like the quotation mark queries, I guess you could call them. Very interesting. I think that's a real good thing.
One of the things that I've found to help me evaluate the overall the overall confidence level of that these texts and writings do is I evaluate different choices. So for instance, on classic.com, one of the options we're exploring is we have an enormous editor workflow.
So when a new vehicle comes into the site, we need to have a vehicle person is expert at that making model. Look at the vehicle and determine what it is and answer some questions about it.
Like what color is it? Has it been restored or is it in an original condition? So what we're trying now is to actually use clip for that. So I have a database of those, let's say, potential colors.
And then I evaluate the image with clip and I say, picture of a red car, picture of a blue car, picture of a green car. And then I look at all of them, and I determine what one obviously which one has the highest the closest distance.
But also, overall, did any of them have a close distance or were they all kind of distant or were they all very far away from the embedding of the query. And if so, then I tell myself that okay, we're not answering this question well.
Right? Like the fact that it had no strong suggestion at all is in a way a confidence factor or a confidence metric in a way. Which is fantastic.
Like you're able to find an answer to my question, which is like broad enough, I think, but like essentially, you can use a threshold on the distance that didn't cross my mind at all. Like, yeah, you're right.
Like you can define kind of like the confidence interval for these distances, right? And you know, which metric you're using and you know your data set as well.
Right? So you could go through the meal of your lab and check, okay, is this a good one? Is this a bad one? Yeah, that's an amazing solution that you just came up with.
And from the perspective of the amount of art of, like when you're building a piece of software, you have to say, how many little artifacts am I creating here? What do I have to actually do? Am I creating a lot of stuff?
Am I just creating a little bit of stuff that works for a broad range of data and use cases? Now with clip, you get so much for free, quote unquote, like that whole question answering system that I used to implement that took a couple hours.
And of course, I'm going to take a lot of tweaking, but compared to training, a bunch of image classifiers to answer the same task, which would take me an enormous amount of effort I would have to have, you know, we have seven different attributes.
So there have to be seven different models, hundreds of thousands of training images for each, a very elaborate process of manually correlating them with clip. I just got all that quickly. And again, it's not super accurate, but it gives you a building block that you can just apply everywhere.
And you know, if at some point, I wanted to find other vehicles like this one, that same model works. If I want to find, if this matches a certain given piece of text, like, is this a Ford Mustang? That model works. It's just, I don't know, it's really, really, really amazing.
Yeah, it's mind blowing that, you know, science, as you said, you know, somebody in the science world thought about this problem, and they came up with some really great solution that you can actually use.
But when you discovered that clip works so well, did you get amazed at the point of going back and reading the paper, or are you not interested in papers? I flipped to the paper. I'm interested in papers to some extent.
I know some people like take a week off of work to read paper and stuff like that. I'm like, wow. You know, I don't come from a math background. I come from more of a practitioner programmer background.
So for me, I actually prefer sometimes to study the code and to understand, like, a lot of times their usage instructions will give you a lot of like subtle information about ways that you should and should not apply it. So I kind of stand that space for the most part.
But I am definitely paying a lot of attention to all, to all embeddings at this point.
And I feel like this is like, especially the multi-volta ones, once they start including the video content, and once we can run audio through there as well, it's just going to be a really exciting time to be alive.
Yeah, I think it's great that you're looking at the code because it's like several levels of abstraction, right? First, you're trying to understand, okay, is this useful? Okay, it is.
How does it work? Maybe what are the limitations? What are the advantages? Then you go to the paper, so where they, of course, beautifully describe the algorithm and they say it's the best. So it beats the state of the art and, you know, over the previous work.
But the problem is that there is always a gap, or usually there is a gap between the paper and the code. So if they publish the open source code, you go there and oh my god, there's like a bunch of additional hacks on top of the paper to make it really work. Oh, I see.
So yeah, it's amazing that you go back and read the code. And are you getting scared of reading like C++ code? No, no, no, no. In a way, C++ is like, it's so much, you know, it's different. It's a different part of your brain.
You know, C++ is so much simpler in a certain sense that every, every line there has a specific action at a specific point in the code.
Like every line there has a certain meaning with, let's say a model and PyTorch, there's a lot of like, for instance, like if your normalization is wrong, right? It's hard to tell that. And it's hard to even see that except for watching a training curve and guessing and sort of hoping.
And maybe that's part of my, maybe that speaks to my skillset, but I definitely think that that the machine learning model is brilliant because it's such a small amount of code that can do so much. Whereas C++ stuff is interesting because everything is excruciatingly carefully defined.
So it's kind of two separate sides, but both you to hold in their own way. Yeah, absolutely, absolutely. Especially when they get combined. So you're like, you're doing some model on C++ because for example, H&SW the graph algorithm is implemented in C++, which looks like C.
So I took a look at it with one of my colleagues and he was like, wow, this is not the modern C++ code. And yeah, it's like basic, it looks very basic in a way. Of course, they use some C++ elements, but like, for example, they allocate memory with like this level of mal-locks.
Yeah, and you're like, wow, yeah, you're doing that. And then some other companies, for example, like semi, which build the aviate or quadrant, like basically re-implement the algorithm and their language of choice, go or rest or whatever.
So because you feel probably better after understanding each bit there and then you can also control it in the way you want, especially after listening to users.
Yeah, and every, I think that it's kind of like they're living in different, different computational spaces in a sense, like what they're expressing and what that line of code does is the complete opposite from the perspective of what is what we're committing to the machine here.
You know, the machine learning models, we're building a framework in which it's capable of learning something in a C++ based or any imperative environment like that. We're expressing everything you can specifically do. It's almost the opposite. It's kind of interesting to think about.
Yeah, exactly.
Hey, Tom, it was really great talking to you, but I was still thinking like if you can spend a few minutes and if you are not averse to philosophy, like I like to ask this question also to each guest on my on my podcast, like considering that vector search is an emerging field in many ways.
We don't know yet if it will fully replace the traditional search or if they will work together. But in general, like what makes you excited? Why are you doing this? Like what, what keeps you going and exploring this field today? I have a very simple answer for you.
I'm tired of writing if statements. So you want to piggyback on some complex models that the trends are. I want to show the machine examples of it working correctly and examples of it working incorrectly and the machine learns exactly what those if statements should be.
I mean, it's the idea that we have to train something by illustrating every possible variation of it is just insane.
 If imagine like on the say on Lookpop when you're searching for money and you see images with dollar signs in it, like that could have been programmed by a human being, but it would take a team of hundreds and it would take them 10 years and then they would finally have the money detector, you know.
Some brilliant dudes took a couple months to express how it could work and now we can solve all these different questions. It's fascinating. No, it's an amazing answer actually. Thank you.
I mean, it's you know, like some people get entrenched in like, oh, I'm so in love with machine learning, but like what you say is that you have a practical need and you also know the limitations of your previous approach, right?
Like if statements like who wants to code if statements or like if you take would say dictionary like somewhere in solar elastic search, you need to manually code that dictionary up and like maintain it.
Oh my god. Really? Is that the best part of your job? Probably not. Defining synonyms is a whole I cannot believe I have to define synonyms. Someone's already done this somewhere, you know, induction rich somewhere and they're just sitting on the ring.
It is somewhere dusty dusty shelves from why not why not embed them inside the machine learning algorithm? Yeah, absolutely. Hey, it's so fantastic talking to you.
Thanks for bringing this user perspective and like, is there something you would like to announce or share with with the audience, you know, anything at all? Just check out lookpop.co and taking some NFTs in your life.
Yeah, and buy an NFT and spice up your life, the digital life, right? Yeah, awesome. Hey, Tom, thanks so much. I really wish you all the best in trying quadrant and implementing it in your product and also like the whole web to your user base.
And I'm sure we can talk later as well and you can share some progress speeds, you know, as you go. Great. Thank you so much. Thank you so much. Yeah, bye bye.