---
description: '<p>Show notes:</p><p>- On the Measure of Intelligence by François Chollet
  - Part 1: Foundations (Paper Explained)  [YouTube](<a href="https://www.youtube.com/watch?v=3_qGr..."
  rel="noopener noreferrer nofollow">https://www.youtube.com/watch?v=3_qGr...</a>)</p><p>-
  [2108.07258 On the Opportunities and Risks of Foundation Models](<a href="https://arxiv.org/abs/2108.07258"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2108.07258</a>)</p><p>-
  [2005.11401 Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](<a
  href="https://arxiv.org/abs/2005.11401" rel="noopener noreferrer nofollow">https://arxiv.org/abs/2005.11401</a>)</p><p></p><p>-
  Negative Data Augmentation: <a href="https://arxiv.org/abs/2102.05113" rel="noopener
  noreferrer nofollow">https://arxiv.org/abs/2102.05113</a></p><p></p><p>- Beyond
  Accuracy: Behavioral Testing of NLP models with CheckList: [2005.04118 Beyond Accuracy:
  Behavioral Testing of NLP models with CheckList](<a href="https://arxiv.org/abs/2005.04118"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2005.04118</a>)</p><p></p><p>-
  Symbolic AI vs Deep Learning battle <a href="https://www.technologyreview.com/2020..."
  rel="noopener noreferrer nofollow">https://www.technologyreview.com/2020...</a></p><p></p><p>-
  Dense Passage Retrieval for Open-Domain Question Answering <a href="https://arxiv.org/abs/2004.04906"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2004.04906</a></p><p></p><p>-
  Data Augmentation Can Improve Robustness <a href="https://arxiv.org/abs/2111.05328"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2111.05328</a></p><p></p><p>-
  Contrastive Loss Explained. Contrastive loss has been used recently… | by Brian
  Williams | Towards Data Science <a href="https://towardsdatascience.com/contra..."
  rel="noopener noreferrer nofollow">https://towardsdatascience.com/contra...</a></p><p></p><p>-
  Keras Code examples <a href="https://keras.io/examples/" rel="noopener noreferrer
  nofollow">https://keras.io/examples/</a></p><p></p><p>- <a href="https://you.com/"
  rel="noopener noreferrer nofollow">https://you.com/</a> -- new web search engine
  by Richard Socher</p><p></p><p>- The Book of Why: The New Science of Cause and Effect:
  Pearl, Judea, Mackenzie, Dana: 9780465097609: Amazon.com: Books <a href="https://www.amazon.com/Book-Why-Scien..."
  rel="noopener noreferrer nofollow">https://www.amazon.com/Book-Why-Scien...</a></p><p></p><p>-
  Chelsea Finn: <a href="https://twitter.com/chelseabfinn" rel="noopener noreferrer
  nofollow">https://twitter.com/chelseabfinn</a></p><p></p><p>- Jeff Clune: <a href="https://twitter.com/jeffclune"
  rel="noopener noreferrer nofollow">https://twitter.com/jeffclune</a></p><p></p><p>-
  Michael Bronstein (Geometric Deep Learning): <a href="https://twitter.com/mmbronstein"
  rel="noopener noreferrer nofollow">https://twitter.com/mmbronstein</a> <a href="https://arxiv.org/abs/2104.13478"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2104.13478</a></p><p></p><p>-
  Connor''s Twitter: <a href="https://twitter.com/CShorten30" rel="noopener noreferrer
  nofollow">https://twitter.com/CShorten30</a></p><p></p><p>- Dmitry''s Twitter: <a
  href="https://twitter.com/DmitryKan" rel="noopener noreferrer nofollow">https://twitter.com/DmitryKan</a></p>'
image_url: https://media.rss.com/vector-podcast/20211223_011252_c0a8e84bf74cac993f87600e13f3d942.jpg
pub_date: Thu, 23 Dec 2021 13:32:52 GMT
title: Connor Shorten - PhD Researcher - Florida Atlantic University & Founder at
  Henry AI Labs
url: https://rss.com/podcasts/vector-podcast/347472
---

Hey everyone, Dr. Podgas here. And today we have Connor Shorten with me who will talk a bit about his research about lecture databases, about YouTube hopefully as well. So I'm expecting a really nice discussion today.
Hey Connor, how are you doing? Hey Dmitra, thanks so much for having me on the podcast. I'm really excited to continue our episode and maybe dive more into the deep learning research side.
 I think our first podcast on Henry AI labs went really into the detail and the practical implementation and the history of Burton Elasticsearch and then all the different vector databases and I think so now we can kind of maybe look more in the research side of things and sort of discuss together about where we think all this vector search engine stuff is headed.
Oh yeah, absolutely. And it's exciting to be recording based on the day when you actually released that video. So obviously we will link it so for our listeners and our audiences. And hey, could you please introduce yourself? Yeah, great.
So to say to introduce myself, I guess I would like to kind of like be reintroducing myself almost every like year. So as obviously I make these YouTube videos and I'm kind of like still discovering my role in deep learning research and still learning myself.
In my journey, I'm in my second year of my PhD. I finished my master's degree where I got started with research on generative adversarial networks and data augmentation, published literature reviews on data augmentation for images and text.
And this has really been my research focus is data augmentation, the idea. Primarily my interest was I started out with when I first learned about deep learning right away, I come from being a basketball player. I played basketball in college and I was ready to go deep learning for basketball.
How can this improve basketball? So one thing about basketball is when you're playing, you want to have a highlight mix tape where you have all your best moves and helps you get the college scholarship.
And so I was really familiar with that process of what it takes to be recruited to play college basketball. So I wanted to build this computer vision system that would crop out, you know, you're made baskets from full game tapes automatically.
And so I came into this problem that everyone has seen where if you try to do supervised learning with small data sets, it does not work. So like annotating data is extremely difficult.
Like you can, if you're doing it yourself, you can probably get yourself like, you know, in my case, I was annotating made baskets and video clips, which is already high dimensional data already, you know, paying to store all that data. So, you know, and labeling it was a problem.
So I said, maybe data augmentation because I'm overfitting this data. So I can try to rotate it, crop it horizontally, flip it, increase the brightness, this whole package of things you can do. Yeah, and orientation. Yeah. Right. And so it worked pretty well.
So I was pretty inspired by this idea of data augmentation.
I really like papers like Francois Chalets on the measure of intelligence where discussing the ideas of like system centric generalization developer, where generalization known unknowns is kind of, you know, matrix of known and unknowns with generalization cases.
So I hold the belief that we can kind of steer the data in the direction that enables more generalization. And the key to mocking more generalization is mostly going to be in the data space.
So I'd say I'm in this data centric AI category, which is, you know, lately become one of the buzzwords of where your camp is. I love things like neural architecture search and different learning strategies and all that, but I really love the data augmentation.
I think there's so much opportunity and research to explore this further. And then. And so yeah, so I have a few ideas of how this could intersect with vector search engines and vector representation learning. So that's on one end.
So that's kind of, you know, my research interest is in data augmentation and a bit of a background about how I became so inspired in data augmentation. So then to say kind of what I'm doing right now is, you know, I've, so I've started doing some experiment papers.
Most of my computing is managed with Google collab, which is pretty nice.
You know, like, you have the Google collab notebooks and then you have the Google Drive integration for persistence and, you know, you can make it pretty far without putting a dent in your wallet by doing it by getting too carried away. And so that's kind of how I'm setting that up.
And, you know, I have, you know, I can tell people about like, as I mentioned, beginning trying to reintroduce myself and figure out my role. So I had kind of like recently, like a high of achieving the best student paper at this ICT AI conference on something about inductive biases.
And then the next day I get my ICLR reviews back, which were not great. So, you know, and that's kind of the journey of this, you know, I'm just setting, setting forward to ICML and trying to just bounce back and stay on this journey of figuring out how to do deep learning research.
So it's definitely a high. Isn't it? It's like almost always like that, you know, like in machine learning, nothing is predictable and nothing is given, you know, like, and you need to be kind of averse to that. Well, not averse, but resistant, right? Like, okay, I'm fine.
I can take risks, but it's like a marathon. It's not a sprint. Oh, yeah, definitely. And just the disappointment of investing a month or two into a research project and then you just start running the experiments. And you're like, oh, this is not working.
And your advisor's on the phone twice a week and saying, how's it going? And you're like, not good. You know, like, so that's stressful. And, you know, anyone else going through that, I can definitely relate to that kind of struggle. Is this by the way, why you do YouTube show Henry A. L.A.
Labs? Is this why you do it? Or is there something else as well? I just wanted to kind of tap into the psychological element of it if you thought about it. Yeah, yeah, I love to talk about it.
I mean, my inspiration for YouTube came from, I guess I was just like one of these people who really enjoyed like we would have guest lectures come to Florida Atlantic University.
One that stood out to me more than anything else is researchers from Johns Hopkins came to, they had built a prosthetic limb that connects to a brain computer interface.
And they have people who have lost their limbs and they can, you know, blindfolded touch an orange and say, this is an orange, this is an apple, this is a banana. And they came to talk to us at Florida Atlantic. And I mean, it was, it was inspiring.
I like, I love these kind of seminars and just, I guess like falling in love with this kind of presentation.
It's almost like, say like to me, it's kind of like an allegace to like maybe like stand-up comedy, how you have someone who gets up on stage and puts the show on, you know, the benefit of the slides behind them. And, you know, I really like these, these kind of talks.
And that's kind of, so that's kind of like the art of it is what I really like about YouTube. I mean, I definitely believe in YouTube as the medium for communicating these ideas right now.
 You know, like, and we'll get into talking about writing on medium and like, yeah, like the different ways you can write on Twitter, you can write on medium, you can record podcasts and put it on Spotify, Apple, and you can write these research papers obviously just, you know, upload it to archive, treat it like a medium of the number of user on archive is probably less than what you get on YouTube.
The content is different too. So, yeah. Yeah. So, yeah, I really believe in the medium and then I just want to see the art form develop further. Like, I'm really impressed with what Yannick Kiltch was doing.
Like, right now he's just released auto regressive diffusion models and, you know, I'm excited to watch it and that's, and that's the fun about it is, is you have this excitement about it. Let's link that as well. It's a YouTube as well or like another show you mentioned. Yeah, yeah.
I think just YouTube, Yannick Kiltch, I think most of our viewers will know what we're talking about. I just want to make sure that I will also educate myself. So, so let's link that. Awesome.
Yeah, I mean, and so yeah, you said that data augmentation is one thing you worked on and I guess continue working on.
It's actually interesting that you did that in CV space, but there is also somehow connection in text, right? Can you tell a bit more about that? Yeah, so I, so I spent the, I think it was this, sorry, I'm getting my dates wrong. It's currently the fall.
So, I think I spent the summer spring of last year trying to transition these ideas into text. I did the image data augmentation survey in 2019 where the sentiment was still extremely hot around GANs, gender, vatricero networks. Everyone was really excited about this real fake loss.
We can generate data and then add that to the data set and then, you know, suddenly we have this very broad coverage for interpolation in our data space. So then I was trying to look into text. Text is, I say the key lesson I learned is that it's harder to be labeled preserving.
When you're forming the X prime Y, it's less likely that the Y is going to have that same high level class labels as you're trying to do things like say, like the starter kit would be random swapping, random insertion, random deletion, those kind of things.
And then you kind of transition into maybe trying to use a knowledge graph to better guide the text you're replacing. And then ideas like say mix up where you cut and paste and glue sentences together. I'm not like a huge fan of that, but it's kind of interesting. Yes.
Might as well have like drop out. It's kind of like a, you know, like I don't think there's a lot of intuition in the data space of why just smashing them together would work so well. But it does kind of work.
And then, and then I really like this category of generative data augmentation is obviously mentioning my start in gendered adversarial networks. And this idea that you learn the data distribution.
So you sample from the data distribution to learn classifiers and kind of classifiers being almost like a appendage of the generative model, which is, which is like what we're talking about with the modules, the supervised learning tasks that you append onto the vector search engine database.
It's like this task of having a generative model or say a representative vector space is kind of like the real context that built into the supervised learning task. Or at least that's the way I see it. And, you know, maybe anyone can leave a comment if they are have a different idea about that.
I think it's ill aimed. But so that's kind of how I see those two things integrating. So to connect this back to text, what we can do is text is we can use things like GPT three or more so what they do is you would prompt GPT three.
So you'd say, you know, please finish this movie review with a positive sentiment as the prompt. And then you can just remove whatever you want from the original data point. And GPT three can generate a new movie review.
And then you can blow up your data set size, avoid the pitfalls of overfitting and that kind of promise of data augmentation. So hopefully that kind of answers the question of how I did this transition from image to text data augmentation. Yeah, it does.
And I mean, why I'm asking also is because, you know, you can also treat these two sources of data in like kind of in a joint training task, right? So you can kind of train the joint neural network.
And for example, when you watch, let's say watch using the algorithm, you watch the movie or cartoon and you see some scene where, you know, one hero is kind of crying. The other one is cheering him up.
You know, now where do you pay attention to? It's also important, right? Because it's the whole scene. Now you need to pay attention, maybe just to that pin on his neck, you know, that he's not happy about. And you know, things like that.
So have you thought about that as well? Or are you still considering them as independent? Yeah, I know. Yeah, I love that idea. Like I think what we're the word that most people are using is multimodal learning. And I'd call that paper multimodal data augmentation.
And you know, just last night Microsoft released a new 2.5 billion parameter image text embedding space. You know, everyone's knows about OpenAI's clip image text spaces and the dolly, the avocado shaved armchair generation. Everyone likes that. So yeah, I mean, multimodal learning is so exciting.
Yeah, I'd say it's going to be an interesting thing with the computation of it and what kind of in what the computation requires, we're setting up these kind of tests.
I'd say, especially with video data, like you just mentioned, I, you know, I wouldn't really want to play around with video data with my collab Google Drive workflow that I mentioned earlier. Yeah. Yeah.
But it's interesting also that big players, like you mentioned Microsoft and I mean, others, they're moving in direction of increasing number of parameters in the model. But when you go to practice and you need to build a classifier, you know, you don't have that much capacity.
Like you don't want to spend that much capacity really unless you're building like a terminator level AI, which will handle all tasks that you have. But probably you won't do that because it's still not there.
So do you also think about that kind of the practical element or are you still kind of fencing the beauty of these complex models? Well, wait, do you see that? Yeah, well, I'll stake my flag in the same campus, the foundation models, researchers, and I think it was mostly Stanford.
They published this paper titled on the opportunities and risk of foundation models, some title like that. I'm sorry, it's not exactly correct. But, you know, this kind of ideology that big companies like Microsoft and Vitya Google Facebook, they'll build these big, big models.
And then what we'll do is we'll use this knowledge distillation interface to compress it into practical use cases.
And so we've seen, I'd say this started with Colin Raffle and the people who worked on my paper with the text to text transfer transform of the T5 model, not showed how you could unify all text supervised learning tasks through the same kind of language modeling style interface.
You just prompt it with, you know, natural language inference and then you give it the input or you say, answer this question, give it the input or you say, re-rank these documents and give them the document. So it's the same interface for every supervised learning task.
So yeah, I'm, and then just one more thing to kind of put in the citation context is this general purpose, like, opening iClip and it looks like Microsoft, I think they're calling it bletchly or something like that.
 But this idea of just having two vector embedding spaces and then using the contrast of alignment as the general interface for any kind of task, because as we mentioned, you can put any task into natural language, any task that you're going to do with supervised learning could be described with natural language.
So you have that kind of interface and the Allen Institute has another architecture called general purpose vision systems that, you know, unifies all these tasks, object detection, semantic segmentation, service, normal estimation, all these kind of ideas are unifying one architecture interface.
So to kind of wrap up my answer to the question, I think it's going to be Microsoft and them scaling up like crazy. Maybe they're going to run it out of internet scale data eventually. I think Microsoft has said that they can train like a 32 trillion parameter model if they were motivated to do so.
 So I think they're going to run out of internet scale data and then the data augmentation will be the next step from going from say like the 400 million image taxpayers that are now open sourced or Luther AI has the pile, which is like 800 gigabytes of raw text if you want to do something with that.
So I think eventually as you go into the 32 trillion parameter and on, they're going to use data augmentation to have these inductive biases about how we can keep scaling the data side of it. So yeah, so I think they can scale the models for a while.
Yeah, I guess they probably they are doing an amazing job, but like they are probably still writing the horse of what Peter Norby called the unreasonable effectiveness of data, right?
So like your algorithm might not be kind of as as nuanced as your data is and so just give it to the machine learning algorithm as much as possible and then kind of it will learn, right? But you know, like in practical situations, this is what I alluded to.
Like you just don't have that much data. On the other hand, you don't want you don't have that much choice and you also mentioned this. This is a very interesting topic of data augmentation in text because in images, you can do like cropping rotation and huge changes and whatnot.
In text, you can do that like so easily. For example, if you say you have a sentence London is the capital of Great Britain, you cannot put Barcelona there. It will not make sense.
So, you know, but like you can still find another example where you could probably swap cities and that's how you build, you know, the augmentation. But then there are other things. For example, if you take machine translation, you know, it suffers from hallucination problem.
I don't know if you heard about it, but like if you have certain like distortion in your data, for example, you call the websites and you also called erroneously the advertisement.
So you glued the advertisement to the source pair, source target pair, right? Now your model is hallucinating about that advertisement when the student has, right? So, and it's flipping facts. It's also switching, you know, object and subject easily. So it's not something.
And again, now I'm stepping on the territory of the model itself, right? But like, and model robustness. But I think data augmentation plays a key role in actually making sure that your model can kind of at least not hiccup on some very basic things, right? So.
Yeah, and we're completely in agreement with that. I think one other part to that story will be how, say, so Facebook has this model called retrieval augmented generation, where the whole idea is to add more context to avoid this hallucination problem.
So to kind of break down three things, you just said, I want to start off with the, yeah, the hallucination thing and transitioning right into that.
So, so I think the idea of adding more context is our best solution to stopping hallucination and maybe using consistency, contrastive loss, loss functions for the fine tuning to, to make sure they're attending on the context.
Because like I recently reviewed a paper on my channel titled open, open, open challenges in open domain generalization, some title like that, where, um, where yeah, these models, you get them the context. So they have additional context in the input, but they just don't read it.
And they just generalize as if it's not there. So fixing that problem is definitely step one. And so then to go into the second thing that you mentioned where you replaced London with Barcelona and that's the thing about tech data augmentation is, it's, it's not label preserving really.
It's harder to find symmetries in the space. It's easier to find these differences. So there's one paper. Maybe I'd like to point readers to titled on negative data augmentation.
And so they're kind of flipping the, so it's like, how do we use augmented data? Should we just keep using this, you know, kale divergence between the one hot class vectors or should we do something different with the augmented data?
I mentioned consistency losses where the loss would be, you know, the representations of X and X prime ignoring whatever the Y label is and negative data augmentation is saying, you know, push them apart.
These are not the same label. We've switched London with Barcelona. And so then I think the last thing, as we're talking about, like the practical implementation, I think you say two things, there's like two directions that which are really interesting.
And I think what you're getting to with the data augmentation is, is you want to prevent overfitting. And if you have, if you're, you know, grabbing Microsoft's 32 trillion parameter model, and you've only got 100 labeled examples, there's no way that's going to work.
So you want to prevent overfitting. And then I think kind of the second part to that story when people talk about this kind of topic is, is like storage and inference cost and obviously training costs. You're going to fine tune this.
So maybe training costs has been solved with prompting where you don't actually need to do any grading to send updates. You just give more in the input context. But then I think inference cost is solved with this knowledge installation interface.
And I think hugging face, man, I think the name of their product is lightning or something like that where it's about inference acceleration. And it looks like they're, you know, they're doing it pretty well. So I certainly bet on hugging face to solve that problem. Oh, yeah, absolutely.
I think they call it infinity, you know? Infinity. Yeah, sorry about that. Oh, it's okay. It's also like testing your memory, you know, like we remember. And I think it's still also like at some point, and I think Elon Musk is afraid of it.
Hey, Elon, if you're listening to this, hello, you know, like he's afraid of that our interface is way too slow, right? And so eventually I will basically supersede us, which I don't think so, but let's see.
 But also like what's interesting, I was thinking that maybe a little bit like developing this topic further, but it sounds you have so much knowledge on this and it's so packed, what you said, you know, like, for example, if we could use the language model itself to help us generate, you said GPT, right?
It's generative model, but there could be some others, which will kind of help us to generate things and then augment the dataset.
But there is one beautiful that I don't know if you've read this paper. It's called what bird is not lessons from a new suite of cycling, holistic diagnostics for language models. And so basically the paper essentially claims that bird does not distinguish the negations.
And that can be super, super sensitive, like in sentiment analysis, right? At least, but also like in machine translation and other downstream tasks. So have you thought about this? Like basically there is actually a now a development.
I think it's also on Microsoft side to try to bring knowledge into the language model. And you can do it in a variety of ways you mentioned knowledge graph, but there are other ways kind of to bring in the structured knowledge.
So any thoughts on that on that topic? Yeah, and this is where I'm just starting getting back into we V8 because I think we V8 is going to be a huge part of solving that problem and adding the additional context. But first I want to raise you one paper.
So from the psycholinguistic thing, I want to point readers in the direction of viewers in the direction of checklist. It was one of the best paper awards at a recent ACL conference. ACL is I think ACL EM and OP, like the top NLP conferences checklist is exactly what you say.
It's a complete suite of tests for negations named entity swapping. And it's really nice to use. It's on GitHub. So yeah, so they have the interfaces for testing for that kind of thing, which I think once you have the test, you can start hacking away, it's solving it.
It's not theoretically grounded. If you have the right test, you could hack away until you pass the test. So checklist is the test for that. But then so yeah, so then the idea of context and and we V8.
 So so V8 is so the vector search engine part and you know, Facebook paper dense passage retrieval is their current approach where they have, you know, the text embeddings, the documents and they're going to go retrieve the context so that you can avoid hallucination, hopefully avoid these kind of vulnerabilities through robustness.
But so vector search engines is what I see as being a huge player in solving that particular problem. And I see that transitioning not just from text, but image text of video text like the idea that you want to add some more context from your database to the current inference. Yeah, yeah.
I mean, V8 is doing fantastic work. Actually, we have a podcast recorded with mob and so, you know, my listener's can actually watch it and then we also had an episode with you where we covered some of the things. And you also recorded a bunch of videos like walking through the feature set.
What caught your attention in V8 when kind of if you can slightly compare to other database vendors? Okay, well, I don't have much of a comparison to other database vendors. And so I'm, you know, apologies to everyone out there working on this.
My experience with it doesn't come from the practical software engineering side of it. It comes from reading these research papers and then being familiar with these ideas. And then, I mean, V8 is easy to use. It's really well, the documentation is great. It's easy to get started with it.
So that was a huge thing for me is, you know, when I first met Bob, first of all, you know, he's a great guy and, you know, meeting this team. They're all really on top of everything and their slack chat is really great. People, you know, pitching in their problems and it's just a great community.
But, you know, what, what did it for me is, so I met Bob and then I spent about two weeks going through their documentation, the quick start, the installation set up, you know, get my data sets in there. And it's just really easy to use.
So I, and then, and then learning about all these other things like the Python client.
Like as we talk about fetching the context, I mean, we want to ingrate that into a training loop where say Facebook also recently released internet augmented generation where they're using the Bing API to bring in the context and then learn with that extra training.
So they have a Python client that lets you integrate that into your model workflows. And then something we talked about in our last podcast, I love the GraphQL interface. I think it's really cool. And I love the web demo.
So you can, you know, get started with the GraphQL interface and you can practice your queries, you know, you know, learn it quickly before you make any commitment of installing your mouse database.
So yeah, and I just think we be it is like a beautiful technology that's making my, my life is trying to do deep learning research just a lot easier. So, you know, it's awesome that they're willing to support Henry AI labs and help me continue making content on YouTube.
Well, at the same time, it's a, you know, it's a tool that helps me do what I want to do with this kind of research. Yeah. And are you like already using via V8 in your research or planning to use? Yeah.
So I haven't really made a Henry AI labs video on this yet, but it's something I'm really excited about. So one paper I recently had accepted in ICML A, not quite ICML, but ICML A, it's application to add it to it.
But it's a, it's a caros, Bert is the title of the paper and it's about, you know, language modeling with caros documentation and caros code examples and, you know, like Syek Paul, Franceschal Leigh, they're going crazy with these caros code examples. And there's so many examples.
Like you could, you have like a PhD and more organized completely online on this caros code examples to me. It's like the most interesting collection of deep learning information on the internet as the caros code examples.
So from there, there's like two ideas is like, can we build a language model that can like debug your caros code for you and, you know, open AI code X. Everyone knows that it looks like the answer to that is yes. And you know, they have the lead code, they have data sets of like lead code.
I know everyone loves lead code. And everyone is looking for a job. Yeah, code X is, you know, able to pass these lead code tests. So, you know, and I, you know, I'd say some lead code tests are harder than the deep learning debugging.
So, you know, it looks like it looks like a pretty promising solution. And so in the second project I have that I'm integrating Weaviate, what to help me do is, is, you know, Facebook is big on unsupervised machine translation.
They did a paper where they're translating between Python and JavaScript without any annotation. So maybe we can translate between caros and PyTorch without needing to, or PyTorch and Jack's even to, without, you know, somehow without much labeling. And this is very much an infant research project.
But if you have that, if you could bring the caros code examples to PyTorch and Jack's and just, you know, help people share this knowledge.
So, so this is like two of my personal projects that I've started integrating Weaviate in and then one of the project that I'm, you know, extremely passionate about and really into with my involvement with the university.
And this is kind of a separate thing that I'm not too heavy on because I don't want to like kind of push the commercial interest too much. It's, you know, and Weaviate is open source. So it's an open source software. We have, we can download it from GitHub and we have it.
So they can't, you know, take it away.
And so, so this other project is, we're trying to build patient information retrieval systems where you, you know, you come to the hospital and they start to record your, you know, coagulation studies, they, all the physiological markers and the genetic history.
And we want to go query the literature maybe. So this is, you know, as a research project and the on Institute has been pioneering this with data sets like core 19 and their system called sub.ai. Salesforce research had a system called co-search. I'm just kind of naming things for people.
Oh my god, I'm not going to describe these things.
So these are like literature scientific literature mining systems where you, you know, you want information about say COVID-19 and or, you know, someone's coming in there with some obscure disease, you want to be able to query the literature with particular information about this patient.
And so this is the information retrieval problem that, you know, we're super interested in as spectrature search engine people. So we're trying to turn these patients into, which is what I have is mostly tabular data.
You might get a little bit of medical images, some clinical reports for some text, but, yeah, mostly tabular data. So we want to encode that into vectors, send those vectors into the scientific literature, and then maybe there's some clinical trial, you know, because it's so much data.
Once you really download, like say the core 19 data set from the on Institute, you'll realize that, you know, 500,000 papers about COVID is nothing anyone could read. You know, I already know this from reading deep learning papers. It's like no one can read this.
And even like, if you go traditional way, and I wanted those at the top in, in this area, you know, like if you go traditional way, let's say you have a keyword look up, right?
So keyword search, you would have to build like some kind of synonym layer, which means you need to understand what you're doing, or you will need to hire somebody to do that.
And that's like an additional step, which kind of like, you know, doesn't reduce the journey for you. You have to do that and this is that you feel like you have more control, maybe, but at the same time, it's very laborious.
So at the same time, similarity search kind of doesn't have that boundary, right? So essentially you haven't coded it and now you, you know, now that the challenge, the complexity moves more into the space of choosing the right neural network and then choosing the right database.
Everyone knows which is the right database. So, but anyway, but I'm just saying, like, but, but I'm just saying, like, do you think that similarity search will completely supersede keyword or you still see some synergy between them? Yeah.
And well, I like, before I get into saying my opinion on this, I'd say that I'm not the expert on keyword search. So, so here's my opinion on it. I, you know, we V8 has a symbolic filtering where you can still do symbolic searches. You can still do the keyword filtering.
You can still have these symbolic characteristics. And, you know, I'm in the same, I believe things like what Gary Marcus talks about, about, you know, it's not really robust to these symbolic queries. What we mentioned earlier, where you insert negation and it might completely throw it off.
So robustness is like not completely solved that. I was reading a paper this morning called from DeepMind Researcher's data augmentation can help robustness. It was like such a on the nose title, like that, like data augmentation helps robustness. So, so yeah, solving robustness.
And I'm, you know, I saw a, I'm not like, I still think solving robustness is a huge issue for this. It's not completely put together yet. Yeah, absolutely. I agree. I agree.
So, but like, yeah, you mentioned you are not an expert on keyword search, but at the same time, I think you were the expert of using like Google, right? So like you still type keywords.
And, and I think psychologically, you still expect, you know, the snippets to contain some of your keywords as a validation that the search engine got it, right? So like otherwise, search engine maybe that just, you know, returns you garbage in return to what you want.
Yeah, and that's why I think like like the page rank, transition dynamic matrices, though, those kind of things that that's like, it won't be enough to just have the vector search engine probably. You'll probably need some kind of like tuning layer.
And that's why, so we've got has the Python client. As I mentioned previously, a research project for this would be to integrate that Python client into the training loop of the, you know, whatever is doing the supervised learning task. So it kind of isn't just retrieving.
It's like when we talked about the difference in information retrieval and approximate nearest neighbor search, it's kind of like the semantics differences between the things you're encoding, where you might be encoding a like the email title and then the email body.
And so you have these different kind of like transitions between the categories of objects you're encoding.
So, so yeah, like the, you know, I still think that there's like a layer of, I don't know how to describe it, maybe like that system one system two, I know people like that analogy, but there's some kind of layer between keyword search and vector neural representations.
There's something in the middle of that. And, you know, I don't know what it is, but yeah, I guess page rank. Yeah.
Yeah, like basically you're talking about sort of even, even after vector database has returned to the nearest neighbors, you still have a sort of liberty to apply a re-runker, right?
Because and that's where your business logic kicks in, like the rules, the product, the vision, the design, there are so many inputs into that process of ranking.
And then ranking obviously is like a huge research area as well, you know, with the click biasing and things like that, right? Yeah, I mean, and it's also interesting. I just crossed my mind that yesterday, Richard Sorter announced his search engine and U.com.
And did you have a chance to check it out? Basically for listeners who didn't check it out yet, so it's a search engine which summarizes the web pages and the kind of documents and so on. And so you are kind of, it makes it actionable.
So just one example, they can find you a code snippet on Stack Overflow that you can actually copy paste. And that's just one example, right? But there are plenty of more. Any thoughts on this? Yeah, well, I mean, first of all, Richard Sacher, his research has been incredible.
And as I mentioned earlier in the podcast, I was listening to systems co-search from Salesforce Research was, he was one of the authors, I don't know who led the project. So yeah, U.com, I mean, it looks crazy.
Like, have I used it quite not really yet, but I definitely believe in the concept and yeah, the research is pointing in that direction. It's exciting.
But do I think like, solely neural system? Yeah, I mean, designing new interfaces around search, started to go around that a little bit as I'm trying to like think, well, I talk, but yeah, the U.com thing is exciting. New spaces for search engines.
It's hard to even completely conceptualize it, I think because it's such a, you think of Google as like this giant, undistructible search engine, but that's really not the story. There really is a ton of research and search engines. Yeah, yeah, but actually, I'm currently working for WebScale.
So, Changes, which I cannot mention because it's my client on the NDA, but we basically have all the charts and we know that Google is like 97%. And then everyone else is close to the bottom. Unfortunately, well, of course, Bing has a couple percent of the market.
And then it kind of, if you go inside a specific country, the split might be different. Like, if you take Russia, for example, Yandex is on top and then Google is following them, but very closely, you know, but overall, globally, Google is just somewhere beyond the sky.
So, you need to kind of differentiate a lot, you know, like you don't want to build another Google.
It's almost like Peter Tills book, you know, zero to one where he says, if you are building another Facebook, you're not learning anything from Mark Zuckerberg or if you're building another Google, you're not, you're not learning anything from the Google founders.
Like, you need to build that one, right? And I think Richard is trying to build that one probably. So, yeah, I mean, it's an interesting direction that he's trying to involve the AI much deeper in the process, probably already surfacing, you know, users. That's fantastic.
Yeah, yeah, I don't have anything to add other than just shared excitement about what you.com will become. It's certainly exciting. Yeah, absolutely. All the best Richard. Yeah, and you actually I wanted to make a slight segue into you shared like a ton of information today.
I wonder how do you keep up with so much stuff happening? Like, what are your preferred sources of information? Like, obviously YouTube is one, but, you know, there is also medium. There is publications themselves.
How did you structure your sort of consumption, you know, parts like the pacing and kind of where to pay, put your attention and so on? Yeah, that's a great question.
And, you know, early days of my podcast, I was doing the Machine Learning Street Talk with Tim Scarf and Yana Kiltcher and Tim asked Jonathan Frank, the author of the lottery ticket hypothesis, the same question. Like, what's your information diet? And I thought it's a really interesting question.
So mine is, you know, like most people out there trying to be good at something. It's chaotic and it gets overwhelming and I get really stressed out sometimes. So I don't know if this is the best advice to follow, but like, here's what I do.
So I, you know, I'm very active on Twitter, like maybe to the point of detrimental to my health, like I checked Twitter, like, all the time. Like, so I'm always refreshing Twitter and seeing the new headlines.
And so I, when I see like an archive link, I'll try to, like, if I like it, I've tried to discipline myself to be like, don't just like it. Like read the abstract, like get a couple sentences in because clearly, you know, the titles caught your attention.
So, so Twitter is really where I get all my news. And then the art form of making these YouTube videos, I mean, like Yana Kiltcher and Tim Scarf that I mentioned, the Machine Learning Street Talk, these kind of, this kind of medium. It's, I watch that. It's pretty good.
I think I watch it on like, Exploratory Street also Alexa, Miss Coffee Bean to kind of go on the list, you know, they're not the only ones doing it well. A lot of people are starting to make really great YouTube videos. And I love that kind of medium of showing these things.
So on my, my work, my like, my workout, say I'm a basketball player and I've got to work on my deep learning skills is it's mostly about reading these papers. My experiments, I'd say the coding part is not super challenging.
Thanks to things like Keras coding examples and like thanks to them, major thanks to them because that saves me so much headache in just getting running. So, so yeah, I try to, I try to read like five papers at a time. I tried to switch, I try to set 20 minute timers, drink a lot of coffee.
And what else do I do? Yeah, I guess that's it really reading the really reading the papers. I mean, if you make paper summary videos and write blog posts, that's also a huge way to retain it. I try to talk to a lot of people also just, you know, I try to keep a lot of contact.
Like I'm organized all this through Twitter. So like, you know, I might just send messages to say, Syek Paul from who makes, I think he works at Cardid and he makes, he's one of the leaders of Keras code examples. I'll send him ideas. I'll be like, you know, I saw this paper on Twitter.
I think, you know, this reminds me of what you're doing. And, and yes, I guess overall, that's my information diet. I'm probably leaving something that I didn't really, you know, prepare something for this, but no, it's okay.
I mean, it's also, it's also great that you're speaking your mind, but and things that really stick, you know, you mentioned them, right? But where on that scale, you would put medium, you know, the blogging platform where it kind of thrives with tutorials.
And sometimes these tutorials, they're kind of okay, but you kind of like, okay, are they going deep enough? But then there are other things where they summarize papers in such a way that they actually try to explain it.
It's almost like popularizing science because you do want to breed that next, you know, generation as well. And maybe you will have some feedback to your ideas because don't you think when you publish a research paper, you know, for the most part of the humanity, it's dry text.
For some, it's just Greek, right? They will not even understand it. They will never, they will never read it. And so, but they still might be curious, like, okay, how, you know, robots make decisions or something like that.
You know, so, how does my car, how does my car keep the lane keeps the lane? And actually today I was driving, I was driving to work and I was like, my car actually switched to the lane keeping mode. And it was telling me that I should not, you know, steer to the left that much.
So it was actually steering to the right. But the moment it noticed that I put my hands away from the steering wheel, it actually started alarming me and saying, hey, I'll sleep or something, you know.
So it's also like kind of caring for you, right? In a way, so it's not trying to do so much more work, in that sense.
Yeah, like, the idea of popular science, I mean, you know, I'm recording my podcast behind a bookshelf, like it makes me look smarter, but I only really, I only really read books like, you know, like the book of, I mean, the book of why is a bad example.
That's a really great book, like technical and I really really like that one. But most of these like popular science books, I'd have to be like on an airplane or something like I, or are in the same with the category of medium articles that are popular science.
Like, you know, I read research papers only, not to like be dismissive of anything else, but that's just like the question of what particularly do I study. And in my approach is very people-centric.
Like, you know, like when, say, Chelsea Finn publishes a new paper on Twitter, I'll go read that because I kind of have been following her thinking, like Jeff Cloon is another example with the AIGA's or François Shalide.
These kind of people like I, like Michael Bronson with the geometric deep learning is another great example. I hate doing these lists.
I never like to do these lists because it's so endless, like the vocabulary you need to kind of assess, like I've left off so many people, but you know, I like the people's centric focus and I try to get to know these people and understand like how they think of these things.
It's like the same thing as you go to the conference. Sometimes you don't go to that specific topic.
Maybe when you're a little bit more junior, you do, but later in your career, like academic or industrial, you actually go to listen to that person because they might not give you any novel idea, but they might give you so much experience that you daily, like really need, right?
Yeah, and just following the timeline of their work, it helped, like their newest work will help you realize, oh, that's their thinking in the past work too.
I kind of see how they're thinking about these things. And it's like, you know, everybody thinks so abstract. They have this idea, this vision, and it can be hard to communicate the vision in writing or videos.
So yeah, just like you said, I think just repeated exposure to the same person is like, hopefully that's Henry AI last thing. Yeah, absolutely. I'm pretty sure. I saw some really great comments underneath your videos, you know, some people were saying, I can't wait for the next one.
So you definitely doing great job there. So could as to you for doing that for so long actually. I don't know for how long you've been doing this, but you have a ton of videos. Yeah, and I really appreciate it.
You know, the people who keep commenting, I, you know, I recognize your profiles, and I do really, really appreciate it. So it helps me keep making the videos and staying convinced of that medium of YouTube being one of the ways to express these ideas.
I'd say like even, even more so than writing papers that you submit to these conferences. Sometimes I, you know, I think making a YouTube video can be a powerful way to share ideas.
I don't know if I want to completely put my flag on that idea because I, you know, these reviews, you do get some really good reviews. Like as I mentioned previously at the beginning of the video, I, you know, I literally got smashed on my ICLR reviews.
They were not good, but I got, I got really high quality feedback. So, yes. You know, you're learning from it. You're learning. Right. Yeah. Actually, one of my managers used to say feedback is gold.
So even if it feels painful, take it because because the problem is that sometimes, especially as you grow in your career, you know, at some point you will be the role model for some other people. Now, where do you get the feedback from nowhere? Because you're the person giving feedback.
But you still need to grow. You still have pains, you have doubts, you have ideas, you need validation. And maybe you're doing something wrong as well at some point. Maybe somebody is intimidated to tell you that because you are at the top. You are like the boss or whatever.
You know, like who gives you feedback at that point? They actually recommend to turn to, you know, professional coaches and kind of those people who can actually steer you in some direction. Right. Oh, maybe you can unload your thoughts.
Have you found yourself in that situation? Or what, what do you think? Yeah. Well, I mean, I'm in a lucky situation where I do have a formal PhD advisor that, as I mentioned, I speak on the phone with very often.
And, and you know, my PhD advisor and I had a relationship for so long that he like introduced machine learning to me. So it's like, I was a basketball player, you know, taking classes. And I, and so this was my introduction to machine learning.
I like, I hardly understood like, you know, like a tea test statistical regression analysis before this class. So it's like, so I'm, I've had the same advisor for a long time in that regard, like a formal academic advisor.
And then meeting people like Bob and, you know, you and I as we talk now, I, you know, trying to reach out and pick the brains of people and see what they think. I guess. Yeah. So basically they are like, they become like, you might have multiple role models.
And sometimes, you know, like they also say, you do not need a physical person with whom you talk, but it could be some kind of online person. Like for me, it used to be for a long time, Elon Musk, because I've been focusing on building startups.
And, and his approach to startups was not like, hey, you know, go unleash yourself, get rid of your doubt and just do it. No, he's so deep into what he does.
Like at some point, I want to record a podcast where I would like to talk to you or talk to somebody to actually explain and kind of does it resonate with you, like he's thinking, like, first, you need to try this before automating this.
You need to repeat it several times to learn new mistakes and blah, blah, blah. So it's like an amazing way.
And he like build this kind of, you know, a thought machinery that he applies to any problem, right? So any problem that lands in his hands, he's like, I can try it step by step like that and see what happens. And maybe at some point it just drops out and you're like, okay, I'm done here.
I'm moving to the next one, right? So I'm not going to waste my time. And he's a super productive guy, as we know. So I mean, sometimes it could be just an online person that you follow. And as you said, you do this on Twitter, like you said, like maniacally refreshing the tweeters.
So just stay stay safe as well there. But at the same time, I think the respiratory time in your life, when you're learning a ton. And later in your life, you will be kind of generating fruit out of it mostly. Or maybe you will be telling to other people and maybe inspiring them more and more.
And then leading some research groups and the work, you know, teams. And that's that's totally fine. But I also wanted to call out your idea that I think is quite instructive for many of us.
And hopefully to our listeners that yes, do go go and read papers because as Andrew Ang put it, he said, if you read a paper every weekend, let's say you have a full-time job, you don't have time to read it, you can read it on the weekend.
At some point, and he also recommended to start coding, you know, like actually you didn't find the code for it, just try to implement the idea, right?
At some point, after reading the papers, you will actually start generating ideas because you will find gaps in the thinking of the authors on all of these papers.
And nobody is doing perfect job there. They're doing the publishable work, right? And so I think that resonates with you as well. Yeah, definitely.
You definitely like switch gears where you become an idea machine like you say where you read a paper and you'll have like a billion ideas for how to extend it. And then you'll transition to this part, which is what I'm learning now.
And, you know, as I'm in my last year, I've been two years in my PhD and the transition for me is going from idea machine to, okay, can you really build the idea for real? Do you really know how to test this? And so, and that transition isn't super obvious.
And it's painful to be going back and forth between, you know, theoretical idea machine.
I'm reading these papers because like in terms of like that flow state of creativity that you get into when you're when you're working on things, for me, personally, reading papers is like the most satisfying thing. I feel very like productive when I'm reading papers.
I might, you know, I feel good. But when I'm engineering things, I feel more pain, man, because it's more painful, I'd say. Yes, yes.
And this is where, of course, you do want to have those oiled well, well, well, oiled software systems that you don't need to waste your time setting things up or running out of disco, whatever, you know, heaven so, so frequently.
So like even the innocuous things like before I had integrated Google Drive with Google collab, and it would crash. And I feel like I've just lost 10 hours of running this thing. So, and that is not good.
Like, this is I think what Joel Spolski said at some point, you know, the co-founder of Stack or Flow, you know, he said like, imagine that you want to print a piece of paper and you log into your computer and it says, please upgrade the driver.
So you upgrade the driver and then operating system says I need to reboot. So it reboots and it basically waits 10 minutes of your time. And then you, and then again, it says, hey, actually, I cannot print because you ran out of something now. Again, it installs them.
And you like, instead of solving the problem, you become the administrator of your computer, right?
And that's the same, the same thing can happen so much, so often in software, you know, development and research as well, because, because I think somebody will put on Twitter, we do not actually choose between big and small, like do a lot of things and do like small amount of things.
We usually choose between small and nothing. And so I guess when those things eating a lot of your small time, right, to nothing, you're like frustrated and you're like, okay, I'm just down the rabbit hole.
What am I doing?
And so I think tools like VEVIates save a ton of time and everybody who is innovating in this space from the direction of usability, you know, like and saving time, shaving those minutes off of, you know, your experience, I think that will save so much time for your thinking as well.
Yeah. And before VEVIate, I was doing a little bit of the sponsored content work, and which for me is great because I get to talk to these people and they teach me a lot. And so this is with the term in AI, which is now a part of you who have packered.
And so yeah, they're building the hyper-pram, like distributed training hyper-pram, reorganization, which what we're talking about, like the administer of the system, they're doing a lot of this work.
And you know, as anyone, I'm sure people listening to this have gotten smoked with the cost of one of these experiments too. So it's not just your time. It's not fun. Yeah, actually, you reminded me of on Google Cloud. It was a tutorial, like a workshop. It was a free one.
They even like gave us food. So you just show up, they video and then they tell you things. And it was a practical one. And I remember one of the instructors, he was not an employee of Google, but he was certified. And you know, like he said, hey, now we're gonna spin the Spanner cluster.
And Spanner is the my SQL planet scale with all the consistency and semantic guarantees using atomic clocks. And there is like a fantastic presentation by one of its engineers that I have in my recordings. I have not published yet because I don't know if Google will try to sue me.
But you know, the idea is that it's a fantastic system. And there is a paper as well. And then the guide, the teacher, he said, well, hold on. Don't spin too many of them because I get the bill. And last month, I got a bill of $4,000.
And Google could not reimburse it because they said, you're not an internal employee. So he was like, it's fun. But you know, to the point when you might. Yeah, it's funny. It's funny now, but it's not funny at all. Yeah, that determined AI calls it lunch and learn.
There are this kind of concept for deep learning, or like I'd say to science content, like even like, you know, with physics and they're going to be doing experiments where it's expensive. So we're not going to each be doing it.
We're going to watch one person do it and kind of gather around as a community. And yeah, I see that as being a huge part. Just like Uber eats coupons, I think is a brilliant interface for it. And then everyone attends the thing. But yeah, I love that kind of.
And then just quickly, so like one thing we're working on at Weve 8. And as people have seen with hugging face data sets and the Kaggle competitions, well, hugging face data is a little different, but it is hosting the demos cheaply. So that so in Weve 8, we're working on this.
The wiki data is going to be the next big release where we have the pie torch, big graph embeddings, which is the graph structure makes it different from say Wikipedia, because it's really good at entity embedding.
 As we mentioned London and Barcelona, if you construct a knowledge graph of Barcelona compared to London, that's going to have a better entity representation using learning techniques like deep walk or note to veck or maybe maybe like a graph convolutional network with an auto encoder loss, but probably deep walk or note to veck is what I would say is, I mean, I'm not completely caught up with that, but anyway, so having that kind of data set, the wiki data, and now it's cheaper.
That's the huge difference. That's the change in deep learning is hugging face is hosting all these data sets, so you don't have to host them yourself. You can just quickly access them.
 And with Weve 8, it's even more exciting, in my opinion, because they're hosting a vector search engine with model inference, I mean, hugging face is doing model inference too, as we talked about infinity where they've got inference time data like milliseconds for these massive models is, yeah, is you don't have to pay for the hosting of these things, which is obviously good.
Absolutely, absolutely. And also not like massive with hosting things, because that's also the cost of maintaining is the cost not to neglect. So absolutely. Yeah, yeah. Absolutely. Hey, it was such a packed conversation.
I think the show notes will be infinite, because you mentioned so many names, so many articles, and that's fantastic. Thanks so much for doing this. I wanted to just still kind of end on kind of a little bit like that philosophical stance, which I usually do.
And I think we touched a lot on that and thanks for doing this. But like in summary, what drives you? Why are you doing this? What you are doing? That's great question. I mean, I guess like, and I've heard, as you mentioned, Elon Musk, I've heard that he says, like, I want to be useful.
That's one thing he says. Yeah. And I guess in the same way, trying to do the useful thing.
And I guess like, obviously, I like these big grandiose visions of things like helping with health care and self-driving cars and helping with poverty and creating housing climate science, all these kind of things, obviously.
So obviously, there are these big grandiose goals that I think we all share truthfully.
But then it's more of a question of how do you stay in the grind of it? And how do you keep waking up and keep getting at it? And so I'd say that kind of heuristic of just trying to do useful things every day is actually a pretty good guide. And so we all share these big visions.
But we need the motivation to pick ourselves off the couch and achieve to do that. Yeah, absolutely.
And it also sounds like you mentioned you played basketball and you continue playing that, right? So that thing, when you do the sport, you need to be persistent, right? And your body sometimes doesn't want to do it, maybe. But you know in your mind that you do want to do it.
And so that persistence, I think, also translates into, you know, the research and keeping up with things, right? Yeah. Yeah.
 And to stay on that kind of analogy, I'd say like the physical pain of basketball is like, you might hurt your knee, you might have some tendonitis, is that kind of physical pain or the physical pain of when you're doing conditioning and you can't breathe, that you're going to have that same kind of analog with this kind of mental work.
And it'll manifest itself in like depression and burnout. And so you have to be like, as you do more training, you get better at the pain of the injuries. So to say like it's like injuries to your mind and the same kind of analog as physical injuries would be.
And I think understanding that and accepting it and dealing with it is important as well. And then it kind of translates into maybe some other region of your brain when you have this page from like, you know, reviews or like your experiment going, hey, why are you can make? Oh yeah. Oh yeah. Fine.
I got to get a cup of coffee and you know, in five minutes, I'm okay. Maybe. Yeah. The coffee is the key supplement. Absolutely. Corner, thanks so much. This was such a fantastic conversation. I'm pretty sure we can repeat it. Have another one.
And I can't wait to see what development you're doing with VIAVIAT and also in all your research projects. You know, stay active, stay hungry, stay foolish, as Steve Jobs used to say. And I think that's fantastic what you're doing. Thanks so much. Thank you so much for having me to meet me. Bye.