---
description: '<p>Topics:</p><p>00:00 Intro</p><p>01:30 Doug’s story in Search</p><p>04:55
  How Quepid came about</p><p>10:57 Relevance as product at Shopify: challenge, process,
  tools, evaluation</p><p>15:36 Search abandonment in Ecommerce</p><p>21:30 Rigor
  in A/B testing</p><p>23:53 Turn user intent and content meaning into tokens, not
  words into tokens</p><p>32:11 Use case for vector search in Maps. What about search
  in other domains?</p><p>38:05 Expanding on dense approaches</p><p>40:52 Sparse,
  dense, hybrid anyone?</p><p>48:18 Role of HNSW, scalability and new vector databases
  vs Elasticsearch / Solr dense search</p><p>52:12 Doug’s advice to vector database
  makers</p><p>58:19 Learning to Rank: how to start, how to collect data with active
  learning, what are the ML methods and a mindset</p><p>1:12:10 Blending search and
  recommendation</p><p>1:16:08 Search engineer role and key ingredients of managing
  search projects today</p><p>1:20:34 What does a Product Manager do on a Search team?</p><p>1:26:50
  The magical question of WHY</p><p>1:29:08 Doug’s announcements</p><p>Show notes:</p><p>Doug’s
  course: <a href="https://www.getsphere.com/ml-engineering/ml-powered-search?source=Instructor-Other-070922-vector-pod">https://www.getsphere.com/ml-engineering/ml-powered-search?source=Instructor-Other-070922-vector-pod</a></p><p>Upcoming
  book: <a href="https://www.manning.com/books/ai-powered-search?a">https://www.manning.com/books/ai-powered-search?a</a><em>aid=1&amp;a</em>bid=e47ada24&amp;chan=aips</p><p>Doug’s
  post in Shopify’s blog “Search at Shopify—Range in Data and Engineering is the Future”:
  <a href="https://shopify.engineering/search-at-shopify">https://shopify.engineering/search-at-shopify</a></p><p>Doug’s
  own blog: <a href="https://softwaredoug.com/">https://softwaredoug.com/</a></p><p>Using
  Bayesian optimization for Elasticsearch relevance: <a href="https://www.youtube.com/watch?v=yDcYi-ANJwE&amp;t=1s">https://www.youtube.com/watch?v=yDcYi-ANJwE&amp;t=1s</a></p><p>Hello
  LTR: <a href="https://github.com/o19s/hello-ltr">https://github.com/o19s/hello-ltr</a></p><p>Vector
  Databases: <a href="https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696">https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696</a></p><p>Research:
  Search abandonment has a lasting impact on brand loyalty: <a href="https://cloud.google.com/blog/topics/retail/search-abandonment-impacts-retail-sales-brand-loyalty">https://cloud.google.com/blog/topics/retail/search-abandonment-impacts-retail-sales-brand-loyalty</a></p><p>Quepid:
  <a href="https://quepid.com/">https://quepid.com/</a></p><p>Podcast design: Saurabh
  Rai [<a href="https://twitter.com/srvbhr">https://twitter.com/srvbhr</a>]</p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20221001_071023_bbd8f38e993da204036dc514900a891b.png
pub_date: Sat, 01 Oct 2022 07:32:38 GMT
title: Doug Turnbull - Staff Relevance Engineer, Shopify - Search as a constant experimentation
  cycle
url: https://rss.com/podcasts/vector-podcast/638830
---

Hello there, that vector podcast is here. We are rolling in the season two of this podcast. And so today we have like a breach, so to say from US to Finland. And I'm super excited to talk to Dr.
and ball, staff relevance engineer choppy fine and that gives to be a CTO at open source connections, the company behind so many tools for us relevance engineers and relevance product managers as I am today. He's the original creator of Quepid and explainer and also learning to rank rank. My shirt.
Yeah, let's search. Yeah, cute.com. Awesome. Great to have you here. Hi, how are you doing? I'm great. Yeah, I'm doing great. Excited to chat about where search is going and the exciting places that are, you know, search is going ahead and everything. So finally, I get to be on this podcast.
I'm really excited to be here. Yeah, absolutely. Long overdue and you are the legendary guest. So I'm super excited to talk to you.
And a lot to cover, but before we begin, could you spare a few minutes talking through your background, how you ended up in search, was it an accident or was it not, was it? It was mostly an accident.
So what happened was so for a long time, the first chapter of my career, the first half was being C and C plus plus developer. And I kind of got really into performance, so optimizing speed and in native code. That was a lot of fun.
And I moved down here to Charlottesville in 2012 from the Washington DC area couple, so I was a couple hours away from my work. And I found that like, you know, I was kind of at the time, especially being one remote employee for an in office company is just a nightmare.
And we had this neighborhood block party and I decided to wear a nerdy t shirt just to see like, oh, maybe I'll meet other developers. And I think this shirt said something like my code doesn't have any bugs. It just has features or something or random features.
And I so happened to run into Eric Pugh, who's the founder of open source connections and sort of one thing led to another and I got, I was like, oh, this seems cool. It's a small company. Always wanted to try out consulting and contracting.
And so, yeah, we ended up getting at the job and getting more and more into search. Yeah, awesome. And you spend there how long seven more years about eight years eight years. Yeah, it's a long. Yeah, a long time. Yeah, it was a lot of fun. Yeah, and you've done so much.
I mean, I was literally in my previous job at AlphaSense. I was my last. So I spend the 10 and a half years and my last half a year. I was focusing on learning to rank. And I could find I could not find a better resource than hello LTR. Ripple on GitHub that you have. Yeah. Yeah.
And it was it was an amazing journey because first of all, I had to learn it on the other hand, I had to build like what we could call maybe an infrastructure pipeline of flywheel of success. Yeah, so you're going to be right.
So yeah, and then you train the model you test and then you validate and so on so forth. Validate with the users maybe a test. It was awesome. I built it entirely on your on your Ripple. And I even thought that said some was the PR or issue that I created. But anyway, I'm sure yeah.
Yeah, it seems like to me sure you contribute a lot to like I know you contribute a lot to keep it too. And I think it's a great work or constantly huddling on you know, keep it and trying to keep it keep it going.
Yeah, and I mean, that's thanks to your curiosity that you created it you you kind of saw the nation feed for it. But also when I came across it, I mean, it was very straight forward to start using it. And of course, it was also learning experience.
But now every time I joined, let's say new new gig, you know, previously silo AI with a large client, you know, WebScale search, I brought it in. I said, there is no other tool that I know we should just try this and then try to write a Tom Tom right now as well. So we have it. Oh, that's awesome.
Yeah. Yeah. Yeah, Quepid has a funny origin story where in sort of like dovetails with my story, but for a long time, opens for connections. And I think this is true of a lot of places in the early 2010s. It was pretty easy to build you we would build these beautiful search apps.
And that would be part of our, our consulting as we build these search apps. And they would be beautiful and they look pretty, but then only at the very end with someone type in a search and you would see, like these results don't make any sense. And then like people panic and they want to fix it.
They're about to go to market. They can't release like this. And they realize of the search engine isn't some magic black box. It's actually this thing that we have to configure and tune and stuff.
And so Quepid actually started because, and there's an old Lucene revolution video that talks about this. But John Barryman, my coworker at the time, and I would go to our client also in Charlottesville, Silverchair. And we were helping them develop these search applications.
And as like they would tune like constantly we go back every week and try to fix something. And then we would end up breaking something else. So I finally got kind of tired of it.
And I just sat there and built like a at the time of Python flask app that was just let's show these search results and like just label them as good or bad. And so we don't have to keep going backwards on on our quality.
And he was I was literally creating the apple he was sitting there like trying to tune search with with our client. Reena Morse at Silverchair. So it was kind of like hacked together in an hour and then we started using it. This is so cool.
And I mean, for me, Quepid, I mean this topic of quality assurance and searches big, I think, right. And maybe under valued. I'm not sure. Yeah, it is. Yeah, totally.
But you know, like at Alphasense, for example, I had access to people who used to be financial analysts or they deeply understand, you know, content. And it's so important to understand content like brokerage versus, you know, sell site versus buy site.
What is it? What is this? You know, what people are looking there for. And I remember one of the guys on that product team, he said, well, this is fantastic. Now I can explain to you what I need in terms of relevancy without getting into the weeds of your algorithm.
And you then hold away and get there, right. So I mean, hold away. That's fantastic. And I remember like at WebScale search, we got stuck a little bit like optimizing our KPI metrics, like one of them is click through rate.
And I remember when I onboarded Quepid, we generated literally 70 juratickets as a result of analyzing first annotating, rating the queries and then analyzing what went wrong there, right. And probably like half of this at least was data related issues, which you would think, hey, this is Quepid.
This is about relevance and not about data. Oh, yeah, you find that stuff all the time. Yeah.
 We would, we would, we kind of had this model at open source connections that worked well where, you know, you, you come in as a consultant, you're trying to, we would, we started consulting in the search relevance basically exclusively and we would come in and instead of sometimes, you know, it's a very data driven process and it needs to be, but on the other times it's like just jumping in.
Let's start with 12 queries and let's label what's what the good results are and improve those. And then we would kind of go through these sprints of, okay, let's take the next 12 queries, let's take the next 12 queries.
 And you just constantly like gradually expand the envelope of what you're tuning and actually worked really well as a practice for improving relevancy without having to spend like sometimes, you know, the, you know, places don't necessarily have months to spend boots trapping like a click stream pipeline and understanding clicks and all the biases and things and that.
Or, you know, and so it's just a really straightforward way to get started on the problem. Yeah, absolutely. And I don't know if you could imagine this, but when I was a consultant, I had like a breather two months between, you know, that client and Tom Tom.
So I consulted you startups, one of them in the US. And so they look at us, we said, you know, you come in and they think you can do magic and I said, okay, maybe I can't, but I will not tell you. So no doubt. In front of you.
So I came there and I said, hey, how are you doing QA and it should need this massive Excel with colored legend and like what not. And I said, well, this is cool, but I think it's not repeatable. And they said, yeah, it's a big pain point. I said, let's do something better, something else.
And I introduced QBIT, it took probably a couple months. Then I lost touch with that startup. So I switched to consulting others. They didn't reach out. Then I reached out and I said, hey, what's the status.
And they said, you will be surprised, but we have moved the whole QA process to keep it as like wow. Oh, that's awesome. Yeah. This is the touch and feeling of what you have created for your use cases, worked for someone else's case. Isn't it amazing feeling. Yeah, it's great.
Yeah, it's funny how that works. You know, if you solve your own problem really well, you know, there's probably other people out there that are like you that have the same problem and appreciate that perspective on the problem.
So, so yeah, I think that that's kind of a truism is like, don't worry about solving a if you have a need, if you have a need, solve the problem for yourself as the most important audience. And it will sort of naturally find the people like you have the exact same problem. Yeah, fantastic.
And now you are at Shopify. Yeah. So how do you structure your work there? This is my how part in the podcast as well. You're building your product is relevancy in many ways, right? And maybe performance of the search engine because there are trade-offs. Yeah.
So how do you structure the whole process experimentation, evaluation? Is there anything you could share and understand that there could be some private things you don't want to share. Oh, sure. Yeah. That's okay.
So for context, our team works on the relevancy of all of the Shopify storefronts, so all the little shops out there. And that's a really interesting process because you could imagine the impact there is very variable per shop.
And we don't, of course, we don't want to like tank someone's sales, but at the same time, if we see something doing well, generally, then we, you know, we want to promote it.
So in the last part of the process there, it's, it's very different than in the past, I've worked on, you know, you work on one search engine, you might work on one Shopify store, so to speak.
And that Shopify, the challenges, there are hundreds of thousand millions of little, you know, shops that you Shopify search.
And you, how do you find an algorithm or algorithms that support those, that, you know, what's going to work well for every possible ecommerce use case, like in some cases, of course, there's a lot of apparel on Shopify.
There's a lot of, there's a, you know, all kinds of things people make up businesses on Shopify Shopify is wants to very much support creators. How do people even search, what do they expect when they search on these stores.
So the good thing is when I started at Shopify, there was already some amount of, you know, data flowing through in terms of like knowing what people are clicking on and stuff. So I was able to pretty early on start developing a click model.
So a click model is something that looks at how users click on results and sort of like in aggregate gives a search result, a probability of relevance for a given query.
And we noticed that people skip over certain product a lot when they search for shoes, maybe for some reason the shoes search shows socks at the top, we know those socks are probably not relevant. And we know that whatever they're clicking on below that are very likely relevant.
And so we can, we started, you know, the good thing that Shopify we kind of could were able to start using that as like a test set. And then, of course, tooling is very key to my heart. So like one thing that we've done at Shopify is we built a large tool tool chain.
We sort of to do offline experiments called called boogie using that data. And it's about what you would expect from using something like Quepid we can take this data we can sort of see like did we improve things to things get worse with our ideas.
And then of course we release to a test we look at our normal conversion metrics and that kind of thing. And then we do a lot of analysis of our A B test and we we will graduate things for production. So at a super high level that's that's nothing there.
I don't think is that different than most places other than you know we have the challenge of so many different shops and things that we have to sort of solve for. I mean, this sounds so fantastic. It's like almost fixing or improving search for the entire e-commerce right and maybe even.
Yeah, that's that's part of the challenge and the draw on one of the reason, you know, I met Shopify is just because it's it's for you. There are people on Shopify who sell 100,000 products. There are people who sell one product right and there are.
There are really there are stores that you use that you may not realize are Shopify stores and then there are stores that are very clearly like Shopify at the end. So that you know Shopify in the URL or in the footer and that kind of thing, but some you know your local.
I think my local running shop. Shout out to rag and mountain running. And then there's a place that you know at the farmers market and a lot of those places you Shopify. But then there's also like larger larger brands that use Shopify. Yeah, that's amazing.
I've seen you recently posted with a link to this paper was it from Google saying that search abandonment costs us retail 30 300 billion dollars annually. So it's like massive massive opportunity for search companies, you know, consultancies and so on. Totally.
But why do you think this is still the case regardless of all the efforts of our like what search community. Is it to say that community is too small and there is like potential to grow it and add companies and so on. Or is there something fundamental that you know still needs to be tackled.
And I think it's it reminds me a lot of where the search space and not just the search space for adjacent things like recommendations or any. I would say like surface on a let's say a website or a product or an app that like is algorithmic in some way.
That's just a it feels like from how people build products. It's just fundamentally a nascent space. So it reminds me of early in my career, when I was a software developer.
And I worked at a couple software companies maybe because I was a CEO developer that fund that really were hardware companies. And people at management levels were used to running hardware companies and but more and more of the value was delivered to software.
And they didn't necessarily understand how to manage software. So you know hardware you might have these very upfront you know classically waterfall kinds of development processes.
And then software you know in the early 2000s we learned about agile and then it was good to be iterative and these things and it's okay you know fail fast. And we can always unlike hardware with software we can hit the undo button.
And so it's a very different practice and a very different style of leadership I think. And I think the same thing is becoming true of these data products like algorithmic data products like search where.
Sure at the implementation level in our at our level you see a lot of people who really we understand the problem we understand it's very experimental. It's even more experimental than like how software you can ship something and you can undo something if you need to.
It's actually like extremely experimental where every week you're shipping something new and you're always looking at metrics and AB tests and everything. And every week you make a completely different product that you go in a different direction.
Honestly I think one reason that one reason that that this is a problem is that organizations structure to ship classic software aren't necessarily well suited to ship like these data products.
And you I gave a talk at at my C's you know the e-commerce search conference in Charlottesville in April about how to Shopify. One of the things that we do to try to help with this problem is really make engineering and data like work hand in hand.
Because many organizations they're very siloed from each other. And that can be a really big challenge because as you make these decisions like day to day like I'm implementing something in my search. Do I go do I you know as I'm writing lines of code do I go to the left or do I go to the right.
Do I try boosting this or implement this algorithm. And I think that's really a little bit slower and a little bit faster. And like those really intricate decisions you kind of need both sides of the data and engineering brain to to do those.
And I only think of very very small handful of companies places like Google maybe meta Facebook like have really mastered this like blending of data and engineering. Most people most other companies who have started to like you know have finally mastered software engineering.
Haven't quite come up like from a leadership and beyond and product leadership overcome the the sort of like hump of of how do we think about data products.
How do we manage things that are experimental and aren't like you know actual projects that are going to take a couple months to complete that we have a very clear beginning middle into.
Yeah exactly it's like beautifully put like it's not like a Toyota you know pipeline where you could say yeah this is where we start we put it all these materials in some people do something and we fix some bugs and off we go with the car.
But like there is no like definition of done in some sense rate. Yeah there's no definition of done it's very much like you're just constantly experimenting. It's not something that's visual that it's like oh we're going to add this button to the UI and it's going to do these things.
In some ways you're not changing you're all with you're like rarely changing the UI you're like mixing up search results and how they come up. There may be UI elements to it like oh we understand this query better so we serve this UI but it's extremely fuzzy and hard to like.
One of the biggest challenges I have actually you know and I've had this in consulting and you know continue to have a shop device how do you coach stakeholders to understand what you even do.
The plus side is it's very much a it's very much like oh it's a very tied to you know we're going to make more money on the other hand it's like not clearly like. So just be in a traditional sense it's a it's a constant cycle of experimentation and optimization.
Yeah absolutely I mean and it requires like a different discipline and like rigor and really even like a Tom Tom for example I work in a relevancy team search search for elements.
 I'm not a ML component or try to but but the thing is that I was amazed by by the team saying hey I'm running this a b test and they compute a bunch of metrics some like confidence intervals p values and they say yeah I feel like this is a good change in the algorithm but it's not proving to be like when we have split our traffic a b 50 50 just doesn't work after two weeks we have to kill it.
You need to go through that rigor if you say just for the moment you you doubt and you say no I love this change I'm going to push it forward it's not going to harm you lost like you cannot do this right.
It requires everyone to be a scientist too and I think traditional I think there's like traditional product and other kinds of leadership that is very can be very opinion driven or have a strong vision.
And I don't think there's a there I think there's still tons of room for that because at the end of the day you need. You need a strong hypothesis and often what you're a b testing is within the context of a larger strategy. Like you know we think we'll get traction if we go in this direction.
 But you have to like you have to really bring science to everyone has to be a scientist and it's not as often an a b test to is not as simple as like it was a clear winner loser it sometimes is a winner in some ways where it's like it's a winner in this dimension a loser and so we mentioned and then like can we go after and slice and dice the data to really understand what happened.
 It's not it's not a often not like a cut and dry story and so like trying to understand the data to even know how to tell the story requires a lot of humility I think if you're a leader to say like okay it's more important like what I learned and like my big idea being being the winner of you know being the clear like thing that won so I get all credit and that kind of thing.
 Yeah exactly this is amazing I wanted to I'm actually rereading your book here I'm a big fan and and once we meet you give me the autograph all right and John very man I think I saw you actually in person listen revolution 2013 you were on stage it was in Dublin do you remember being in Dublin if you.
 Yeah yeah that was fun yeah I remember they had this huge rugby stadium yeah yeah exactly this was the coming back to I think I talked about Quepid there I think my colleague from Silvercher arena came out we talked about Quepid yes and and I and I got blessing for Luke from Andrew Balecki there I said would you be okay if I continue because he didn't have time and he said yes yes please please oh cool and and then later it ended up being part of the game.
 You see and earned a lesson committed to Tomoko Ocita who is now driving massive changes day on Gira and oh yeah yeah that's true yeah I've seen his name a lot yeah yes fantastic and and in this book why I brought it up I was just reading one of the first chapters where you so beautifully said analysis so let's say in lesson lingo how you process the input text yeah analysis shouldn't map words to tokens it should map meaning and user intent to tokens yeah I mean this is amazingly put and you go there later explaining how you balance the precision versus recall as you do modifications to the analysis chain you know whether you're standing or not and stuff like that but it's not like many people even viewed that way I think not that I viewed it that way I was always like yeah what should I tweak to make it possible but there is a related topic on this front you know query and content understanding mm-hmm how how does this thing connect in your mind yeah I think so first of all I think it's funny how the work that we do shapes sort of like how we do certain work shapes our perspective on things because I think when I would you know writing that book is sort of like my early part of my relevance career at open source connections and this still happens like you kind of get brought to a client and it's like okay we have we have this app over here we have this indexing pipeline you just work within this box that is the search engine and so I became quite adept at like how can I hack the analyzers and the query and everything to be really to do like all the crazy things I want to do like and really could I take in a taxonomy and sort of map to a conceptual understanding of of the language not just a not just like the words themselves you know people think about analyzers they think about stemming and they think about lower casing but more and more it was like oh I only I can only work within this box it as a search engine and whether it becomes like plugins or whatever how can I how can I massage the text coming in and the queries coming in so that they they mapped to each other and so in that in that context it's it's like you know people you may have heard Conway's law which is like you end up shipping your org chart like how you structure your projects is very much tied to the organizational structure of how you sort of of how you do things and so the consultant slash relevance team it really only works in the box that is the search engine and makes the magic thing magic more magical and so how it's when I think about that often it's sort of similar to how people think about relational databases you're creating a structure of a database to answer certain questions in the same way using analysis and how you create fields you're sort of like structuring an index or a view of some documents to answer these natural language queries that come in and so everything is sort of like thinking about massaging this database to really to really get to that into rank results in a way that sort of like gets closer to the questions that users are answering and I really concrete example of that is uh you know I think this comes up a lot in that actually this is my one of my earliest first projects was if you take some let's say some medical knowledge into a your indexing like questions or medical articles and you have a there are taxonomies out there that are like mesh is one medical subject headings that say like okay this article is an article about um it let's say something in the cardiovascular system it has to do with the heart and it has to do with like the left ventricle so you that's a taxonomy it's hierarchy and um if I can index that and I can index that taxonomy a certain way so that if someone if I take a query I also map the query to something in that taxonomy let's say cardiovascular system heart rate ventricle if I can engineer the similarity in the search engines so that it uh it kind of uses the analysis to be like oh it has so many taxonomy nodes similar that makes it more relevant but maybe it has one or two dissimilar that makes a little less relevant if I can sort of like zero in on a on on that uh then I'm really getting closer to sort of meaning that I am to uh you know whether it's like a stemd version of this word or not and you can create tokenization pipelines that take terms like let's say uh myocardial infarction which is a heart attack and sort of like uses synonyms and other things to say oh it's actually this part in this taxonomy uh and therefore you know we we we sort of expanded to these taxonomy tokens and uh same thing at index time and so I got very adept at sort of massaging data in that way but I think like when you take a step back and you think about if you have access to full indexing pipeline as most teams do and you have access to the full query API and everything um really you're doing the same exact thing you're sort of like massaging content as it's come in comes in in some ways you have more tools if you can do it before it gets to the search engine and the same thing with queries you might have some ability to apply an nLP model or do some kind of any recognition before it comes in so philosophically you're really doing the same thing you're trying to map um you're sort of at one side you're mapping documents to queries and on the other side you're mapping query to sort of the document structure and you're trying to map those two together in a way that creates a ranking function that that does what you want it to do yeah absolutely um I think it was Daniel Tanklank who summarized his 20 years experience as comparing sets of documents right so like is this set of documents better than the other and then everything else comes as input you know was it query understanding was it content understanding whatever yeah it's it's amazing absolutely absolutely yeah and it's uh all of these things come together and the search engine is kind of like the core driver and you're trying to massaging this similarity engine to to make that quote-unquote cosine similarity what you want it to be yeah I've recently ran across one case uh so in map search you could think well what what people do type there well they do type addresses they type um coordinates um they also types uh questions they can see how do where do I go hiking here you know in this area stuff like that not something we can handle right now but maybe in the future we will and the case was um there was a company a search with company name so right we support points of interest search B.
O.
I and our search engine focused on so you have a meaningful part of the company name I don't remember something like white mice something and then had like less meaningful parts like limited south Africa you know things that would repeat across a number of company names and I was search engine because you you have the feature of minstreet max right it it it it focused actually on less meaningful components and so we bumped some overlap higher just because how TF idea by the way works it's also a lot of work they're going into understanding why does this TF equal to this number I need to figure out idea right um yeah I mean and I was like so I went on Twitter and I tweeted like I came across another use case where maybe vector search could help because it would actually focus on the meaningful part hopefully because you have the attention uh mechanism in the transformer models right like bird and others so presumably it would focus only on the right part and it would find it do you think do you think this was a moment of despair do you believe in this?
 uh I I mean I do believe in it to some extent like I totally believe that's a valid thing I also think that like sometimes the document frequency itself is really interesting because it's like it gets at the idea of specificity in the query and so if you search for something and it's just like the document frequency is sometimes a poor measure of specificity because it's uh it's not like it's actually you know just because something is rare in the corpus doesn't necessarily mean it's it's uh it's it's actually more specific to the users intent and some cases like that is just like uh thinking of when when we would do uh we did a project for a Riley media to kind of help with a project similar to Safari books online that people might be familiar with and people don't like if people search um job ascript or book job ascript books it just so happens that just how titles are written if you write a book on on uh react you're not going to put JavaScript in the title but react is conceptually you know about JavaScript and so uh what's really interesting is like I you know you type JavaScript books react might be a great great react book might be a great JavaScript book but you have to understand react in the context of this broader concept of JavaScript even though that exact term is put in title so uh this concept of term specificity is really useful but it's often like uh the the way we get at it with document frequency it can be can be really invalid you know not great and to your point about like the attention mechanism that's that's really that's really interesting because um yeah I sort of I could see like conceptually how uh how that can really like tie you sort of like zero in on the concepts that are most important to a to a document and one challenge with like one of the reasons I think Bert is so found like transformative is traditionally like for years and years even going back to the early 2000s with like lead and semantic analysis of these these things and and then we have word to vac eventually these sort of like techniques uh they're really great for like and some ways like increasing recall or getting at like a rough semantic sense of of what's what's uh you know what's there but when you at the end of the day it's like not helping me really get at the higher precision kind of component of search that really like traditional search engines thrive at and still are really good at like you have you know that this is a shoe I don't need to see socks just show me the shoes this is a shoe and you don't have that like fuzziness that you get in a dense vector representation where everything is kind of compressed down and fuzzy uh but what hurt and those kinds of things really do with the attention mechanism I think it's really like turning that on its head where it's like actually there are these parts that we could get at where it's like the precision of these related concepts it's like we know that um we know that the A the most important part of this document is this part that talks about JavaScript or it's you know the JavaScript-iness about it and and when we search for that we can kind of zero in on when that dimension of it as opposed to being a fuzzy concept of um of you know programming languages and JavaScript if that makes sense I feel like it's like zeroing in on like what makes this this thing precisely interesting as opposed to traditional dense vector representations which have been more fuzzy and castifying cat out a five wide net kind of thing and more focused of recall yeah exactly I think and you you're reading a nice paper with what problem does Bert hope to solve the search in 2019 and of 2019 and you really well-compared there uh inverted in the xpar search method with uh war two back and then you basically allude to the fact that Bert probably gets the aboutness of the document uh better than uh war two veko tfidf right because in war two veko you essentially have like a window that you slide through yeah to your early example if this book react never happens to be near JavaScript because everyone knows it's JavaScript right then you will never find it using using war two veko maybe it will be too distant but with Bert it tries to embed the whole document right or like you know chunks of it averaged and so on so it might yeah and and you have to do like if you were to use war two veko you'd have to like sort of implement your own attention mechanism in a way you'd be like uh okay what parts of the document are okay first i've got to throw out a bunch of front matter and and matter and and junk and like with word to veko you'd have to somehow engineer to like okay we'll look at these paragraphs and uh maybe i need to focus it on these ones more and throw away some other ones uh and you don't the aboutness of that gets really blended whereas the amazing thing yeah you're at the amazing thing about like about Bert is how it's a ability to really zero in on the aboutness of like where each each token position it's not just like the paragraph has you know where the document has an embedding each token position has an embedding so it's like if i take a question i can really zero in on like oh this is the part of this article that is most similar to this you still got challenges with like the fuzziness of dense factor and it's you know maybe not precisely the words you're looking for but just the fact like that's just mind-boggling that each token position of a book might be an embedding i mean it's a it can be a beast to to man and should deal with but it's it could be a really powerful concept yeah absolutely and plus it's a mass model right so it can predict what should be the the token and that masked out position and then it can actually predict entire sentences i think there was one of the side yeah effects of it right so it could become generated yeah totally yeah exactly so it's pretty amazing yeah and so today as we roll into this you you follow up on this trend of sparse versus dense you know i think a lot of discussion is still going around how dense will enter this sparse search world at larger scale so how do you feel about this and of course there is hybrid search as well it's a hot space yeah and i know there's a lot of open source projects there's like milbus there's companies like pankham there's qdren there are all all of these systems that are doing dense vector retrieval and it's also just like a fun problem if you were in search for a while to think about approximate nearest neighbors and like how you solve that and i know for a long time it's been sort of you know a side project of a lot of people i know for you Dmitry and Max you guys had had a lot of fun in the billion vector challenge um it's it you you the first thing to ask is like why do we need these extra databases and it's interesting to think about because you're thinking like i we just talked about why can't we you know we can match map tokens to meaning and that kind of thing you know a lot you know and why can't we do that why can't we just apply the same techniques to the dense world why can't we use a traditional search engine and if you think about it what what you're they're very in some ways they're very the data structures underneath of them are optimized it's like yes you're both in both cases you're sort of like mapping query meaning to document meaning like fundamentally the task is the same but the data structures that you would use for a dense system where everything is clustered into like 200 and maybe 256 or 760 or whatever dimensions are very different than a sparse index where you know you have and it's something like a a last searcher to get a loose-seeing traditional index really it's a it's a the dimensionality is way way higher so it's like you know you could expect hundreds of thousands of words each each word is its own dimension and if you think about that like you're going to have situations where you know words follows zip zip-slaw zip-slaw which is you know the occurred in english the occurs in every word and you get gradually gradually falls right off a cliff and then you get like cat occurs in 1% of documents and then you go keep going and you get like specific terminology like feline occurs in 0.
1% and it really falls off a cliff and so the sparse vector indices are really optimized for that use case of like I have a I have a term and it basically points at a very small handful of documents that contain that term and I can do that look up very quickly I can fetch those I can score those and then I can sort of like get get a get a score whereas what's interesting about the dense vector case is it's more like I go in with sparse vector I go in with a single term and I get like or maybe two or three terms I look up in this giant data structure and I get this this by looking up and I can get like the you can get the handle to all the things that match that and I can sort those and get them back with dense vector the query isn't two or three terms it's some value in a larger 256 or more dimension vector so off the bat right there is like that's a large 256 terms and a traditional surgingian would be a large query and really you're looking up in a in a index that is itself that dimensionality much smaller dimensionality where every document has some amount of value for each each dimension so it's not like cat where it occurs in three things it's like all billion documents have some level of if cat was one of the dimensions some level of catness and if you just think about how you would build a data structure it'd be a very different thing and that's why that's why they people build these you know completely different data structures and why doing nearest neighbors on this large scale data is very important because you do want to get like some some sense of like similar conceptual meaning in this in this compressed vector space but that in and of itself kind of gets at the pros and cons of each because if you get this sort of like compressed representation you don't specifically have the word cat or even direct necessarily direct synonyms of cat that you've created you have like a rough statistical sense of like catness or animal-ness that you're kind of clustering together you've lost it by compressing to smaller dimensions you've lost some precision just by definition but you've sort of expanded the net of what you might bring in so that's like a pro and a con of the new dense vector representation whereas continuing to use a sparse vector representation it's a much more precisely managed look up and so they there's not some future where you throw away one or you throw away the other more and more the reality is like hybrid retrieval where you're using both data structures to serve search results to give people like some kind of relevant results and in a mix of both perspectives of like expanding the meaning to baby mean other things or snow-staying in this more precise world of sparse vector meaning or sparser meaning yeah it's amazing how you put it like it struck me house in a simple way you can explain complex things I mean the sparse vector yeah you said hundreds of thousands of you know terms in your term dictionary when I work with alpha sense I once counted we had that billion in because you feed like I don't know millions and millions of documents and they do vary quite a lot of course there is some overlap like financial legal like revenue right would occur everywhere but then as they describe different verticals in the industry you know healthcare versus I don't know pure finance banking investment firms and stuff they they have different lingual there and and that's amazing like how you put it right so if I had a vector let's say billion size right now I have what I have much less right so I have like 768 dimensions maybe 1024 I heard the recently one commuter in in Elasticsearch is trying to push the scene to upgrade to 2048 or something oh wow and I didn't see that that's that's amazing yeah yeah I think it was my share you were so yeah and this is amazing but I guess you're right and also there is another thing that comes to mind we had a podcast with Yuri Malkov who is the creator of one of the most popular a and then algorithm station SW here article navigable small world graph and when I ask him this question so let's say you have this geometrical search right similarity search and in the case of ecommerce you also want to have filters so you want to say I want to read choose you know this size in stock and so on and and he he's surprising that he said these contradicts contradict the to each other so so much that I cannot even imagine creating a generic algorithm that will cover this case because essentially he said it could quickly degrade to traversing the whole space of points because as you said you know about as a dimension you also have these filters as a dimension right you could say yeah cluster these points on color cluster these points on size yeah yeah imagine doing this up front I mean this is not a generic solution and then he was he was just blunt and saying this is not possible I don't see how you could do this and yet the vector databases claim that they have done it and yes you can go and at scale but I I sense that there is some some truth he didn't maybe potentially there you know some edge cases where it doesn't work or maybe it goes over a second and it's acceptable I don't know but how do you feel yeah I mean it feels a bit like over complicating a solve problem it feels like we've I sort of I suspect that will be in a world of of sort of hybrid more hybrid retrieval where you're using a traditional filter for those kinds of things because I think like I feel like we dense retrieval is the missing piece of most people's search systems if not for anything then just like I think like first pass like often it was like case for a long time that people would would do like first pass like the m25 scoring and then like maybe apply some learning to rank algorithm I sort of feel like that's going to flip to be it could flip to be like first I'm going to get this dense vector candidate list because it's compressed it's like it's actually more recall oriented and then I'm going to use sparse vector techniques to filter re-rank and these kinds of things but I also think like just for speed and ops like one thing that's you know it's just a practical concern is like they're solar, elastic search have such huge install bases and a huge practitioner people who know how to scale them and I think with new dense vector techniques I'm not sure people are going to like completely throw out their elastic search or solar install just to have this new functionality and in fact you know as elastic search and solar sort of adopt more of these things I think more and more people are going to say oh that's cool I'm going to use this in addition to my elastic search or solar so I sort of feel like we'll end up in this world where where yeah elastic search and solar don't give us as nice of a set of feature features for that but it already works pretty well for 80% of what we do anyway and we just kind of want to tack on this extra bit so that feels more of a like my expectation of what the future would be then then we'll like throw out the existing systems and adopt something new yeah this is very interesting opinion of course not downplaying the displayers that you mentioned I haven't talked about it as well seven databases exist today and you neural network no neural frameworks like Gina and Haystack which is of these database but I agree like I think the future might be in flexibility that okay if I'm already with solar why don't I just use the you know and then plug in and try my luck maybe just wet my toes so to say right I don't want to jump to entirely new world of new database that I don't have experience with but if you haven't had that set up you start up let's say I know some startups when they when they want to go that direction with neural search they do consider VESPA or VVA or Pinecone you know and that might be a different use case as well by the way this is entirely new big topic but it's not only search right search is still being figured out and some companies do it but then you also have machine learning pipelines like recommender systems oh totally yeah yeah that's a great use case so it's not like for search up of course like you have this huge install base and stuff so I mean especially you know for established companies like Shopify or whoever else but you're absolutely right there is there is a lot of opportunity for for this at in so many places like pure question answering applications or places where you know you're using it as a backend component to do some kind of inference for recommendation systems I think it's a fantastic I think it's a fantastic thing I do I do think like more and more a bigger practice will evolve to scaling these things out and understanding them from an operational perspective so yeah I definitely think it's an interesting landscape to watch and I think like I think the the other counter even to what I said is like like their solar and elastic search are established for their use cases but this sort of like I was saying before this sort of like world of building these search like or data driven surfaces or personalization driven surfaces it's just wide open like I look at my phone I have the limited screen real estate it needs to show me something relevant for me or relevant to you know I think you know peloton for example peloton the fitness app I want to do a workout it's gonna I'm gonna go to the app it's gonna it's not gonna like it does have an navigation but it's also just trying to show me something on the screen that's gonna be relevant to me so engage with it or Netflix or all of the UIs we use these days they're not really like point and click they're driven by some kind of smart algorithm and it's not necessarily like a classic search use case where it's like point click filter then search with relevance so I do think that it's a wide open space and honestly I think it's a it's an under appreciated space and I think it's a space that in some ways if I was maybe if I was you know I'm just thinking of this now and I speak speaking completely out of out of hand but like if I was to advise like a Pinecone or somebody I might say like let's you know stop talking about yourself as a vector database let's start talking about all of these ways that are really you know I think I in my book I talk about relevance oriented applications or like I forget even like relevance driven enterprises and I think like a lot of these applications are really sort of like completely sort of relevance oriented applications which really whether it's really personalization or recommendations there are things that are about ranking something to a user for given context or maybe for given query or question and I would focus on the universe that stuff because I don't know if anyone's really speaking from a product perspective about how what is the engine that drives that and I think that could be really exciting product or open source space or whatever just just really begin.
 This is a great advice I might quote you on the upcoming keynote that I need to deliver at Haystack Berlin because this is a great great thought because in many ways you know one thing is that when people come back to me as they say what's the difference between vector and neural search and I'm like there's not much difference it's like vector is probably mathematical stance and then you know it's more like if you're deep learning engineer or researcher so you like to take it from that angle right but then you put it so beautifully like maybe if we focus too much on this technical level saying you know this is vector search that is and yeah totally it's all sexy you need to bite and we don't focus on like use cases and how things enough I think enough and how this could complement each other you know it's not like vector search is trying to kick out spar search it's not going to happen by the way you know the prey search is not supported in vector search maybe it will be but it's not right now you cannot just say there's also there's also a set of problems that are I mean to this day people use tree based models for I'm going to mix some kind of similarity with some kind of statistic about my data you know I think like tabular data so to speak has consistently been dominated by tree based models which is a completely different thing from deep learning and neural search so and those things integrate pretty well with like the learning to rank plugins in solar and elastic search where you can plug in a vector similarity into that kind of tree based model but the opposite isn't necessarily true this is very interesting so diagram like we spoke a lot about and I'm sure we could speak more about how to engineer a search engine you know let's say if you start up you don't have clicks right you you don't have feedback from your users maybe necessarily in that form you can engineer now you have a dense search you can still engineer by tweaking analysis chains and crafting scene dictionaries but once you are over that launch you know and you gather that data natureal move is to start looking into something like learning to rank and you you spoke a lot about that I was just quickly googling you know you you spoke at I believe reading buzzwords haystack you spoke somewhere in San Francisco Bay area like how to turn you know ranking into a ML problem machine learning problem we also spoke about Bayezian well yeah and then there is also I've recently learned well not that recently I think it was last haystack or maybe the previous before that learning to boost how do these methods come together where do you start for those who maybe haven't have only heard about it but they didn't try it yet and do you also think maybe a connected question do you think that we will marry you know the dense retrieval signals with learning to rank in some way does it make sense yeah yeah so yeah so I think a lot of companies they think it's easy to go into the learning to rank process thinking that I'm gonna this is the you know knock on wood it's easy I have a model I'm gonna train this model with my clicks and everything and and search will magically get better what's interesting is if you go back to haystack talks about learning to rank and other comforts talks where the number one place people get stuck and they spend their energy is on the training data less so the model the features and all of that stuff and if you think about it it makes sense because one of the biggest problems with sort of training data is it's just horribly biased towards whatever the old algorithm is you're always clicking on you people are always clicking on what the old algorithm showed them regardless of you know if it was good or not there are it's getting some clicks and the stuff that might be amazing but it's on the 10th page is never getting clicks so how do you optimize search in that context and it sort of doesn't matter what model you use or what feature you use until you get like really well-labeled training data you can't really make much progress um so what you can do to get started on it is at a minimum okay we know that it's the training data is not perfect but if we just look at like the top end the top 10 or so what's actually getting clicks we might be able to start to learn some stuff there about like what differentiates them so you might start to think you could think about this is um the learning to rank is there are many ways of learning to rank but if we just start to think about it as a classification problem within the context of these uh these results are that we do have click data on what's differentiating them like getting being seen with a lot of impressions and no clicks and lots of things and things with lots of clicks and you start to see the features that separate those um and then you sort of like know at least you're knowing like within the context of your search filter bubble what's sort of like differentiating relevant irrelevant and you can kind of use that to rank um but at some point you do need to realize that like I am working within this filter bubble with my original algorithm and all I'm doing is sort of tweaking up a few things tweaking down a few things how do I bust that filter bubble and get different kinds of potential relevant results in front of users to sort of like see whether or not they'll click or not um and that's that's really like sets you know that's really probably the big big challenge that people have with I mean honestly not just learning to rank but any algorithmic search works they're doing yeah absolutely I mean I when I was doing it using your hello LTR Rappl I think I focused a lot on the mechanical aspects like okay what is pairwise what is list wise I need to read london mark papers to understand get into the width of the algos but then I think I spent maybe too little time figuring out the data part and like head versus tail I think grunting your show wants to say torso as well and I'm like a torso yeah what's that so like do you have any advice for those who are starting like do they just need to be born data scientists or do you think that they're really like that to set yeah it helps I guess for those like you once but like tool set or methodology or a book or whatever like what yeah yeah so um I this is like a this is a big focus of AI powered search a book I help write with trig ranger and max or whim and then ML powered search was just the training I'm doing because I I think I think like a lot of the focus these days is on cool things like dense vector retrieval bird and these kinds of things and to me that's like taking out an old model and putting in a new model but the problems outside of it to get the training data are still really hard and there are a lot of techniques people can use and I think sort of the thing people aren't talking about enough in in the space is active and reinforcement learning and that's what I talk about a lot in my book and my training is this idea of you know how do we strategically explore new potential relevant search results for a query but still maintaining exploiting the knowledge we do have because we don't want to completely just show people random results and how do we play with that boundary a little bit in a strategic way and not just like here's a bunch of results oh it hears like a random one um and there are processes out there to do that and out you know one uh one that's very near and dear to my heart which is a very practical thing for people to learn about is what's called a galsian process and a galsian process is just a different kind of regression so it's the same way we're learning uh we're basically learning to rank we're learning like given a bunch of features like the title bm25 or maybe some dense vector similarity or some other you know the popularity of the document we're still learning from our data what is you know what function of that and probably predicts relevance and what doesn't but what's interesting about a galsian process is that any given point it knows how certain it is in the prediction so like obviously points that are in your training set it's going to have high certainty that the the variants the sort of like the gout that's where the galsian comes in the galsian um distribution at that point is very small it's very certain when you go a little bit farther out from a point that you have information about and it might sort of like try to connect the dots between that that observation and maybe one down here but at the end certain the kind of grows and grows as it moves away from an existing observation and that's interesting because what this model is doing is it's sort of like both predicting relevance for arbitrary points in the feature space and it can do that because it can see patterns like oh it seems like there's a cluster of training data over here where it's like things in this realm are more relevant than things over here on the bottom left but when it's in between those data points is where the uncertainty really lies and it can say well I'm gonna I think we should probe here I think we should try to select the document to show the user to get more information on that is uh uh we feel with a reasonable set of confidence is probably relevant but we're not entirely sure because we haven't exactly observed that yet and that's really where you can both explore the training data and explore the feature space so if you introduce a new feature into learning to rank you could say like oh let me try different combinations of this and then as you start to get out the general pattern you can try things in between to really understand how that feature interacts with how um how users are interacting with data so that's really why it's active learning it's very much about like the model itself yes you're you're training a model like the model itself can know its own gap so that you can sort of like imagine as you're serving search results you can go to this model and say not only are you I am not only wanting what you're most certain about I want like strategically where you want to explore and you can show those results to users too and start to gather clicks and information on that and to me that's a really exciting field of of where search and information retrieval and all of these fuzzy relevance interfaces could go and do a lot of amazing work yeah it sounds fantastic it's like a mathematically driven wave expanding your click base right and it still and it still sounds very experimental to me because nothing is given you only have from what you explained to find a student correctly you know it's like it's still an experiment we could run a nab test is that how you would do it also so like you you basically your model is essentially a reflection of the data choice you made and now you explained a Gaussian model to do that right and then you run an nab test is that right yeah you could do you could set up lots of different ways of doing it so you could be you could have your ab tests that are that are going on within those ab tests or let's say a classic ab test is like if I search for shoe I get ranking a or ranking b I can select actually there's there's a lot of creative ways you can do it but sort of like a classic way of doing it might be to say in the third slot I'm going to put the explore item so in every other slot I might have my like my traditional LTR model that's ranking results using lambda mar or SVM rank or any of these sort of traditional learning rank models and then we're not even learning to rank some other solution that you know works well with your features and then you slot in that like third result that's going to like explore a bit it's going to be different or as many results as you want another completely different option is to use it as a means to drive um in search results it's not like we show people just like 10 results anymore we give people lots of different there's different UI widgets that are like off to the right you might have something that looks a bit more like an ad or suggestions or in in product recommendations or a product search we might have like sort of similar products to those that you searched for like different kinds of prompts and you might get sort of explore in those spaces too to kind of get more click data and traffic to just sort of like explore what might be relevant so it may it depends a lot on like how you want to drive your UI in your specific use case and what might be appropriate and this is help me understand this this is different from click models right because we also have the click bias problem and we could introduce or redistribute the click weight in a way to those unseen items this is absolutely right is this different so it's different so um so there yeah this is like I'm talking about step two of a process step one before you even get to here is you don't just want to take like if you search for shoe and you notice something gets a certain click-through rate it's not necessarily you don't necessarily want to take that raw number of clicks because even within those things that you're showing users different just there is something called position bias which is people might scan top to bottom and they're just going to click on the first result more than they're going to click on the second result and there's lots of reasons for that even when they notice both they're just getting they might say oh this algorithm must know what it's doing um there are people scan top to bottom um and there are different reasons people are just like will click the first result more than the second result and so on and it's it's a it's an interesting phenomenon of like psychology about how people process uh search results that are even shown to them yeah exactly and by the way just as you explained this it occurred to me that have you noticed how um you know the interfaces changed like you go on youtube watching these shirts there is no way to search them right you just click and you watch and watch it because I think at that point first of all there is no bias you don't know what's next but I think the goal is also more like entertainment it's not like um I have an information need right I'm actually searching for something but I guess sometimes and I think you also spoke about it search uh blends with recommender systems because we actually don't know and user might not know what they're looking for sometimes maybe sometimes they do sometimes they don't like it's an explorative search which means it could become a recommender system which means you could plug in those explorative results exactly and it becomes a very um that blending it can be very uh interesting it's also can be challenging because searches also a very intentional activity and if you do something uh let's say in a an a dense vector representation there is some relationship that in a general sense like when you train on Wikipedia it makes sense that these things go together but maybe in the specific domain this specific uh profession there's jargon and it turns out those don't go together people will notice and they'll complain about about these things um a sort of like actually domain independent example of this that you sometimes see is sometimes um things that are opposite sexually occur together so you get like um I want to cancel my reservation or I want to confirm my reservation those sometimes co-occur with the same kinds of words and sometimes in these retrieval situations you might be able to get away with that and like a recommendations context for people like yeah whatever but when I'm searching it's like how how dare you not understand me and people almost get like offended by it because it's almost like going to a person at a store and asking a question and given the exact opposite or something yeah exactly I think my wife was recently doing a search in one of the uh you know grocery apps and uh everything gets delivered home today even in Europe and she was searching for oil and she was saying hey your uh vector search you know research could be applied here probably so the the top result was tuna fish and she was like why uh maybe maybe because oil is one of the components it's inside the oil right so what do you want but she was looking for a category of things so like breads right and she was getting yogurts yeah a sudden yeah so I think that's probably a negative example of an explorative search or maybe not I'm not sure but I think it is like you're a puzzle to the user not to see breads on the on the page and seeing yogurts yeah exactly yeah yeah that's actually a good like example of a the traditional search engine kind of doing that or it's like oil but it's tuna and oil whereas uh maybe a dense vector search might actually work better until you get like motor oil it's like so yeah both sides have to be tuned carefully because yeah search can really searches one of those things and I think the article we talked about a while ago about like the Google article actually talks about this not just in terms of loss revenue but in terms of brand retention because people will not come back to your store if they're like the search doesn't get me the seems dumb so it's it's uh it definitely yeah people people notice when search is not understanding them yeah 300 billion dollar opportunity for everyone yeah out there so in this maze of things learning to rank density well we still need to also um get concerned with how we manage this project right and yeah a lot of a lot of ideas here and thoughts uh but like I'm particularly interested in this um in the search engineer role transcending itself to something else for example it used to be I don't know I was tuning I was a solar relevancy search engineer I guess a few years ago and I was just reading these XML files and tweaking and tweaking and then you know you know indexing search pipeline and so on but today you mentioned this data science came into play and it's still being integrated what what other aspects do we need to think about how should we form search teams uh I believe you have a blog post on that as well we will cite this in the show notes oh yeah a Shopify and I know the Shopify yearning blog I know Eric Pugh at Opus source connections talks about a lot too um yeah and I can't say I have all the answers because if you like it's a you're right it's a brand new space I think it's an interesting thing to talk about I think there's two principles I think about when I think about a search team and you can't you have to do both and it's like it's like uh building a plane while you're flying it do you always you can't be that I remember at Opus source connections sometimes we would get in projects that would be very almost like two infrastructure focus uh and then other projects that would be two only building the experiments and solutions um and what I mean by that is sometimes the infrastructure folks experiments is more like oh we're gonna gather we're gonna spend nine months gathering clicks and processing it and trying to understand what's relevant before you've been touching a model or or tuning relevance wherever and then the other end of the spectrum you have systems that are just like we're not even gonna try to understand what's relevant just tune things and you know yolo ship it and hopefully hopefully things hopefully things look good and what you know and honestly both of those are anti-patterns because obviously in the case where we just like study the problem we never actually deliver anything and uh not just as a consultant but as a practitioner working on a team your stakeholders are gonna lose patience they're you're not gonna have much success well the other hand um I've seen like I've had I had one project where spent months and months and months developing experiments they did have the ability to AB test but we didn't really have any ability to understand or dig below underneath like what was happening at a query level or anything where we just spend months and months experimenting and through a dozen experiments at the wall for AB tests none of them turned out to matter and I suspect in that case it was turned out to be a performance issue or a UX issue that was actually more a problem and really what you have to be doing in this relevant space is shipping experiments all the time with whatever for structure you have to support them while simultaneously like changing the engine that you're using to like understand the quality of relevance so as an example you might do something like start out with Quepid and start shipping things just incrementally with Quepid getting people's feedback as bad as it might be knowing it's wrong um and start shipping changes but at the same time with you're doing that with your right hand and with your left hand you're kind of going in like oh we have to start gathering click data because eventually the Quepid experimentation might hit diminishing returns or might get it really subtle cases that people aren't gonna be able to easily tell me the difference and if you're not doing both you're you're really gonna get yourself into trouble yeah and it's it's amazing you really described it as a not an individual level experience but like a team level experience right like and now well everyone can figure out okay add the data scientist at the UX person at the product manager at the research engineers and work together in one single concert right yeah totally yeah and that's a tricky thing to build because um so the first that yeah you do need all those roles um and you need a tremendous amount of data literacy so not only do you need uh you need those roles you need like probably a good strong core of engineering and data working together so that's probably a good place to start but as you add as you eventually add like someone like a product manager like what does a product manager on a search team do um and that's a really interesting question because I think it's quite different than building other features a product manager on a search team is constantly looking at data trying to let's just say at the query level because it doesn't have to be the query level could be a user or whatever is trying to say like here's a cluster of problems we have or opportunities maybe it's this kind of search a search for um colors in products or a search for this type of terminology and then have some like has to have the ability to do the constantly do the analysis of that data advocate for data that they need to get implemented and then um understand to some level like when they work with their data and engineering team what are the experiments like let's think about half a dozen experiments that could treat this problem prior to ties and triage in terms of reward effort trade-off and um and really plan out how we do those experiments and when you do that planning it's not just about planning the like nuts and bolts of how we get this experiment into production like we built this pipeline we do these things it's also building the like how will we measure how we answer the questions about those experiments um and that's a pretty I feel like that's one of the toughest roles that's a unicorn that is it's hard to hard to have someone with all of those skills but it's also really essential to really be able to have a really successful search team really accurately put I mean I'm still learning the product manager you know roll myself but like that's exactly right you know like you need to generate the insight for yourself you you're constantly like a detective work you know you keep looking yeah that's a good way of putting your detective uh you have to be a really good detective and then you have to like you also have to like figure out where you're going to go digging as a detective what am I gonna maybe I need to set up the like a team of manual laborers because there's something on our click data that's not quite right or or do something different with our click data and it's like you really have to be able to understand and appreciate how the nature of your evidence yeah exactly and and maybe to add to that like when I used to be an engineer what do you do daily you open Gira and you say what's the next ticket on my name so somebody thought about it somebody says what needs to be done they don't tell you that this might be an experiment but like it's given right with product management I don't open Gira and I know what to do every day I'm like let's think uh you know okay look at metrics uh look at query logs see what the engineering has done what experiment we just completed try to combine this pieces into yeah what did we learn from that experiment and like what might be the next step yeah yeah and and also subscribing to bold changes sometimes it's easy to kind of go step by step evolutionary you know but sometimes you need to jump over you steps yeah requires boldness and then messaging that and saying hey we need we need this I know it's almost like going after it makes me think of going after like then you know to get a get um get grant research for like most you know in the US if you if you want to do some big research project at a university you go to a government agency and you give this big proposal and for these big bets you almost like it's almost like that where it's like you yes we have this like side over here that's just constantly evolving whatever it currently works but then like for these big bets you almost have to think about it in terms of we want to spend x amount of time researching this area to see if this direction works out and then as part of that you also have to be like these are the early tests the prototypes before we build the big thing to know if we should invest even further and that's that's a tricky thing I think that's something a really good product manager can sort of like coach the stakeholders and thinking about these things of like and and thinking about them as bets and not thinking about them as like sure things that we know they're going to work out this also really important yeah exactly yeah just one example came to came to my mind that was it eBay when they didn't have type of head when they added it they they tapped into something like a hundred million dollar market you know because because you reduce the the time spent in each search session right you might get the faster which means you will get the faster transaction or like get faster so yeah totally probably probably was in involving product management thinking what if we do this but it's yeah it's like outside of box thinking yeah totally totally and I before we close off I mean I really enjoyed this conversation bag and I think we could speak entire day you know my engineer having like a lot of fun now like really getting into this but I love asking this question and you partially answered it during this podcast the y-question what really like you've done a ton it's not just that you imagine doing things or told someone to do you actually did it yourself like keep it learning to rank plugin explainer you know books all of these really physical almost physical objects right books are it wouldn't matter yeah so and but you still keep going and going and I mean you talk at conferences you push so much material on LinkedIn and Twitter I barely can fall up like what drives you in this space that's a that's a great question I think I think what gets me excited about this space is how hey it feels like the future of how people interact with with computers like searches the Google for example for a long time people call it Google like it's really a command line interface but it understands natural language and I feel like more and more interfaces are this like fuzzy interaction that's search like and it's this thing creeping up on us that people aren't quite realizing um and then the other it that just makes it a fascinating field of like the intersection of data and engineering and product and UX and you have to have all of these parts of your brain working together to help sort of like understand and solve the problem um it's really just a it's a huge intellectual challenge but I you know more you know more foundationally just like I find like interacting with interesting and great people in the field also just drives me is just how fun it is to interact out there with people like you Demetri and other people who are just like also get excited about the problem and like to nerd out about it so that also kind of drives me is just the social aspect of sharing my crazy ideas or products or books and getting feedback and like continuing the conversation yeah and I think it's endless you're doing a great contribution there but it's like endless journey in many ways right so many facets so much totally dimensionality yeah totally absolutely and of course I think people want to learn these things I myself as well from time to time I'm subscribed to a course and I just have the blast of I don't know four weeks two months whatever not for the certificate but for the for the knowledge and for that feeling of connection to that knowledge and with that I want to ask you if you have any announcements yeah so I'm doing a course with sphere sphere is a fantastic company that is sort of trying to build these like next level courses you know it's not your basic utemy course we're learning some basic things it's really like it's almost like a master class with a professional and they are really focused on machine learning engineering right now so they've recommended systems and all these things that I'm doing an ML powered search course and it really covers a lot of these things that we've talked about starting from you know just appreciating the relevance problem to building up learning terrain models and really focus on the problem of ranking and then also discovery of doing feature exploration and training data exploration to try to figure out what's even relevant beyond the sort of filter bubble of our current search algorithms so if you're interested in that catch up with me at it's get sphere.
com and you can find the ML powered search course and then of course like I all of my other things out there AI powered search written with tray and max and relevant search of course hopefully still still relevant so to speak and all the great stuff out there that I think people find interesting and useful and of course I also want to continue to plug open source connections they have great training consulting courses I was you know a key part of training of that team as a great place as a resource that you can go to so yeah this is fantastic announcement and also thanks for that and I also want to say that I enjoy the reason I enjoy reading your book relevant searches not only because you share a bunch there like for example indexing songs I was like what inverted index yeah you can if you want it your way of writing is very thorough it's like you create a network of thoughts as I go through the text and say we will talk about it later but let me spend a few sentences still explaining what I mean and I'm like it's like a conversation and yeah I try to be conversation on including like the typical like bad jokes and sorts of humor exactly I'm also learning on that side so that's that's fantastic and that gives that feel and keep keep going keep doing this I enjoy pulling what you do and connecting once in a while you sometimes give me a really good advice on you know how to oh sure is the title in the blog post or should they venture into this or not and things like that that's amazing this cross pollination so I'm enjoying it a lot and I recommend everyone to subscribe to your course google of course link it thank you and have fun have fun oh definitely we'll do awesome thanks so much Doug I enjoyed it and see you soon hopefully in person yeah yeah same all right bye bye all right take care with