---
description: '<p>Topics:</p><p>00:00 Introduction</p><p>01:12 Malte’s background</p><p>07:58
  NLP crossing paths with Search</p><p>11:20 Product discovery: early stage repetitive
  use cases pre-dating Haystack</p><p>16:25 Acyclic directed graph for modeling a
  complex search pipeline</p><p>18:22 Early integrations with Vector Databases</p><p>20:09
  Aha!-use case in Haystack</p><p>23:23 Capabilities of Haystack today</p><p>30:11
  Deepset Cloud: end-to-end deployment, experiment tracking, observability, evaluation,
  debugging and communicating with stakeholders</p><p>39:00 Examples of value for
  the end-users of Deepset Cloud</p><p>46:00 Success metrics</p><p>50:35 Where Haystack
  is taking us beyond MLOps for search experimentation</p><p>57:13 Haystack as a smart
  assistant to guide experiments</p><p>1:02:49 Multimodality</p><p>1:05:53 Future
  of the Vector Search / NLP field: large language models</p><p>1:15:13 Incorporating
  knowledge into Language Models &amp; an Open NLP Meetup on this topic</p><p>1:16:25
  The magical question of WHY</p><p>1:23:47 Announcements from Malte</p><p>Show notes:</p><p>-
  Haystack: <a href="https://github.com/deepset-ai/haystack/">https://github.com/deepset-ai/haystack/</a></p><p>-
  Deepset Cloud: <a href="https://www.deepset.ai/deepset-cloud">https://www.deepset.ai/deepset-cloud</a></p><p>-
  Tutorial: Build Your First QA System: <a href="https://haystack.deepset.ai/tutorials/v0.5.0/first-qa-system">https://haystack.deepset.ai/tutorials/v0.5.0/first-qa-system</a></p><p>-
  Open NLP Meetup on Sep 29th (Nils Reimers talking about “Incorporating New Knowledge
  Into LMs”): <a href="https://www.meetup.com/open-nlp-meetup/events/287159377/">https://www.meetup.com/open-nlp-meetup/events/287159377/</a></p><p>-
  Atlas Paper (Few shot learning with retrieval augmented large language models):
  <a href="https://arxiv.org/abs/2208.03299">https://arxiv.org/abs/2208.03299</a></p><p>-
  Tweet from Patrick Lewis: <a href="https://twitter.com/PSH_Lewis/status/1556642671569125378">https://twitter.com/PSH_Lewis/status/1556642671569125378</a></p><p>-
  Zero click search: <a href="https://www.searchmetrics.com/glossary/zero-click-searches/">https://www.searchmetrics.com/glossary/zero-click-searches/</a></p><p>Very
  large LMs:</p><p>- 540B PaLM by Google: <a href="https://lnkd.in/eajsjCMr">https://lnkd.in/eajsjCMr</a></p><p>-
  11B Atlas by Meta: <a href="https://lnkd.in/eENzNkrG">https://lnkd.in/eENzNkrG</a></p><p>-
  20B AlexaTM by Amazon: <a href="https://lnkd.in/eyBaZDTy">https://lnkd.in/eyBaZDTy</a></p><p>-
  Players in Vector Search: <a href="https://www.youtube.com/watch?v=8IOpgmXf5r8">https://www.youtube.com/watch?v=8IOpgmXf5r8</a>
  <a href="https://dmitry-kan.medium.com/players-in-vector-search-video-2fd390d00d6">https://dmitry-kan.medium.com/players-in-vector-search-video-2fd390d00d6</a></p><p>-
  Click Residual: A Query Success Metric: <a href="https://observer.wunderwood.org/2022/08/08/click-residual-a-query-success-metric/">https://observer.wunderwood.org/2022/08/08/click-residual-a-query-success-metric/</a></p><p>-
  Tutorials and papers around incorporating Knowledge into Language Models: <a href="https://cs.stanford.edu/people/cgzhu/">https://cs.stanford.edu/people/cgzhu/</a></p><p>Podcast
  design: Saurabh Rai <a href="https://twitter.com/srvbhr">https://twitter.com/srvbhr</a></p>'
image_url: https://media.rss.com/vector-podcast/20220830_070827_46ba9c40226c9b5c8e39886c99b0aea3.jpg
pub_date: Tue, 30 Aug 2022 07:27:26 GMT
title: Malte Pietsch - CTO, Deepset - Passion in NLP and bridging the academia-industry
  gap with Haystack
url: https://rss.com/podcasts/vector-podcast/599924
---

Hello there, Vector Podcast. Season 2, we are relaunching after summer and it was a little bit of break last episode was from Berlin buzzwords and today, coincidentally, we have a guest from Berlin, multi-peach, a studio of Deepset, the company behind Haystack.
So we're going to be diving into what I call a neural framework, but I wonder if Malta would give a different picture there, but still very interested to learn and dive into multiple topics there. Hey, Malta, how you doing? I'm good doing great. Thanks for having me today.
How are you doing? I'm good. I'm great. It's still summer. It's super hot as we were exchanging before the recording. It's super, super hot, but I like it.
So yeah, I think before we dive into what is Haystack, I really like to learn about yourself and what is your background and how did you find yourself in this space of what we call Vector Search? I wonder if you describe it differently, but I call it Vector Search, Vector Search players.
So can you tell a bit about that? Yeah, I'm sure I'm happy. So I would say my background is mostly in NLP engineering, what I would call probably these days. And during my studies, I basically had no clue about NLP. I think it wasn't really any part of our coursework or something really a thing.
And for me, all then started basically after my studies, went to the research project in the US, which was at the intersection of machine learning and healthcare. And the big, big focus there was on numerical data.
So we were basically trying to find signals, patterns, and laboratory measurements for kidney disease patients to predict some kind of risks. And there was all the kind of numerical data.
And NLP wasn't really really scope of that project, but there was for me, that basically one kind of event that made me then get in touch with NLP and eventually fell at fall in love.
And it was really in this project, we tried to predict a lot of these risk factors through a lot of, I would say, quite fancy modeling to get some good signals. And at the end, it kind of worked.
We were able to predict some risks, but when we then talked to doctors and showed them these results or asked for their feedback, they said, yeah, yeah, that's all correct. Yeah, but it's not really new. We knew that before. But this part here, this is like, this is an interesting one.
And this is what we do there. And that was basically the only small part where we, where we looked at written notes of of doctors during treatments. And from a modeling perspective, that was really, I would say, nothing fancy, nothing advanced, nothing where we spend a lot of time.
But at the end, it was the point, I think, where the, the doctor's physician saw the biggest value. And that kind of got me to think again, thought, okay, well, like, just this kind of data source, it was something they couldn't really access before.
And now with this, like, very simple, native methods, they somehow saw a value, a new thing. And that's basically where I thought, oh, what, it's cool.
What can you actually then do with more advanced methods of, if you have more fancy models, how can you make this kind of unused data source than accessible. And yeah, basically, realizing this, the power of it.
And that's basically when it then started digging deeper, working more on energy, at some point, then set left research, because I was really interested in seeing these models working the real world.
How do they work at scale? How can they really then solve problems every day? And basically, and came back to Germany, worked in a couple of startups, always just say, NAP at scale, kind of intersection, a lot in online advertisement, recommend our systems.
And then eventually four years ago, then we started sort of Deepset. And together with two colleagues, we found the Deepset basically because we saw this big motion appeared was kind of piling up.
There was a whole like still pre transformers, but there were early science, I think, on research that, that things are becoming more feasible and super interesting things became possible.
At the same time, we also saw that there's this big gap, you know, like things becoming possible on research side, didn't really mean people were using it in production in the industry. And I think we were at this, this interesting bubble back then.
We did it, we applied deep learning models at scale, saw how that worked, but also saw how much of work it actually is of manual work to get it done.
And basically up the early days of Deepset were mainly around, how can we bridge that gap, how can we get latest models from research into production in the industry, what kind of product tooling can we do. And can we build to make that transition easier.
Yeah, and that's basically how we, we ended up in the, in the startup world building building out Deepset. And, yeah, initially, that was really more about we saw this problem. We had a couple of product hypothesis, but we didn't, didn't like say place a bet on directly on one of them.
We rather said, okay, let's, let's go out there. Let's really try to understand for one year what are really repetitive use cases out there. What are really the pain points of other enterprise teams that are working in that field and then kind of settling on a product and then building it out.
Yeah, that's basically after one year, how we ended up in search.
 And of course, I would say really the one use case, the dominant use case, there was present in every company that we worked with and that was really a big say, valuable use case, where the push not only came from the developers who wanted to do something better, but also actually from the, from the business side where people saw big value inside Eric.
I use Google every day, where can't we have something similar in our product or our internal data sets and and that thing was something that got us done really interested.
And on the same time that on the the tech side, basically learning more and more about the pain points, why is it actually so difficult for for people in these and these enterprises to build modern search systems, what could you actually do to help them. Yeah, that's fascinating.
Actually four or five years ago, could you have imagined that an L.P.
would cross paths with search because like in many ways, this bar search, which existed for many, many years before was in some sense, I sense it that way in mailing list, let's say a patch is solar mailing list, people were dreaming about applying an L.P.
in some way, compared to what is happening right now.
 I don't want to downplay those efforts, but I'm saying things like you could embed a post tag, part of speech tag on on term level, and then use that during search again, you need to run some kind of parser on the query, and then use that payload information to filter through let's say adjectives and verbs or something bad, you know, I don't know if there was any practical application in place, probably there was.
But again, if you compare that to what is happening today, you basically have a vast array of models right in deep learning models that can be applied directly to search using vector search approach, could you have imagined this happening when you when you were about to start the company.
No, I would say I was I think what we we had big big say dreams about N.A.P.
and we we were true believers that that things become easier and say more feasible in production, but that was more actually under I would say transfer learning side and making models to say more easily adoptable to certain domains for search, I think that was for us.
And only then on our journey where we kind of realized, oh, like that's actually two interesting different fields kind of connecting over time right and also I felt from at least from my perspective, from a community side from the people who worked on information retrieval.
 I think for a long time, a big, like a lot of skeptic people, I wouldn't be talking about any key or dance dance retrieval for good reason right because I think there was also like a lot of hype around deep learning and still what's a lot of promises that were made like that it will just outperform space retrieval out of the box.
And then I think many of these promises were not hold for a long time.
But I think then basically there was another phase where I think people realized, oh, actually now it's kind of starting to work and not only just in research and these ivory towers and lab settings but actually also in reality at scale.
 And I think that was then fast also here, the moment where I've got really interesting and I think since then just crazy to see how things are progressing when thinking about a multi model search or now just was like more I say going away from document retrieval to maybe something like question answering which we do a lot.
And really really crazy to see what's possible these days and I couldn't have imagined that it's going so fast. Yeah, and there are a lot of contributors as well, of course. I just happened to give a talk about players in vector search.
I will link it in the show notes, which was just published with C's. London IR meet up, but even that during that presentation, I felt like I'm scratching the the tip of the iceberg in some sense, I know there is so much happening.
And in Heystack, like did you have a vision for the product, like you said, you didn't know what the product will be, but you knew sort of the repetitive use cases in a way, right, and also challenges, can you share some of the early day challenges that you saw.
And do you think that they are solved today or are they still kind of like in the mix of we need to fix something's there. So I think that was basically all about this first year of Deepset, where we did these learnings where wasn't that clear.
But after that year, I think we had a lot of clear insights and at least for us, a clear vision also for Heystack, what we want to want to solve there.
And I would say the big challenge, the big problem that we focused on that we saw in the industry was having just all these get up technologies and.
And basically Heystack is trying and always as I would say as a design philosophy design principle has two things in place that try to bring these data technologies together in a meaningful way.
And what I mean with that is basically if you think about search it's what say really it's a lot more than then model right and it typically you have factor databases.
And you may be chained together multiple models, you have something you want to do at indexing time, you have other things you want to do a query time.
And for each of these say kind of components that you need at the end, there are so many different options that you're that you can plug in and often it's hard to say in the early days.
And then you know, do I go for elastic search or something like Pinecone electrical database, do I go for this model or that model, do I need a, I don't know, just the retriever in my pipeline or do I actually also need to add a re rank or something else.
And we just saw that teams are aware of actually spending a lot of time on. And then we're doing these things together manually. And even when they had it once there was and constant or maintenance work or iterations where they have to exchange one component of the system.
And that was really just slowing them down a lot and sometimes even then causing that a project got. So over time, not really ending up in production, but kind of dying at the prototyping stage, because it just took so long and and things got kind of sidetracked.
And with hastag, we basically tried to solve that and having very clear building blocks like, for example, the retriever, which very clean their face.
And within that you can swap a lot of different technology models and the same for a slew vector database document stores where you can very easily change between something like elastic search, Pinecone, we veate and whatnot.
So I would say that's the was the one thing this building blocks and trying to get the focus of developers back on making these creative decisions what they actually want to have in their pipeline, trying it out with with anti users, rather than just spending time on doing things together.
And the second thing is I would say very deep concept also in hastag up pipelines. So really what we saw is it's not just one model. It's typically a couple of steps that you want to have there.
So in hastag we started early on having direct as to click graphs where you can have different notes and basically when you have a query or indexing time file that kind of hits the pipeline, you can root it for this graph. That can be very easy. There is a set of a query.
I do put it to a retriever and I get back my documents or can go basically quite complex where you say all like depending on the query type.
If it's a keyword query, I rooted a certain path in my graph, my pipeline, or if it's a question, maybe I go a different way and I have different models, I'm basically involved in my in my search request. And these two, I was here, the core principles in hastag up. That's very interesting.
So that second thing they are cyclic graph with a love for very complex scenarios, right. Like as you explained, we couldn't principle support question answering use case side by side with the kind of like normal search with theory, rankers and stuff, right. Is that correct. Exactly.
So that's what we basically learned from customers like when we saw there was a big interest in something like question answering and people say, wow, that's amazing. Can we use that for our website or for our product here. But doing that switch in a production case is quite tough, right.
Like if people are used to do keyword queries and they know I know I have to enter your keywords to get basically my results.
And then from one day to the other, you switch to more semantic queries, maybe more questions or also I think dance retrieval, if you really have more sentences that you use.
It takes some time for people to adjust and we saw that in a couple of scenarios that basically the traffic kind of requests that come in. Start a lot with keyword queries and then over time slowly shift towards more semantic queries.
When people realize, oh, I can actually also ask a question and all this like, like Google.
 And then there's a trend, but you need everything to have an option your system to allow both for certain time and and hasty basically with the query classifier where you can initially basically classify is that a question or a keyword query or you could go with also semantically like what a topic level saying all like this is a query for certain type of category in my my document set.
And then maybe do something different. And like early on Hey stack did it integrate with any database per se was it like the last search back then. Yeah, like the basically starting point was the last search was the very first document store we had.
But the last search back then didn't I believe didn't support neural search right so how did you actually gel these things together. Yeah, that was just that kind of coming in over time right so it was. Think the the era where elastic search was for us was really.
We came from a question answering use cases a lot and there was really like how do we scale that how can we now. Ask questions not on a single document and single small passage, but how can we do it actually on millions of files and.
And the 25 work as a retriever step before that was was okay was not not too bad and that's kind of how it started and then very fast evolved into into a say back to search direction. Where we had them a files basically as a as a next document store.
In combination with some some SQL database for for the metadata and so on and then it basically kind of. I think took off on the lecture database side with the nervous we via a Pinecone and so on and so forth open search today is also part of the face deck. But that was I think then just.
Half half here after we launched a stick. Oh yeah, that's awesome. That sounds quite quick. I know that BBA was also emerging about the same time. And then and then neighbors I guess as well. Yeah, that's that's that sounds super cool.
And was there any as you were approaching your clients or like prospects was there any specific use case that you would be demoing with because you knew this would trigger the aha moment like question answering or maybe a specific domain where you did that.
Yeah, I would say we were for us it was a lot around question answering back then that was really very great that I think many of these aha moments. As to remember we were at one client and when this meeting and it was like on the in the financial domain.
So we're interested in asking questions on financial reports of certain companies and basically accelerating their analysis.
And at one point in this meeting we showed what you can do with question answering ask these questions and they also like suggested own questions that we should ask and they work so they were that point and convinced oh like that's not fake. And like smoke and mirror here.
 And the basically the boss of the department was standing up and shouting like wow that's that's amazing and went out of the office and at the office next door and and carried over colleagues and said like you have to see that and that was actually even before we started building hastag but was these kind of moments were very important to see like this is something.
That is not just fascinating for for techies like we were but also say business people and users see that value and see value and their work for it.
 I can imagine that and it's like a class of what we call knowledge workers right it's something that you spend so much time on crafting this queries and I have spent some time in the full text finance I would say at alpha sense and remember some of the clients they had accumulated Boolean queries over a period of 20 years right and they were like so long it's like several pages.
 When you when you when you slap that into solar it runs for three minutes because our index layout was not what it is today and was not very optimal and it's crazy to see what what people kind of start doing as work around right so we are at a similar case with a with an airplane manufacturer was not financial domain but really on some more maintenance level analyzing basically issues that come up maybe in certain technical areas and they also have like this crazy Boolean search queries and people just became experts and crafting that but it took them really long like asking for sending one query creating this query I was taking easily like minutes.
 Yeah exactly and so what hey stack is today can you can you elaborate a bit on the architecture and maybe if it's possible if you find it easy if you put if you pick what say use case actually I recently I was talking to one stakeholder who wanted to build a chatbot but it was a very specific domain so that chatbot would actually ask you some kind of philosophical question.
 So I think it's a very difficult then like questions sort of a little bit like distracting you from from what's going on let's say you are on a conference and in a lot of things go through your mind but you don't register maybe what's going on you don't get see the value and and that Zenbot might kind of ask you and well essentially allow you to pause and reflect right.
What I realized is that yeah I could pick another shelf model let's say question answering bird or something but it probably wouldn't work on what I want right my domain is different and I had an electronic book with this Zen type of statements.
So this one question I'm hinting to is kind of fine tuning or maybe even right retraining right but where would I start with hey stack and can you walk me through the architecture.
 So as mentioned earlier into core principles are these building blocks and using this building blocks to assemble pipelines and I would say the core we come from is question answering and search but by now I would say the framework has evolved a lot in that direction if you have a lot of different notes and can support a lot of different use cases going to translation zero short classification.
And you could produce these notes in isolation or you can kind of assemble them and use them within your search pipeline.
So usually I think what what our users through and how they start is now they often come with a kind of search use case pick one of the standard pipelines that we have so we can very easily the few lines of Python created pipeline for no it's a question answering or maybe dance retrieval.
 Pick a document store you pick one model from for example the hackenface model hub and and we give some recommendations on which models might be my people starting point and then it's very easy actually to just put your files into a into a pipeline can be PDF files we do the conversion basically for you there's a note for it.
And just have a basic say demo system up and running in a few minutes and that's often already I think a good good starting point if you are maybe also new to that field if you just want to try it quickly out on this kind of ebooks that you mentioned.
 And get a get a first let's say quality of understanding how good piece of the shelf pipelines for my use case get this first data point and then basically enter the I would say next next steps typically in your project if you see all like this is promising but not enough for really going to production.
And then typically go more in this experimentation mode they say all it's now maybe evaluate compare a couple of different models let's maybe adjust this pipeline a bit or add a re-ranker maybe or go maybe to the to a hybrid retriever pipeline where we come.
Basically have a 25 retriever in parallel to a dense retriever and we join these documents and hastic has a lot of functionality that makes that easy to to basically change a pipeline as you wonder very quickly and then evaluate if that gives you any any benefit.
 If these say of the shelf options and combinations are not enough for use case then yeah you can go down the fine tuning route I would say we have also have a source the notation tool labeling tool where you can create training data and basically fine tune parts of your pipeline retriever or reader for question answering.
So basically I would say everything from a quick prototype tool. Let's do some some experiments here and there to then going and production and deploying it with a with a basic rest API until basically.
 Sounds cool and so in that experimentation mode I guess one one one aspect is like fine tuning you mentioned right the other is kind of like what building blocks I could plug in right and I know you guys have really good documentation is there something like a tutorial or or some kind of walk through that would even help me discover is a user what are the options.
So we have a couple of different different tutorials showing you what kind of notes also you can use like many people are not aware of for example options that can do it indexing time that might be helpful so.
For example, like enriching your documents with metadata can be incredibly powerful later at search time because you can then filter on your search space to make more categories that that you're interested in. And there we have for example, the stories that show you how easily you can.
For example, classify documents that you index to certain categories and then later on at query time use these categories to narrow down your search space filter for these categories.
And on the model side, say if you are now you know that you want to have a say QA model reader and you know interested in what model you want.
I would probably suggest you just go to our benchmarks page which is linked from documentation there we have a couple of comparisons in terms of accuracy and speed. But also we have most of our own models on the hackenface model hub which appears to find this information and model cards.
Yeah, that's awesome. So you guys in addition to open source version that I could I presume could host completely myself right I still have a bunch of questions on that open source side but still you also offer the cloud version you call Deepset cloud is right.
Can you explain what users get with that I presume scalability but maybe something else and I think we can we can leave a link to in the show notes as well for those users who want to try it out.
Yeah, basically hey stack the open source predictors will be a Python framework and you can do everything you want there to prototype the experiments and if you want also go to production with it.
 But you also found in basically in addition to that people want something more like they want to really host the platform where it's really end to end and basically you have faster workflows so really what's covering the whole lifecycle of an application from early prototyping to running many experiments and parallel getting more guidance.
What's on from your eye perspective on what to launch investigating certain documents in a faster way. Then to OK now I did all these experiments and I want ever kind of one click path to production and I don't want to bother with any scaling and basically a productionizing on my side.
 And this is basically what what we do with these at cloud so if you imagine as a host the platform the cloud the SaaS platform where you develop your NAP applications and can easily bring them to production and monitor them afterwards so really the I would say whole life cycle and especially what's going on getting your.
Getting your NAP pipelines faster to production as you would probably do it on a just Python level and then continue monitoring them and having this close group is to later want to maintain them.
 So it sounds cool and since it's kind of like so with open source version I presume I could do kind of a local development on my PC right and then go and use some deployment pipeline to deploy with cloud version I have sort of like managed haystack right and now thinking about developer experience are you guys moving more towards cloud tools as well you know like for example.
A code editor could be in the clouds or the changes and click click the button and off it goes I don't even need to download it locally right or or do you see some other trend with your users.
 No like we maybe that's also an important point so it's still a developer platform right so we are not in a low code no code space and what we really try is basically giving developers the option to customize components and that then goes through coding and and there we have for example editors directly on the platform where you can.
 Edit for example just the young definition of pipelines and quickly switch certain parameters if you want to do that and then it's basically there's a hosted notebooks where you can also easily kind of open these resources like a pipeline and we automatically create some Python code of it in notebook that you can then.
Then edit as you as you know it also from haystack open source.
Adjust the sort of certain component debug it maybe at another one and then it's basically just one Python line again to move away from the Python code in your notebook to the production artifacts to the pipeline that is then deployed and then can run production.
Yeah sounds cool and if a user has some as a user I mean it could be a company right so let's say they have an established tool set you know maybe if the usage maker maybe they don't maybe use something else. How do you reach these tools said that is kind of outside of haystack do you have to.
 I would say in most cases not so you will I mean what were very very basically stop I would say with with the cloud is when you have your pipeline to NAP service and you have your rest API that you expose that's kind of where we stop so there's a lot of I would say stuff in a company that is built around it when you're into your product and also on the other side of where do the files come from where does that.
Data come from how you think it into into a Deepset cloud. But within that space we rather see people. Customers who appreciate it that's like fully integrated and they don't usually then. Want to stay on on sage maker if they are on it for these NAP use cases so from our perspective.
There are the other are these more generic solutions that are not specific for NAP the car work for any kind of machine learning. But if you really have cases where you want to be faster on your NAP use cases.
Want to have more say support on that side that's basically where where Deepset cloud and comes into play and to give you an example your think of experiments should evaluate these pipelines.
And then you have to do give basically a lot of options to investigate predictions and what do these metrics actually say and this is a thing is something that is usually missing and solutions like sage maker.
You have to then really combine with many other tools and build in there like a lot of extra stuff. And that basically comes all together already with Deepset cloud. So get it right so Deepset cloud with offer me sort of an evaluation tool set right.
Can I get the same in the open source version or it's not present there. You can basically evaluate single pipelines also in the open source version.
The difference is that basically in Deepset cloud you have a full overview over your project where we track all your experiments you can kind of compare them.
Launch easily 20 experiments in parallel and this is actually on large data sets and with open source I think and generally you would need to provision a lot of machines GPUs to run that in parallel.
And that's basically what one thing that we offer and Deepset cloud and the other is basically the I would say just the you I love layer over it. So of course I can work with what Hey stack on and get basically a report around my experiments again maybe a panel state of frame I get some metrics.
What we do when as you on top in Deepset cloud is allowing people to interact with this kind of data more easily like finding examples of queries that fail that. Or that are successful getting feedback from also end users so collaborating.
Basically the the persons who use that search system at the end.
And now that's also what I think what we what we saw a lot that yeah you can extract your predictions and maybe it's like a CSV and then you shared with your next colleague who then I'm kind of rates or give say human evaluation if these queries makes sense or not.
But again this is like a lot of friction you have them a lot of these sees these are exifies floating around.
And what we would be what we do is I think bring this together again having it in one place that you can also in future easily reuse that for other experiments and even use it for training and and have it in this in one central place.
Yeah sounds amazing from what I gather this sounds like a end to end ML ops platforms specifically for an LP neural search right.
Exactly you have thought through so many things not only the developer side of things like experimentation but also you know debugging and actually going through the feedback from stakeholders or users. And then communicating with them.
Yeah and I think this is like something that is missed in many projects like this like end user collaboration and from our experience this should really happen in in a very early stage of a project that also kind of continuously when when you move to production and even when you are production.
And I think this is something which is if you don't have the right tooling that's very annoying to you probably like just building a demo like a UI for some search system.
If you are not a front end developer if you're an LP engineer it takes some extra time and even with something extremely these days it's still is then annoying to do it properly and if you're an enterprise maybe draft some access to it's permission words.
But it's so important I think when you look at what projects work out at the end what pipelines more customers go to production.
It's really a big criteria I think in the early days like sharing a demo with your colleagues and end users really the first pipeline you have more or less giving it to the hands of users and seeing what what they think about it and how they use it.
And there were so many examples where NLP engineers thought they they knew what people were were searching but after these kind of demo sessions or like sharing it I want to see what what people actually do there.
And then they realized oh like they use a lot of key work queries or they never put a question mark at the end or they have a lot of misspellings what else. So I think there's a lot of early learnings that you can make as a developer from these demos and understanding it out.
And also I think on the other side just creating this early aha moment this kind of wow effect and some trust on the end user side is also crucial.
So I would say that's a cycle one point very early demo getting this initial feedback and then probably the second point that we see often is when you then had a time of running your experiments tuning your pipeline kind of the way to production.
I think then at some point a second phase where you you just do again some manual evaluation with end user so not completely relying on on machine learning metrics.
Because we think there's some kind of metric blindness in the industry sometimes you just kind of get obsessed with your one metric that you optimize in these experiments and whatever it is just increasing it from experiment experiment.
And you go to production and you realize wow okay this metric is doesn't say say anything about the user set of satisfaction that I have in the end.
And there are so many examples from our customers where just handing out this pipeline showing kind of like search queries and results and then collecting some easy kind of thumbs up thumbs down feedback and.
And then trying to correlate is that really what we also saw in our experiments in our metrics and in the thing in many cases was that either the pipeline was not yet ready for production and they were like it's.
The far less accurate than we thought or also case where it was the other way around where teams thought are stuck we will never go beyond and like a for a for one score of 60% we do not here it's it's not working.
And they kind of handed out this this predictions or like get this demo and then people actually don't like notes like these predictions are perfectly fine.
And when you then dig deeper I think it's often that engineers not look enough into the data I think I'm just kind of rely on this high level metric. And the thing especially nowadays. These metrics only tell the part of the story because you're like for question answering also for search.
If you have a relation data set and let's say you always label the exact answer for certain question or query. There's just so many ways how you know. Can give a correct answer for for question that is different to this label so to give an example.
And we have many customers financially domain so typical question there is. How will revenue evolve next year and maybe in your data set and the evaluation data set you labeled. It will increase by 12%.
And now at the prediction time your model maybe finds another passage or generates the answer and says it will significantly increase. So like there's no overlap at all from a lexical side still both answers make sense and and are correct and we can probably debate now which one is more accurate.
But in many cases there is they basically give the same same answer semantically. But they're just formulated very differently and that's where I would say traditional metrics fail.
So yeah, we need better metrics and we basically did some research work on that and also part of the haystack where you can do like more semantic answer similarity or as a metric.
But it's of course also just I think looking at your data and looking at these predictions and seeing if they're really wrong on or if they're actually okay and maybe it's some problem of metrics or you are labeling process where maybe you need to collect more different options that are okay.
Yeah, I totally agree it's like it's it's a challenge of intersecting user language with whatever machinery you have to answer that right be it's part search be dense search doesn't matter like users don't care what they care is that their language is understood and often enough it's not.
Especially around things like bird if we go dance bird model doesn't understand engagements right there was a research paper on that and that might actually harm. There was even a Google example where it's showing the opposite like you say I don't want that but they say yes you actually do.
And then take that medicine which might be harmful. And then the metrics is essentially what I get from what you just described essentially you might have offline metrics right let's say and DCG or precision or recall whatever and then you have online metrics right.
And actually crafting the online metrics is is also our an art and it's never ending journey and just recently I came across one blog post which was shared by a former Netflix engineer.
 I will make sure to link it in the show notes as well describing click residual metric right so it's what is you expected success on on on on that let's say segment of your market whatever on the queries versus what you got and then people still keep trying and trying and trying but just doesn't deliver so you could have these as a low hanging fruit to fix your system right and so.
 Do you see that maybe that's already happening in haystack or do you see that that might happen that I as a user might be able to describe my metric let's say in the form of Python or JavaScript code whatever plug it into haystack and let it measure what I want and kind of mimic the online metric in substance.
 So I think like providing kind of custom metrics yeah yeah yeah yeah and you can can can do that to some degree already like plugging in basically like a Python function and forwarding it that's the one way I think the other is probably on a on a note level so you can imagine this pipeline they're providing at some point they can be it answers or documents so you can also easily kind of add custom notes where you say I think this this note should now I will compare it to whatever you want or like maybe you're not on the setting.
Kind of write some locks somewhere like I don't take some some signals from from from the early query. To an extensive the way you can monitor it so yeah I think there's that's probably one of the kind of next steps where.
I see it's more and more online metrics more and more online experiments I would say right now where we see big parts of the market I think that's the.
More and that phase of developing experimenting finding the pipeline getting it initially to production and having their video I would say smooth journey and having a fast path to production. Kind of high success rates for these projects and I would say it's very right now focused on more.
But yeah let's say further down the road if you really think about the whole and add up life cycle. I think on the monitoring side there's this logic and one online metrics but also then things like data drift my queries actually shift into a different direction to these.
I think a lot of our query profiles and think of what I actually these use case how how how can we describe the query distribution and this can be on a formal level like.
Second questions was keyboard queries but could be also on a topic level to understand what is a profile at point a we can match it with certain pipelines but also is that kind of changing over time.
 Yeah yeah you you somewhat anticipate like expected my question or sort of partly answered my question and my next question about where do you see the biggest effort in haystack and and Deepset cloud going let's say beyond amelops you know tightening the knobs and making sure that this flies and works correctly.
More towards I know you guys also hiring a product manager so sort of like more on vision side and connected to that if you will what do you think is missing on the market today still.
Maybe in understanding maybe in perception level maybe in tooling you already alluded also to things like metric blindness right and and and maybe when users get stuck and thinking that this is a wrong system but actually it's not they just didn't look the right way and things like that.
Yeah and there's I think the ton of works to left I think we are we already talked about it I think things progressed a lot in the last years it's crazy to see but still I feel it's with the in the middle of it or just starting and so much more work and things you can improve and then do better.
 Yeah I would say for us right now there's like a lot of different directions but I think especially on the on the open source side we want to improve the developer experience also like simplifying the first steps within haystack I think it can be still overwhelming and I really want to make sure that.
Get as many people to the first aha moment like using all your own data asking a few questions comparing spa students retrieval and really experiencing this first hand I think this is one of the things we work on.
 Then a lot around multi model so we recently added support for tables within haystack so I think one interesting direction right now that you can can really query into these kind of tables in your documents but maybe also further down the road into your SQL database as another data source and then of course everything around images videos audio and it's also interesting for us I think for.
 Our customers it's typically less important than can attack some tables but still I think it's interesting interesting options that you can do there so I think that's like a lot on on open source side and deep side cloud are we really into lounge basically the experiments module that was one big step forward there and now it's a lot around giving there also guidance and suggestions like.
 Like for example now I have the experiment I ran an experiment I've like a lot of these metrics I have a lot of data that was somehow generated but as it's not a single model anymore it's like a pipeline I really want to understand as a data scientist okay like where where should I not focus on or like where what's probably a good way forward to improve this pipeline is a rather the retrieval problem is a rather.
 Another note that I should improve is maybe something wrong with my evaluation data set should I go back to labeling and like giving these kind of at least making these kind of analysis easier as something that we work on right now and then I think further down the road that will be for us a lot expanding in this world ML of life cycles what we talk about right monitoring without just making it simpler to integrate it at both ends so.
Basically on the one side ingesting your source data more easily and thinking it more easily into into deep side cloud so that you can say I know either maybe I have a wiki system that I use maybe I don't know I use notion or maybe I use.
 Confluence or either I know another elastic search class that way I've already my my documents that I'm interested in so we have in there kind of smooth connectors that you can can import your data and directly work on it and then on the other end if you have your API now how can I easily get now a kind of search bar or search functionality in my final product so there's a lot of things and then everything around fine tuning a few short learning with large language models that some of you are quite excited about because as we mentioned I think right now there's already made a big step forward that a lot of use cases where you don't need to train at all anymore and then maybe that's a misperception that you also see in the market I think to the typical users come to us and say like oh yeah this use case how can I train and then we usually ask did you really need to train your own model like have you tried this and that like these kind of combinations and kind of models that are out there certain sentence transformers certain pre-trained QA models or anchor models and they're like no but like our use cases are different and that won't work and in many cases it does or at least they're surprised how good it is already and maybe it's enough to get started on it and so I think that's what I think we have to do is misperception still I think there are then also these cases to be fair where fine tuning still helps right and where you really care about if you percentage points better accuracy and where you then go down and say let's now start labeling let's collect either like we in this manual labeling process or maybe from some more noisy maybe real time like a production day data where you saw what people search what they clicked how can we use that maybe for training that's something where we see big potential probably for for next year and basically want to simplify this domain adaptation to have less manual effort and basically more automated way of training it and that I think was also about that.
So I think that's a good way to do that and then the direction of maybe that's language models.
 Yeah sounds cool and if we go in even in look even further into the future would say I don't know five 10 years out do you think that haystack at some point may even start suggesting the user what to try you know if you go and set up a KPI for yourself right your end goal and then through the chain and that I see that you know then say yes something is going on there.
Then it would actually suggest you also to try some other model do you think it's possible or do you think it's a wrong direction at all like to you drive and leave this to the creativity of your users.
I think it's a combination of both so I definitely think that helps to accelerate and certain parts of your work so especially I think suggesting what experiment to run next or what it could be something you can try.
So I'm a big fan of that and I think we don't need to go probably like five or 10 years down the road that is happening already sooner so I can and hasty and Deepset cloud.
 And maybe just like one thing we are so we have our company something that we call Hockey Friday so it's like one Friday every month where every person the company can work on whatever they want so really hacking on crazy ideas trying stuff out and I know that this Friday people are working on a generative model where you basically give in you describe what you want like what kind of pipeline so you can type in.
And let's say I want documents such pipeline that works on legal data that is very fast something like that.
And the output is basically a YAML file that describes this haystack pipeline which you can then easily kind of load and Python try out and also write a load and then Deepset cloud and run it there.
So we are experimenting with right now and and of course some time for the down the road I could see that you can take what's like signals from from what we know from what worked on certain domains and and basically use that in into this maybe a generative process.
Yeah it sounds cool actually reminded me of the time when I was doing my PhD something like 12 years ago a bit more. I had a collaborator who wrote a paper on taking taking the user text and converging that into C++ code.
 And the use case I don't remember exactly all the details of the use case but I remember it was some way in the airport so like they do a lot of this routine work and instead of repeating it you could actually build a smarter system right so you think this could be the future of haystack or maybe the industry at large.
Yeah at least I think it's like one if you want element that helps accelerating right so if you also if you look at the core pilot right now I like it a lot for calling and I'm still in many cases surprised what what co pilot suggests you're as a as a note on the code level.
And I think something similar as also posted on the machine learning site and you are not only a generic correct code but really something that fits for for use case and to describe it.
I mean I think it's like if you think about the big up picture I think it's one piece that helps you in your workflow. I think it's there's still like many many other pieces that we need to get right and that won't be that's it a holy grail at the end.
What I really believe in is that you need a framework or a platform where we want to call it where you can easily compare things on your data and I think this helps a lot then and creating transparency in the market creating or creating a lot of things.
 And also like kind of trust for your own use case that you are not basically doing a technology choice before you actually started working on your use case and that I think holds for vector databases where maybe today this is a good choice for you but maybe I know one year down the road maybe you want to switch this I think this market is so early and that's very hard to place a bet right now on one of these technologies.
 So modeling side there's like so much crazy bus around large language models and can firstly see the trend going there but it's also I think very important to to understand if that's really useful for your use case now how it compares to much smaller models and and that this should be easy right this shouldn't this shouldn't be big part of your project it should be rather you will try and you will try to do it.
 So I think to think about options you want to try maybe getting some suggestions as well there but this would be I think this is a human creativity part as well and then the actual say swapping of components and comparing their making them comparable I think that's nothing where you should spend time as a developer on.
 And like connected to the question about future maybe I'm closing off on that we recently built with my colleague are netalman a multi model and multilingual search demo right where we used clip model of the shelf without any fine tuning on web data and it showed us really really amazing results right so like where keyword search cannot find because simply metadata doesn't really work.
It doesn't have it and it's multilingual right so and it type it the same query with neural retrieval and it gets it.
Is there anything stopping haystack to move into that direction as well sort of like crossing the boundary of only text right so like you did say multi model in the context of let's say queering a table but I could also query an image.
Do you think that high stack is going in that direction as well.
 I think we're like we're right now working on it so we have a first case where we want to support where you have a text query but you can query also into images from the result side and then basically now the other way around would be probably one of the later ones they have an image as a query until I want to find different media types, I'd say.
But yeah this is like definitely what we're right now working on. I think I also think we need to think always see what are the big use cases and what kind of customers you have and how do we use it.
I think with images there's a lot of interesting use cases mainly in e-commerce I would say that's cool. So we are already supported to some degree and will support more I think in the next month.
 That's great to learn and that also means that I need to adjust my classification because I've been presenting what I know about the players in in vector database and neural frameworks and specifically for haystack I put NLP as the main vertical and I think largely you guys still advertise that as the main vertical but I think nothing stops you from.
Switching that to multi modality right so NLP computer vision and maybe even speech at some point.
Yeah totally I think our approaches there's just a bit like doing one thing to quite a depth first and then moving on to the next rather than let's say starting with very high level basic support for all modalities and then kind of growing all of them.
So what we rather did in the past and still doing is very deep support for texts and we haven't there everything in place before kind of moving on to the next. That's a bit of a philosophy question maybe a strategic questions what you want to do it.
So this field multi is changing quite a lot right so a lot of things generative models really big large models models that I don't know even how to use yet you know like dali.
Yeah I think that's a bit of a bit of a question about the young just kind of experimental interest but probably there will be some use cases. Where do you think else the trends are going in this space.
Yeah so we like want one big trend I think for sure is these large language models and everything around it and as I talked earlier about it. Where is it right now and is it already today really usable is it already kind of worth investigating them comparing them for for your own use cases.
And I think there we are I would say still in an early phase it's look at for example GPT 3 and I think it's primals to the quite nice analysis earlier this year where compared embeddings from GPT 3 to more standard size transformers.
And there we think we saw that the performance is is not bad but it's also definitely not our performing regular size models which are a thousand times smaller cost a few dollars and not thousands and tens of thousands of dollars for for your influence costs.
So I think that's it's basically right now is the let's see case by case that it makes sense for use case. But if you think look a bit further into the next years.
I'm pretty sure and convinced that this is only a matter of time until we see more and more large language models really in production also in search pipelines in production.
And I think that right now it's this phase of figuring out how can we make them really more efficient more some more reliable so that we really can trust these these results there.
 And I can easily update to new knowledge and and I would really but now look a lot into and what I'm personally quite excited about is is now this I think area of research around retrieval based NLP so yes on the one hand side kind of scaling up the models making them bigger because we think learned and over last years that they are good few short learners.
And that's of course exciting because you can just take these models and kind of throw a task at them and they will perform so less manual work of of annotating data creating domain specific data sets and so on.
But I think we also saw that they are not very efficient and there are these other problems.
 How do you how do you actually now teach not to be free about recent events or about your own domain knowledge and typically I think these these data sets that you that you want to search in they're not static right so there's a constantly evolving and you really want to retrain these crazy models every few days or weeks just to kind of catch up with us.
And yeah, I think it's like where this stream of retrieval based or achieve augmented models is super interesting and I think there's a lot of cool work. We work just this week but from from Patrick with this publication around the Atlas model.
So there's basically idea can we can we somehow remove the say the memory part from these big models and it kind of outsource it to a database to an index and then at a query time we still have like a large model.
 Yeah, that's like kind of complex reasoning, but it's kind of basing the generation on some retrieve documents and that can be useful for search but can be also for a fact checking or other use cases and and long story short, I think they have interesting they did love interesting experiments and that paper that show that you can actually outsource quite a bit of these parameters of this memory into into a vector vector database and and still keep the few short capabilities of these giant language models.
And I think this is like a super cool route like yeah larger models but still not putting everything in it not not blowing up parameters parameter size unreasonably. Let's do combining it with now let's say an external document base or knowledge base.
Yeah, I think it's the topic attached upon it's fascinating that on one hand let's say you have a model right and if you if you keep retraining it or fine tuning it on on latest data, you may run into this I think it's called catastrophic forgetting right so.
 Like things that we as humans know that I don't know what is liquid kind of on high level without going into chemistry and it's not that we think about it every single day when we drink water but like it's not that we actually forget it if somebody asks us right no matter how many news or papers whatever the red books right we still remember the basic facts and and I think what you just said.
 With the Atlas model right so approach outsourcing that memory into some database that you can maybe even control and say okay these facts need to stay I never want them to go away no matter what right this are like basic principles and maybe they exist in every domain like finance or healthcare and and so on.
And and yeah, I think this is interesting direction. Yeah, actually all these facts change right can also be that over time you have to adjust effects or knowledge and this is way easier I think if you have it explicitly somewhere in documents not so much in the process of model.
Yeah exactly exactly and like maybe just one example that comes to my mind is like CT CT's change names right and so you could still go back and say what was the name of that CD between you know 1995 and 2000 right something like that.
Yeah or presidents of nations also change right so for this kind of queries I think you want to make sure that you're up to date change it.
Yeah, and I think maybe coming back to search understanding the context will play such a huge role once these models become even more mature and available and knowledge aware.
 But but the the challenge of extracting contacts from the query still is there if I say who is the president of the United States it might you know conclude that i'm asking about now present but if I was couple programs above already saying setting the stage about specific period of time in the past it could actually reason that I'm maybe not asking about presence right.
Exactly could do this reasoning or could you ask a clarifying question right or like or like here.
 A couple of options that you mean this like as you may want more to win a human conversation yeah so I think it's called conversational information retrieval right and I think that we might start seeing this blend of what probably today is called chatbot and a search engine but it could be a search engine which is just clarifying.
Yeah, I mean I think it's it's I think also is in the field I think we are seeing that the search what we understand undersurge is evolving right so it's not so much anymore so I think about web search engines.
A few cases you were still you search search and you click on the website and then you will search somewhere your information. But in many cases we will kind of zero click search now where you have your query and within the search results you already find what you want at and i think this is just.
Yeah getting more and more popular that you're not providing say there's the route to go to another knowledge source but you're trying to really answer the query directly and there's no need to go further.
I will also try to remember to link one paper maybe it's like a series of papers from Microsoft where they try to embed knowledge into the language model and that's.
I think it's a very interesting direction as well as also embedding knowledge graphs into the model right because one way as you said and I think that trend probably still there that yeah you can keep adding parameters more and more billion.
More and more billion three lines but at some point it just simply becomes an on practical in practical right to to have such a large model in production and then how do you find it. But again it doesn't capture the relationships well enough right if you didn't explain it.
Absolutely and just thinking of it we have actually a meetup end of September so if you're interested or anyone here is listening where a news Rimas will also talk about exactly that topic how do you.
And then we'll talk about the corporate knowledge into a language model and and that would be our end of September as well just look for maybe can link it in the show notes.
So our open NIP data series absolutely will do that gladly my favorite question I know you touched many times as well during this podcast which I really really enjoyed.
But what what else drives you beyond you know you have a role as a city or you have a role as a pioneer in this space and maybe educating and reaching more and more people. Is there something else that drives you sort of beyond the tech itself in this field.
I mean that I think my my excitement my passion for NIP is clear. I hope that came came through.
But yeah for me like the technology is the one thing but then really seeing how you solve problems with that like how you can make annoying work of financial analyst faster and better like just seeing that either say first and because they are customer or it's a more indirectly.
And I know that this is now kind of possible. So I think it's like still a big driver for me personally and I think I want to think I absolutely love about open source that it's not just paying users commercial users where you kind of see that.
So we are really this huge community by now from haystack where there's so many different people with different backgrounds, different use cases and it's.
For me often like just end of the day really like scrolling through and all get up issues kind of questions that come in or on Slack when are we on discord.
And what what people are actually building with that and and it's really cool to see what they kind of use case come up with but also how far this actually got that it's.
And using so many companies all around the world from big tech to classical enterprise to start up to build their products on top. And that thing is a stick to one of my biggest motivation boosters that you can get at seeing the community appreciating using it.
And and probably also like on tip beyond get up just recently ran into a guy in a bar who he and Berlin who who used haystack and let's definitely something I never would have imagined a few years ago.
And this kind of happens or what we said of a glass here when we find a bit of a vision and thought about some goals at the company offside.
I think one of us for the open source side that people start putting say haystack experience into their job requirements or the other way around people putting that in the CVs and we thought oh, I guess this is maybe three years down the road.
But then a few weeks afterwards we saw these first job postings where this was required and also TVs where this was mentioned.
So I think it's just cool to see how you can leave a footprint and beyond let's say you are immediate bubble but really kind of spreads it's open it's all digital it's kind of connected in the world right. And leaving those kind of footprint is what I enjoy.
And yeah, search in I think as a domain in us just for me really interesting because it's so diverse as you can go in many directions can dive very deep into an IP can think a lot about the user side at the end for what use cases you can make it work. And can think a lot about scalability.
It's just I think the one of the most for my point most exciting and diverse applications of technology right now. And and one way I think you can really relate to like really can think okay what what is actually possible what kind of information you can make accessible.
And that's that's obviously the beauty of it. Yeah, it's beautifully put thanks thanks for sharing I know some some of the guests that I asked this question would probably think hey why is this philosophical question I'm just you know doing it I like it but that's it.
But I think it gives so much to towards you know you reflecting on what you do because that might also influence your choices in in the tech or in how you approach your users what message you send and so on and so forth and maybe reconsider some things as well.
 And an open source part you reminded me of one story when it was my first time visit in the US I think it was 2015 and it was a patchy coin I was crossing on the traffic light you know on the pedestrian crossing and it was like this narrow avenue you know not narrow white and every right and select takes on my account like few minutes but it's of course not minutes maybe 20 seconds.
And I think it's really bouncing to me from the other side of the road saying I know you I was like no it's impossible it's my first time visit you know I don't I'm not a public figure.
How is it possible and he said he because you build look it's one of the open source kind of you seen in the extriders that I used to work on.
You know which I inherited from its original creator Andrej Blyetski and that's it he didn't stop to say anything else but but he made my day you know and I think what you felt in the bar was probably similar knowing that that person uses haystack and you know it's amazing.
Absolutely because it's just it feels very honest right it feels like it is is not because we know it's crazy marketing or anything like that it's just like a really like a natural community thing and and just building something that's useful for others.
 Yeah exactly which probably reinforces you and gives you these well in this case direct feedback well not only specifics of your of your platform but actually the fact that they're using it and relying on and building a business and that tells the two decisions you made in the architecture and so on and so forth that's amazing.
Yeah I mean like from a company perspective that's one of the fastest feedback cycles you can have right and like seeing diverse use cases diverse developer person on us how they approach things what they're struggling with. Yeah also that angle it was fast yeah absolutely crucial.
I think it's the best and it's like I think it's Elon Musk who said the best setting is when your user fell in love with your product and once you just succeed so yeah there you go amazing and I've enjoyed this podcast so much is there anything you want to announce to our listeners.
 Yeah we just the meet up I already mentioned so if you're interested in LP that's happening in September it will be hybrid so you can join online but if you're if you happen to be in Berlin we also have a small on-site event and then yeah of course if you haven't tried hastag yet maybe check it out on GitHub.
 As a prompt every promise you can get an easy first pipeline up and running and just give it a try to try to question answering if you haven't if you're more coming from traditional search and down on Deepset cloud as mentioned we just released a big new model on experiments with still an early stage with with the product but we have an early access program so if you're interested if you're having a lot of use case that you want to bring to production in a fast way we think about how to scale it how to actually find that pipeline how to collaborate with with your end users and get some feedback there just reach out to us and then we can can get you on the on the early access program.
 Amazing thanks so much Malta I've enjoyed again saying this and this was deep and thoughtful and we will make sure to link all the all the goodies that you mentioned in the show notes and I hope to meet some day maybe in Berlin maybe somewhere else but absolutely yeah let's make that happen and I totally enjoyed our conversation as well so thanks but for having me.
It's definitely interesting fantastic all the best with haystack and and with your research and development. Thanks a lot thanks Malta bye bye bye. you