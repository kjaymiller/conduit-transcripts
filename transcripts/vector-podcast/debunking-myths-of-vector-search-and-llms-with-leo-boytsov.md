---
description: '<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=0s">00:00</a>
  Intro</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=91s">01:31</a>
  Leo''s story</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=599s">09:59</a>
  SPLADE: single model to solve both dense and sparse?</p><p><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=1266s">21:06</a>
  DeepImpact</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=1798s">29:58</a>
  NMSLIB: what are non-metric spaces</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2061s">34:21</a>
  How HNSW and NMSLIB joined forces</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2471s">41:11</a>
  Why FAISS did not choose NMSLIB''s algorithm</p><p><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2616s">43:36</a>
  Serendipity of discovery and the creation of industries</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=2826s">47:06</a>
  Vector Search: intellectually rewarding, professionally undervalued</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=3157s">52:37</a>
  Why RDBMS Still Struggles with Scalable Vector and Free-Text Search</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk&amp;t=3616s">1:00:16</a>
  Leo''s recent favorite papers</p><p></p><ul><li>Leo Boytsov on LinkedIn: <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.linkedin.com/in/leonidboytsov/">https://www.linkedin.com/in/leonidboytsov/</a>
  and X: <a target="_blank" rel="noopener noreferrer nofollow" href="https://x.com/srchvrs">https://x.com/srchvrs</a></li><li>Leo
  Boytsovâ€™s paper list: <a target="_blank" rel="noopener noreferrer nofollow" href="https://scholar.google.com/citations?hl=en&amp;user=I79y2i4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate">https://scholar.google.com/citations?hl=en&amp;user=I79y2i4AAAAJ&amp;view_op=list_works&amp;sortby=pubdate</a></li></ul><p></p><p>Lots
  of papers and other material from Leo: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=gzWErcOXIKk">https://www.youtube.com/watch?v=gzWErcOXIKk</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20250117_030137_0d0e98d093e79861b9dc85d445adcf1e.png
pub_date: Fri, 17 Jan 2025 15:07:51 GMT
title: Debunking myths of vector search and LLMs with Leo Boytsov
url: https://rss.com/podcasts/vector-podcast/1852660
---

Hi everyone, Vector Podcast is back with still with season three and I'm super excited to be talking to my guests today and there is a connection with this episode between this episode and the episode that we recorded with Yuri Malkov about one of the most famous and popular vector search algorithm I can ask WL and I'm talking today with Leo Boytsov, who is the senior research scientist at AWS and he is also a co-author of Animesleab and Animesleab is today used at Open Search and probably some other places that I actually don't know and I hope to learn it as well today.
This is just exciting and I think goes without saying that the whole field stands on the work done by people like Leo and Yuri and others who actually develop the core algorithms and popularize them, improve them over time and then the story unfolds from there.
Hi Leo, how you doing? Hi, thank you for introducing me, it's a great pleasure to be able to podcast. Yeah, it's my pleasure as well to have you. Traditionally we start with the background.
Can you say in a few words your background maybe how you got here and what's your story in search vector search and maybe LLM? Yeah, sure, yeah so background is pretty long. So I've had a rather long career, honestly. Well in my current capacity as you mentioned, I am a scientist at WSAA labs.
For one year I was working on co-generation about this year, earlier this year I moved to a Q-console team, Q-console team works on question and switch at bots that answers questions about various AWS services.
So we can ask like, I don't know, it's like where's my EC2 instance things like that and how I set up things. But I have to make a disclaimer that today I do not speak on behalf of AWS and I cannot talk in details about my work there. So as I said I had a really relatively long career.
Yeah, so most of nearly all of my life, I have been a computer science geek with a passion for building cool stuff and solving hard problems. Yet my professional career started in rather mundane fashion. So I started working client and service of where for financial systems.
This was not my favorite subject, but pretty much the only one that was paid reasonably well at the time. So I had to do a lot of front end and back end engineering using various SQL databases.
I was not satisfied with my career, but luckily I got really interested in algorithms, in particular retrieval algorithms. So I started working on this topic with the algorithms first part time, then full time. But largely as a software engineer, less as a researcher.
And as a software engineer, work for various companies, including two tiny startups in the Russian search engine and the Yandex. So later I moved to the United States and work on the search engine PubMed, International Center of Biotechnology, information.
First again, that was a common topic in my career, started working with I was doing a lot of front end development. But about the class, 40 years I worked primarily on the T-Roll, the core engine. In particular, I invented a pretty need to speed up weighted bull in the T-Roll.
And around the time I also realized that it would be hard to get to the search position without a good degree. So that motivated me to apply a bunch of universities and eventually I got accepted by Carnegie Mell, which was a huge lock. But yeah, so I did my PhD studies there.
And during these studies, I worked on a mix of machine learning and algorithm algorithms without any deep learning. So the vector search or rather similarity search was a part of my graduate studies. So yeah, I didn't use any deep learning though.
It was a mix of classical machine learning, VortoVex style neural networks and digital. So what is an interesting part of that story is that my advisor, Eric Nyberg, he worked on question answering.
And together with his theory and his participated in development of IBM Watson, that's an amazing trivia playing system that 2011 defeated human champions. So that was like one reason why I chose my advisor. It was like such a cool topic to choose.
But pretty quickly I learned about the system and realized, oh, like it's actually really just not just but it's largely such engine on steroids. So Retrieval, IBM Watson, I have a blog post about that if anybody is interested.
But then Retrieval, it's basically really Retrieval based extractive question answering systems. So if you want to improve question answering, you need to improve Retrieval. So that's how I got back to working on quality algorithms.
And again, I saw an opportunity and why big research question was, how can we do information Retrieval using more advanced techniques rather than lexical search with BIRN 25?
And because before birth, nowadays like everybody just uses like word-based models or any like other transform based models to create dense vector embeddings and they are quite effective, that was not the case when like 10 years ago.
So whatever we had there was pretty ineffective Retrieval. And so my thought was that because the single representation was not effective Retrieval, those need to be somehow combined and assembled. So you basically don't get a single representation.
You use a combination, you use combine similarity and then you treat this similarity as a black box and then you apply generic Retrieval algorithm. So this was a pretty in hindsight that was a pretty ambitious project that required working on both design and effective similarities.
And Retrieval algorithms. And that's why we, well one, that's where that animously library turned out to be very useful. It was instrumental to this work. Although it was created for somewhat unrelated people.
Okay, so that was an overall rather bumpy, right things didn't work well initially and I got a lot of help from other people in particular from my author, David Norva, who proposed an amazing improvement for one of the algorithms in a thermosleep.
Yeah, and so we published and opened after my graduation. And when I was writing my thesis, yeah, I was found like a bunch of issues with my previous approaches and realized that I could also use like a H&SW like algorithms which were not like core part of my thesis work.
And I got even stronger results, but that was like a little bit too late to publish and use otherwise. Moreover, that the similarity that I used was a sort of a face palm realization that that similarity that I used, like Retrieval, completely as a black box. And it worked with most effective.
It was more effective than B125 on the collections that I used. But I didn't realize that this black box similarity was actually representable by another product between two large sparse vectors by another former author Chris Dyer pointed this out.
And if I embraced this sparse vector approach from the get-go, it would have been a much easier problem to solve from both engineering and scientific points of view even without work. And okay, it could have produced some more impact. But yeah, a little bit too late to dwell this now.
Okay, and enough with that I graduated six years ago. And since then I haven't working as a researcher scientist and engineer on deep learning in what specifically I had in training models for specific initial computer vision and Retrieval.
Despite this diversity, things have come a full circle and are working question as being systems once again. Yeah, that was pretty much involved. Yeah, amazing story. Yeah, thank you for that.
It's like the story tends to repeat itself, but at the same time, if we find the topic still exciting and it seems like you are still very interested in question answering and improving building blocks of that, it's kind of cool, right?
So that we are able to come back to some of the topics, pick them up on a different level.
That's amazing. And yeah, there is a lot to unpack. I almost wanted to ask you or the moment you spoke about spars and dance. I wanted to pick your brain on what's it take on the model called split and split V2.
I don't know if you're familiar with that model, but basically, you know, there is always this discussion should we take lexical search, combine it with dance search and then do some kind of hybrid formal on top and then how do we even learn the parameters of that model, right?
Depending on the domain.
But then there is a drastic sort of approach. Let's not do that. Let's just take a complete model which can handle both and then you can also support what the dance search doesn't support like exact phrase searches.
What's your general intuition about that? How do you think about this? Well, that's a super interesting question. I have one clarifying question though. So, before I answer, you said that some people who want to have a single model that's doing both.
Could you elaborate a little bit on this? Well, I guess maybe it's not that they wanted, but it's like the development when, instead of sort of, you know, combining these disparate sources of results, you know, one coming from lexical search, which is kind of like well-known BM25 driven, I guess.
And then the other one is more like more modern in a way that everyone wants to get exposed to dance search. And then you need to somehow figure out how you combine the results, right? So one is designed maybe for precision lexical.
The other one is designed more for recall, right? Because the vectors are not, they don't have as many dimensions as these far specters.
But then you still need to figure out, okay, how do I combine this to? And usually people cite reciprocal rank fusion in what I hear, but there are other methods as well, like even clustering based. But then that's one approach. Another approach is just stop doing that, I guess.
If I really understand what split does, and then you encode with split your data once, and you retrieve, you know, you use its capabilities to also retrieve exact phrases, right? So, effectively, ideally, you don't need the lexical matching engine anymore, but maybe I'm completely wrong.
I'm just, I wanted to hear your opinion on that. Okay, well, let's get it. Using your words, it's a lot on back here. I'm still not quite sure what you mean by having like a single model.
Although, maybe I love me try to maybe start answering questions and you can drop me and guide me into the other direction if needed.
So first of all, we have what's interesting about Nashville language is that, and that's very different from computer vision domain, is that we usually represent, we can we have multiple ways to represent text.
So in computer vision, usually it's just like each image is a traditional representative of actors that was the commodity theme.
 But in the in a national language processing, we started with the so-called bag of words representations where a document was represented by basically a sparse vector where you will have either zeros and ones, which means the specific terms present or not, or maybe weights, not just zeros and ones, but weights.
But then, with development of deep learning, and I actually started a little bit earlier with people, people learned how to represent text using fixed size vectors. And that was like using principle component analysis.
And this is not a very natural representation for text and it didn't work really well initially. But now we're having good results. So we have like two representations and there are different approaches to combine those, of course.
One is just if you want to do the T-wall, you can indeed just do the lexical base search, you can do a kidney or a snabestation vector representations, and then you can somehow merge the results. You can use ranker.
But you don't have to, and that's the so-called hybrid search, but the hybrid search can exist in different versions.
So if you want to combine it sort of in a single model, why don't you represent each document using both sparse and dense vector? And when you're computing the similarity, you can compute the similarity between sparse parts, between dense parts, and then combine them somehow.
For example, using a weight. And that's in fact what I was trying to do in my thesis as well, because I was doing, my similarities was basically an ensemble of several similarities course for at least two representations. And that could work.
There's of course modern instantiations of this, and there's a paper, I think both are by some glue people, where they did exactly like this, they combined splaid and some dense vector embeddings.
And that can work apparently a little bit better than, or sometimes maybe a lot better than basic representations, like each representation specifically. So with both approaches, of course, there are issues that you mentioned.
So I don't know what the best approach there, and I don't have a crystal ball regarding what's the best path forward. But with dense representation, the clearly the problem is that you have to pack everything into the fixed size vector.
And as your document is getting bigger, you basically the vector size, the amount of information you can store is the same, but your document increases in size. So you would possibly expect some deterioration in quality.
But another reason why you can see deteriorating results just because some like you have fixed representations, the number of words is huge.
And like in regular person knows like around like educated person knows about 30,000 words, but in reality, like internet has millions of words, right? And the words are not just only words, there are things like product identifiers, right?
If you want to, and sometimes people will do products, they will search something they want to buy, and they would you know copy paste those, or type them in, and then they got squished in the in that dense vector.
So it cannot be precise.
 There is an interesting paper by author by Neil Srymer's, sentence board author, where he has a, in like some experimental and even theoretical evidence that as the collection size increases so the dense vector search can deteriorate just because there would be some false positives and measures due to you know the excruciating a lot of information together and they fix size directly.
So yeah, I mean, it's quite possible, but I haven't seen like a fall off of this work, so I don't know how much of a problem it isn't in practice. And coming back to the sparse representations, so yeah, they could potentially use all this issue, but not necessarily with displayed like models.
Well, the problem with display is that displayed models, they create those sparse representations using the, not the words themselves, they're using sub word talking.
So as a reminder with like models like with transform models, they create this sort of new sort of vocabulary that has some complete words, but most words are incomplete.
 So like they have like extract prefix, suffixes, parts of the words, and this is your new vocabulary and the difference between these new vocabulary and the actual vocabulary that people use or use on the internet is that it's limited to, it can have like 50,000 talking, maybe 200 talking and some of the advanced modeling models, but we really have like millions and millions of words.
So of course, that would also lead to some deterioration in quality false positives, and especially if you try to represent, represent long documents using this fixed size vector. So it's sort of sparse in more, it's more sparse in some ways, but it's still fixed size vector. Doesn't make sense.
Yeah, it does.
I mean, it's very insightful, what you said that like basically to make my question much more succinct, I could ask, you could we just use splaid for everything? And like instead of, you know, combining different approaches, just use splaid, but you basically answered it really eloquently.
You said that splaid itself has limitations, right? For example, that would not allow us to properly embed all variety of the language and then obviously dealing with longer documents is another issue.
There is an interesting extension to this, so I was just recently listening to a presentation on the extendable splaid where they extend the vocabulary of splaid by eddy entities.
That's one interesting direction of work, but another interesting direction is like the so-called like deep impact models where they take a document and they do document expansion using like, you know, the doctor query style models.
And then they for each talking, I think, in the document they are learning a weight. And so this is like a little bit more less limited, I think.
But in the end, I think it's whenever we, yeah, so basically if like to be able to handle those like rare, we need lexical representation to handle, you know, bigger vocabularies. And it's probably hard to model with just fixed size vectors. Yeah, it makes a lot of sense.
At the same time, we also know that, well, it depends on how you model this, but lexical approach, like vanilla lexical approach would miss semantic links, right, and sort of understanding of larger context, because all it does is that it kind of looks through the VM 25 model at the words.
And sometimes it just pays attention to some words, but doesn't pay attention to other words.
And it may miss the main point of the query, right?
 But of course, this model still worked for a new work that Yandex, you know, it best, this model's worked previously, probably by virtue of you training the users that, hey, don't give me the full sentence, just give me like, you know, specific words, like chopped list of words that I need to look up.
And that's how I guess inverted index worked out.
And of course, you need to have on top of that, you need to have very smart reranking strategy to pull up the documents that are really relevant, right?
But I guess today we have we have this new, well, I keep calling it new, but it's not maybe necessarily that new, but it's still fairly fresh development of dense, dense retrieval that not many companies, I think, have been boarded in the products yet.
But it's a very interesting direction, and still you need to combine the two worlds, right? So it sounds like from what you said, the only way to get better quality is to combine this approaches rather than try to develop one single holistic model to handle everything.
Oh, I, yeah, it's a great question. I actually don't know what's the best part forward is. So I highlighted the, the deficiencies and advantages of different approaches.
But I also want to comment on the deep impact model, the deep impact model, I think the way, maybe I described it, it was, it sounded like it is like a BM25 model, but it's actually not.
So maybe we should have, like we're talking about sparse representations, like learned sparse representations, because it's a bigger topic and it's much bigger topic than most people realize sometimes.
So people know BM25, people know dense vectors, and these are, these simple things, but there is a lot in between.
So first of all, what you can do, and that's what people did, and even the doctor query is the most famous way to do so, but it was actually not even a single group of people who proposed this.
So what can you do?
We can take a model, a deep learning model, contextualized model, maybe not necessarily contextualized, but contextualized models, they do better job because they look at the model as a whole, the document as a whole, they don't like look like a devidue chunks of document, right?
So they kind of can understand what the total meaning of the document.
And then they, they propose new keywords on new terms. So some like synonyms, synonyms that could have been in this document, but they are not. And if you add these documents to the new, if you add, sorry pardon me, if you add these terms to the document, then this missing synonyms are there.
You can index this document. So basically this is document expansion. And you can do document expansion. And that helps resolve that lexical mismatch, mitigate lexical mismatch between query and documents. And I claim it's easier to do this expansion.
That there are like of course approaches that do query expansion, basically adding synonyms at the query stage. But why claim is that it's much harder to do this accurately because there is much less context.
So this is one, you know, this is one direction of fixing things and creating sparse representations. Like there is a split model. What does this play? What does this play model? It's completely sort of it's doing something completely different. It looks at the document.
And there is a vocabulary, like bird tokens. And for each token, it gives you a weight. It looks at the document sort of understand this meaning that says, all like this is like, this is a word, a prefix or word. It should have this weight. And that's how you get a sparse representation.
But with deep impact, you're doing something slightly different. So you take a document and you do this document expansion. So you add words like synonyms. But then you don't index this document using build 25. Why? Because build 25 is clearly old style and it doesn't take like context to account.
So instead of that, you train a transform model that would give you weight for each term in the document, in the expanded document. And then you use this for it. Oh, that's very easy. And that's called deep and that's called deep impact models. Yes. Yeah.
We should link that I guess there is a paper for that as well and should be able to link that. Yeah. That's very interesting. And it's also interesting that what you mentioned about the dance model sort of not able to capture everything that you want them to capture.
And yet, this becomes a building block in the application phase, like for example, in Rage or a Givalogmented Generation because effectively, the only method that I heard so far off, which is circulating a lot is just chunk it up.
You chunk all documents up and then you hope that the chunk size is less or about the same as capacity of the model, right? Because otherwise, it will chop off the end and you will lose the part of the meaning.
Or you also apply some methods like some level of overlap, right? So you can then index a few more chunks in the same entity and then try to query. And then interestingly, you can generate questions out of chunks that these chunks might be able to answer.
And then you search those questions instead of the chunks themselves, right? So which comes back to what you said about Dr. Query, I guess. So it's very interesting that like we are sort of like standing on a set of building blocks that themselves should be optimized and optimized and optimized.
But I guess we already in the phase globally when everyone is trying to derive value from LLMs and Rags and everything, right? And yet, we can stumble upon some really tricky situations. Like you explained. Oh, it looks like we have a lot. Yeah, it looks like we have still a lot of research topics.
Yeah. A lot of answering questions. Yeah. I wanted to a little bit digress from here to the work you've done at NMS Lip and I want to read it from your from the GitHub repository. It's a non-metrics space library.
And I did spend some time in my rework life, you know, when I was studying mathematics and we did study a bunch of, you know, metric spaces. I have never realized I would never really like imagine that this highly theoretical stuff would now connect so deeply to practice and it's amazing.
But can you tell me why it's non-metrics space library? Isn't it so that the whole idea of, you know, vector searches that we choose some metric, Cousin or dot product or whatever it is. And we are, and that's how we express the semantics similarity. Great question.
So the reason why it is we decided to not limit ourselves for to metric search because we felt and that's also a feeling of other people is that metric search is is limiting. So it's not expressive enough. It turned out to be true to some degree but not as much as we hoped.
And indeed, in many cases, so and why we're doing so, the representation learning was not as developed as it is now. So we felt like, you know, we need to be able to, people will engineer those complex similarities and we need to support individual using this complex similarity.
This did not happen.
But what I think happened and that's I want to connect this to my statement that in the end of my graduate studies or rather after defending a thesis, somebody pointed out that the similarities that we were using were basically representable as the sparse similar product between two huge vectors.
So it's some sort of it becomes similar to either deep impact or split. And in fact, so the similarity is the maximum product. It's not the cosine similarity. And the the search like the search procedure is called maximum inner product search.
So basically, you want to retrieve documents that have the maximum inner product between query and the document.
 And the and this is not this is a symmetric similarity measure in some sense symmetric, but it is not it is it is not a metric and it's not easily reducible to the to the cosine similarity and to the creature searching using a science similarity is actually fully equivalent to searching using the Euclidean distance for the inner product you can reduce this search to a cosine similarity and Euclidean distance, but turns out that this reduction affects efficiency.
And then somewhat like bigger topic for a discussion, but what happened is that people say at at Lucene, who are maintaining Lucene, they were adding support for the maximum inner product.
And Vespa did this and they did this through this trick to reducing of of maximum inner product to cosine similarity and of two. And I argued that there is research showing that this is suboptimal and there was a discussion in this as a result they basically didn't do this.
So so a long story short, I think a lot of things are so non-metrics similarity search as in general turn out to be not so useful, but there are some instances like maximum inner product search where we do have things that are non-metric entities widely used.
Yeah, that's amazing, but I think I hope that equally as I'm learning, I hope our listeners are also learning on this because often times when you plunge into a new field, let's say, then search, all you see is what is being popularized and you know you may go down the rabbit hole.
So I'm really excited and thankful that you are able to share and much wider perspective over things. And then also most interestingly, you work and you say you're a co-author of an MSLIP besides other authors.
Your collective work is also now used at like for open search, engine, which I believe I also had a chance to test at some point and like it's a C++ library that is then somehow loaded of GVM and basically then searches is performed using H&SW.
Can you tell me a bit about that like that story of how did you end up you know connecting these to an MSLIP and H&SW and here I will probably link to the episode theory that it's quite popular today.
 Yeah, well first of all I have to say that I mean it was popularity like close to 100% of an MSLIP is certainly due to the development of H&SW which was Eurisk creation not mine and we affected it in only very minor ways because I think the I mean we provided the platform and yeah so I think one trick that you reboard which I borrowed from KGRAF was the efficient algorithm from him or she checking but that was it.
So the end of the sleep but end of the sleep it was like creation of several people and it was like has like a rather wild story so it was never planned in the sort of random how we developed.
 So in 2012 I attended the conference where I met Billik Nidhan who was working on and he was doing his PhD on similarity research and we found that like a lot of like we shared some interesting particularly in the written algorithms and we decided to do some joint project together and then my initial interest was how do I support not it was somewhat academic topic no metric search is as I explained before it's still like largely more like academic interest because a lot of things are really metric or at most maximum winter product search which is sort of almost a scientific narrative almost metric.
And yeah so that was basically purely for academic interest and I connected it to the to the machine learning because I saw an opportunity to use machine learning to support generical algorithms that would do a k-nearest new research with non-metrics simulator such as scale divergence.
 Yeah so we did it as a machine learning course project and we published paper at Noribs and it could have stopped at this point but then I also like that conference I got like I met other URIs that conference or just treated that I met with some of with Yuri Adir Quothar, Alexander and they worked both work at Merrill Habs company where they developed small world graph approach and that was like a general version and so Alexander was really interested to prove that whatever algorithms that we have in Emma Sleep and we were tackling generic search in generic spaces for generic similarities and he was eager to prove that the graph based algorithm I actually truly generic and this is why he and his student that you know created the first version of a small world graph in Emma Sleep so basically contributed this version and that was a really super slow I spread it up by both 10x and that was the version that we used to win the first in Benchmarks so it was pretty good but it has issues and one issue that was fixed thanks to Yuri sharing with me some early version with H&SW and I looked at the code it was not as like fast version that he created later but already there was fixing something and maybe he didn't realize he showed me that piece of code and I realized oh like there is actually still an issue in SW graph so SW both SW graph was improved and Yuri like contribution is W2 and Emma Sleep so it greatly it was like a huge contribution like big step forward it won the second NNBG Mark competition they proved SW graph was I think the second the second algorithm I have a screenshot of this somewhere which I sometimes included to my job talks and the H&SW also influenced face because they realized oh like they actually knew about K graph and knew about the graph based retrieval but there was one important reason why they didn't you can ask me why but anyway so it influenced face and a lot of other people and of course yeah that that Yuri created that was Yuri's work like a huge impact in the rest of his history but I think Yuri shouldn't complain no he has a great career first he has great career first Twitter and now it's at OpenAI so yeah it's a magic story just a close of the wolf white it face did not implement the approach you had this is this is a really interesting thing because that's one of my favorite pieces in this story well turns out that the the graph based retrieval algorithms they had long history so a lot of this was rediscovered on the the pruning heuristics and like the basic algorithm they go back to papers in 80s and 90s but people did not use it and one hurdle was the inability to efficiently create those K K nearest neighbor graphs and K nearest neighbor graph it's a simple concept you have data point and you have you need to find some data points that are nearest neighbor of these data points and then you connect to them module or some post modification of this graph but how like you know if you have end points it is in the brute force approach and squared computation how can you do this efficiently how can you approximate this so the way how it was done before people were coming up with fancy algorithms how to how to approximate a disappointment and those fancy algorithms would not particularly scalable and K graph I think is not particularly scalable we played with it we actually incorporated K graph in primitation into enemy sleep and it was indeed hard to create large graphs because it's a nitty-fragurithium and yeah it's not it's not very scalable but what what Yuri and Kothar did for as small world graph and it was before she was done we were while they were at Merrillab's company they realized that they can combine the interval and creation of the graph and they can do it efficiently like in what like using modern terminology embarrassingly parallel fashion and that was I think one key missing block that prevented graph based algorithms to become practical yeah that makes sense yeah it doesn't like what what excites me in this story that you shared is that how serendipitous the this like the discovery process is right and like something that feels like random leads to I don't know creation of the industry right you could you could largely say that the new industry of you know vector databases and vector search and now rag on top of that is created because you guys worked on practical implementations of something that was also that that stood on no shoulders of you know some of the inventions and research done before that and so it's kind of like natural progression but I mean it's amazing how you know it's just on the verge of you not meeting someone at a conference could basically need to possibly not creating an industry right quite possibly I think well thank you for the kind words and of course it's not because of us the if not for us I think other people who have done this I yeah so I with them but anyways so I think we did useful work and clearly people are using a tremendous double you lot and people using it mostly even even though it hasn't like a lot of issues but it was still ended up being used rather widely and the one reason why it was used so widely because people who needed a library the Python basically a library that would do Kenyar's new research and it would do it from Python and people often take these little things for granted but say initially I honestly didn't have Python bindings and to participate and then benchmarks and have something useful you would need to have Python bindings this Python bindings were written by Billek I didn't I didn't create those bindings so he created those bindings the first version so there you go that library was possible to use and at the moment there were at the moment the it was not such a big choice of libraries to do Kenyar's search so in terms of the competition there was anoy which was slower noticeably slower there was another library flan which used similar algorithms to anoy but it was less optimized and it was also slower through three wall there was a keg graph but it was not like so easily usable and yeah basically that was it and later came face but it came it was only released I think a year a couple of years later after well definitely after yeah it took several years for face to appear and people started using it so at some point there was vacuum and I honestly filled it now like other approaches that are taking over yeah so yeah it in summary there wasn't a lot of serendipity but I wouldn't take credit for it in the industry it would have created without us for sure yeah yeah maybe or maybe not and it's also well I think it's quite a photo like a better work typical of the end turn not to recognize the impact they're making because the moment they do recognize that that's probably end of story so like you need to be constantly sort of low ego and pointing at the goal and I think this is what you're doing and that it feels like this is your approach but you also do do quite a bit of impact I could ask a ton of questions obviously and I could relate also to the fact that what you explained about some of the struggles like how to optimize these algorithms because at some point I did embark on participating in billion scale and thench marks and I I think I failed miserably but at the same time I did have some code which worked on a small scale and one of the building works there was hnsw with very simple I would say with very very simple intuition that you just make several I think several passes through the data set and you try to bind points in space that are closer to each other and then you would push them to some common bucket I called them shards and then you would build a nation s double index for those shards the only thing that I couldn't figure out is for those shards I still needed to have an entry point to quickly sort of identify which shards I should go down you know through when I when I when I find similar similar documents for the query and I did attempt to modify hnsw code in the enemy sleep you know to to like get me only first layer of the graph so that I can pretend that that's my layer for entering the shards I just ran out of time but I see it was very exciting and also thanks to the organizers we had access to really beefy machines which I think I had I haven't been giving like good use I was mostly burning the you know CPU capacity and memory but I think it's an exciting field and what what I hope is that like with the vacuum that you mentioned that it doesn't happen that this torch will be carried forward and then someone will get excited about and not afraid of trying new things in this space are you yourself still like looking at obviously you're looking after enemy sleep but is there something that particularly excites you in this field that you would be working on or you are working on?
 Yeah great question so first of all yeah I am not sure if if I would do any work on vector search in the I haven't actually not maintaining an enemy sleep pretty well recently I'm just didn't have a lot of time and there was an issue with building the so I will still fix and support later version of Python for sure I was like you know like piece my piecemeal work I found like say half a day to fix like Windows build something else popped up yeah so it is an exciting field while it's also become really busy and another thing is that I still see the focus main focus it's not it's not very appreciated so the like you said you said I mean there's a really nice voice that all like that helped industry to be created and maybe it's true to some degree it is what yeah is it appreciated by you know your potential employer no it's like zero appreciation so it it isn't it's it's it's it wasn't still is somewhat niche topic and most people are of course I interested in how do you solve intelligence in that you know broad sense of the word how do you create models that can be seen and if the model like how you can combine them that this is like you know this new agentic ecosystem and yeah so all that stuff that really excites people it it isn't this you know plane of or space of large language models machine learning deep learning intelligence you name it yeah so that's why I do have ideas I did tested some of them but you know things usually don't work but yeah I don't have time to think systematically about this issues yeah but I guess at the same time you did create the base for for you know for other people to innovate and I think it's I think it's highly appreciated really I also wanted I also wanted to pick up the topic that originally sort of interested when you interested me when when you popped up on my LinkedIn you know feed you you made a statement about relational databases trying to implement search feature or sort of capability and sort of like miserably failing and that maybe you didn't use the word miserably it's it's my word here but I wanted to do a little bit like expand on this like why do you think they tried to do that and also while they were trying that what went wrong yeah great question well first of all I definitely wouldn't say the word miserably because the it it has been success to some degree definitely and it's not it's not over until it's over so the people are working with this so what I have been observing for many many years and I as I said I did start my career as as a person working on databases and doing a lot of writing a lot of SQL so but the the typical database is a very different beast from what you typically need to do information to you so the first of all they all like the early databases or they are oriented they achieve some tradeoff between the so they need have got throughput in both inserts and updates and they need to be able to update information pretty quickly and also it should be pretty reasonable and they also support really like the the data the data can be pretty complicated that what they call that SQL schema there can be multiple tables and all of that needs to be supported and so of course there are tradeoffs to be made to make it possible and again to support generality support efficient updates support efficient inserts but at the same time if you're doing the TV system a lot of of this is not necessary so say for you want to do like keyword-based retrieval you only need to all at high level is somewhat a simplification but you need to memorize them in which documents you have which keywords and then you have this so called inverted index where for each keyword you have a list of documents where these keywords appear and it's much simple structure it permits much more efficient compression algorithms so it's again it's it's a different beast and and also in terms of efficiency of updates once you compress data and once you represent it in a special way it's it becomes much harder to to make these incremental updates for which those early databases were applied so clearly there is a disconnect it was somewhat removed with the introduction of so-called columnar databases but it's still like with columnar databases I believe they actually do not favor those you know point updates anymore they they are best to be used for bulk updates and so basically once you're doing bulk updates yeah you're sort of in this search engine area where you ask you you change things in in rather large increments changing the access in rather large increments and you don't you don't worry too much about your information is being like really up to date you can wait maybe a day maybe a few hours but it doesn't have like an instantaneous update of the database so this is a different trade-offs so yeah um well of course there is a disconnect and this is why it's it was always hard I believe to add to add like food tax indexes to regular databases but another issue with the the disconnect is that like again the retrieval often needs like really different set of specialized features so if you have a relational database system it's pretty hard to support this for example like deconization if you need to do deconization in multiple languages yeah so of course that's part like you know the creation of those specialized tools with a lot of features like you've seen and VESPA and databases are catching up but there is still a gap and you know it's probably like going to be really tedious to to support yeah full set of features like you know you need to match VESPA so yeah these are like my five cents on this stuff yeah but I'm curious to sort of a little bit the understand why do you think databases are still trying why are they trying to encompass this seemingly disparate ways of searching right when you actually if basically like you explained if you need to have a fully blown search engine that can support multiple languages tokenization and so on you better be using the likes of recene VESPA and you know maybe elastic search on top of the scene and so on why why are they still trying they want customers it's of course advantageous to be like you know one stop shop so they come to specific provider and they have everything so I listen to a podcast the roxette co-founder which was the roxette the one was acquired by OpenAI but I think you recorded that podcast before they were acquired so good timing and you can clearly hear that message all like we really want people to come and use our solution so we have hybrid search we have some support for ranking we have this and we have that yeah I can't I can't argue against this being convenient so definitely something something very useful customers yeah yeah just a small correction he's not a co-founder I think he's well VP of engineering or used to be a VP of engineering in roxette but yeah I mean he's he brings the story and I encourage listeners to listen to the episode he brings the story of you know roxDB scalability issues from from Facebook and how it underpins you know the the further journey at roxette so I feel like we could discuss for five hours and I'm actually a big fan of Lex Friedman podcasts where some of the episodes really really long and you can listen to them for weeks and and I think I really hope that we can record with you sometime later as well as you know as you have topics to share but is there something Leo that you want to share I don't know it could be a paper you've read that particularly excites you maybe a book or anything else that you want to say yeah I think we yeah great question so so I was interested a lot recently very recently I mean maybe the last couple of years in how LLAMS can be useful for search in one particular interesting direction is how do you use LLAMS to to train smaller models for retrieval and ranking for me personally it's a very exciting area of research yeah as far as distillation is concerned there was several interesting papers on the topic there was but basically the lot of of that work revolves around creation synthetic data synthetic queries based on the documents like we have a document that creates the queries and queries that you asked that that pretty slash question and the answer is in documents we have a positive relevant document and you can sample negatives from from your collection and train them while but there is also a line of research where they they would try to create both queries and documents so yeah in summary the that whole that whole not in summary but that that that line of research was particularly interesting to me although there was some work before LLAMS to create synthetic queries it was not particularly well-used technique but one paper that stood out was the in-part paper from a couple of years ago and we have reproduction of this paper in that paper had a a pretty quick fall-up from the there were several several authors from Google they called it Protigator where they showed how this technique can be improved and there was another fall-up from the with the same first author now she transitioned to the mind and now they they showed like like oh like now we do it like somewhat better but the they they found one issue with the synthetic query generation approach that not always the the document that was used to create the queries the most relevant document so you would think it sort of makes sense that if the question is being answered by this document it is the most relevant document that turns out if you ask a question there can be other documents that that answer this question and they can answer that question even better and so they solve this problem using you know a relabeling approach so that basically the due to the will they generate synthetic query from some document they they do it you and they do then they look at the top say 10 documents and they they use another LLM to decide whether these documents are relevant to the query or not yeah it's also very interesting paper as well I yeah and finally the last couple of papers that I encountered were regarding creation of either creation of documents either just joined here with queries or based on the queries this is also very interesting for long yeah that's amazing thanks for sharing and I hope we can link all of these papers in the in the episode you know yeah absolutely because I think one of the goals of this podcast is to continue to be educational resource not just entertainment maybe some people potentially viewed as an entertainment entertainment and then good sense of word you know when you want to sort of a little bit like break away from your daily routine and then listen to some of the insights and and we heard a lot of insights today from you thanks a lot for sharing Leo and I wish you all the all the best in in your in your projects and your current projects in your future projects and yeah I mean I would be all equally excited to talk to you at some point as well because it does feel like you have a lot more to say than I'm able to contain in the in a single episode yeah it's my pleasure thanks a lot for inviting me I enjoyed the podcast I enjoyed our conversation very thank you very much Leo and good luck bye bye