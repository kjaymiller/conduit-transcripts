---
description: '<p>Topics:</p><p>00:00 Intro</p><p>01:54 Things Connor learnt in the
  past year that changed his perception of Vector Search</p><p>02:42 Is search becoming
  conversational?</p><p>05:46 Connor asks Dmitry: How Large Language Models will change
  Search?</p><p>08:39 Vector Search Pyramid</p><p>09:53 Large models, data, Form vs
  Meaning and octopus underneath the ocean</p><p>13:25 Examples of getting help from
  ChatGPT and how it compares to web search today</p><p>18:32 Classical search engines
  with URLs for verification vs ChatGPT-style answers</p><p>20:15 Hybrid search: keywords
  + semantic retrieval</p><p>23:12 Connor asks Dmitry about his experience with sparse
  retrieval</p><p>28:08 SPLADE vectors</p><p>34:10 OOD-DiskANN: handling the out-of-distribution
  queries, and nuances of sparse vs dense indexing and search</p><p>39:54 Ways to
  debug a query case in dense retrieval (spoiler: it is a challenge!)</p><p>44:47
  Intricacies of teaching ML models to understand your data and re-vectorization</p><p>49:23
  Local IDF vs global IDF and how dense search can approach this issue</p><p>54:00
  Realtime index</p><p>59:01 Natural language to SQL</p><p>1:04:47 Turning text into
  a causal DAG</p><p>1:10:41 Engineering and Research as two highly intelligent disciplines</p><p>1:18:34
  Podcast search</p><p>1:25:24 Ref2Vec for recommender systems</p><p>1:29:48 Announcements</p><p>For
  Show Notes, please check out the YouTube episode below.</p><p>This episode on YouTube:
  <a href="https://www.youtube.com/watch?v=2Q-7taLZ374">https://www.youtube.com/watch?v=2Q-7taLZ374</a></p><p>Podcast
  design: Saurabh Rai: <a href="https://twitter.com/srvbhr">https://twitter.com/srvbhr</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20230311_070307_5788fcdf763e7dd822dd4b0bbb59f9b6.jpg
pub_date: Sat, 11 Mar 2023 19:38:10 GMT
title: Connor Shorten - Research Scientist, Weaviate - ChatGPT, LLMs, Form vs Meaning
url: https://rss.com/podcasts/vector-podcast/861832
---

Hello there, vector podcasts. Season 2, and to them super super super excited to have a reappearance of Connor Shorten on vector podcasts. We recorded like a year ago about that time. Something's changed. He is a research scientist at Semi Technologies, the company behind VB8.
Here you can see an episode with Bob, and here you can see the episode with Connor as well. And back then when we were talking, Connor, you've been a lot into basketball. Do you still play basketball? Yeah, I still play a little bit.
And I'll add also to that that I think you also have podcasts with Eddie and Laura, also in the queue of we've read. We'll add that. Exactly.
And I remember like you've been big on computer vision, data augmentation back then, and you first like guinea pig task was you know some capturing baskets in the basketball game. And I wonder if you continued working on that at some point.
Yeah, I think about it every now and then, but I've been so captivated by the natural language processing and the tech search honestly. I still think about image search a bit, but yeah, the tech search to me is just it's just so exciting. It feels like there's so much that you can do with it.
And yeah, it's really been it's been an intense year. I've learned so much and I think it'll be a totally different podcast with respect to like what I'm this talking about. Yeah, yeah, absolutely.
I actually love to start also by asking you, what do you feel you've learned in this year that has changed something fundamentally in how you perceive vector search today versus back then and year ago? Yeah, that's a big question. I think I'm definitely with we V8.
I've learned a lot about having like kind of the user focus, the product focus definitely way more engineering understanding of the distributed data system, replication, cap theorem, all these kind of things.
So like the knowledge of the engineering around it in addition to sort of the machine learning research about like how to vector representations get optimized with deep learning models and then you know, this whole retrieve and read research.
And overall the space is evolved in such an amazing way and it's just really exciting. Yeah, absolutely.
I've been I've been also following all different things reading papers, you know, implementing clip, but I still feel like I miss out on so many things and I really hope we will cover some of them today.
And we on the verge of I think maybe witnessing a change in the search paradigm, you know, with chat GPT. I first I wanted to sort of get your first reaction on this. Obviously you tested it. I also tested it actually with when I published my recent blog post on neural search frameworks.
And I was like just stuck on creating a title and I asked chat GPT, can you come up with a title and came up with a reasonably good title and I actually used it without editing. And I read a bunch of other stories, you know, like for example, how you can avoid fines, for wrong parking and stuff.
But then there is this discussion going on, you know, like how it may change search.
But before that, what was your impression of chat GPT? Yeah, well, I think like everyone else sort of in in this like reading about say Google's flan model or, you know, that we've been kind of reading about a lot of these large language models, but we haven't actually really gotten to use them.
 I think Facebook's OPT model was on hugging face and I played with that and back in back at the time, I was mostly like the few shot learning part was like the part that was so exciting where you could, you know, give it like for example, of a task and then it could just instantly learn the task and that's like pretty surprising for people who've been doing supervised learning optimization for a long time.
And so mostly my thinking was a few shot learning, but this chat GPT thing, this reinforced learning from human feedback, this like I mean the way that it can talk is just mind blowing.
It's I'm so amazed by and I think yeah, it's really unlocked a lot of thinking about the importance of prompting to me and what prompting means. I used to think that was just kind of like a task description idea, which it still kind of is, but it's like the nuances of it are so much.
And yeah, I'd really love to like dive into this topic of large language models and search and I have a few different dimensions of how I'm kind of thinking about these two things relating to each other, but since I've brought up prompting, I kind of want to stay on this one quickly.
So Bob and Jerry Lou showed me this thing called GPT index and GPT index has this strategy for prompting GPT for summarization.
It has other things, but this is one thing that just really stood out to me and there are like two strategies you can use to summarize long text with the large language model.
 You can either create and refine where you go paragraph by paragraph and you say like you start up by please write a summary of this long text, you'll receive a paragraph by paragraph and then it iteratively updates a summary or you can have this tree where you you know you chunk it up like you know as a tree and then you couple it like recursively and then build up the summary that way.
So this kind of thing about like how we use these large language models all of it is so interesting and so I guess kind of yeah, let me pass it back to you and I'm curious like how do you think large language models will change search?
Yeah, I mean I'm still kind of learning it and I having you know built search engine before vector search you know using like TFIDF basically.
I knew the cost of doing it wrong you know or sort of focusing too much on precision and then paying a huge bill because of that.
 So like our search engine for example back in the days when we indexed on sentence level in alpha sense would eat something like half a terabyte memory and you know memory was never cheap like it was very expensive even back then and so we had to figure out ways to retain precision, not lose recall or maybe even increase recall because there was a problem with this precision oriented search and stay within the budget right so when I think about language models myself and I also worked at silo AI at one large client you know applying these models at web scale.
 The problem at web scale is that you really need to go sub-second and not just sub-second you need to go like 10 milliseconds or so because all of these adapt because you have so many components in the search engine it's also multilingual it's also serving a specific country you know with that specific latency requirements and stuff and and then there is indexing how quickly you can index things right because you may also face bottlenecks there so these these are the things that I keep thinking about but also the thing that we talked a year ago in the port in the same podcast vector podcast is that you know the models like trained by Microsoft for instance I can hardly imagine deploying them today in my practical setting because they will have like billions of parameters and so they will be probably slower and also how do I fine tune them how much server capacity I will need to fine tune them and so that's why I thought you know from the discussion with multi-peach right he pointed me to the Atlas paper where they basically are able to with a few examples fine tune the model so quickly and it will have substantially less parameters so it becomes more practical you know both on fine tuning side and also on serving side and these are the topics that I keep thinking before I enter the is it chat GPT is it sexy is it cool is it answering my questions you know can I actually deploy it and not have angry faces from DevOps saying hey you just crossed all the like we are low margin on search and you are just you know way above that so sorry we cannot deploy this so these are the questions I'm thinking about a lot yeah that I think there's a couple things in pack and no one's helped me develop the abstraction around the end-to-end search framework more than you so thank you so with the with the pyramid diagrams and these kind of things it's so helpful and yeah you mentioned like the approximate nearest neighbor then one up you have where I see is the information retrieval layer where you have the you know dense vector search BM25 split covert that layer and then at the top you have like what I think is going to be the chat GPT layer that's like that would be my current predict and we're going to talk about neural search frameworks that they can do more on the wv8 podcast yeah well maybe to just say a little bit one of our favorite partners that we've been working with is neural magic and neural magic is doing sparsity inference acceleration where they've recently one of their papers is about getting the 175 billion parameter GPT model to run on a single GPU I know that you know you can probably compile these large language models on like Nvidia Triton server and do it that way but I think that this sparsity acceleration for CPUs is just incredibly exciting for that particular dimension of it and yeah I think what you said inspired so many ideas yeah I sort of like like what I value in your approach is that you run probably like a basketball player converted into a marathon runner with the same capacity you have to play a game you know that you basically run super quick and fast and long distances you know on the research side and I love this approach really really because it opens up a lot of opportunities I sort of like because I come from the engineering background yeah I did my PhD but it was like 11 years ago so I most of my time I spent in production you know great systems and every time you just try to move a little bit like okay let's add this and oh the cost is this oh sorry okay it will take me now to two more weeks to index my content so and we have for this what is the use case so you trickle back to like almost like product level management and so you will get these questions inevitably like okay why are we doing this like what's the actual trade off what's the benefit of bringing this into production right and but at the same time I'm fascinated by this I mean this will not stop for sure right would you agree to that statement yeah I think and there's uh so I know Hagen Faces recently published their they open source of data said they did with surge AI on getting these um human annotations to train the reward model in the reinforced learning human feedback strategy so I think they'll they'll be an open sourcing of the data of the data that you need to train the models and then yeah I think pretty soon they'll be open source versions of it I think open AI um I I'm very curious about this like kind of data flywheel idea whereby open sourcing the model they spend a ton of money on letting you use it for free but then they get the data of how you want to use it and so very curious how that leads to more to a better model my PhD advisor is a world class expert in class imbalance like understanding that machine learning models they do not perform well on long tail you know if you have an imbalance data so it's a lot of like the bias discussion things like that so I'm I'm curious maybe it helps the long tail getting all this data yeah it's still not exactly how it will get better I think one thing I've said previously is like there was this paper from Emily Bender and um caller is the last name sorry it but it's called unmeaning understanding in big data and it makes this argument that it's like language models by predicting the next token will never achieve meaning because it's like an octopus underneath the ocean of two stranded islanders and it's just mimicking their language but if it if something like a bear is to show up on the island and it goes help a bear then the octopus is like oh I don't know what a bear is like yeah I'll do more but I think what we're seeing with the reinforcement learning thing is that it's like it's acting it's there's there's this other paper called experience grounds language it's about you you need to it's like the levels of sort of developing meaning and one of it is like about the importance of acting acting in your environment I'm I'm kind of going around right here but I also see like this causal inference stuff and uh Judea Pearl has this ladder of causality where uh it's you act you make interventions but then the the the the top of the ladder of causality is you can understand uh counterfactuals and so that last part I have no idea how that's going to be achieved yet but I clearly chat GPT is now like acting so it's different from the yeah yeah the next word thing yeah I think what coming back to chat GPT like what um impressed me maybe the most is uh so I had I had this problem uh I was I was working on billion scale and then search algorithm with with the group of researchers and and engineers like almost a year ago so I invented this this algorithm I called it candy like of course you know not not meaning my surname but in any case with a k um it's all open source and GitHub I'll make sure to link it and so the the problem was that it it would work on 10 million vectors it would work on 100 million vectors but it would choke on one billion it would basically run out of memory uh and and I did it entirely in python right so maybe I I should have chosen in retrospect some other language but in any case I wanted to make this work um I couldn't I ran out of time and I ran out of computer resource because it was given to us by Microsoft um for a limited period of time so what I did is that I pasted that code into chat chat GPT and I said yeah first of all I tried to paste the whole thing but it said well it's too long so I had to focus on a specific part where I think the the problem you know kind of lurks and and it gave me the answer it said okay maybe try avoid using non-py arrays as much as you do try to pre-allocate them try to reset them and actually I did that I just didn't paste that portion of the code which was doing this so the the system didn't know that but it was on the right on the right track but then when I did it a year sorry a day later the answer changed the question was exactly same but the answer changed and that kind of make me really like uh what's going on like is it learning as it goes can you explain this part like have you seen this in his behavior like was the casting generation of the yeah chat GPT sorry I was like I was trying to follow along with the I think we're going to talk about like approximation error with the AN and search as we scale it and I know we're coming back to the chat GPT but I'll be uh yeah so it's like uh it's like a tree decoding where uh it it has a probability density on the length of vocabulary and you can take several paths through that tree for what you're going to output and uh you often randomly sample through the through the tree if that makes sense like um yeah yeah me does but I mean the answer was kind of like in some sense these two answers were complementary to each other right and and maybe I could go on and say hey what do you mean by resetting can you because it didn't provide any uh code examples it would just say reset and I was like what do you mean by reset I don't have such a method like like like so I I think that that was maybe impressive part of chat GPT and um just to close off on that there was a recent discussion on on relevancy and matching text like where a lot of these search people see uh there was um there was this argument against chat GPT that let's say if you go um you know use uh duck duck go today you will see the links right you can go and examine the links and you can actually verify the information to some extent maybe not to full extent but to some extent in chat GPT you can do that there is an answer that's it so it's it's quite a jump from being able to kind of seemingly check the is it trustworthy to well you have no way to do that what do you think of this aspect yeah that's brilliant I it makes me think about like well very broadly it makes me think about artificial general intelligence compared to super intelligence sort so to say and like I think about the artificial general intelligence and like because open AI they've published web GPT and instruct GPT so instruct GPT is like the reinforcement learning from human feedback part and then web GPT is like the like the whole idea that we're super excited about at wevea where you search for context to append to the input and then like if you say like please uh ground your answer in this information and then it's a paragraph about like how the BM25 algorithm works like I use this personally that way to hybrid search and understanding it and so like if you give it the context it's so much better and so I think I suspect that chat GBC under the hood does something like a Google or a Bing API search and so it's like general old but um yeah this idea like so so so then this idea of super intelligence it uh because I've been like can I use chat GPT to help me write like you know blog post survey papers things like that are relevant for trying to be a master of search and what I need from it is more so like citation recommendation right like I needed to go into like uh Leo Boystov's publications and parse it out for me and help me understand what he's done so it's like the specific information and then yeah the real I mean u.
com also has a really brilliant thing where it's uh search engine on this panel and then the chat GBC on this side so it's like a user interface problem I think yeah yeah but but I mean maybe even yeah I totally agree with you that user interface definitely creates the bias uh how we like how you use traffic lights today they go like red you know yellow and green they don't go upside down right and like if you see an upside down you will you will think well this is a wrong uh traffic light uh I'd rather not cross here you know but like it's kind of like similar here like with the search engines we are used to seeing you know URLs and and being able to click there but of course if you take Google or I guess being does that too they also pre-generate this answers answer boxes right so you can answer you can click there but I don't think you have a URL to verify you know the source of this information if I'm not wrong yeah yeah so they already playing with incorporating this knowledge from a language model right and they they they look at you and of course they also want you to spend more time on their page which is probably not good but we'll not discuss that so they don't share the traffic further but but the thing is you know they still play with the idea okay what if we try to answer not just with the URL and summary but actually with the actual thing right with the actual answer oh so that comes into like the extractive versus abstractive and like whether you want the question answering models that classify the answers in the context yeah and yeah I think that still has a place for sure I mean it's super lightweight as I mentioned Neural Magic they just did a sparse question answering model that can run on CPU super fast and yeah I think that approach is also just gonna be more cost effective for a while yeah exactly but you mentioned BM25 and I'm curious like I've been trying to approach this hybrid search topic but I think you went ahead all right so and I was just wondering like what's your take on this topic like can you a little like intro it to our listeners but also why do you think it's a good idea to to build like a hybrid search you know combining keyboard retrieval with it's with a you know dense retrieval yeah awesome I started by saying this has just been like the most satisfying project I've worked on since I've joined Wevegate and just being a part of this team and it's been you know like a big team working on hybrid search and it's just been an incredible experience so I guess starting yeah the motivation is BM25 has this it builds on term frequency inverse document frequency by adding like this binary independence model and the IDF calculation and then you also normalize it for the length of the document that's just like these subtle differences that make it different than TF IDF but you could also use TF IDF in hybrid search if that's what you were after and so then you also have the vector search and then you have this rank fusion so so we look we found this paper where they have seven different strategies for rank fusion it's like rrf board uh I don't know come some but in the end we just went with rrf reciprocal rank fusion which is just erica's recently published a blog post that shows the equation and just tells some of our thinking around it but it's where you just combine the ranks compared to say combining the scores because you know BM25 has a score particularly and vector search has like a distance at return so you might look at some way of like linearly or non-linearly combining those scores and I've done some experiments with with kind of my thinking around it was like okay what would be like an optimal alpha per query would that be you know maybe like a conditional model like so I tried this with the few shot learning of gbt3 where you you run a few examples of the optimal alpha and then you try to see uh you know how would you like to wait BM25 and dense vector search given this query and see if that is productive but I found and this is a very interesting thing because I think people have this idea that BM25 is like very interpretable but in my experience it hasn't been that I when you do when you're doing longish queries in long documents and maybe we can talk about long queries or short queries but I find that trying to decode why it why BM25 was better than dense search for some particular query I find that to be impossible and maybe someone will prove it wrong and I'll look forward to seeing that honestly but like there's this example that we have as you know erica was developing the weviate error demonstration of hybrid search where the query how to catch an elaskin Pollock and the idea being that the dense vector search contributes the disambiguation of catch that it prefers to fishing and that BM25 is specific to elaskin Pollock but I haven't been able to just like inspect that kind of behavior as I look through the beer benchmarks that just I'm super excited to talk about that and how we've been evaluating it but you know let me let me pass it back to you and ask about your experience with them BM25 or like the keyword and the dense search particularly because then I'd like to kind of take the topic to just arbitrary combinations of retrieval methods not just be in 25 and say DPR or whatever yeah I think I remember even before the dense search appeared on the scene we were experimenting with sort of like making TFI DF which is BM25 is like an addon like BM25 I think stands for best match so period so solved problem solved but you know like one of the questions the the reason I love working with product managers and at the moment I am a product manager so I took the other side of this thing maybe we can talk more about it in the Weaviate podcast but you know the reason I love talking to product managers is because they don't know anything maybe they don't know that much about algorithms as you and they don't code maybe as much as you but they do care for they are the stakeholders of the end result right so when they go out talk to sales or to the end users they will not get a question which alpha you have used now coming back to your to your example right they will say hey I typed cat three times in my query and I still see that the document that mentions it once is at the top how can you explain this I will try to link there is a consulting company I think they're based in Boston actually by the way I just forget their name key and via something so they have a really great presentation on haystack life I believe where they go super deep and I recommend you watch it super super deep on how TF IDF screws up our understanding of how things should work what they don't you know and they go by you know how many times you know the word cat is mentioned in the document versus how many times it's it's mentioned in the query and you can do all this combinatorial you know combinations and then they kind of like explain what you would do to kind of solve it right and you you basically develop this situation another another thing is that I found useful and it also mentioned in the relevant search book by Dr.
 Nbal and Jerry Bareman that you can you can use like if you would use like let's say elastic search or similar system or solar you could actually build a function which explains the query step by step right so it basically prints you the tree of how it actually came up with that final answer with that final score and how you know that specific field like for example at TomTom we would I cannot go into much specifics what we do at TomTom but basically the geographic search right so you type some destination let's say an address or maybe a P.
O.
I name point of interest like a shop and it's multilingual as well right so obviously your query may hit by accident sometimes in a wrong language field and so the only way to know this is to print that query execution formula if you will right and so you will see okay ah it hit in that in that let's say I don't know a French uh but I wasn't intending French I was doing German or something why did it do that and you start reasoning about how did I create the tokens because when you tokenize your text it's same problem is as in then search in a way like when you split text into paragraphs or sentences there you need to split the tokens how you do split the tokens is dependent on how you model the semantics of what you are converting to to a token so you should not convert string to a token you should convert meaning to a token so if you capture meaning in that token then you're done in a way but then coming back to your question I cannot answer it fully now but I highly recommend that that talk um by can be so you know like you need to you need to see how term frequencies and inverse document frequencies play together and also like in BM25 versus TFID if you have the term saturation issue which is kind of mitigated in BM25 to some extent right so meeting that if you have two documents um sorry if you have two terms which occur like one is like million times and the other one one million plus one TFID will be unable to distinguish between these two but like BM25 is still sensitive to these things and that's why it's a little better right so I think it solves this term saturation issue I don't know if I answered your question but no yeah I think um so yeah a couple things I really want to continue on this TFID versus BM25 and then adverse displayed to it I think you're I think this like pseudo relevance feedback is that like the phrase I give to show that like um if you're searching with BM25 you say if you had added this key like you have the gold document and you're like how would I have modified the query to produce that document is it so I think that yeah that's one way another way is to how would you modify the indexing that's more in your control right so how you would modify the indexing for example you would in some cases you can remove the applicates or something right so like you don't you don't need them or something like that you can you can or you can split the term by numbers or something right if they happen to occur inside the term something like I'm making these examples but I'm saying that you have more control in the indexing than in the query but in the in the query you can model like query similarity for example right so yeah oh that's super interesting yeah the the way that you do like the text preprocessing like stemming stopper removal all that all that that bag of that's what I hope dense vector search can kill all that I hope you can just like anything can go into it yeah and but um yeah and so I think there's this this thing called like decoding the latent space of a vector search model on that other idea of what query would have produced this where you would take the you would train a language model on document query pairs and then it would generate a query that would have matched the document maybe that's useful but but I'm also I'm very curious what you think about this idea of split vectors so split vectors is like you keep the mass language modeling head and so you encode the thing into the vectors so the mass language modeling head always only takes in a vector as input you always would mask out whatever the mass token was and then send just that vector to the mass language modeling head that will produce like a sparse distribution over what would replace it and so I think the the idea behind split is that you do that for each token and then you just kind of average all the vocabulary distributions and that gives you a sparse vector that represents like the like happy euphoric ecstatic like the kind of synonyms behind it do you like that kind of idea yeah yeah uh uh I like that the fact that I think we can step back from like this dense vector limitations and go back and try to capture what sparse vectors do because if I don't know if you watch the episode with duck Turnbull but he actually shed the light on on this really well by saying hey if you if you take the keyword retrieval inverted index you deal with like probably hundreds of thousands of dimensions unless millions unless billions like in some of the indexes we had at least million per term right so that's like million positions most of which are zeros because this term doesn't occur you know in in specific doc but like doc id but like it occurs like in a few and so in dense retrieval you sort of like compress all of these to let's say 256 dimensions and inherently you lose the precision right so it becomes more like recall oriented rather than you know in sparse you you basically like what also it means spars is that this is probably like a little bit like going back to n and algorithms right so like an inverted index it's basically like a hash table so I have this term it's like order one look up in the hash table and then you leapfrog you use this leapfrog algorithm implemented really well in leucine for example how you jump over long strides of consecutive doc id's because you don't really need to examine them in an antiquity let's say if it's cat and dog you know you know that cat occurs in the document id5 well I don't know like 10 let's say and for dog you are on on on three so you can leapfrog all the way to 10 you don't really need to check all this in because they will never occur together so for or query that's another story because that's a union but for and query it's an intersection so you always need an intersection you can then stop early because you don't need 100,000 results on the screen right and I'm still actually curious on how would you know when to stop because what if you didn't find the document that is even more relevant that what you have seen so far but that's like a matter of debate I guess but then you start scoring them and then sorting them were relevance right yeah sorry if I'm a little behind them so is this referring to how you can use like an inverted index to calculate the BM25 scores so I would you know with my document collection if dog appears I you know dog and the documents so that when I'm calculating yeah yeah but like the the the the I guess the comparison I wanted to make to dance search that like an old vector search is that they are on the on the base data structure first of all you have a choice of the algorithm you want to use but let's say we take hnsw which is the most popular right also implemented in v8 I know but like you don't know when you enter the first layer you don't know where exactly you will end up like so like with hash table I know exactly where I'm entering and I know that I'm exactly in the right place right and you know you can also expand your query with synonyms then you enter more more points in the hash table and you start traversing all of them in parallel and you come up with the answer but in dance search you need to like accept the uncertainty of navigating that graph you don't know where it will land it has certain limitations and trade-offs and then it will pull up you know some nearest neighbors and probably you should be happy with them because oh otherwise you need to do it twice so that price and so on you see what I mean right so like they are fundamentally different also on search side oh in like this stochastic nature of the yeah and also I read this paper called OOD disganan that talks about how much the distribution shift can impact the graph based vimana so vimana is like hnsw but you flatten it so there's no longer the hierarchy of layers it's like all the same thing and then you can put it on disk and it's like a little cheaper run I think yeah it's fascinating the whole indexing the part that that's like kind of the the meat of this especially from wavy aspect of that's where I see and in addition to you know the ux and making it like a very developer friendly to well there's a few sides to it because there's also the distributed database part and you know all the written and go laying the concurrency control you know the replication of the backups like all these kind of things like that so it's definitely like some things to but that approximate nearest neighbor search and I know that you have this experience with you know I've listened to a ton of your talks and you're you introduced me to the a and n benchmarks but yeah that I see that there is being like three levels of errors that come that propagate up there's the errors from hnsw and say product quantization then there's the errors from the vector representations to begin with and then there's maybe the errors and like the question answering model so if you wanted to have like you know natural questions open domain qa you're looking at like three layers of cascading errors that are sort of unrelated to each other yeah exactly people really brilliantly that you like and I think if I may summarize it you know I anyway to you know kind of where this hat of the person who is creating this doctor search pyramid and stuff I'm not the only guy doing this but I keep doing this because it helps me to stay comfortable in the topic and sort of okay I'm looking at it from this angle and if you accept it stay with me if you don't you know you may you may as well augment it or something like you did earlier with some levels and you know like it's just you need to accept that uncertainty like you explained and also that uncertainty that you know like in this can and paper they they explicitly show that in hnsw you may have unreachable nodes and they counted something like 1000 nodes were completely unreachable from any point in the graph like no matter how you search how long you search what are the values for your e f and m parameters during index construction and search you just don't reach them and and that's I think that's somewhat similar to the inverted index search where you have like one million uh doc IDs per term how do you know when to stop it's also like you may never reach the documents that you should have visited but you just deliberately decided to stop you know prematurely because you don't have time you have to you know return the documents within night and 10 milliseconds so you have to make trade-offs um but they are ordered naturally in in the increasing order of doc IDs right they're not ordered by does this question answer anything does this does this document know anything about cats or it just not mentions them and passing you know does this document knows anything about tweeter does it describe tweeter or just says you know please contact me on twitter here is my twitter handle right like complete noise uh so so you see what I mean right so like there are I think in both approaches like on fundamental level on data structure level we deal with this fundamental limitations like gravity law like you cannot jump off and and fly to moon or to Mars right without additional like thrust and devices and stuff yeah so do you feel the same like does it resonate oh yeah well firstly thank you that you just explained that concept to me for the first time I'm just I'm just now alive on the podcast understanding that concept but yeah it's very it's very cool like the um sorting the inverted index to prioritize documents maybe by clicks like clicks would be like like the most sensible thing if it's like web pages so to say and you sort the documents and then you yeah you have some kind you could probably calculate how much time you have to search and how much that lets you go into the invert index yeah super interesting I I think it's very interesting for wevia with the with the hybrid search in the beam 25 index because I I know the inverted index has been explored because we have this uh like neuro symbolic search where you would annotate properties like you you're searching through let's say you have a billion sneaker images but you've also labeled the color they are so you have red is the color and then you can use that to filter the search so there's definitely been some foundation in pre filtering and integrating uh these kind of symbolic inverted indexes with h and s w so it's not like the first time we've yet it's ever exploring that but I yeah there's definitely nuances with the beam 25 because of the cardinality of how many terms you like with the document I think you're splitting it I don't know 300 words right like 300 300 words per property so the just the size of it um I mean starting to go into the thinking around like the sizes of things it inspired me when when you're mentioning the compression bottleneck from sparse to dense I was thinking like okay let's say we have 384 dimensional vectors that have 32 bits per uh vector position like what is that is that is that 384 or 324 or 32 or you know like that it's still a massive common tutorial space right yes exactly exactly and and and you said like is it even the model that captures everything we need to capture right it in all of these are numbers of course it's kind of number representation of the models understanding well understanding in quotes of of the objects that we index uh but I guess like for me like um and you're way ahead in this I feel like that uh with VBA development like um of me you know what matters to me when I was like a search engineer day to day is what tools not necessarily tools as in specific programs but like tools as in algorithms approaches I have to control the process right so if somebody comes up and says hey can you look in this query can you debug it first of all like explain queries one brilliant way of doing it and that's where you start but then once you understood aha there is a problem that it hits this field or I give too much of a boost uh in this situation what should I do so you start like tweaking these parameters and you have these tools in your hands right you can do that in vector search I I don't know like I have like probably fine tuning as one tool right so like if clip stops working on these images I can go and fine tune or bird um but what else do I have like I can also tune some parameters in hnsw or gskn and so or something I can make all these thousand nodes reachable like they didn't this can and I can choose disk over RAM if I want to save on you know on cost and stuff but what else do I have as a control to actually go and debug and fix that specific query like what has been your experience on that or maybe thinking uh yeah I think you've named them all I mean I know I've seen like um like the tuning of the EF construction as you mentioned with hnsw and I guess something that I'm really excited about with these beer benchmarks and maybe I can introduce it now because I think it helps with this idea of model selection in terms of the user's perspective on how can I debug my system how do I fix my search system so the beer benchmarks is it's about diverse text retrieval so you know it's like arguana NF corpus track covid is the difference is instead of saying that the search image net is going to be ms marco which is you know like 10 million being passages and like a million labeled query so it's like the image net idea of like this general source of it like image net is like a massive collection of images labeled in a bunch of categories so it's like it's like is ms marco the search image net but it seems like instead we're going for diversity with beer and I think also if we all if we want to talk about intent intents and instructions further I think actually beer is I think beer had another there's latte like L O T T E capital T's like they go they go beverages right so yeah so there's like an equivalent to beer and then there's also miracle which is for multilingual so there's a lot of these like diverse text retrieval and then and then it's expanding where you would label it with the instructions as well and I don't remember the names of these data sets off the top of my head because it's very new but I know this paper called task aware retrieval with instructions and I think there's a model another paper with a model called instructor so this is idea where you also label with the intent but but anyways let me go back to the focus on like how does a user debug the search system and say how can I fix it so the idea with the beer benchmarks like one idea would be that we could test several different models and you could maybe say like okay well I'm building a nutrition I'm building a nutrition search apps I'm like I'm like bodybuilding.
com or something like that and so you would look at the NF corpus results and you would see the performance of the different models and that would maybe help you take a different model off the shelf but then what you're saying with like fine tuning it I suspect that fine tuning is going to be a super powerful lever I think if you find like and maybe later there's so many topics I want to talk to you about.
 Like with the idea of I've been building a Wevey-A demo of the podcast search so I've been taking the Wevey-A podcast parsing the transcriptions and putting them in there in my temptation to like fine tune it and start thinking about this positive negative construction for that I mean I think in general with Wevey-A we're kind of you know letting like you know we use open AI models co-hear models, hugging face models and it's like we're not really training the models but it's just such an interesting thing to tune I know Gina AI's fine tuner is extremely interesting that I do find myself like constantly pulled in that direction of like wanting to train models.
 Yeah absolutely I've been when we when we presented Mewves at Berlin Buzzwords last year now we actually said we also have Mewver which is the component to allowing you to fine tune a model we kind of like don't have it for prime time but I've been like really fascinated kind of coding a bit of that and and checking how well it can can work in a more generic way you know because I think fine tuner allows you to plug in several models you know and like because different models have different inputs they have different like setting to train and fine tune and so you need to be aware of that like Clip is that is a kind of two tower in a way right so you do need text you do need the image but I think I feel like coming back to the question like what tools I have I feel like fine tuning and I feel like you agree to that the fine tuning is one way that should be more available to the masses should be more available to the users in a way that they are aware of this tool and they know you know best sort of like know how how to use them and also pitfalls you may fall into and I think this is what you brilliantly described like a year ago in the context of computer vision like data augmentation right so like it's one thing that you can feed you can feed some manual examples but how far you can go and like in your basketball example like you've been manually labeling some examples like you run out of patience in a way right okay you can hire people to do that but is that scalable probably not and also new trends come up like if you take a business specifically working on e-commerce or I don't know full text document search you know things come up every week maybe right so like I don't know Tesla releasing cyber truck and you don't have it in the in the model so it actually like in your example what was it with the ocean and like yeah I hear you say like how to catch an Alaska Pollock and then let's pretend that Alaska Pollock is a new fish that like you maybe with vector search you may try to find what could be the most similar object but it may also be wrong right or in the case when the distance is so big that it doesn't make sense anymore to consider this as a candidate right so yeah so this is this is very interesting like and I hear that you you really want to like dive into fine tuning topic as well right yeah well that idea is amazing because there this argument and I also when I interviewed multi-peach he gave me these three reasons to favor the retrieve then read approach to large language models and one of which was this idea that you can swap out the information to update it with new information cyber truck becomes a new thing and then you can put it in the context and now the language model just has the reason across the context but then as you say the embedding model doesn't know about the new thing so the embedding model you know also isn't going to pick it up and so yeah I think that continuous updating one idea that I'm just incredibly excited about I haven't figured out how to make this work yet but the idea would be you you're the ML ops problem of this is you need to re-vectorize your data set which yeah so the solution maybe is that you could vectorize like a thousand representative documents and my hypothesis is that the proximity graph from I want to say Vamanum or Southern H and SW because I barely understand graph neural networks let alone trying to make it a hierarchical neural network but like if it's if it's the proximity graph maybe you can it's like it's like it's like a psycho again it's very similar to like image to image translation or any kind of you know it's a vector space to vector space translation and so you you know you input the vector output the change in vector and so can you vectorize like a thousand and then propagate that throughout the graph it or throughout the corpus and maybe that proximity graph has some kind of bias that facilitates the optimization task or maybe the graph neural network thing is too much overhead and you're better off just having like a transformer that takes into vector outputs a vector but yeah that this idea of like how do you continually update your embedding models it's fascinating right yeah yeah especially the ML ops aspect of it as you've mentioned like if if we were to insert new neighbors into the existing graph right would that change it favors something more recent or would it like break something that we didn't want to break and things but but in some sense if you think about coming back like we are still in the realm of this hybrid search topic in a way right if you look at BM25 OTF idea of approach right so if you compute so you're I so you term frequency is only dependent on this document right so that's fine it's kind of the independent of all other documents but your inverse document frequency is dependent on the whole corpus which is indexed in that chart by the way that's another like big topic which is kind of like crossing the boundary of is this just infrastructure issue in slash engineering is this kind of like research issue and it's like it's fuzzy it's it's it's it's a blend and so for that chart you're gonna have that local idea unless you build a a higher level cache which will keep track of each individual chart's idea and roll it up to the global idea and like if you look at Apache Solar I think I believe they had a country module or something implementing this where you can actually implement a global cache with IDF which will live on top of the chart and now you're coming back to MLOBS you need to make sure it never dies because if it dies you go back to like the chart level IDF and so that becomes dependent on okay I have managed to stuff stuff a lot of documents about cats in this chart so the IDF is like this and then I stuff a lot of documents on dogs here so they become like unbalanced if you if you know what I mean so they it's not a healthy mix of term statistics in your collection right and that will influence a lot of things like you may say in some cases it's okay but in some other cases it may not work if your query contains you know both concepts and they are unequally represented somehow in your in your collection right so so I mean does it make sense I mean so it's like you do have limitations also and and not limitations but maybe I should pose it in more positive way research tasks right so such challenges what should we do and I hope that in some sense dense search is pushing us to think more and more about this and maybe some things will back lash from vector search back to the you know classical keyword retrieval and maybe some new data structures will even emerge to to tackle these things yeah I think that idea that you're describing on the IDF caching it's super interesting I think it is it is inspiring me just thinking generally about how we're trading knowledge on this thing in general and having this podcast and having this content and this communication and how we've you know done like our first iteration of BN25 and yeah like learning so much about the index structure it is really really interesting I was thinking about like oh well how about displayed vectors could could we just kind of update the mass language modeling head to get the new terms and would that be easier than this kind of global cache I like idea and is it more forward thinking and then yeah it's really interesting I think maybe one other idea is this thing called colbert which is like a token level representation thing where it's like they call it late interaction where first you do the you know the standard vector search but then you keep the token vectors for each of the vectors and and then you do that and then they've had efficiency improvements on that so like I think they in the original colbert they they've recently published this paper I know Christopher Pots and Omar Kataba I'm sorry I don't know like I'll show my best like no give everyone credit all the time but in this paper they describe it like the original colbert is like in 154 gigabyte index compared to like one gigabyte with other methods and so so yeah like efficient indexing and I'm definitely running a van but it is like a big thing to unpack there's so much depth to this and that's what makes working in this field so exciting is that there's so much opportunity so much to explore yeah yeah and so much unsolved as well I don't I don't know if you wanted to continue a thought oh no sorry yeah I was just yeah I mean we are branching out but like actually one thing that you just reminded me there was a maybe I should start writing a book or something because like the moment I remember this I should write a chapter and then keep adding and then publish it maybe you can be my author or something yeah that was just thinking it was was it like 10 years ago on Berlin buzzwords there was a presentation by one of the engineers at Twitter I don't know if he's still a Twitter and I forgot his name I remember he was German working out from San Francisco and he basically coming back to that issue with you know sorted document ideas right what they did at Twitter first of all you know the scale of Twitter is such that you cannot possibly store Lucine index on disk and then go and retrieve it because well it's just way too slow right what they did is that they moved the whole index into memory right so they had to rewrite Lucine to kind of like this memory friendly data structure and one thing they did in particular is that as tweets come in each tweet is a document it gets its unique document they deem and they would append this new document ID to the postings list in the end right so for this term so they would decompose it into terms back and then they would know okay I need now to update that specific terms posting list so the posting list is just the array of dock IDs so they would put that Twitter tweets dock ID in the end and as the new searcher comes in searching tweets they would read backwards from the end they wouldn't read from the beginning of so basically what they did is that they kind of like encoded the temporal nature of tweets and people what end users wanting to search and view the tweets which are the most fresh so like like I don't know if like you are the heavy user of Twitter I do you know like on Twitter like when I log in and I check my timeline like usually I see something super fresh and then I keep scrolling but like not like anti-props to Twitter but it's it's a nightmare to search on Twitter like when I search something I know existed like a week ago there is no way for me to find it unless I know the exact tweet ID right and so at some point I was even indexing tweets actually direct messages I had with few people you know in solar and then basically searching them so because it was way faster than searching them on Twitter because like if you have 5,000 direct messages scrolling through them will take half a day so because they keep loading and loading so basically what I'm trying to say is that they optimize the data structure for the nature of usage of Twitter in such a way that they bias to the recent tweets and they don't care if you will have to spend a day retrieving like super all tweet like it's like so my new user use case for them for the majority of users 99% of users will only want to see and consume the latest thing so in some sense this is kind of the effect of optimizing to the usage like what you say we could optimize you know like split or or similar you know sparse lllm or something to kind of like learn you know that latest beat and maybe there is a high chance of it being retrieved as well so we might as well bias the system to that but then of course there is catastrophic forgetting thing and stuff like that.
 Yeah there's no is yes not an easy problem to continually update the mlm head either it would be maybe worth adding that this mlm head insplay doesn't need to be like a billion parameters well maybe a billion would be good but it doesn't need to be a hundred billion or like yeah that's such a fascinating nugget of system design you just shared at the Twitter thing and yeah it's really interesting I've seen this other company called perplexity AI that Ravine Shrinivas is I think he's a founder CEO of it and it's cool because he was he worked on curl with Peter Rebele on this contrastive representation learning for robotics where they're you know they're doing the same kind of idea vector optimization to learn a state space for robotic control on so I think it's really cool that now he's working on the search space too but they have it's like the other approach is like natural language to SQL it's something like that where like instead of and I'm getting a little off topic but it's like kind of related to Twitter and it's about like putting tweets into you know data stores and then parsing natural language queries into the SQL but so that's like another idea I guess is like you would parse the query yeah I think I'm already explaining what do you think about that idea like you you take the query and you turn it into an SQL query in like that's it yes yes I know what you mean it's like it's very similar I think deep said did that right so you can or maybe it's opposite I'm not sure but like if you have a probably the same if you have like a table right you know with fields and rows I don't know let's say list of mountains with their heights and so on so you can actually have a question what is the tallest mountain in Europe or Asia you could turn that query in natural language into SQL command and say you know select you know mountains from this mountains table order by height reverse right descending and so I like this idea and in fact actually I've I think first of all this is already doable right so I'm fine just stood with like with the Deepset doing that in haystack but I also came across this idea during my PhD research because so the problem there I believe was that it was like these engineers working on building aircrafts and so they had to read a ton of manuals but once you read the manual you still need to go and look up that specific number somewhere in the database right so so basically they do like multiple multiple hop approach and that may take like forever like you first of all you need to crunch through a ton of you know text material and then somehow summarize it and then okay now I need to go and look up that that number in the database but what if you could ask a natural language question to the manuals then convert that to a SQL command which would know to go and look up in that specific database table and give you the answer so like the manual doesn't have it but it has some instructions how to find it and then you would kind of like convert that into through this metal language convert that into SQL and then get that answer right and this was like pre-dense retrieval in era obviously but I think I still feel like it has the merit to like well I guess two things I think first there's this problem where you search like for error line manual some specific detail and it's like in result seven like it almost got it like it's not like not in the top 100 but it's seven and to that problem is where I think this GPT index like recursive summarization or create and refine summarization I think that'll solve that problem and yeah well so I then coming back to this idea of natural language to SQL and like structured unstructured data on the other end you can also parse the tables into text and so I've seen that done too there's like wiki tables to text and so me personally my favorite application is is scientific literature mining and searching through scientific papers and so you could parse out the tables to turn like the results tables to turn it into natural language and I mean there's so many fascinating things so it's like with a knowledge let's say like a knowledge graph the idea of the knowledge graph is if I have Demetri Khan host the vector podcast is a product manager at Tom Tom I with knowledge graph I can you know I compress the representation of all these facts into one structure compared to having the set of sentences right and yeah so maybe if I can kind of plug something I've done so I have this paper that will be published pretty soon it's about it's in the Florida Atlantic University PhD it's an interdisciplinary team with the College of Nursing and a local healthcare system so we have electronic health records that describe COVID-19 patients and we're trying to predict survival outcome treatment forecasting prognosis all that kind of stuff and so the the thing that we explored in this paper is let's switch from the structured tabular data to parsing it into natural language text and let's turn it into like clinical narratives or let's do this thing where you do if X if feature name equals if feature name equals then label right yeah so there's a paper from the University of Wisconsin called language interface fine tuning where they do that same idea but it's you know like the UCI machine learning repository data sets so so I think I know that I've taken like a walker and also to think it's cool it's cool I'm sure now listeners will be like what but I know like it's it's also what I heard from my listeners for example in the podcast is that they actually do use this episode as an educational material so that's why you know if we can stuff as many links to papers and your work they can go and study this yeah go go go I do some rise I guess the question is like how are we thinking about structured and unstructured data the deep learning systems you could parse out the structure into unstructure and then you have the transfer learning is really easy right yeah yes or you can keep the structure and then maybe you can learn a better representation thanks to the structure and with that question my interest has been really heavily in these causal digs and this idea of creating structured causal relationships between variables I still have no idea how that really how you can take like Wikipedia text and turn it into a causal diagram but I have an idea of like if and it comes back to this agi versus super intelligence idea if I have a super intelligence and it's reading search literature I want it to have some kind of causal diagram of our current model of search stuff so like it has some model of how BM25 is index the limitations of it's blade this representation this MLOVs problem it has like some structured representation of all these problems such that when the new batch of archive papers or tweets you know however the news is coming into it or experiments right it looks at its causal diagram to say like this violated my this this claim like because that's the thing you see a paper like autoregressive models as as search engines or you see like what's the name of that where it's like transformers is a differentiable search index like you see some title like that that violates your causal diagram of why things are the way they are and that's what like inspires your interest so that's that particular angle of it is yeah yeah I'm not mostly thinking I haven't explored this topic myself yet but so let's say if you take a language model like bird which was kind of like you could say statically trained once on Wikipedia or news content right but the world is changing every single day right your model doesn't so what you could do is that you could introduce knowledge back to the model and I'm still like on the on the brisk of kind of exploring this I think new tremors talked about it recently like how you can incorporate knowledge in the language model so for instance what like the way I see this before I even like read this paper so I could probably try to invent reinvent the wheel is that so the language model might figure out that the question is about the president of the United States that specific one let's say Obama something but then the question is is Obama still the president of the United States and so now the language model is kind of like hentik app that says well I actually don't have last I know like chat gpd does that right like I was trained by 2021 so I have no idea what happened in 2022 sorry goodbye but like it could actually say it could say I figured out the context I know roughly what you're asking this is the person I know this person I know that what what the president means I know the the country United States but you're asking me a factual question so what it could do is actually it could go and ask a knowledge graph which is updated without recalculating the the embeddings which is solving them all of this problem right so it's it's it's another data structure you know it's a knowledge graph it's being updated as we go and so it goes and says hey let's coming back to your question on on structured language like in in graph systems you also need to form your query in a certain way so it forms the query in a certain way and traverses the graph and then checks is Obama the president the answer is no it goes back all the way to maybe a language model I don't mean some other layer and basically presents the answer to the user right yeah so that's just one thought before even dove into this topic of incorporating knowledge in elabs I would probably think like that yeah I love that you brother that knowledge graph it's like and that's kind of like GBT index as well as laying chain I can't believe I haven't brought that up until now we can talk about that more in the neural search frameworks discussion on the review podcast but like this idea of different kinds of external memory and I don't know what's wrong with my brain today and I keep like branching into completely I don't think it's wrong I think it's the right setting it's just not suitable with the coding or something but um like so I was recently talking with Shukri who just joined we've got as well about um about this idea of metadata re-ranking so one approach is you have the xg boost re-ranker where you take in the bm25 score the vector distance and then also symbolic features as the input to the re to the xg boost re-ranker so the thing he was okay do we want to store this metadata in weveate as well or do we go get it from redis or feature store something like that where we get that kind of property and so it's like the knowledge graph the idea connects to that because it's like okay are we going to build the knowledge graph in weveate should it live in weveate or should we plug weveate in with something like Neo4j or or is it a top level controller like the neural search frameworks thing you're describing where it's you know something that hooks into weveate and hooks into Neo4j relational AI tiger graph I don't know all the rdf ontology technologies but you know like it has separate and it's a higher level that picks between the indexes so it's yeah it's like what kind of technology is built and weveate and that's not even really up to me you know exactly but I think it's kind of fun to brainstorm with you like what like we kind of like intuitively find this limitations together and at the same time this limitations may lead to future discoveries like on engineering and research and like when I was giving this keynote at Haystack where by the way weveate guys will surprise and another guys as well like I didn't I didn't feel bold enough to say this but I think I will say this now at least that I feel like engineering and research are kind of like indistinguishable in the amount of intelligent power you need to put into this to solve it because it's not like given right like if this data structure inverted index is designed like this and you do have the the issue of early termination because you cannot like waste so many CPU cycles then like okay without reading papers can you go and solve it like being just an engineer so to say no you can't it's like it's it's super hard like you need to start coming up with like new vector space model which was invented when in 60s 70s I don't know so like can you come up with like completing your model it's it's it's equally hard as in research when okay you know that SOTA is now this can I beat it somehow but it's not like you're just beating sort of for the sake of it maybe some people do but like I would take a stance of not doing that like I would try to solve an existing problem right so I do want to surface as you said more relevant document to the top or maybe even the passage maybe in a number so I keep pushing for that so both of these to me they're like they require so much intelligence so that they become indistinguishable in some sense like what exactly are you now solving the MLOPS problem are you solving the you know the inverted index data structure limitation problem or are you solving how do I retrain the embeddings how did you train the model or fine tune the model and I don't recompute the embeddings because it's a way to expand so it's to pay the bill yes does it does it resonate with you like what what are your thoughts of that yeah our ct oeddy and delocca has written about product engineering and like on this meta on this meta of like how do these decisions get made and it's like I think there's a book called change my office I have a bookshelf behind me I used to be in podcasts and I'd be like it's that yeah yeah I still have it actually yes but it's like it's like ask your developer is a title something like that about and well okay so that maybe maybe I got a little off with this idea of research and engineering I think the the scientist is very like a metrics oriented in a different way like the the engineer like the the diversity of the tests and the data collection is more important when you're the when you're the scientist sort of uh yeah the the engineer needs to build like smoke tests sort of where whereas I see the scientist needs to like have a very rigorous data collection kind of because that's sort of how I see the distinction and responsibility sort of is that makes sense yeah it does it does actually yeah you you uh you gave a very good distinctive you know feature what I was trying to say is that like in engineering you still have a plethora of options like it's combinatorial explosions in certain cases there are also mundane parts in both of these right so like we are not talking about them but like they do exist but like you do have these points like okay should I branch this way or that way should I step back and rethink and and that's yeah but I agree I agree you you gave a really good example of like in research I do care about data so much in engineering it's probably the quality assurance department is going to worry about okay what data we're going to feed into the system to try to kind of maybe break it and see limits and where it breaks what do we need to fix um or is it kind of like stable what it proved enough to release you know things like that so but yeah I think if I can stay on this a little more I think this like generalization testing like the industry of quality assurance but 4D learning is is going to be really fascinating I'm like excited like how I think when we first met you had written this um not all vector databases are equal and I thought that was so insightful because it was like a you told the story of an emerging market and that was so interesting I really look forward to seeing like the story of the emerging market around generalization testing I think like um like with the beer benchmarks that kind of thing where it's like you create some million scale data set and have the NDCG recall precision with all these queries I think maybe also this idea of like AB testing with models is going to be more popular I was when I went to Neurops this year and there is this talk from Dr.
 Juhau came about interaction centric AI and how that might differ from the first paradigm of model centric AI where say you judge the image generation model purely based on like inception score for shade is tends to feature spaces in real images and then to data centric AI which is like I think snorkel AI is very responsible for like branding that term and making it so popular but it's like you're really focusing on the curation of data like your language model is like mosaic and oslatus pub med gpt it's about like you have this particular data and you like clean it and you make it awesome and then I think interaction centric AI is like a new way to evaluate models where it's like AB testing driven kind of or like how quickly can you perform a test I don't know if I've gotten too else topic but no I think it's it's exactly the topic to focus on if we are serious about you know putting these things out in production like you do need you do need to have and provide an evidence to the stakeholders that and to yourself that this dust hold water and we can release it and it's not going to show something you know in discriminate to the users that they will be completely you know puzzled and stuff or maybe you know there are all these numerous examples when like Google search when they I think incorporated some distilled version of bird when they would flip the meaning and they would say you do take this medicine but actually in the prescription it says you do not take that medicine or vice versa you know because it's not sensitive to negations and stuff so like I totally agree I'm with you on that like how do we QA quality of sure you know that the systems that release and I think the open AI team did that brilliant trick in a way that they said hey here is the chat GPT go test it and they get like million users in the first few days because they actually do need some extra brains to do go and test in different like scenarios and see where it breaks maybe it doesn't make sense anymore so yeah it's my understanding that's how like scale AI became the kings is that you know like labeled data like mechanical Turk I think Sir J.
I.
 is something that's emerging that I've been seeing yeah it's really interesting yeah exactly um yeah um I was I was wondering um you you also worked on this podcast search and you had the opinion that Whisper has some bottlenecks I wonder if you if you want to like tap into that a little bit yeah so I'd love to tell this story so uh so it comes the kind of story behind it is uh so Boris power at open AI tweeted uh so they they cut the prices for the open AI embeddings and and Boris is pointing out how cheap it would be to index a massive podcast like the Joe Rogan podcast so that's how I was like hey I have a podcast and you have a vector podcast and we did also but so I started to be you know I started doing this where you you know you take the audio files then you put them into Whisper I also tried like uh descript is something that I like a lot I've been using descript for a long time for editing videos and so it's like you still because you it's very the podcast transcriptions you still want to edit them a bit you you have like uh and like like if you were yes how I'm pausing right now I'm talking about but the transcriptions it's not quite what you want to like index to this idea of like how do we create a knowledge base from these podcasts because these podcasts is so like we've covered so many topics and it's so it's kind of easier to do it like this than to be writing all this down and then and also it's very collaborative uh like the podcast you get more people involved it's like a community building thing is so yeah that idea of creating knowledge bases out of podcasts like what would you write your interest on a scale of one to 10 of having a vector podcast I mean I would love to join to join the you know to join the geek here so because I do like I was rewatching the episode with you Danny here we go and you were like exploding with knowledge right like in a way you're branching out a lot today as well exploding with knowledge because you you read all these papers you try things you share you know like google collapse and stuff but like like how do I tap into this knowledge like it's it's very synchronous right I have to like there is no way to like random jump into hey where did he talk about you know that model from Microsoft like I don't know unless I have the time code I don't have a way to do that right so yeah yeah and I what that's what inspires me so much with I want to fine-tune these models so badly just on the uh turn taking as the positive labeling and yeah I think can you expand a bit more on that what do you mean uh okay Conor says I want to talk about the turn taking Demetri can you expand on that more on what that means Conor okay it's like this is how you do the positive thing that's like potentially like that yeah yeah yeah and like if you want to have more examples of what Conor said like you could like augment with Conor's uh statements uh oh like sentences yeah yeah and just the I feel like the potential of it is crazy I also think like we're gonna see it like hooked into say Spotify are these big platforms that organize podcasts and I think it'll help you like discover like because something else it's like I love how you do this vector search podcast and I'm also doing a vector search podcast and it's like who else is out there doing like maybe a recommendation podcast or like you know like it's like this kind of discovery about the people because podcasting is very like collaborative it is a medium right like it is not like you you can't do it by yourself no like it's like it's it's almost like the thing uh like stand up comedian so anyone who is presenting you do need the audience because you simply do not generate the 3d-ness of your thoughts in absence of people like it's very hard to do and then same thing happens here right now like when we exchange like I like I have like a full shade of these notes and stuff right so I wouldn't be able like what like do I do you know these things do I know some of these things you know it's like a vote if working in your memory but like coming back to whisper like just to get it right you you're saying it's still a bottle neck in your opinion in what way okay well I'd hate to be like quoted as saying it's not good if it's not the same thing which I value you know it's not yeah so if you're creating a podcast search app you like there's still needs to be a little more parsing I don't know if you need to find I don't know if you need to correct one and then fine tune so because I've also been playing a little bit more about chat gbc and as as I've been learning about this kind of like sequential prompting from gbc index and chain about learning like how you can get chat gbc to maybe clean up a podcast transcription but there's like still a pretty fat pretty difficult manual cleaning effort in the middle of that yeah actually I can resonate with that like I've I've worked with one startup helping them to do speech to text right and first of all one one issue is very similar with low resource so to say languages in an OPs that if you don't have a model trained on a lot of examples or maybe they've been trained on some TV shows and you are doing and a user speech stuff you know the topics are different the style is different everything is different and so it breaks and so I was also eluding to the topic of fine tuning there but exactly what you said the problem was the output was so noisy that I had to write and what I called like an LPLayer which would go and you know change things for instance if you say 25 and it actually spells it out with letters you you will collapse that to a number you know but sometimes it would do it in problematic places and you're like oh no don't do that don't do it here you know so like it's just like an aftermath you know thing and you would wish that the model having enough context and knowledge about the world should do it right as it transcribes rather than you do this as as a aftermath yeah yeah exactly I'm thinking the same way and he's like it's a text layer afterwards yeah yeah exactly yeah super cool and then maybe like as we're wrapping up the podcast if you let me quickly tell you about ref to veck and sort of the pivot into recommendation and well so to start off ref to veck is about and it's about utilizing we VH data model a little more so we VH data model is designed where you have different classes so this class could be products this class could be user so you know like tables and SQL we have different data objects like the high-level ideas of designing data objects and then you have graph relations between them like user-like products so the simplest thing is that then you can represent the user as the average vector of the products that the user liked and then you can rank with you can re-rank with that or you could just search with that vector that that could be your search vector or you could have some other search like restaurants in Boston and because I live in Boston and you know like oh sorry I didn't mean to give away Boston in the query say I my my query is Italian restaurants and because it sees that Connor likes restaurant I don't know like some north and Italian restaurants one way that like it knows that I'm in Boston so it will it can personalize just using that vector to re-rank to only show me restaurants in Boston because if you show me a restaurant in Chicago it's like useless so so so that's kind of the first idea is this kind of like average the vectors to get the centroid but then there's this idea where and I learned this from talking to Martin Grootendorce about his burtopic library and I highly recommend people check that out it's such a cool way of visualizing vector spaces but this like HDB scan clustering so he was describing the difference between HDB scan and k-means clustering for how they produce centroids but and so HDB scan has this very cool like density clustering thing but regardless of the clustering you use I just I like HDB scan a lot but so let's say we get three centroids like I like Nike shoes a data shoes and Jordan shoes and you have these three centroids so you can use those three centroids is three average vectors from their respective clusters to re-rank with as well have some kind of diversity and results and then there is just this thinking around so so yes there that that's the recommendation pivot and then there's this idea of like top level index and I'm stealing that kind of terminology from gpt index because what gpt index does is to represent a long document you have again that tree summarization where you could say this is for obviously right and it's for and you summarize these two and then summarize one and see if this like top level index where you search through this layer first and this layer and so it's like if you're asking question like what was Barack Obama's legacy and then you have the symbolic filter of the titles of the Wikipedia pages and you have where title equals Barack Obama like that top level search will like super simplified the search space because now you're just looking in the Barack Obama article and there at all Wikipedia so I think reftivect also in the use of having constructing top level indexes by you know having document has passage has passage has passage again in the we get data model it can be it's just like I think it's a really interesting way that we're trying to use this cross-reference graph structure to move embeddings through the graph another idea and I know like doing a thousand ideas like it could be having like a graph convolutional network where okay so you have user-like product has brand okay let's just make it a three-class graph like that and so you have this this graph and you need to send the you need to aggregate the embeddings through the graph so now it's like should we just average should we try some kind of like nonlinear graph convolutional network and the graph convolutional network being beneficial because a graph network can handle like arbitrary number of inputs that's sort of like isn't like a fixed input size like transformers you would like zero-pad it to 512 tokens or the convolutional network is it's like kind of flexible but generally it's like very flexible to the number of inputs and so I hope that was an okay tour of reftivect and I know I'm trying to get in a little bit start it's amazing actually and I think I hope we can maybe discuss in subsequent episodes as well because the topic of personalization is also very interesting and like for someone who says okay we just have this fixed vectors computed from the content how the hell we can actually bring the user and this is what you've described this is what I perceive from it I think this is an excellent topic and this kind of opens up opportunities for vector search to appeal to to the you know search engine builder so maybe some other engines like recommendation and so but I think we have a ton of material I really love talking to you maybe before we close off is there something you wanted to announce to to the audience of vector podcast oh yeah I think it was so we have toured a lot of things but I really hope that you check out the weve 8 beer benchmarks repository so this is a recent effort around hybrid search coming back to that in a long conversation I really thought it was forever ago but like the hybrid search thing has been tested with with the beer benchmarks and so there's there's like scales there's like small scale beer medium scale larger scale so right now there's the larger scale and some medium scale I'm at smaller scale and some medium scale and right now we're working on the backups but this is all based on so we've got 1.
15 had backups where you can you know back up the weve 8 instance to have like a file that lets you just restore the weve 8 instance so you don't need to import the data it's like you know it's like with the face indexes how you can just read index so so now what you can do is you can just load the weve 8 index and so why this is so exciting to me is I've I've always been really interested in like hug and face data sets or papers with codes papers with data like this organization of data and I used to think with like weve 8's Wikipedia demo that it would need to be like live always hosted like you click try it now and then it's like boom you're in the console and you can query it but I think with these with this repo where you just download the Docker file for weve 8 it's like three it's like two lines of code where you do Docker compose up and then Python restore the name of the data set you want and I think that's just as easy as having some always hosted demo so yeah I hope and I think the other thing is with with hybrid search another thing that excites me so much is it's like if it's vector search only it's like you could argue well why don't I just use the face index then but I think because it's got the BM25 and the vector search is starting to offer more value with like how it can help you with your information retrieval research yeah and in general that's just something that is very important to me is trying to figure out how to connect with the information retrieval research I think the beer benchmarks presents a really exciting way to do it I do have some ideas on how users would be interested in it because I think the idea of beer benchmarks is maybe you look at it and you say okay NF corpus or trec covid or natural questions like very similar to what the app that I'm building but I think with chat gbt you could probably loop through your documents and generate queries gold like those would be the gold documents for those queries and you can do the same kind of evaluation testing where as you mentioned you want to see how that approximate nearest neighbor error cascades into the representation error and see what that means for your particular problem so I hope people check it out I hope find an interesting yeah that's a that's a ton super packed thanks so much what I like in this discussion compared to to the last 20 years ago is that you continue to explode with knowledge and I hope you will continue doing that thanks so much for your time today corner and yeah looking forward to talk more yeah thank you so much to me try to feel like the vector podcast is like the super bowl of search podcast so thank you so much thank you so much Connor yeah enjoy your day bye bye