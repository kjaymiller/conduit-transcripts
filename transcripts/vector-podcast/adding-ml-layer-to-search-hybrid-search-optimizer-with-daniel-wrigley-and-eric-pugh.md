---
description: '<p>Vector Podcast website: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://vectorpodcast.com">https://vectorpodcast.com</a></p><p></p><p>Haystack
  US 2025: <a target="_blank" rel="noopener noreferrer nofollow" href="https://haystackconf.com/2025/">https://haystackconf.com/2025/</a></p><p></p><p>Federated
  search, Keyword &amp; Neural Search, ML Optimisation, Pros and Cons of Hybrid search</p><p></p><p>It
  is fascinating and funny how things develop, but also turn around. In 2022-23 everyone
  was buzzing about hybrid search. In 2024 the conversation shifted to RAG, RAG, RAG.
  And now we are in 2025 and back to hybrid search - on a different level: finally
  there are strides and contributions towards making hybrid search parameters learnt
  with ML. How cool is that?</p><p></p><p>Design: Saurabh Rai, <a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.linkedin.com/in/srbhr/">https://www.linkedin.com/in/srbhr/</a></p><p>The
  design of this episode is inspired by a scene in Blade Runner 2049. There''s a clear
  path leading towards where people want to go to, yet they''re searching for something.</p><p></p><p>00:00
  Intro</p><p>00:54 Eric''s intro and Daniel''s background</p><p>02:50 Importance
  of Hybrid search: Daniel''s take</p><p>07:26 Eric''s take</p><p>10:57 Dmitry''s
  take</p><p>11:41 Eric''s predictions</p><p>13:47 Doug''s blog on RRF is not enough</p><p>16:18
  How to not fall short of the blind picking in RRF: score normalization, combinations
  and weights</p><p>25:03 The role of query understanding: feature groups</p><p>35:11
  Lesson 1 from Daniel: Simple models might be all you need</p><p>36:30 Lesson 2:
  query features might be all you need</p><p>38:30 Reasoning capabilities in search</p><p>40:02
  Question from Eric: how is this different from Learning To Rank?</p><p>42:46 Carrying
  the past in Learning To Rank / any rank</p><p>44:21 Demo!</p><p>51:52 How to consume
  this in OpenSearch</p><p>55:15 What''s next</p><p>58:44 Haystack US 2025</p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20250321_110308_985bc30944ce48882d237ba24dea55a4.png
pub_date: Fri, 21 Mar 2025 11:33:23 GMT
title: 'Adding ML layer to Search: Hybrid Search Optimizer with Daniel Wrigley and
  Eric Pugh'
url: https://rss.com/podcasts/vector-podcast/1951801
---

Hello there, Vector Podcast is back. Same season 3. I think we are about to wrap it up with few final really interesting episodes.
There I have the privilege to talk to the open source crew Eric Pugh who you have seen in the one of the previous episodes and you guessed Daniel Riggly joining us to discuss really interesting topic on hybrid search and optimization. Really really excited to have you both on the show. Hello.
Awesome. Awesome. So as a tradition we start with the intros. Eric everyone knows but Eric feel free to introduce yourself. I mean great to be back to me tree.
I actually I'm a little late getting here because I realized I was driving to the office that I forgot my mug that you gave me the other year. So I actually called Daniels like I'm going to be a little late because I got to go home and pick up the mug and bring it into the office.
My wife keeps it and we use it when we go hiking but I was like I'm going to bring it into the office and show it off since this is my second podcast to do with you and the mug that you gave me two years ago three years ago at this point. Yeah probably three years. Works great works great.
So yeah super excited to be back here and you know kind of talk about some of the work that we've been doing with the open search community. So exciting. Yes. And Daniel welcome. Can you say a few words about yourself your background? Absolutely yeah thanks. It's great to be here.
I'm super excited maybe a little nervous but I'm sure it'll be fun. So I'm Daniel I'm with open source connections. I started out as a search consultant back in May 2012.
 So almost 13 years now and I'm here to share some of our experiences that we made in our most recent project together with the folks of open search when it comes to hybrid search how to optimize hybrid search and also what's necessary to optimize hybrid search namely query sets and judgments but I'm sure we'll get into that in a couple of seconds.
Yeah thanks Daniel. I'm also nervous but I also know that you know when I release in the episodes I enjoy them. It's just it's just fun really. So I was thinking like hybrid search yeah we did discuss and I think community discusses it at large and various forums.
Erick also reminded me of the episode with Alessandro Benindetti that we just did it really was worth.
Yeah I was really just curious maybe step back from that topic a little bit and discuss the importance of hybrid search and what is it in your own words where do you see value for it compared to how we used to do search before. You want to take it Daniel and then I'll follow up.
 Sure yeah so I think we see hybrid search especially in this project as let's say the process of blending traditional keyword search and also let's say modern search approaches based on language more or mostly called either vector search or neuro search and I think the benefits of it are probably you follow it or I guess you can you can group them into two groups.
 Looking at the end user we always want to provide the end users with the most or the highest quality results right so search result quality is what we strive for and traditional keyword search always lacks of let's say finding related things that may not really contain the specific words but similar so laptop and notebook is an example that I think we ran probably a million times in demos maybe even more than a million times so if notebook is not in my product description I will not find it when I search for laptop and the other way around and that's where let's say blending the two techniques really shine because it enables you to not only find where your keywords are in but also find related stuff to augment the result set and I think that with that with that large benefit of course come a lot of challenges because it always is let's say non-tribal how to actually blend the traditional techniques and the more modern techniques so that's where the challenge between or the challenge behind hybrid search actually lies.
 I mentioned two groups for which there are benefits of the end user we want to provide the end user with the highest quality results that's one group the other group is of course we as the ones providing search applications I mean we somehow need to profit from providing better results and it then is always different or yeah different in which let's say scenario in which industry we are working so the monosperm transparent one is always e-commerce the easier the end user the consumer actually finds stuff in your online shop the easier is for them to buy stuff if they buy more stuff more easily of course we generate more revenue and that's kind of the benefit then that comes with providing better search results and the other way is we don't want to let's say manually tune systems let's say indefinitely so of course I can go ahead and say laptop is synonymous to notebook and PC is maybe broader term of laptop and rules like these but that's kind of work that is never done if I have a changing catalog that I don't know old products get thrown out of the product catalog new products arrive so it's a never-ending challenge for me and I don't want to let's say spend my work for us always manually hunting these rules and thinking about what made the users mean when they start for something I want something let's say intelligently looking for the right things in my index and that's what the neural part of hybrid search enables us so I think these are definitely maybe the two groups that benefit and how these two groups benefit from my perspective yeah that's really good intro Ericy water take it yeah I think it's an interesting journey that we've been on the last few years and I sort of look at hybrid search as a little bit of a like a course correction right so keyword search been around forever well understood frustrations are well known and then vectors came out and all these new products these new vector databases everybody was really excited about them and we all said oh okay let's go use vectors and we leapt on that and got really excited built everything using vectors and I think maybe we went too far that way over into vector land and we started after we started getting some experience with vectors we started realizing some of the problems with it right like doesn't matter what you query you're gonna get some search results right sometimes zero search results is the right answer right you know interesting challenges around you know faceting or pagination or highlighting can be weird right so you know I think that there are some definite challenges in vectors and we all went over that way and I think we've seen it in the last two years where all the vector databases were frantically adding keyword like search and all of the keyword search indexes were all frantically adding vectors okay now we have these things as like where do we go oh hybrid search right hybrid search and you know hybrid search popped out and you know hear me out I think hybrid search is just good old federated search from the late 90s and 2000s where you had two search engines with two we send out two queries and then you brought them back and you're like how do I merge them together and sometimes you do terrible things like two lists of results right we was sometimes we would try to link them up together um it's the same idea whether you're going to one search engine you're making a keyword search and a neural search and bring them together or two totally separate see keyword search engines you're still bringing back two lists however I think at least this time around how to merge the lists of results together seems to be going better than when we did it back in federated search right uh and I look forward to talking more about like some of the ways that we bring hybrid you know build our hybrid results set together um part of me really kind of wonders why ranked reciprocal fusion wasn't a thing the last time I did federated search back in the 2000s right like doesn't seem like that crazy of a concept why didn't we do that right but we didn't uh so I'm a little more optimistic about the value of it but um it i think hybrid is a little bit of something old coming back because we're back to the same problem I literally have two search engines two concepts for how to do information retrieval and yet I want to blend it into one yeah that's exciting topic I think to me a hybrid search opens doors beyond sort of what I think Daniel just explained you know the semantic connection between you know keywords and so on is where you go multi-model right of course you need to go there carefully probably but if you do miss metadata on a particular you know image on the product you could reason about it using the image itself and maybe also video because we have video alarms as well they're more expensive of course to run but you know sky's the limit so to say if you want to go there and go um yeah so I think I think in that sense hybrid search uh unlocks many more avenues to explore including in e-commerce I think right yeah yeah yeah I mean I love that we are actually getting away from the old just straight up bag of words that was keyword that served us for a long time but still was just a very rough approximation of what people want right I mean BM25 you know people say it's not even the best algorithm it's just as fast as the one that we use uh vectors is sort of this idea of there are richer ways of understanding user queries and the content tech and just going beyond taxes the you know it's absolutely wonderful right lots of different things I mean some point we'll do a vector search on usage patterns right to figure stuff out right like it'll be the the mode will be activity won't be video or image or something it'll be activity you'd be like oh yeah that's the person I want to talk to they have the same activities as me based on whatever it is that they do right so but those kinds of things definitely are expressed through the vectors um I do think that hybrid is an amazing thing for right now for the next few years uh I do think though it's also a little bit of a bandaid in the sense that we're still leaning on keyword search for you know various use cases and if we were to look 10 years out I think an ideal solution is that we're not doing hybrid anymore just have a better approach to search something beyond vector plus keyword something better that still supports the zero results is the right answer you know some of these problems that vector using vectors gives us right we would have better better approach and not this slightly band-aid I have two different ways of searching and then have to wedge them back together yeah I like for now hybrid's exciting yeah I like that I like where you're going I wanted to also I wonder if you saw that blog by Dr.
 Enbal I will make sure to link it where he talks about uh rrf you know reciprocal rank fusion and he shows on a like handcrafted example that uh you know if let's say neural search brings relevant result to the top of few results keyword lacks and doesn't so it basically brings noise when you combine the two you will end up having kind of half noise half signal and it will look terrible it will look terrible right and where do you stand on this like only there was a way yeah yeah it's only there was a way of at our understand not just blindly yeah you know blindly matching things um and I'll hand it over to Daniel and just a moment I do want to call that I really liked uh your previous episode with all a Sandra where uh I can't remember was you were all a Sandra but you kind of I think it was you Demetri said yeah that your engineers were looking at hybrid search and they kind of looked at it and said when you strip away the fancy words like ranked reciprocal fusion for blending things together you're like that's just round Robin right and you know round robin is not necessarily a round blind and it's blind round Robin right it's not round robin in in your middle school when you had to pick teams for dodgeball right the people picking knew who the best players were so at least you were at least divvying the best choices and at the very end those last two kids you know you knew they were the worst choices they were the noise in the search results right but you were that round robin at least had the benefit of knowing what was good ranked refresh ranked reciprocal fusion has no sense of whether the those results are good or bad right it is literally blindly picking them in some order with no sense of uh of what that is and as you can imagine blindly picking is going to leave lead you be pinched potentially a very weak dodgeball team right and yet that's what we think of a state of the art yeah so Daniel what should we do in this case is there any solution it's it's a good segue into what we actually tried and explored and experimented with um so in our most recent work we tried to come up with a systematic approach to optimize hybrid search specifically in open search um so in open search actually right now you have linear combination techniques at hand so that means you have two normalization techniques you can choose one the L2 norm the min max norm um they are basically both there so that you can normalize the scores from keyword search into the let's say space of vector search so that you can compare apples to apples more or less here and not apples to oranges because as we all know vm25 scores especially if you have if you have like wired field weights they are unbounded they can be in the dozens the hundreds the thousands so you don't really know upfront in what range you are operating and also you can't really compare the scores from one query to another query so that makes it really difficult to combine keyword search scores with any other um let's say search mechanism together with these normalization techniques the L2 norm them in max norm you have three combination techniques at hand and that's basically just three different means you can apply the arithmetic mean the harmonic mean and the geometric mean so that leaves you with two by three so that's already six parameter combinations that you can try on and then you can define weights um so how um how much neurosurge weight how much keyword search weight do i want to have in my query they always add up to one so you can say I want to go with 10% keyword 90% neural or 50-50 um thinking of let's say 11 of these weights so maybe you start with zero keyword and a hundred percent neural and 10% 90% and so on and so forth so that gives you a range of 11 multiplied with the six parameter combinations that we already had gives us let's say a solution area to explore of 66 different combinations which is pretty manageable so we defined optimizing hybrid search as a parameter optimization problem and we picked the most straightforward approach that you can pick and we just tried out all different combinations and calculated search metrics based on judgments and then we just had a look at which one is the best combination um for our experiments we used the ESCI data set um so that was released by amazon a couple of i think 18 months ago or something like that as part of our taglet competition um this data set comes with queries comes with products and most importantly it comes with judgments so we basically have everything that we need to really try out different um parameter combinations see how they work what results are retrieved um can calculate a couple of metrics compare these and then see which one is the best um parameter combination and um that's what we call the global hybrid search optimizer so we try to identify the best parameter combination globally for all the queries that we are looking at in a certain defined subset of queries so that's kind of the first step um the very very straightforward approach that we applied that's not really something um let's say scientifically um so first decated there was just a very brute force approach to see um what's in there also learn how results may be shaped or turn out differently when we increase neural search weight or increase the keyword search weight which normalization combination uh technique is usually the one that's best to retrieve the results and so on and so forth so um we started out with what I call reasonable baseline so searching across um I think five or six fields so title category color brand and description some bullet points so ecommerce data set like pretty basic stuff um and we calculated our metrics with that baseline so um I would call it uh probably not the best baseline you can come up with um but a reasonable baseline um you could come up with so we didn't want to let's say just create the weakest baseline because that's not really difficult to let's say outperform so we wanted to create a reasonable baseline without putting let's say a man here in finding out what the best baseline is um that's called okay right um we got decent results out of that and then we ran this global hybrid search optimizer and that outperformed the baseline already um across the metrics that we had a look at so better in DCG better DCG better precision at 10 were um these were the three metrics that we had a look at and that was nice to see because that already gave us um let's say assurance in a there is a straightforward approach that everyone can use because it's really easily applicable um it gets you good results and it also gives you assurance that there is something too neural search when switching to it from a keyword based um search engine or search application to a hybrid search um application but um as always when you apply something globally there are winners and there are losers so um some of the queries really improve by this hybrid search optimization step the global one but others didn't so we took this one step further and thought about how can we um really create a process that dynamically per query now predicts what the best parameter set is and that now is also like going in this direction that dark mentions in his blog post right so that's kind of a query understanding approach to hybrid search so we're not just blindly applying one parameter combination that we identified on a thousand queries that we explored we are taking one query analyzing this one query and then saying based on a variety of experiments that we made what is for this individual query the best parameter combination that we cannot apply so that we are not really globally applying something but individually dynamically per query and um to already maybe yeah give you the results um of what we did and then go into detail how we did it um the dynamic approach outperformed the global approach so we managed to identify a set of features we trained a model or multiple models actually and by applying this we were able to predict the best neuralness in that case or the best neural search weight for a given query based on um the results we got off the global hybrid search optimizer so we basically recycled all the different search metrics on a per query basis that we got did some feature engineering trained models and then used these models to predict what is the best neural search weight for this query and with this dynamic approach we even saw increases up to 10% in one of the metrics of the three that I just mentioned DCG and DCG and precision at 10 yeah that's very exciting thanks for for sharing this whole you know end-to-end picture pipeline I'm particularly interested at least at this point in time about the fact that well first of all your dynamic approach outperformed the global one right and that seems to be thanks to that query understanding part right can you talk a bit more about that uh and also did you check those predictions manually for example does it make intuitive sense that system picked that's for that specific query it picked more neuralness I mean is it like is it like a natural language question there or like some remnants of it or is there some other interesting findings that you could share possibly oh yeah so um let's first maybe outline what we did exactly and then dive into a couple of observations that we made on the way um so we started out by creating what we call feature groups and then we created features for these feature groups so we looked at three different feature groups um one was the query feature group the next one was the keyword search result feature group and then the semantics search result feature group for the query feature group we had a look at the length of the query the number of query terms if the query has numbers in it and if the query has any special characters in it so we kind of thought of ways figuring out when is the query maybe more specific when it is more when is it more broad query a narrow query and then we will just come up with rules like well a longer query is the more specific it is and maybe if we have more specific queries we have less results that's where we may want to augment search results with neural search results on the other hand when we have a very broad query we may have a lot of results these are short queries and then we may want to let's say only use organic traditional keyword search results yeah so we just came up with a couple of assumptions on our side and then with these four features for the keyword search result feature group we looked at the number of search results the number of hits we got when we executed the query with our baseline search configuration so the one searching in the six fields and then with something like hey if we have zero results then this is maybe a perfect scenario for neural search because then we want to augment zero results with zero keyword results with what comes from the vector search application the other two features we had in this group were the the best title score we had in the keyword search result so if we have a strong title match maybe that's an indication of we don't need as much neural search and we also have a look at I think the average title score in the top 10 was was another one so if we have like a high average in the title scores that's maybe a good sign for no augmentation needed with neural search results for semantic search it was similar to the title score so we had a look at the best title score and the average semantic similarity based on the title that we had indexed so by looking at these three groups we thought of well we now have a representation of the query on its own the result set based on keywords and then the result set based on neural search and that was kind of our starting point and then we did loads of experiments having to do with what's the best feature combination when we train a linear regression model or a random forest regression model what does regularization play for a role can we optimize the model training aspect with that so we really did a lot of iterations with these we also had a look at a large query set and versus a smaller query set to see if that also provides different aspects to it if we just randomly sound the 500 yet 500 queries versus 5000 queries so we did a lot of exploration to really pick out and make sure that we are not really let's say randomly receiving the uplift that we saw but actually really making sure that there is something to it and that we can go out in the wild there for example on this podcast and share our observations and be kind of on the safe side that they can be reproduced hopefully in other scenarios as well so that's kind of on the let's say how did we do it here so the features feature engineering how we train our models so we have a look at linear regression models and random forest regression as a starting point because we thought let's have a look at simple models first and if that works we can still have a look at the more complex ones and that's maybe already the first observation that I can share so linear regression models and the simplest form random forest regression slightly more complex form and then this is the last model iteration that we did last week we also have a look at gradient boosting methods and interestingly they all were almost the same from the model performance perspective so it wasn't like the most complex ones really give you the best results and that's kind of a very reassuring feeling because we need to calculate a couple features right per query and that adds latency to your query execution and especially in e-commerce where every millisecond basically counts we don't really want to let's say run multiple queries to calculate our features just to have like 0.
3 percent performance increase so it really has to be worth the effort so that's kind of a nice observation so we don't have to go for the most complex model architectures we can stick with the simple ones and don't really lose a lot of performance if any the linear regression model and the random forest regression model they actually scored absolutely equally when calculating the search metrics so they predicted the NDEG scores slightly different so that's how we did it we predicted NDEG scores by adding neuroness as a 10th feature in that case and by looking at which neuroness scored best and that kind of the neuroness we went then with for testing efforts afterwards so that's kind of the first interesting observation that we made we also had a look at different feature groups so what happens to model performance if we focus only on query features or only on keyword search result features so training models within one feature group only and not taking all features into account the interesting part here was that the combination work best so not always the combinations of all in nine features together with the neuroness feature but at least some of the key word search result features some of the semantics search result features and some of the queries features so they were best but looking at the query features only and these are the simplest ones to calculate they weren't part of these models from this performance aspect so you wouldn't really lose a lot if you only shows query features for your predictions so that was another nice observation here that if you went with the highest performance in terms of let's say inference speed and also speed of calculating the features beforehand you don't lose a lot of search result quality so again you don't have to go with the most complex approach to get reasonable results out of that which was I think the second most important finding at least from my perspective because that gives us again the assurance that when putting this into production we don't assume let's say hundreds of milliseconds added to your query latency if you stick to the the simple features.
 May it means that there's like room for growth with this technique right we're not maxing out this technique just to get started we can start out and then as we get more sophisticated we have room we have milliseconds to burn to do other cool interesting things ask an lllm to characterize the query or something like that right we've got room for bro but I will also like this lesson and I think it kind of resonates with what I've seen in doing a mal previously is that start with like simpler solutions and try to kind of maximize ROI you know by upgrading to a more complex one and you need to set some thresholds because like as you said Daniel you know just you know adding 0.
03 won't get it right it doesn't it's not worth because also when you bring in neural search it means that you need to build that parallel index of things right like you need you need to compute and maybe GPUs as well and someone will need to pay for it and I guess the passing Daniel we're doing this project I kept asking Daniel I'm like why is re-indexing with embedding so slow like where's my turbo button like really why is this still a problem it's 2024 almost 2025 why does embeddings take a long time yeah I remain a little confused why it's so like don't we just turn a knob and make a GPU go faster and then re-index with embeddings is just the same speed as re-indexing with keywords yeah but but also what Daniel says now but also like the fascinating part like one thought crossed my mind as you explained it Daniel is that in some sense you've built some sort of like a reasoning engine if I may call it that way maybe it's not fully you know reasoning like I don't know LLM start to do it that way or something but it's like the engine that looks at the query and examines its features and makes some some conclusions and then it looks also at the results it's not like you just you just you understood the query you set it over to the retriever side and then you hope for the best that there will be best results right but in some sense you you basically do this dynamic sort of reasoning on top of everything but but the lesson there you said and correct me if I'm wrong is that just by looking at the query features you could already achieve good results you don't need to look at the result features yes yes but wouldn't it be nice to look at those yeah I do love the idea of looking at both sides right we tend to focus on queries because I think that's the viewpoint of our industry we are very query centric in the search world it's all about the query and what can we get out of the query we really don't look at the results that much except to say are they good or bad and we're not particularly good about factoring in and what did the user do back into our algorithm and yeah I love that this is a little the dynamic thing that we're doing here I think it's a pointer to bring in more dynamic aspects to our algorithms where they actually can start evolving or changing or being very specific to very specific query types use cases time of year right and today that's very difficult to do only the most sophisticated to teams have sets of algorithms yeah but I also feel like I like what you said Eric like looking at results you know and reasoning about results and also what you understood about the query might lead to much better final representation of what you show to the user right because there are so many there are so many factors also beyond the query and results right like as you said season or you know you observed some patterns with the user the recent purchase history and so on and so forth yeah I mean it's very fascinating and also like if I continue to draw this analogy with lm world you know when you ask lm to to think through what it has done it may correct itself right by just looking at what it has produced because lm's are as someone said calculators for words so if you if you give it itself its own output and ask yeah exactly yeah like I can't wait to write a search algorithm that understands what they did the last time the user didn't like the results and so when you get a similar query for the same user do something new right try something new because whatever you were doing before the user didn't like yeah joke about if the user hates what you're giving them you might as well just return random docs because that'll be better than whatever you're doing right now yeah yeah at least you you have a chance there right with the random so what question I sort of have though in what we described how is it different than learning to rank other than learning to rank is about ranking one list and here we're ranking two lists is do I just conceptually have the role of learning to rank wrong between what learning to rank is and how the dynamic hybrid optimizer was working so I mean we are not re-ranking results right so that's what typically learning to rank does but what we are kind of doing is we are learning when when to let's say increase the weight on keyword search results or on your search results right so it's kind of a learn to lend learn to blend learn to search the new technology or we just done with the learning to nomenclature right and we like optimizer better than learning to blend maybe yeah so I'm not the one who's most creative in coming up with clever names so maybe it's maybe it's time for not learn to but blah blah blah optimizer and that's kind of how we ended up with the hybrid search optimizer right now but I wouldn't really have a good let's say argument against calling it learning to optimize hybrid search or something like that because that's kind of what the dynamic approach does right as we gather more data more clicks more of that those go into the features right we even use the the language of learning to rank right feature engineering right we use that language and we're building a model and you even mention right linear models in a forest and you know those are all the the words that I think of as oh it's learning to rank so interesting I think that's just it's interesting to see learning to rate maybe come back in a new way yeah yeah and we're learning to rank something you can apply on top of the hybrid search optimizer right so it's not like we have any kind of substitute here so that's kind of still I think a very valuable tool in the mix and that's just now one way to really figure out what's the best way of getting to reasonable hybrid search results.
Yeah, but I was recently also thinking about this. I wonder what's your hunt on that, but learning to rank sort of depends on the training data and you usually collect it from the past.
You don't collect it from the future, right? And so as you move into the future and patterns change, you carry over that past weight that can actually go against the intent of your reasoning engine. And that's where I think that a lot of work needs to go in all of these directions.
But as you optimize your retrieving and your reasoning engine, you know, your query understanding, maybe you should dial back the LTR a little bit or maybe you need to retrain it right there right then.
I don't know or retrain frequently enough so that you don't lose the invention strengths, right? Yeah, I think that's our challenge in a lot of these things. The historical approaches versus the predictive approaches. Right.
And, you know, which ones do you go with and how do you discount the historical if you have a bunch of new interesting data? Yeah, yeah. But I also like the limitations of the physical world, I think from investment books I've read one key key takeaway lesson is that no one can predict the future.
If someone claims that they can do, they probably lie. But again, I guess there is still room for being more dynamic. And is there something you guys want to also show? I mean, is this something we can look at visually or? Well, theoretically we can. No pressure.
I mean, so this small demo here and I'm going to show the results first and then how we get to these results. It basically takes in a user query. My search application now is this Jupyter Notebook. So it's not the most sophisticated search application. But it calculates the query features.
Then with these query features reaches out to the model to get the best neural nest what these search features. And with that retrieved response, the query is built together and then sent to open search.
So we're just going to have a look at a couple of examples first and then we can have a look at the code. So again, this is now a part of the ESC iData set. My index has like 20,000 documents in it. So it's not large. It's only a subset of the ESC iData set.
And when we send queries, now in this case, water proof jacket. In this method, the method first, as I just explained, calls out to the model. It retrieves neural nest score and then builds the query together. And then we have this HTML display here.
As you can see, there are not images available for all of the products. But what we can see here is that what a weather proof, sorry, weather proof jacket, it gets a 50-50 search waiting. So in this case, 50% keyword 50% neural. If we go for weather proof jacket for the women, the weights change.
So now we have 90% neural and only 10% keyword search weight. And then, and that's because the query became much more specific, meaning that since we did add women there, we are not expecting results for men or for kids, right? Is that exactly.
So that's kind of what we can now infer maybe in what the model picks up here. So we have a longer query. We have a more specific query.
So it's not really looking at the words, I'll say, the model, but at the features like query length are there numbers in it? Are there any special characters in it? And so another one, weather proof jacket black. And we also see that maybe results that we wouldn't really expect are in the top here.
But again, it's only a small-ish proof of concept kind of thing that we are looking at here. But we can see different queries that are similar from let's say, meaning standpoint of view, they retrieve different weights in that case. And that's kind of the interesting thing.
And we can go for something completely different as well. iPhone case. And we see light, nice iPhone cases throughout the query and that goes with neuro.7 and keyword. 3. And I don't know what's iPhone 15, role, max, A's, black. So that would be a very, very, very specific query.
And here, again, the neurosurgery increases. Whereas when we go for a very broad query, so that's maybe one characteristic of the model that you that you can almost feel is the more specific we get, the more neural weight it gets, but also other features to play a role. Yeah.
Yeah, that's very interesting. And that's how the open search query looks like. So let's say the interesting part is that one here.
So we have keyword query, which is like I explained before searching in these couple of fields with different field weights, best fields query, multi match with the end operator. And then we have a neural query that retrieves the A100 based on the title embedding that we have.
And then the hybrid part is actually the search pipeline that normalizes. In this case, with the L2 norm and combines the results with the arithmetic mean based on the keyword search rate and neural search rate and that are here, passed in as variables that another method.
Yeah, predicts with model inference in that case. Yeah, that's kind of the small but built in Jupyter notebook prototype that we have. Everything we build is like Eric just mentioned open source.
So we have a public repository that contains all but this one notebook actually, but everything you need to train the models, do the feature engineering, calculate the search metrics. So yeah, everything you actually need. So running this with the ESC IDATA set is possible for everyone.
And if you want to apply with your own data, that of course is also possible. So that's kind of what we are looking at next. So adoption in the industry.
And also, hooking it up with the other part of this project, which is namely the, let's call it the evaluation part, calculating implicit judgments based on user feedback.
So clicks queries, stuff like that so that we also enable not only everyone to optimize hybrid search, but also enable and empower everyone to well come up with judgments if you don't have any because that's kind of the basics you need for any query.
Yeah, and that's where keep it comes in shameless flag. So one of the things we actually have a reference implementation. Some of you all may have heard of chorus, which is reference implementation for e-commerce search. We did it in solar. This we have an open search version.
And some of the stuff that you're seeing is sort of bleeding edge, hot off the presses. But we're working right now on getting the chorus for open search edition updated with some of these scripts and notebooks.
So you can just check it out, run the quick start, and then have everything and start playing with it. So you don't have to build all the steps yourself. So you can see how all the pieces fit together. And so that's available. Be great if we can add a link in the line or notes for that as well.
Let's do that. And just want to also to understand this search pipeline and all this, you know, mechanics of hybrid search that you guys implemented. Is it like a plugin to open search? And what's the plan for it? So I guess you spoke to that. Like let's give it to as many users as possible.
What's your idea there? So hybrid search is available in open search. So with the let's say the basic share so you can create a pipeline and say 70% keyword search rate and 30% neural search rate. You can also define these on the fly.
But we currently have the limitation that we although we can hook up the model within a so-called ML inference pipeline in open search. This ML inference pipeline can as of now not pass the predicted neural and keyword search weights to this pipeline.
But feature request is already out there and I assume that in one of the next open search versions we will have the possibility to not only what's already possible hook up the model within open search so that it from within open search you call out to the model to make inference.
You will retrieve the predicted neural and keyword search weights and then you can use in your search pipeline. So there is already an implementation plan out there.
There are open feature requests and if anyone wants to give these thumbs up to prioritize that within the open search community that would of course be greatly appreciated and I'm sure we can include these GitHub issues as well in the show notes. Yeah, for sure. Let's do that and we will call out.
That's how it all out to the community. Please vote if you care. I hope there will be enough people who care about this. Yeah, exactly. And then you said everything is open source.
Does that mean that the training scripts, the algorithms, the choices you make you can make there is also open source, right? And we can link to that as well. Yeah. Yes.
So everything we did, I mean we of course didn't include all the thousand experiments but all the helpers at least that we used to run these 1000 experiments. They are all in the repository out there for everyone to have a look at and maybe come up with even better ideas than we had.
So we definitely always love to hear these as well. Wow. This is amazing. This is a we can true sense. You speak up to your name, open source connections. That's amazing.
To me, it's a ton of work that you could choose to hide as well, right? And work only with your clients and nurture and iron out and everything and make a ton of money and continue on that. But you choose to open source because you believe in the power of the community as well to enhance it.
That's amazing. Well said. What's next? You already said a couple of words, but what's next? And also do you want to address our audience? What do you expect from that? I think one of the things is as we've gotten into this, we've found some rough spots in open search.
Open search has a strong ML component, ML Commons project, but integrating it in sort of new and interesting ways like what Daniel was showing. We're finding some rough spots. It is interesting to me. It does bring in a mind the do search engines.
What we call a search engine need to evolve to be more of an ML engine as well, right? I mean, it feels to me like search has been revolutionized by machine learning.
And as we move into this direction of more calculating building models evaluating data on the fly, do our search engines need to support those use cases and go beyond just the I get a query, I get documents, I match them up and that's it, right?
Is there another layer of computation that we kind of need in the search engine versus bolting it on in some other environment with an ML ops pipeline and all the rest? And I think that's interesting.
 One place where I think open search is definitely breaking some interesting ground is all the machine learning aspects to it, but data processing and building models and all of that needs to maybe be a first class citizen of what we consider a search engine versus something done by some other system elsewhere because that's a lot more complexity and you know raises the barrier to adopting these things.
So I look forward to things like a hybrid optimizer just sort of being like what you do when you build your search engine, of course, you turn on the hybrid optimizer.
If it meets your use case and you have the judgements and other features that you need, right? Versus oh, a major engineering project that we're going to do this going to take us six months. So yeah, yeah, yeah.
And you know supporting that, you know, as Dan, you highlighted the search quality evaluation framework that we're adding to open searches, really exciting, would love to come back to Metri and talk all about that on another show. Yeah, let's do that.
I'm really excited to dive deeper into eBall because I think in so many ways, you need to start with eBall, especially if you have a search engine right out there to establish that, you know, baseline for yourself and then learn an introspect where things work or fail. Yeah.
Make sure some of these models don't go and produce terrible yeah, back shit crazy results, right? We had the risk that you know, boosting up on neuralness might be terrible, right? We have to understand that.
And so we need to be much better about evaluation, right? We can't take our eye off the ball of speed, right? Query speed does remain just sort of top of mind. We also really need to be good at evaluation. And I think for a lot of search teams, that's kind of a new thing. Yes, absolutely.
So the pooling and closed search agents. Yeah, and we do want to do what other thing I want to shout out. Yeah, one other thing I want to shout out is that the next haystack conference, the Save the Date last week in April, right? Last week in April, Save the Date went out.
And we are looking for talk reviewers, people who want to be reviewing the talk proposals. It's a double blind process. We get a couple people from the community. So if that's something interested, reach out to David Fisher. He's running that process. And call for proposals will be out.
I'm curious if this year, haystack in Charlottesville might as well just be called hybrid stack. Like, are we going to have two days of talking about hybrid? Because R.A.G. Rags, you know, that's last year. Now we're on to hybrid.
Or is there going to be something new? So it's going to be interesting to see what kind of comes out of the community for this year's haystack. It's also very interesting to see this dynamic or this evolution because I think two or three years ago, hybrid search was at the top of charts.
Everyone was discussing it. And then Rags proceeded it. And that just tells me that maybe we didn't dive deep enough into the topic, which is let it go. It passed and people thought, oh, that doesn't work. We need we need something else. And now it's Rags, Rags, Rags everywhere.
But then Rags also comes with its own, you know, limitations. And now we have a new, you know, evolution level, right, with hybrid search, you guys are optimizing. That's amazing. I was actually waiting for it. I'm happy to see it happen. And thanks for sharing the story.
Let's come back to the eBall when you guys are ready. I also expect that you will publish some blog posts around this topic about this topic. So I'll, you know, I'll be happy to promote those as well and read them, of course, and learn. Terrific. Yeah. Thanks. Thank you very much.
Thanks for coming to the show today and sharing this story. It's very exciting and also showing the demo. I like notebooks because they love you, you know, too quickly. They say things and have a feel of what's going on. I'm glad that Eric still uses the cup that we gave him as a gift.
It's like, like, you're really see, you really see the gift you gave like coming back and saying, you did this and the person you gave it to is still enjoying. That's amazing. So awesome to me, Trey. Thank you very much. You thank me and Daniel on. Yeah, thanks for having us. Yeah, thank you.
Take care. Bye-bye. Bye.