---
description: '<p>Topics:</p><p>00:00 Introduction</p><p>01:04 Yury’s background in
  laser physics, computer vision and startups</p><p>05:14 How Yury entered the field
  of nearest neighbor search and his impression of it</p><p>09:03 “Not all Small Worlds
  are Navigable”</p><p>10:10 Gentle introduction into the theory of Small World Navigable
  Graphs and related concepts</p><p>13:55 Further clarification on the input constraints
  for the NN search algorithm design</p><p>15:03 What did not work in NSW algorithm
  and how did Yury set up to invent new algorithm called HNSW</p><p>24:06 Collaboration
  with Leo Boytsov on integrating HNSW in nmslib</p><p>26:01 Differences between HNSW
  and NSW</p><p>27:55 Does algorithm always converge?</p><p>31:56 How FAISS’s implementation
  is different from the original HNSW</p><p>33:13 Could Yury predict that his algorithm
  would be implemented in so many frameworks and vector databases in languages like
  Go and Rust?</p><p>36:51 How our perception of high-dimensional spaces change compared
  to 3D?</p><p>38:30 ANN Benchmarks</p><p>41:33 Feeling proud of the invention and
  publication process during 2,5 years!</p><p>48:10 Yury’s effort to maintain HNSW
  and its GitHub community and the algorithm’s design principles</p><p>53:29 Dmitry’s
  ANN algorithm KANNDI, which uses HNSW as a building block</p><p>1:02:16 Java / Python
  Virtual Machines, profiling and benchmarking. “Your analysis of performance contradicts
  the profiler”</p><p>1:05:36 What are Yury’s hopes and goals for HNSW and role of
  symbolic filtering in ANN in general</p><p>1:13:05 The future of ANN field: search
  inside a neural network, graph ANN</p><p>1:15:14 Multistage ranking with graph based
  nearest neighbor search</p><p>1:18:18 Do we have the “best” ANN algorithm? How ANN
  algorithms influence each other</p><p>1:21:27 Yury’s plans on publishing his ideas</p><p>1:23:42
  The intriguing question of Why</p><p>Show notes:</p><p>- HNSW library: <a href="https://github.com/nmslib/hnswlib/"
  rel="noopener noreferrer nofollow">https://github.com/nmslib/hnswlib/</a></p><p>-
  HNSW paper Malkov, Y. A., &amp; Yashunin, D. A. (2018). Efficient and robust approximate
  nearest neighbor search using hierarchical navigable small world graphs. TPAMI,
  42(4), 824-836.  (arxiv:1603.09320)</p><p>- NSW paper Malkov, Y., Ponomarenko, A.,
  Logvinov, A., &amp; Krylov, V. (2014). Approximate nearest neighbor algorithm based
  on navigable small world graphs. Information Systems, 45, 61-68.</p><p>- Yury Lifshits’s
  paper: <a href="https://yury.name/papers/lifshits2009combinatorial.pdf" rel="noopener
  noreferrer nofollow">https://yury.name/papers/lifshits2009combinatorial.pdf</a></p><p>-
  Sergey Brin’s work in nearest neighbour search: GNAT - Geometric Near-neighbour
  Access Tree:  [CiteSeerX — Near neighbor search in large metric spaces](<a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.173.8156)"
  rel="noopener noreferrer nofollow">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.173.8156)</a></p><p>-
  Podcast with Leo Boytsov: <a href="https://rare-technologies.com/rrp-4-leo-boytsov-knn-search/"
  rel="noopener noreferrer nofollow">https://rare-technologies.com/rrp-4-leo-boytsov-knn-search/</a></p><p>-
  Million-Scale ANN Benchmarks: <a href="http://ann-benchmarks.com/" rel="noopener
  noreferrer nofollow">http://ann-benchmarks.com/</a></p><p>- Billion Scale ANN Benchmarks:
  <a href="https://github.com/harsha-simhadri/big-ann-benchmarks" rel="noopener noreferrer
  nofollow">https://github.com/harsha-simhadri/big-ann-benchmarks</a></p><p>- FALCONN
  algorithm: <a href="https://github.com/falconn-lib/falconn" rel="noopener noreferrer
  nofollow">https://github.com/falconn-lib/falconn</a></p><p>- Mentioned navigable
  small world papers: </p><p>Kleinberg, J. M. (2000). Navigation in a small world.
  Nature, 406(6798), 845-845.; </p><p>Boguna, M., Krioukov, D., &amp; Claffy, K. C.
  (2009). Navigability of complex networks. Nature Physics, 5(1), 74-80.</p><p></p><p></p>'
image_url: https://media.rss.com/vector-podcast/20220131_090127_be85ef047356dd187c4b22fb3a9286be.jpg
pub_date: Mon, 31 Jan 2022 09:41:27 GMT
title: Yury Malkov - Staff Engineer, Twitter - Author of the most adopted ANN algorithm
  HNSW
url: https://rss.com/podcasts/vector-podcast/377082
---

Hello, vector podcast is here and today we're going to be talking to the author of H&SW library and algorithm. It's one of the best algorithms out there, one of the most used algorithms in vector search. And today I'm talking to Yuri Malkov. How are you doing? Hi. Hi. Hi.
So yeah, my name is Yuri Malkov. Currently I'm working at Twitter. There's a staff from my engineer and the content understanding and research and recommender systems. Yeah, please know that during discussion I don't represent like Twitter's point of view. The views are of my own.
Yeah, so it's great. So yeah, you already began introducing yourself. So I was wondering if you could tell me a bit about yourself, your background and then maybe we can also move into discussing the algorithm itself. Okay, sure. Yeah, so my trajectory of moving to ML is quite typical to Russia.
So yeah, I got good physics education in Nizhny Novgorod and there I did the PhD in laser physics. So there I was doing experiments on teravat lasers. So that was fun and like that part of physics is like considered to be like sexy part, similar to computer vision in machine learning.
And I was lucky to have good supervisors. So one of my supervisor which was like mostly a supervisor of paper. So he helped me. Is now the head of Russian Academy. So yeah, I had good supervisors.
In addition to physics, I was concurrently working part time in a startup that was building distributed scalable search systems based on insights from real networks. Yeah, that worked ended up in several papers on predecessor of H&W.
And the startup, yeah, unfortunately the startup was closed before even I got PhD. So yeah, I decided to focus on physics after that, but after I got my PhD degree in physics. So I, like there was a choice for me like what to do next and to proceed with career and physics.
I had to go abroad, like I didn't want to go abroad. I want to stay in Nizhny Novgorod. So I decided to just like switch directions and to network science there. And then I got a really good grant from the Russian fund. Alpha Phi, which now is not present anymore.
So I could do like research by my own. Like this pretty good salary. And yeah, I also joined companies, like computer vision companies to get to insight into why people actually use like similarities to your algorithm and machine learning.
And I worked at an television and later anti-club, which is the company that is like doing big brother for Moscow, like Moscow surveillance.
And later I joined some some VIS Center in Moscow and I worked with like Victor Limpitsky who is one of the well known personas in Russia and in 2019 I moved to US and now I work in Twitter to recommend their systems and content understanding, like board models. Oh yeah.
So you probably also use nearest neighbor search in your work or. Well, I can mention it. Yeah, well, not really. So I'm focused on the so I can work Twitter most of the time. I can have last half a year, I spent on improving search relevance. So that is mostly the ranker.
But that is closely related to the nearest neighbor search. Yeah. So you mentioned like basically the background where you've been in Russia, it was like kind of related to computer vision. Of course, you had physics background by education, but you also kind of worked in computer vision startups.
So what was your impression of this nearest neighbor search problem and like, how did you think about it when like, did you read papers to understand like what was done in that area? I think that areas pretty like developed right in in in the papers like like NSW itself, right? Like navigators.
Well, so like in the startup meta labs, I have been working, I think I've worked for six or seven years. So it was quite quite a significant period of time. And then we started just like from distributed search. So the idea was like we do it from scratch.
So like we don't care what I've been done before. So we have an idea. So there are like distributed hash tables like port or other stuff and we want to do it, but with similarity search. So that should scale to better base. And there that's like very different approach from nearest neighbor search.
And like most of the time we spent like developing this algorithm was not even like nearest neighbor search. That was closer to this symbolic filtering, but with like arbitrary filters.
And only at some point of time, like we had a realization that oh, like that is similar to what people actually need. Like there are a lot of papers of on nearest neighbor search. So we switch direction and like the most cited publications are on nearest neighbors. Yeah. Yeah.
I don't remember was it on your paper or somebody else's paper. I saw a paper of my old friend, you reliefs because he actually defended his thesis like in PG thesis in this space. So when he was doing it, I think it was 2009.
I was like, I was considering this like a pure mathematical problem without like maybe direct application. But then he gave a talk at Google, like you know, Google tech talks. I don't know if they still exist or not, but like he presented this problem and like they did some optimizations as well.
And then I think I think your paper sites it or maybe someone else's I don't remember. I was like really surprised to see, you know, his work also kind of in the same line of things that now lead to vector search essentially. Well, yeah, I think I saw his work, but it seemed like more theory.
Like if you look to history like of like graph approaches so like. Now it's mostly like rehashing of old stuff. So definitely new things, but like there is so much work done before like Sergey Brein worked on nearest neighbor search with like GNIT. So that is also like good work.
There were there were previous work on graph search, I think in 1993, which like aren't that much different compared to like current though, like they have also problems with scalability at that point. So I think yeah, so that was. There is a large number of like previous work in that area.
But you said like you didn't concern yourself with reading too many papers before you started inventing this new algorithm. Is that right? Yeah, sure, sure, we read papers, but they were not really relevant. So we read papers on network science.
And so we tried to so there was a problem with building like this, no navigable small roles. So like not every small network is navigable. Like most models are not. So we wanted to build navigable small and there were also didn't understand like.
Like what what was the criteria like what is like how we could make it and we reinvented like this. Dillon or graphs inside the company and after like you reinvented like you know starting to search and see there are lots of papers who did the same. Right. Yeah. So yeah. So we went the other way.
Yeah. So now that you mentioned this thing like can you actually please introduce this concepts at least on high level to our audience like what is a small world what is like what white it's to be navigable kind of a little bit like more to the user facing level if it's possible.
Well, like navigable small world so you have a large network. And so navigable small world that means you can find paths between like arbitrary elements in this network using which is a logarithmic scale. So the number of hopes can be done with the rhythmic and you can use only local information.
And do like something like greedy search like greedy searches allow and if you can find like the path and the algorithmic steps to your network is navigable. And that small world part like why is it small small.
And that's like history how he's historical reasons so there was like a famous like milligram experiment where they they send letters from one person like from random person to some target person.
That was kind of greedy like greedy search for connections very similar to this and that that's called like small world experiment so like a small world. And real networks like people have like real networks have low diameter like human human connection networks.
And they are navigable like at least according to milligram experiments and like subsequent experiments. Is it kind of related in common terms like to six handshakes that you need to connect every random person with another random person on the planet.
Yes, yes, so that's that's like that experiment is pretty sure I think it's done in the 60s so yeah so. And so the navigable part is basically like if we put this in the context of search right so.
So let's say I have local information I'm here I would like to travel from here let's say I'm in Helsinki I would like to travel to New York like how do I travel right I need to go to the airport.
From the airport I will travel maybe to some city in Europe from there I will change you know the airplane and then fly over to New York.
I'm making it a little bit more complicated there is a direct flight to New York from Helsinki but okay maybe that wasn't right is that analogous to navigable part.
Yes, yes, so like generally like that you can pinpoint that but if you start and finish in like small local airports which usually don't have connections, my magic connection so they connected to hops.
Yeah, and that is one of the model of navigable small roles so there are like Kleinberg's model which doesn't have hops so you can also build navigable small walls without hops.
 But they have polylogarifmix coedizian so if you want to have polylogarifmix coedizian so maybe I'll ask you to provide some references later so especially for those who want to dig deeper into the smithematics like you mentioned these different algorithms like many of them are new to me at least so I'm sure to our part of our audience.
 Part of our audience as well and I wanted to also ask you like on the context of your invention like what was the input so you said like you had a lot of data right from computer vision but like was there something else like dimensionality or some other constraint that was kind of tough for previous algorithms like a LSH or you know any other.
Well, there LSH didn't even work so we worked with like three structures we have to like how will you do LSH.
 Yeah, for and for LSH so I thought that those are not practical algorithms so even when I spoke with people who like were writing a lot of papers on LSH they like expressed doubts and whether those algorithms are practical so they are not learnable so they cannot take advantage of the data that you have so like that.
And like what what they told is like they see as quantization as just a better version of practical version of LSH.
Yeah right and so actually I'm really interested like how did you set up to invent the algorithm like I can just give you briefly like in the recent billion scale vector search challenge.
We had like a small team and one of our team members actually implemented like a small change in product quantization layer like basically how you shuffle the dimensions in the vector and he achieved like 12% recall increase over the baseline you know the Facebook sell algorithm.
I didn't like have that much knowledge I've read your paper I've read other papers and so I was just thinking okay if I if I would start from first principles how would I solve it like I know nothing about this problem right so like how can I solve you know the search in multi-dimensional space.
And so I actually implemented a very very simple algorithm using your algorithm as one of the components maybe we can talk later about it but like how did you start inventing H&S W.
Well H&S W had a pretty assessor so it has like an NSW it's also called MSW or SW graph in different places like depending on where you look so and there I just so it had problems.
So it had several problems but like for like if you don't think about distributed setup the main problem it had poly algorithmic scalability with a number of elements and that killed the performance on low dimensional data.
 So there were like comparison works like one by Leonid Bytes of where he evaluated different algorithms and like its performance really like it didn't perform that well on some data set and the loss was by many orders of magnitude so it could be like one like 1000 times slower than the best solution and yeah.
So the work on H&S W were targeted at just improving the previous version so it wouldn't have this problem and like ideally would perform the best on all setups. So yeah and that that that that has been solved.
Right but like you still needed to add that magical age in front of it so you made it hierarchical like what what pushed you in the direction of making it hierarchical and what what did you think that it might work or was it like as a result of experimentation that it proved to work.
Well yeah that that that's that's yeah that has many ingredients in it so for for one thing when I worked with the startup mirror labs so we had a different problem with distributed index that NSW had a pleasant quality that the hubs that are created in the network are the first elements.
So and the for distributed system you would want to add new nodes to the system and you will have much capacity like increase the capacity of the system but because all your hubs on in the first notes like in the older notes because they have been created before new nodes even existed.
 The traffic is routed through the same old notes which make it not scalable and we spent quite a lot of time on figuring out how to solve it and there at some point I've noticed that like our NSW approach is pretty similar to skip list in terms of what what has been what is being protest as final network.
The idea is like if you if you create a skip list for one D and create the NSW for one D and then like for skip list you just merge the all all links regardless of player you will get a similar network in terms of like degree distribution like distance distribution well all major properties.
 So but skip list doesn't have this property so you can add new nodes and they can have like they can have higher levels and like your traffic will be a road through notes in your form like across your distributed system so and that thing we knew like from the startup that there is a like equivalence but that was only for the problem of distributed search.
So it would still use the same polylogar if Michael like 3d search algorithm like which doesn't think about like what is that how many links you have on a note so that was shelved for that reasons in the startup but then so after ID PhD so like I wanted to publish a good paper on network science.
 And there like it was and I like there is there is a result that we can create a new navigable networks which a method which was not known before so I tried to publish it in nature so it was rejected like nature physics also rejected that it was rejected by editors then in scientific reports was rejected after a review and then like it was finally published in plus one.
And I think like I really like this paper so that was like the most surprising result I think I got but yet it's not really decided.
And as a byproduct of this I did a comparison to other navigable small world methods and so like maybe I have maybe like this approach with like the old vision that you can apply like you can look at the real world networks and replicate it and like computer system and they will be.
So I replicate that the work done like scale free navigable navigable small worlds which are very popular thing till the moment all.
And so that the performance was really was like very bad like extremely bad and the reason for that that if you have a scale free network and scale free means you have a power low distribution of degree and usually they like there is a.
 coefficient gamma and like the best cases was gamma is close to two but gamma close to two means that the scalability with the size of the network so the degree scale is almost linearly so when you have a like a greedy search for the hub so when it goes through the hub like it to play it's like a huge portion of the network so you have like linear scalability instead of like ultra logarithmic so log log again which is.
 They like the number of hope is log log in but at some point you evaluate to like almost every point in your network and you have like really bad performance and that like that after that realized what was the problem with NSW and like I thought all like we already have a solution for that so because keep least doesn't have this problem and so yeah after that I implemented the prototype and it worked.
Working on the like C++ version and the evaluation. By the way when when you started implementing your prototype was it initially in C++ now it wasn't in in in in Java and Java because Java is your favorite language or what was it Java.
Because the distributed system like that was implemented in Java so that was close so like it was easier to integrate like if you like maybe you were thinking it's easier to integrate in Java right.
Well I just know how to code it in Java so I code that several times for NSW and that all Java code was released.
So yeah just code it and then like I had to transfer it to C++ to make it efficient and like yeah and so there is like Leonid bites off so who who is a maintainer of an MS leap so I have been in contact with him for quite a while and yeah so it was implemented in the library.
Did you like collaborate with him to to to implement it using the enemy sleep sort of the most efficient way or.
Well first of all like the ideology of the library is very close to like what we have been developing so it's not only focused on like typical distances like L2, Cassian or even like inner product.
So yeah it makes sense to compare on all those distances and Leonid also had a paper like in a bench like on all of those so we can just implement a new algorithm and run a bench.
Right and come so that that was like a really good point and it also wasn't implemented in and benchmark so if you add an algorithm so we can like.
Go through all sets of benchmarks yeah yeah yeah so like it was kind of easy for you to evaluate where you algorithm stands against other algorithms right so like yes right right and so what was the. And you also had a call to write maybe maybe you could introduce him as well like on your paper.
Oh yeah that that is midrida shunyan so. So that's that that that was my peer in the like physics lab he also got PhD the same year I did so yeah so. Yeah so we decided to team up with that so he helped a lot on he he did he did the all evaluation so he integrated it with other code.
And here on the evaluation on the like clusters that we had. Yeah at that point nice so so back to the invention like as you've been inventing this elbow did you have to make a lot of adjustments to the core of the algorithm as you have been evaluating it or was it like.
You know the first shot and it was it. Well not really so there are like two changes compared to NSW in the national SW so first one is the idea of layers so that's all most of the problems with like low dimensional data and.
Yeah also improve performance like in most of the tax that like most of the distributions even like but maybe not much like high dimensional data but still when I ran the whole like suit that was there was a few data set on we should perform worse compared to.
VP3 so that's from Leonits you then I thought that wasn't a big deal but like communicated the results with Leonit trying to convince him that like we don't need to have that much algorithms.
But he was not convinced so then we added like an improvement with the heuristic for selecting the neighbors which like I personally knew from the work on spatial approximation three.
That made that that made the transition to skip list exact so it made an exact so you can build the exact skip list in one day using this heuristic and after that so yeah that that that that addition improved the performance yeah.
I remember please correctly if i'm wrong but like I've read your paper actually really really closely so I printed it and you know like I was reading with the pencil actually making notes so remember like at some point was it so that you agree to them it also needs to prove that it will converge.
 Or like because you keep resuffling the points in some way right like as you build it you use multiple threads like in order to kind of build the actual paths between the nodes between layers right so like do you need to kind of still somehow make sure that it will converge on all dimensionality so all spaces or was it was it not necessary.
 Well so the algorithm is pretty stable so the result is like how many threads you can go that is an empirical result so I was surprised when I saw it but you know even like for NSW the first algorithm even if you start to do like to use I know 40 threads from a single element I can found no no no drop in the recall.
No drop in the recall or speed that was a bit surprising. In terms of stability. So the main way to make it stable is just like to avoid avoid exploring so like use use proper parameters big enough there are ways to make it stable in.
For corruption so when when but that that that that is pretty costly so if you bootstrap the graph so if you like do iterations like similar to an undecent I think you probably know that I can make it stable even if it's corrupted by a lot.
So that is done only for updates so like when you update your kind of corrupting the graph and well in the like a channels WLIP.
So for updates it wasn't specifically made to be like very stable but for just construction it doesn't have to be like that's stable doesn't have to conversion all situation just keep the parameters high enough and it wouldn't diverge.
 Right yeah because I remember like and I'm also curious to hear your opinion so then I after your paper I started reading other papers for example the Microsoft's zoom algorithm and then later they called it discount and with some modifications so they were comparing to etch and SW at larger scale something like billions of nodes billions of points in the space right.
 And so they they were trying to minimize the cost that that that it will incur because basically as you build the H and SW you also use memory quite a bit right so I wanted to hear your opinion on that part and then they what they did is that they I don't know if you're familiar with these papers but what they did is that they offloaded portion of the retrieval to to an SSD disk.
And so they kind of combined your algorithm with like additional layers and then they kind of resolve to full precision when they go to SSD disk but they don't don't do it in memory. So they do use quantization right yeah they use quantization exactly.
 That's a very popular approach and that makes sense so it's so basically you have a hardware limitation so that you can can store but you have a hardware here are here so you have like not so big RAM and like lots of SSDs and maybe like if you have distributed system you have access to other nodes.
So yeah that's a clever use of here are here that makes sense. But at the same time like your algorithm was taken into using to popular frameworks like files right so like files is not a single algorithm like one of them as H and SW and then.
Actually don't know how they did it did they take your C++ dependency directly or did they implemented do know.
They very implemented from scratch so like I talked to them once so they said they tried different way but like in the end it was like pretty close to the like the initial C++ library don't have some different there is some if something's are implemented differently in fights.
So for instance there is a thread pool like in channels WL for keeping track of visited elements so when you have a new search if there is like a map like think of a bitmap for which knows which notes in the network are visited.
And the channels WL it's kept in memory all the time and when you have like a new search it will just like peaks from the pool and then face like it creates it once per search so there are much searches more more effective.
 Yeah yeah yeah by search yeah batch search is another feature that sometimes is implemented in vector databases but did you like expect that your algorithm would become so widely applicable like do you know that it has been re implemented in several languages like for example as part of vector database called V ev8 it was implemented in goal.
And there is a database called quadrant it it's implemented in rast and of course all of these implementations also add like crowd support so they you can actually update right because in reality in database you need these features.
And then they also added symbolic filtering on top of that so it's also inside your algorithm like did you did you expect such popularity. No no like I thought that we will publish the algorithm and like we will win the benchmarks and we're clearly seeing.
But though at that time like just before we published the benchmark there was a like competitor Falcon which also published the benchmarks of widget better but like for Falcon targeted like not like that much and I thought that well Falcon was only like for few specific metrics.
And yeah actually it also was done by like person from the same school which I went so it was in jarrison stain so I talked with him a bit and I thought that like we have open source to code so we published the paper so like people will quickly just like iterate on top of that and like improve.
 But yeah so it took much more time to others to improve upon it compared to what I've expected and maybe that was due to lack of interest maybe that was to some inertia so I don't know like looking at the how many startups and solutions are popping out right now it seems like that like the most of the interest came much longer.
Like much later yeah to like to the point when it was released so it was hard to predict it back then.
Yeah do you think that an MS leap has to do something with this success that it kind of maybe an MS leap was somewhat visible and then when you edit your algorithm there and show that it performs you know those people who followed this library at least knew. Okay there is a new algorithm.
I think yeah well that helps so when the MS leap is a good library so it has some audience I think the most attention came from and benchmarks. So because yeah well an eye is like what was I had a lot of attention by that point and that benchmark was done by the same person.
Who did an eye so yeah I think that draw some like traffic to the libraries and yeah also I think the idea of algorithm was like understandable and so. So that also like affects the usage so if you understand something you are more likely to use it.
Yeah yeah it's Eric Bernerson right the Swedish guy as he says the sweet who is stuck in New York City yeah I think he implemented a no way originally there is also like a presentation by him where he explains not only the annoy algorithm but also.
 So how intuition doesn't work in multi-dimensional spaces anymore like we think that like in three in 3D world where we leave now right like the further the point away from you like you can actually see it somehow perceive it but like in multi-dimensional space it's not like that I don't know what's your view on that by the way.
So like does geometry perception changes in high dimensional space.
Well yes yes so there are like many interpretations of this so people who work with nearest neighbor search they know about it so like if you have like if you have like many dimensions even small perturbations there they can go like far.
So you all have like so to find nearest neighbor you need to have like a huge cover sphere yeah like when you divide divide the space so yeah that makes the problem complicated and that that one of the reasons why all the practical methods are approximate.
 Right yeah yeah yeah so like you do need some approximation in order to find the points and so yeah I mean it sounds like so when you when you mentioned and then benchmarks was it you who submitted the algorithm for the benchmarks or was it Eric who picked it up and he made it kind of available in the end and benchmarks.
No no I did a full request to edit. All right so it basically yeah yeah so you pushed it forward yourself right so it wasn't like you just implemented and then you waited for it to be discovered so to say.
No yeah definitely so like the one of the like decisions to use in the most sleep was that the most sleep was already integrated in an benchmark so adding that will be just like adding some code in an benchmark that like pulls the algorithm and.
And then the tuning of the parameters so that was but that was simple to do right yeah and so as you did that like what were the results like of that of course an end benchmarks it has a number of parameters right for example like even indexing speed.
Not only like recall versus QPS trade trade off like was there some specific kind of metrics that were hnsw excelled over other algorithms.
Well at that point of time there was like no logging of the construction time and memory consumption and the like the initial version in the most sleep it had like clear focus on the performance like recall to speed ratio.
Yeah and well you know it's hard to do proper benchmarking so like there are a number of scenarios somewhere you have a limit on memory somewhere you have a limit on the construction time sometimes like you don't care about them at all you just care about the speed.
 You can also care about like multi thread performance or you can care about like single thread performance like maybe different scenarios so it's pretty hard to go proper benchmarking and the depth like like I did a decision to just focus on the recall and don't think about construction and memory.
Okay I see yeah and so and and basically when you when you did that like was hnsw like at the top of the competition at that point.
Yes yes it was like a top and on many many benchmarks it was like it was a huge cap compared to the next competitor so not so maybe for a globe I think this Falcon there was still there there was a like significant. Difference yeah but many.
Yeah also like at some point after that there was a real release of key graph algorithm so which like decreased the difference but it was still on top of it.
Yeah so did you did you did it make you feel proud at that moment when you saw the big gap and like this is your invention for how did you feel about it.
Well that felt nice for sure so yeah so we published the paper I think like pop when the paper was finally accepted so it's also felt like really well so I think it took. Like two and a half years to publish the paper well.
As they say in the US I think every rejection brings you closer to the goal so it sounds like you've been rejected in multiple like journals that was not that was still published.
Now that was a single journal it's just like yeah one revision took one year so that is that is palm year so transaction of pattern analyzing a machine intelligence okay.
So like we follow the practice and physics and ignore ignore the conferences so and we also need the for the grants we need to have journal publications not not confidence publication so we sent. To Pamy and had few revisions there but each revision took a year.
Wow this is super long why do you think it was like that like why why would reviewers be so scrutinizing like your submission. Well I don't know so like I actually talked with the editor so I was very angry after the first result so and it seems like just a problem is how.
So publications in computer science are organized so that's that's not only that journal there are so many journals which have. This problem and like when I looked at the Twitter like when some discussions were like oh I got like review invitation for like this like the national journal.
 They said I have to write review in 10 days oh I never gonna do that so no like no way I'm writing a review in 10 days and yeah so in physics it took it sometimes took a few weeks to get the review and journal in journal so you send it and thank you for the months you can already start like writing to review like what what what takes so long yeah and yeah in computer science.
But journals are very slow conferences are also slow there's several months to get the review and like people saw that we are using archive yeah so if there were no archive I think they have already they will just.
 They will create new journals yeah exactly like they should be any monopolies right in that sense like maybe go and create your own journal but then the question is when the problem is when you a PhD student let's say you have a chicken act problem right so you haven't proven yourself yet you need a publication to defend your thesis right so that's the trap.
 Well it's also known how how can this all so if like they created like a new conference conferences like I think I didn't remember I see a lot or I see a lot was created not that long ago they could have created the journal as well yeah the same people said like we don't want to do conferences like conferences you have a very tight deadline that means like if you miss it you'll wait for another year and that is like not not.
Great let's create a journal and now you have a continuous like a spectrum of time when you want to send your paper no deadlines there are no deadlines for reviewers yeah you could almost review yourself.
Yeah I mean like during the review period on the conference you can get like 10 papers at the same time so you have to review them like in a batch but if you are working with journals you get a review like from time to time yeah like your your load is distributed.
 Yeah so by the way what is your take like I think new IPS conference they decided this year they decided to hold all reviews publicly so essentially anybody can see you know the comments from reviewers and there is like a discussion between reviewers and authors and everything is public do you think it improves the process somehow or not what's your take on this.
 Well I think that makes sense so that opens well that sets the bar for reviewers higher because if you know that your review will be read by some random people you want to make it better and spend more time on reading the paper it also helps to understand the review process from outside like for if you're a new reviewer you want to understand how to do proper review you can just read reviews by other people.
 And that is helpful and you can also like if you're if you want to publish a paper you can find similar papers and read the reviews for those papers and understand like why they are rejected or accepted so that that helps I don't see like much problem in that that fights against against the corruption and some places in science are corrupted so.
 Yeah it kind of brings transparency at least with the process and also as you mentioned someone can learn how to do these things right so I think it's also useful and maybe it prevents situations when the paper is rejected outright because the reviewer has some bias against this topic or you know I mean at least transparency is good I think yeah.
Are you publishing today by the way do you have any publishable work do you intend to publish. Not much so I'm working mostly on protection like maybe next year I work on something publishable we are last last thing I published that wasn't some song so for on both estimation.
 Yeah but like I've noticed like you are very active on hnsw github like when I when I posted my question and maybe we can discuss that as well if you are kind of curious on that kind of you responded really fast and so it means that you still continue to allocate chunk of your time to to look at you know issues and pull requests on on github.
Yeah so like I wish I would have done it better so I miss some some things from there but yeah I tried to update this library so I think that well when I designed hnsw so there was some design decisions and even if I see like some algorithms outside improve upon that I think they are not.
Aligned with the design. So and I skip them one of that is like hnsw tries to avoid global view of the network so because it's it's a descendant of. Distributed algorithms so like it's like it's not good strategically if you have like a global view well sometimes it helps.
Like there are papers where you can and that we should make that the pass from the entry point of the network to every node is in short so you can make it but that is that breaks if you're doing assertions for instance so like you cannot have a global view and dynamic nature at the same time.
Yeah so that that's why I ignore some of the stuff there's also a focus on like custom distances so even though the hnsw lip supports only free distances is pretty easy to implement what distances like you want and I believe that there will be a shift in like what distances are being used.
 After some time because there are problems with like those like those simple distances you mean like a sign cosign dot product this type of distances right or yeah yeah it's more a problem that you want to embed everything like you want to embed an entity into a single vector representation so and that has limitations like as you probably know that.
 Like transformers are based on attention and there before there was a like a last year with attention for translation and without attention of didn't work well because it like compressed everything to a single vector so I believe that in some time there will be at least set distances so where your object and query represented as a set of like as a set of which can be shuffled and doesn't change the structure.
 So for a user that can be like set of user interests for a document that can be a set of like subjects inside the document for the query it can be like different parts that you want to have in the query at the same time but those parts like might not be ordered and when you embed something you are that you make it ordered and like for instance when I played with clip.
 So there is this for so I thought that like it can do what's your which is nice so you can like have an image and like see like what are the words are which text is closest but definitely struggles with the notion of like what words are there so what is the first word yeah yeah so like geometrically or like in different languages it might be even different geometry of words right like should you read left to right or right.
Right to left and then like you also need another dimension of language they are guess.
Yeah, we can represent it as like bag of words maybe ordered bag of words is something encoding as all people do now for text but that that I have so I think like an end would need to adapt for the situation in the future and keeping the stability to add new distances is like is important.
Yeah, so are you are you working on on this personally or are you like welcoming pool requests as they say you know to implement different distances. Well, I'm welcoming pool requests for sure because those are very application specific well it's pretty easy to implement like I don't know.
 Or some simple distance so you have like a set a set of distance you just select which are the closest out of the set so it would like many many to many somewhat similar I think to what colder does so though they can I think go without it but essentially you all you'll need a set to set distance right yeah make sense I was since I mentioned it twice already I was wondering like to pick your name.
 I was wondering like to pick your brain on what I was thinking in this space like an and trust me it's absolutely very simple algorithm that I came up with the only problem is that I chose Python as the language and so Python has this little bit weird virtual machine kind of how it does the garbage collection and so what I suspect maybe it's also bugging my code but on billion nodes I cannot actually make it conversion reasonable memory so I'm going to say.
And so it runs out of memory like on 995 million and what I did I was really what like I took the input set of points right so the points are like 128 dimensions or 200 dimensions.
Essentially I pick a random point the first one not not random the first one and then I on a sample of points I compute a median distance right so basically what's what's the kind of average distance between between all of them in a player wise fashion.
 And and so then I use that as as a filter to build what I called a sharp right so essentially I decided to split the billion points down to controllable number of charts let's say 1000 charts right and so I pick the first point and then I say okay which other point is close enough so like within that median distance to this point.
 And so I joined them together in the chart and as the chart reaches 1 million so basically if it's like 1000 charts each chart roughly 1 million points that's a billion points right and then I will close that chart and I will run H and as double you on it so that I can actually have that chart as a hierarchical navigable small world graph.
And and it seems to converge like at least on 10 million it converges on 100 million converges it runs out of memory on one billion but I think it's just some weirdness in how I do it in this big loop or overall points.
But when I reached out to you on on GitHub like my idea was to also access the first layer of the graph so that first layer where the query will enter I could use that.
 And as the sort of entry point across this 1000 charts right so because I don't want to load all 1000 into memory I want to load only sufficient amount of entry points so that I can quickly check which chart is closer to my query and then go inside that and use it as W what do you think about this idea it's very simple I think.
Yes well that that that makes sense so that clustering it seems to be so like you did you have like a cluster the points into 1000 clusters and then they select the clusters and. Well yeah that that makes sense I think like historically there were other papers that suggested something similar to.
And then I think in flam so that was one of the distributors strategies that they suggested. Yeah well that that that might work out so that though that depends on on the scale so and so that also well for production system you also want to replicate those notes and so right.
Okay maybe like let's come from a different way so that you can also start to very small pieces so it might not be needed in this case I can want to balance so but on the top layer you can also use like as in this Microsoft paper that you mentioned also there are other papers like from young so.
So I have a paper this those guys so you can use a in you can start into maybe not the short you can. So if you want to divide your data set into like million clusters and use like a higher index to decide on which chart you want to select it right yes.
So though like if you're if you're not talking about like. So it's not a scale so probably like doesn't make too much sense.
 But yeah yeah you can do this yeah I mean I'm still hopeful to kind of keep trying it I have another friend who is like on Twitter he actually recorded like a YouTube video where he said here here and here you make a mistake like this is why you lose memory like you should never allocate objects inside loops you should pre-allocate them as NAMPA erase and so on.
So with his modifications it still runs out of memory so like I need to kind of move forward and I'm still kind of like hopefully I can do it in Python but something also tells me maybe I should move to more kind of memory controllable language something like rast or C++ I don't know.
 Well I'm not sure so like so using something like so you probably using C++ libraries from Python like NAMPA or torch yeah something like that so they should not click memory so those are pretty pretty controllable yeah yeah it is definitely my code somewhere in the loop it probably just computes too many time like like basically the hottest part of the algorithm like in terms of profiling it right is like.
Like when you can so you pre compute the median distance once right and then you use that value all the time so it's kind of it's okay it's just an object so it doesn't allocate much but then as you extract the next batch of points so I read the one billion set in one million batches right.
I sense that there could be a loss of memory because like it's a binary file and so you say in NAMPA you say from this file read the next batch right so like you you provide the kind of offset and so I sense that maybe there it loses memory maybe not I don't know.
Because I've noticed that in files library they use a map to do the same thing I'm not using a map. I can also use a map so NAMPA if you read the tensors from NAMPA they can also have map options so you can load this map in NAMPA.
But even if you're using if you're reading we are like open like open file is like read binary it should not click memory so it should it should you do read them it's just like.
 Yeah so it must be something super stupid then in my code that kind of like really obvious to somebody like you like okay here is the here is the point you should not do this but like for me it's like I invented this basic idea but then like pushing it maybe like like it works on 10 million and I'm okay but like the task was as part of this challenge to do the billion scale right so this is like you crawl you crawl the the mountain.
 Without the top in a way but yes there is a top of course it's only one billion points but yeah I mean it keeps me quite excited to keep doing it of course I already see some maybe need for improvements for example how how do I make updates right so let's say new point comes in and I have like 1000 charts predefined so I need to find either an existing chart or create a new one at some point so that that part I defer to the future but I'm not sure if I can do it.
But like maybe I still need to push push harder to just converge it first. Okay you can profile for memory so we can like loop some operations in the code that you think that can leak and the profile the memory for those.
Yeah I've been doing that like actually I also come from the world of Java so in Java it's like quite straightforward in a way there are also tools in Python when you plug in this memory profiler it slows down your computation significant. So you have to wait like 10 times more to see the.
 Yeah so I'm not a fan of profilers so like recently I found a video like a talk on YouTube which explain why we shouldn't use profilers and that was like the profilers they become obsolete when the code became like not multifreaded but like with multiple pass so when I'm going to release pension so pension was super scholar so your operations are out of order and when you look at the profiler results like I don't understand them so when I was developing Asian as WL by having to use profilers so I just like wrote benches for operations and like I had like baseline and trial so they usually work in the same memory so the like index is the same but there are different implementations of search and like your your speed can depend on memory how you allocate the memory and with those benches you can measure something like up to 1 or 2% of difference.
And when you like do a lot of benches with one or two percent improvement you can get like 20% improvement 50% improvement. Yeah but like I never used profiles and like I never saw like in my life that people use profiles and like get really complicated insights from using profiles.
 Yeah I agree like we did like so we're building also building a search engine with like we had like by design we had like billions of documents and each document was just a short sentence like a statement from a document real document and of course we were running out like we were running into all this garbage collector stop the world problems and so on.
We were running this profilers I think one of them was J rocket and others and like when you see the graphs you're like okay so now I know yes it leaks but what should I do so or it tells you that your code is using like byte arrays too much like what can you do other than that right.
 Yeah and for performance it's even worse so you see that like this model takes a lot of time but like in a multi multi threaded world that like it's not for sure so you can improve it that like and that happened so quite quite quite a few times so people went then to me and said like your analysis of performance contradictor profile.
That's okay right because you didn't optimize for the profiler. Yeah because profiler cannot like so it cannot say to you what would happen if you change something. Exactly it's just a snapshot. Yeah it's just a snapshot.
 And like coming back to H&SW what are you hoping to achieve like maybe in some midterm future for example like widely cited work which where the re implementation as W is when they add symbolic filtering so like what would it take in your original paper in your original algorithm to add symbolic filters how does it change the dynamic of that graph and search.
Well it seems like for me like so I can correlate interest to and then and interest to symbolic filtering so like I think two years ago I haven't heard like people talk about symbolic filtering and and but now like it's a hot topic.
Like from different places people want symbolic filtering that is like for targeting so like for ads yeah you can want to have some targeting for the audience or some other filters and but I see that as outside of the end so.
So as I said when working on a startup so our first application was doing something like symbolic filtering and there it's easier in some sense because like as you said there is a problem of this distances and high dimensional space and this problem there is no such problem in symbolic filtering.
 So symbolic filtering you have a query that have exact result and like if you write the SQL query so it can be optimized to work efficiently and but the I and I does a very different job it does approximate yeah filtering it can kind of mix them together so if you add like so you have a distance and like you add some.
Like prefix for that which somehow captures the symbolic filtering and you can build an index that also like takes takes account and like there are some other people who suggested to do that as well.
So but the problem here like and yeah that can help so during search so if you filter by the symbol and like you can easily add filtering so when it's in a W does filtering for the leads like can be done the same way.
 Yeah you can extract like only elements that pass the filter and there is some guidance on the graph because you create it with it but for me like I don't know so you have like huge number of possible filters so what will be the metric and how would you balance it with the like approximate network that creates a lot of problems I think yeah.
I thought that the best solution would be like to keep this like to some extent but focus more on like how do you can sharp the index according to those like.
 Great theory, don't that they are sharp so you can like to a skill queries like for instance like there are some queries that can work well with this filtering like if you're most of like like I know 20% of the elements pass the symbolic filter so that is fine you can use it but maybe there are some queries for which like I know already like one of a million passes them and those are in different parts.
 Yeah exactly space so for them you can see in real time so you like you search and you see that it doesn't perform well for those and you can just build a separate index for them right because you know those are small those are people want to find them maybe there are enough maybe they're out of a billion but if you have three and a element so there's like a million of them.
So you you you cash them like build a cash index for those on the fly so that is like discrete optimization problem and I think that's a bit outside of the index because index is like.
Yeah so it's focused on the different part yeah yeah and I really I don't think that other algorithms like and an algorithms can like somehow avoid this problem yeah exactly yeah I mean it sounds.
 Yeah what what you say like you find a stunt correctly it like a little bit like a and then contradicts this kind of the nature of symbolic filtering in some sense but still people do it right so for example in VEV8 and in quadrant they did it right so like you and then Mildbus as well but it's funny like in Mildbus they use.
 Fies and then other algorithms right but they say we only support you know integer fields but we don't support for example strings yet so we are working on adding strings which means essentially they're designing like this graph somehow in such a way that okay it doesn't support strings yet maybe because it's not so easy to to to to edit right.
 Well I'm not sure so that also depends how you measure the performance like if you have rare queries like which don't have any result so like you probably algorithm doesn't even work on them but you either are rare to you measure the like overall recall and you don't see like any problems so definitely you can build a solution maybe like some simple with like filtering during search.
But like it's sure it will fail on some points and that is suboptimal in terms of latency. So if you if you talk about existing solution maybe maybe they have like a really good solution which I just don't know I looked at few and that was mostly like filtering inside the graph.
Yeah, like if you if you if you if you have really rare elements which are like distributed across the search space. Evenly like in different parts so it will struggle because you need to just do brute force of the whole.
Yeah exactly I mean to me it sounds like computerial explosion like if I add more and more symbolic filters like essentially I'm introducing like new subspaces in my space right so like I need to like push this points somehow close.
To each other within that specific symbolic filter but if I add more of them now I have like the kind of like multi-dimensional space of filters right.
Yeah and you you have a really high dimensional space of filters but you don't really know like the distribution of queries for those filters should be very different because those are user distribution.
 So that also will make the problem more complicated so it still can work if you if the especially distribution is kind of similar so it will work if you crank up the parameters of the graph use more connections but so there is a mismatch so during query your distribution may be very different and you need to think about it.
So like how you balance those inside so you have like two types of distance and how you balance them you want to balance it so the the query distribution.
Yeah that's that's this field like I think this field of vector search doesn't make you excited that you you contributed to it like how do you feel about this field that is emerging right now.
 So I think it is very important so right now I'm working mostly on applications how to like get advantage of this and so there are many applications which cannot be done without efficient search like there was a paper for deep mind like was quite recently where they used search like inside of the network and well.
That makes a lot of sense and I think yeah there will be more papers and there were papers before that paper but there will be more papers that use an inside inside a big like a huge and all people.
 Yeah, for example, like this learning to hash methods, I don't know if you heard about them so like there are like when I when I tried to kind of put everything into their buckets like how many different types of algorithms exist like I didn't know about learning to hash it seems to be like one of the recent developments.
Are you following up on that as well or. Well learning to hash so like I'm not really following that so learning to hash was before H&S W okay there are algorithms and when I talked with people who did like what specialize on product quantization and review the papers.
They told me that like learning to hash never reaches the performance of like post quantization like at least at what that was at like few years ago. Yeah and yeah maybe like now it's solved. But like when I talk about a and then inside I think about like about graph and.
Yeah yeah and yeah and so one interesting thing also can happen like with graphs so what what is like what is an additional advantage of graph nearest neighbors search engines is that you can change the metric.
So for instance if you are doing multi stage ranking like the you have like and you have multiple multiple candidate sources like for search you have something like like like the m25 also you might have embeddings like with similarity search.
And those are like three that the separate sources and then ranked. But essentially like why do you need an end like for the first like from from the beginning. You need an end to speed up the ranking so essentially you can rank all the documents using your have a ranker.
But you cannot it's too like too expensive to do so you can add an an an is basically for vector search that is you distill everything to vectors and you have the same objective. And you have like a way to specify the interactions.
But you can look about the other way so you know you have a graph and the graph are just the candidates and you have like a low simple metric now you have more complicated metric on this graph and you have like a final ranker that also can be searched on this graph.
So that means you don't supply like a set of candidates to the ranking but rather you supply interpoints and the graphs so you have a graph which is well, which is built trying to capture the similarity for the ranking.
And like when you so instead of filtering like from one stage to the next stage, you can just switch the metric and the graph. You had light metric which is like vectors.
Now you have more complicated metrics so you hydrate the features of the elements and the graph and like traverse and like now you have a really complicated metric. Yeah, like you have it, but you still you just have an interpoint on the graph so you explore it and you can.
Well, you can fix some mistakes done by the previous layers. Yeah, so it's not exact filtering so that's yeah, that's another like maybe unique feature of the graph methods.
So it sounds quite exciting like have you have you thought about publishing this idea or like I mean it sounds quite quite unique. Well, it doesn't make sense to publish an idea without implementation.
Yeah, for sure, but maybe you can influence those who who would like to experiment on it, at least those who will watch this podcast, I think they will listen they will they will probably pick it up. And use graph algorithms for sure.
Yeah, I mean, it sounds like all of the NN algorithms they have like advantages and disadvantages, right? So it's not like the all of them are uniquely outperforming, you know, the others.
So there is like a division like if you think about like quantization algorithms, so they are kind of orthogonal to graph algorithms, so they they quantize so they can speed up a compressed like I'm compressed to save the memory and speed up the computation.
But like older algorithm they just use something like IVF so and that is like one layer filtering and you can use graphs instead of IVF right so we can use graphs and add like quantization and Faiz did that before. Yeah, I think some others also did that.
Yeah, and the thing and then like vector databases actually offer it as one method like like milvis, for example, like they offer IVF and then you can choose like if you want to do exact KNN or if you want to do a and then so you can actually configure it in different ways.
 Yeah, I mean, just sounds like you're without maybe realizing much like you are at the core of what's happening in vector search in some sense, of course there have been other multiple contributions, right, but like for some reason exactly your algorithm has been picked by many vector databases, there are like seven of them, so actually wrote a block about six of them and then seventh kind of knocked on my door and said can you also add at us.
And so when I when I was going through different databases like in Java implemented in Java or in Python or you know in Rust and go all of them picked your algorithm for some reason.
So like maybe it was easier like it's a combination of how easy it is to implement how transparent it is like to understand right and then basically it's stability so it's like a combination of things.
 Yeah, probably like I'm not totally sure so yeah the initial library also was implemented as a header on the not the initials or that was a second library so there was a problem with HNW implementation and NMS lip so it so like the NMS lip format was a bit restrictive like for efficient operation so it converted to flat memory format.
And so that that makes made construction slower memory conceptions bigger so was implemented as a header only library so header only library was inspired by N9 so like by the success also and I think that also might have contributed because it's very easy to like integrate it.
So there are a few files it compiles in some seconds. Yeah, maybe maybe also that help the library surface simple and easy to integrate. Yeah, yeah.
 And I mean it must feel kind of cool to have this impact but but I also also hope like you you will continue kind of maybe doing some publishable publishable work in some fashion and doesn't need to be a journal which is rejected five times but something else is this something that you are planning to do or well, that depends so like I cannot talk too much about my work in Twitter so maybe maybe we will publish something so that depends on how it goes I mean I'm near even nearest neighbors.
Yeah, not not only but yeah. But I it's hard to predict now if it works well. Yeah, at least the idea that you mentioned like I mean if it's outside Twitter for example in HNAS W library like the idea of this multi stage ranking sounds quite exciting.
Well, I think it can be implemented only by the teams who own the rankers and all the whole pipeline. I think it can be implemented as like as you need to hide you need to hydrate the features and like on the fly and have feature hydration is very specific to application.
Yeah, but not only inside the production environment. Yeah, that makes sense. Yeah, so maybe it will call for creation of data sets and kind of this benchmarks if the industry chooses to move in that direction.
 So like there are some obvious problems with data privacy with that so it's hard to publish something well, you can think of a toy problem so like you have like you don't do actual like work with users but maybe we do image to image search and you have like a huge transformer model on top of that or maybe like something like Marco and the smart car maybe it can be done with that.
Like experimented maybe so. Yeah, yeah, I think we went really deep today, Yuri, I think it was really really cool, cool talking to you.
I always like to still ask kind of this question orthogonal question of why like it's a little bit more philosophical but like if you're not a verse of philosophy like why would you say like this field attracted you like. Like in your own words.
Like I didn't have much choice, it just was like I got my first job offer and that was in this field.
But that's that's about scale like people like scaling and like many games when you play on like Android or other stuff they're based on scaling so you do like a little action and there are huge consequences of those actions like destroying something or like that is scaling.
And this is just like a pure scale of how how how do you scale machine learning applications.
 Yeah, so on one hand it kind of was predefined as you said you found the job on the other hand you still were curious to implement that algorithm so like it wasn't like somebody said okay you have to do it right you could also choose a job of like okay I'm just coding nine to five and then I go home.
But like you still decided to implement an algorithm. Yes well that that was a fun job so yeah so like you were not scared by the challenge itself right maybe was it was it like motivating actually.
There was no like that much push like from the like from the company itself so we could we could do whatever we want inside the company so it was very relaxed.
Yeah that might be actually a really good background to invent things don't you think like if you if you come to work and somebody says no you cannot do what you want you should do this and it might be kind of too restrictive.
But here like they've been both challenges and also that freedom to solve those challenges.
Yeah there are like two components first of all you need to have like freedom and do long long term stuff so like without worrying of like what are you going to ship into production soon the second is concentration of talent. So you have like high concentration of talent so people can share ideas.
Yeah if you have this mix so like there will there will be innovations for sure. Yeah it sounds like you had a combination of all three components that you mentioned right so like talents and also yeah.
Yeah yeah I also saw that another company is like yeah like in some song there was already a strong team and there were like lots of innovation so there are a few startups. Which came from our lab and there was a really good paper. Yeah so that's a that's a recipe for innovation for sure.
Yeah yeah I'm really happy that it turned out so well to you for you and your author as well. I think he continues to work also in the industry at least last time I checked. And so I really hope that you will get some really cool pull requests on hnsw that will pass your criteria.
Well yeah most of them pass is just like I would love to have more time and I'll try to allocate more time. Yeah looking and checking them. Yeah it's really really great I really enjoyed talking to you Yuri thanks so much for allocating your time also in this Christmas time.
But yeah I mean all the best to you in the future also Twitter and hope to see some published work at some point but I don't know it just I enjoyed reading your paper and and. Kind of also then read read your code and it's kind of like it feels like you've put a lot of effort there and and.
It it also influences the industry so much today so maybe you are not kind of realizing this every single day but like yeah you should know this that there are so many databases that use your algorithm as one of the base lines in production. It's really cool work.
Yeah that that that that that was great that there was success yeah maybe one thing like I would note that the idea is the rain cares so that was partially implemented and there needs work. So here the work on ER maybe you know like.
By using the and then as the for the final rain care so yeah it's just like so I felt that I need to cite this sure I need work for sure I learned this idea like maybe not was changing but from from him.
 Yeah yeah sounds great I mean I've also interacted a bit with him and and it sounds like he's very knowledgeable guy and he has very strong opinions as well so maybe we will also talk with him on one of the episodes but yeah I'm glad that you guys collaborated and yeah it's a fantastic result for for the industry as well and and probably for your profiles well not probably but definitely for your profiles.
So yeah thank you so much for your time and yeah I hope you will have a relaxing time over the Christmas and happy new year as well so thank you very much for your time Yuri. Thank you. Yeah bye bye. Music