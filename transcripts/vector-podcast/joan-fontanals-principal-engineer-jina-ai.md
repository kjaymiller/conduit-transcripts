---
description: '<p>Topics:</p><p>00:00 Intro</p><p>00:42 Joan''s background</p><p>01:46
  What attracted Joan''s attention in Jina as a company and product?</p><p>04:39 Main
  area of focus for Joan in the product</p><p>05:46 How Open Source model works for
  Jina?</p><p>08:38 Deeper dive into Jina.AI as a product and technology stack</p><p>11:57
  Does Jina fit the use cases of smaller / mid-size players with smaller amount of
  data?</p><p>13:45 KNN/ANN algorithms available in Jina</p><p>16:05 BigANN competition
  and BuddyPQ, increasing 12% in recall over FAISS</p><p>17:07 Does Jina support customers
  in model training? Finetuner</p><p>20:46 How does Jina framework compare to Vector
  Databases?</p><p>26:46 Jina''s investment in user-friendly APIs</p><p>31:04 Applications
  of Jina beyond search engines, like question answering systems</p><p>33:20 How to
  bring bits of neural search into traditional keyword retrieval? Connection to model
  interpretability</p><p>41:14 Does Jina allow going multimodal, including images
  / audio etc?</p><p>46:03 The magical question of Why</p><p>55:20 Product announcement
  from Joan</p><p>Order your Jina swag <a href="https://docs.google.com/forms/d/e/1FAIpQLSedYVfq"
  rel="noopener noreferrer nofollow">https://docs.google.com/forms/d/e/1FAIpQLSedYVfq</a><em>iwvdzWPX-blCpVu-tQoiFiUJQz2QnI</em>HU1ggy1oyg/
  Use this promo code: vectorPodcastxJinaAI</p><p>Show notes:</p><p>- Jina.AI: <a
  href="https://jina.ai/" rel="noopener noreferrer nofollow">https://jina.ai/</a></p><p>-
  HNSW + PostgreSQL Indexer: [GitHub - jina-ai/executor-hnsw-postgres: A production-ready,
  scalable Indexer for the Jina neural search framework, based on HNSW and PSQL](<a
  href="https://github.com/jina-ai/executor-h...)" rel="noopener noreferrer nofollow">https://github.com/jina-ai/executor-h...)</a></p><p>-
  pqlite: [GitHub - jina-ai/pqlite: A fast embedded library for Approximate Nearest
  Neighbor Search integrated with the Jina ecosystem](<a href="https://github.com/jina-ai/pqlite)"
  rel="noopener noreferrer nofollow">https://github.com/jina-ai/pqlite)</a></p><p>-
  BuddyPQ: [Billion-Scale Vector Search: Team Sisu and BuddyPQ | by Dmitry Kan | Big-ANN-Benchmarks
  | Nov, 2021 | Medium](<a href="https://medium.com/big-ann-benchmarks...)" rel="noopener
  noreferrer nofollow">https://medium.com/big-ann-benchmarks...)</a></p><p>- PaddlePaddle:
  [GitHub - PaddlePaddle/Paddle: PArallel Distributed Deep LEarning: Machine Learning
  Framework from Industrial Practice （『飞桨』核心框架，深度学习&amp;机器学习高性能单机、分布式训练和跨平台部署）](<a
  href="https://github.com/PaddlePaddle/Paddle)" rel="noopener noreferrer nofollow">https://github.com/PaddlePaddle/Paddle)</a></p><p>-
  Jina Finetuner: [Finetuner 0.3.1 documentation](<a href="https://finetuner.jina.ai/)"
  rel="noopener noreferrer nofollow">https://finetuner.jina.ai/)</a></p><p>- [Not
  All Vector Databases Are Made Equal | by Dmitry Kan | Towards Data Science](<a href="https://towardsdatascience.com/milvus...)"
  rel="noopener noreferrer nofollow">https://towardsdatascience.com/milvus...)</a></p><p>-
  Fluent interface (method chaining): [Fluent interfaces in Python | Florian Einfalt
  – Developer](<a href="https://florianeinfalt.de/posts/fluen...)" rel="noopener noreferrer
  nofollow">https://florianeinfalt.de/posts/fluen...)</a></p><p>- Sujit Pal’s blog:
  [Salmon Run](<a href="http://sujitpal.blogspot.com/)" rel="noopener noreferrer nofollow">http://sujitpal.blogspot.com/)</a></p><p>-
  ByT5: Towards a token-free future with pre-trained byte-to-byte models <a href="https://arxiv.org/abs/2105.13626"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/2105.13626</a></p><p>Special
  thanks to Saurabh Rai for the Podcast Thumbnail: <a href="https://twitter.com/srbhr_"
  rel="noopener noreferrer nofollow">https://twitter.com/srbhr_</a> <a href="https://www.linkedin.com/in/srbh077/"
  rel="noopener noreferrer nofollow">https://www.linkedin.com/in/srbh077/</a></p>'
image_url: https://media.rss.com/vector-podcast/20220119_090157_f67877f44bb32ae14fd380d9328691ec.jpg
pub_date: Wed, 19 Jan 2022 21:02:57 GMT
title: Joan Fontanals - Principal Engineer - Jina AI
url: https://rss.com/podcasts/vector-podcast/366298
---

Hey everyone, Bector Podcast is here and today we are continuing our quest to study more about Bector Technologies and Beding Technologies platforms and today I have a guest from Gina AI, his name is John Fontanelles and he is a principal engineer at Gina AI. Hey John. Hello, nice to meet you.
Yeah, nice to meet you as well. Thanks for joining me today and I'm really really excited to talk about what is Gina AI? I know something I used to use some kind of predecessors of Gina AI in some sense, but not like Gina AI itself.
But first of all I would like you to introduce yourself, your background to our listeners and to me.
 So, well I studied engineering degree in Barcelona, not computer science, from general engineering with my AmiExelectical Engineering, Mechanical Engineering, but I got into software engineering because I was related with robotics and then when I started my professional career I did software engineering at different companies and industries and then I got more into that engineering machine learning and these kinds of fields and then I also did some work on traditional search, on web search, engine so on and then just live brought me to Gina which was a good step in my career.
Oh yeah, cool.
So, what caught your eye in Gina AI as a company and maybe as a technology as a product or maybe the team?
 So, for me what caught me the eye, it was like the technology and the vision of I see that vector search embedding a semantic search in general can revolutionize how we understand search and can bring it to the next level also and adapt to different kind of data or so on and go beyond the typical search bar that we are so much used to.
Yeah, yeah and I mean but Gina is like more than just embedding or kind of it's more like a ecosystem right like it has like marketplace it has many different building blocks and components.
 This is what I think most of the people that might be hearing us might be wondering much because it's a question that we receive a lot so we are not such another vector database as the ones that have been created in the podcast so we are treating the problem of semantic search and we are seeing this as a then-to-end problem and we are trying to build an ecosystem to help the business and developers to develop their own neural search based engines and for that we are trying to build a ecosystem from the core to our document types where we are also recently and launched this fine tuner project to help you with fine tuning your models for your search applications so we are building a whole family of products and projects in this around this area of neural search.
Yeah it sounds quite ambitious and it sounds like all of these building blocks are really needed for anybody who wants to venture into embedding world of semantic search or you know kind of bringing the power of this deep learning models.
 So it goes beyond only only embedding your data and searching through it you may want to cut it into different pieces, you may want to re-run it at the end, you may want to join different modalities together so we are trying to give and make it easy for the user to develop these applications so that they speak the same language and we hope that they will all speak gene language.
Oh yeah, oh yeah, for sure.
And GNI is open source, right?
 Yes, so can you speak a bit more like towards the business model or kind of how GNI kind of makes money in a way like so basically it's open source, anybody can go and download it and basically leverage in their work or is there something that like you have some products for which customers can pay and kind of right now we are right now we are completely open source everything that you can see in our report and stuff each open for everyone.
Yeah so so okay and you are like mostly working on back-and-side of things so you're not interacting with direct customers right? Is that okay? I'm working mostly on the main products and what do you hear about use cases like how do they translate to your level of kind of day-to-day job?
 So most of our solution engineers that say that are closer to clients and users they bring guys their pain points on how they are trying to solve users needs and some of the main use cases that we are trying to solve come from textile search, e-match search, multimodal search that is something that we are trying to excel at that is going beyond only just using search and text or images to search maybe trying to have a combination of walls to power search to the next level.
So they might like bring some kind of use case that you need to figure out on tech level right?
Yes kind of translates to you but on the other hand like you said it's open source so it means like there is like a bunch of like GitHub issues coming in right and if you have like Slack or I don't know if you're using Slack anyway.
Yeah, so like probably every day like somebody you wake up and there are questions there right? So it's also clients in a way right?
 Yes for me my users are our clients and we have to listen to them so that's the big point of open source in my opinion is this direct feedback from the users we can you can correct your direction and you can measure if your APIs are or your design are too complex for the user to rush or whatever so this direct feedback is really useful.
And to this point it's manageable.
Yeah yeah but it's also like I guess I also alluded to this and one of the podcasts was with Bob One Lloyd from from semi like it's also sometimes maybe to give up with all the questions right? Like if you get all these questions when do you find the answer to kind of really deeply answer?
Yeah fine time for answer them yeah.
So we are trying to grow our team into knowing that the community is something that makes us special and it's important for us to take care of our community so we are all trying to keep an eye on the community.
Yeah yeah yeah I remember like when I was developing like search code we were using like Apache Solar and I had to like customize some parts of solar and listen and I remember like in order for me to kind of get up to speed I had to go to this mailing list right?
 And so there are like thousands and thousands emails actually Apache Solar was super active you know like in sillies in many ways and and I was like how can I keep up with all these questions but like I do need to somehow keep up and summarize maybe what what is being asked there in order to understand it's useful for me or not because when you ask a question on the mailing list or like today on Slack sometimes you need to be ready to pay back right?
If somebody help you in the community like you sometimes need to also pay back so it's like it's a game.
 When this is seen in the community I think it's really pleasant for all the team when community interacts with each other and none of the no one in the team has to jump in because they so they help each other that's when I think the community really scales and really open source goes to the next level.
Yeah it's kind of regenerating itself and kind of the cultural element of it so and the community drives you forward I mean just driving force of the project from the interaction point and the feature wise as well. Yeah sounds good.
So John tell me more about GNI itself like as a product let's say as a technology stack like what can I do as a user you know using GNI and yeah like is it self-series and so on.
 So the main point of GNI is that we want to be with the user from the minute they are experimenting with their search application so for instance we are written in Python and we have a really nice API in Python to build with your documents that can treat with any type of data, text, images, audio, video and we are trying to build a really easy to use API for this for you to run locally your solutions.
 The first experimental facing to wrap your code for processing loading the files and for processing the images or whatever and embedding them searching to do a process many as neighbors or exact nearest neighbor search then once you have this we make it easy for you to wrap it in some microservices what we call executors so first phase you deal with these document array types that we have come with then you come with them to the next layer is you have it with the executors so you wrap your logic in different microservices and then we put it in what we call a flow that is kind of a pipeline that is really to scale locally or remotely or even with Kubernetes so that you have replication and scalability taken care for.
So we are trying to bring you easily from your day zero of development to the production system.
Yeah yeah sounds good sounds comprehensive and like what if I would like to just use like a hosted version can I use a hosted version from Gina AI or do I need to do an operation? There is no hosted version at this point yeah so it's basically I need it's like a Lego type of thing right?
Yes exactly.
I will have a nice deployment. And we have even this marketplace as you said this with this helicopter cutter so you can share publicly or privately with your colleagues or with the community your meeting blocks that you may think they are useful for you.
Yeah modern deep learning models that you have packed processing, copy-done, re-runking, even back to research research.
Yeah so and how does it align also with like companies or hubs like Hagen phase you know Hagen phase is also very famous on model side right? So like let's show somebody picks a model and wants to bring it to Gina what's the process there?
 So it's quite I would say having phase it's quite inspirational for us in this sense in this marketplace community and place it is quite similar but um Gina is this marketplace is related to our executor so it goes beyond only models so it's any subsystem enabled and block that you can that you can build that is able to be part of this of this Gina pipeline for us and we are trying to make it user-phone for you to localize it and use it in any way in a simple API and we're still working to make it easier every time.
 Yeah of course because actually you know it tends to get a lot of time you know the infrastructure part like how do I bring my model let's say I have a custom model and I want to bring it inside Gina right so it serves as a embedding layer so how do I figure out all this scalability or latency parameters and so on so I think so the first thing is to get it working we are having to we expose these with these executors that have some API and to that read requests with some maybe I inspired with this fast API approach and then you have with this row you have the parameters to replicate to scale and so on you you may run it in GPU whatever yeah yeah so like you can choose your cost kind of like factors right or based on your cost factors you can choose it's CPU actually and then latency and for some models actually CPU is fine so yeah I mean why not yeah it depends also on the user needs so for instance we are also seeing that neural search main not all is not needed to be only for these big giants with this big amount of data and big amount of resources so any company it's more company can benefit from the power of the neural networks to power their search so they may not need so much require so much resources or they may not require so much speed so it's about and so we are giving the power more or less to use yeah and kind of flexibility of the platform so because essentially if they wanted to do it from scratch then they would probably need to figure out similar things like component isolation and scaling and yeah like an algorithm like a quality checks and so on and on the algorithm side you said like you have exact search as well as in exact search can you talk with more and kind of mention maybe some algorithms that you support so yeah so right now natively we support as the main native quite optimized version of the and exact nearest neighbor search but then for instance one of these building blocks can be any support wrapping any client for any other vector database but for instance we just realized our own and approximate nearest neighbor solution we have two of them for instance that we have developed so much so we have one that is based on hsw plus a postgres indexer a postgres database for to require the documents and then we have built our well we just released and in Slack the community can start enjoying it we have and build what we call pcolyte which and works with product quantization but also has support for hsw you said pcolyte or how do you spell that pcolyte pcolyte or pcolyte or it's like product quantization light version yes we are preferring options that's why with preview and and and in what sense is it light compared to product quantization no I have not been involved so much in this project right now so it's a new thing but it is light in sense of that it is quite and and it's quite native to work with our document type so it's not so general as any any object but it is really built to integrate very easily with jinnah how I see like with specific kind of scheme on or document types and and it's also open source and do you do you like obviously you can provide the links or we can also link in the show notes but do you also like have some kind of latency analysis for this algorithm like hasn't been conducted do you know yeah there is some benchmarks that you're gonna find in the readme I cannot have the I don't have numbers in my head right now yeah but I think for portion of our audience it's gonna be interesting to check out because as you know like actually my team just completed like participation in big a n n I don't know if you heard about this competition so it's like brilliant scale approximate near near snail search so we invented like a new algorithm called body pq I will also link in the show notes like the blog post about it so we increased recall by 12% over five model so yeah five algorithm so yeah I think I think it's great that you guys also inventing I don't know if we are testing to this billion scale I think we are more in the million scale yeah actually we also ventured into billion scale but in the process we figured out a solution for million scale so it's not for billion yet we don't know yet if we can generalize to that level but I think we can with some additional research well this is the first version so for sure we will try to improve it yeah yeah awesome awesome this is great and have you also helped customers to like train models no but we don't we didn't help customers well we did from our solution point of view but this is an interesting topic because this is something that of the this is one of the pains that we found quite often with our users like it was easy for them to go to the to that level 70% let's say of accuracy with any deep learning model that all these tech giants have developed right but we believe that this last mile this transfer learning part is important and we are and when we realize we started this project that is we called well between we know it's already released a fine tuner maybe we can share that as well where we try to make it easy for users to to fine tune their models for our magic learning search applications and they are and it is also framework agnostic for fight we support fighters tensor flow and paddle padder so we realized this pain point for the users that once we have everything running at so on the quality was not as expected and this and we are trying to get to help the user in our ecosystem to get to this yeah to this level by using this fine tuner so basically can you can you explain a bit more about fine tuner like basically what what input do I need to provide as a as a user into this so fine tuner it could feel similar to any fighter dataset for instance but we are trying to put our documents as our as the main citizen of our ecosystem so you have to wrap your any that has any of your data into our document types which is really easy so it's something easy to learn and easy to use and then you can fit your models and we have made it easy for you to use the most typical of those functions we are trying to introduce hard negative mining we are trying to make it easy for everyone to solve the common problems when having and when training for and for search applications and we are also trying to make an interactive labeler that helps you interactively with through an easy to use UI and tag similar objects so that you can go together with them yeah yeah so like kind of I mean fine tuning can be a pipeline by itself right in itself so like how do you get this data samples that you want to fine tune on and you might have them with full launch or during test after launch and it's like a you know the cycle and flywheel of success so to say right so do you cover like the full workflow until production including production or is it like pre-production so for now we are using the just embedding model and just to get embeddings that get better semantics out of your your data set of your specific use case but we are in a really thing it's 0.
2 release or something around in there so there's a long way to go yeah for sure but I mean the direction is fantastic because that's exactly what what addresses the the real need uh any user like fine tune like it's all fancy to take like a hugging case model or whatever but like fine tuning it to the level when you're users beloved that's a different story yeah that sounds great but I also wanted to come back to you like you mentioned that ginae i doesn't kind of compare to vector databases but I do get sometimes questions like how do these systems compare to each other and you may may or may not know I blocked about all vector databases I knew to that point and turns out they've been six and then the seventh one knocked on the door so it's also now on the blog but I didn't cover ginae i i didn't cover deep sets haystack because I thought that ginae and haystack they like layers above a vector database is that the right thinking yes I think it makes sense we are we might try to develop our own solutions for the use case that we may feel more worth so that is I mean the one is out there but yeah I think it's right we are trying to I think vector databases cover one of the parts or one of the challenges maybe one of the main challenges of vector research or neural search but we try to see the whole scope and the whole pipeline so in ginae we can use you can wrap some client that will use any of the big vector searches big data bases of how they're have you done any integration with some vector database not ourselves right now but it would be we might do it in the future okay yeah because for now you did mention that you offer ginae and and and algorithms which to me sounds like a core building block of vector databases but then of course in vector database you have many more things right like where do you store objects how you store them what about filters and so on but we are trying to cover from the for instance we are not some some people for some use cases and just exact years neighbor search might work just fine and they don't need to worry about configuring fancy ANN models for their recall speed and requirements so I think there is room for everyone so I think it just you have to offer what is right for the right use case and the right need yeah of course and by by the way what's the core programming language used in gina like in ginae so our core programming language is python because we are more like since we are this pipeline and we are like a glue and ecosystem most of our operations are wrapping models that run in optimized languages or something and that also python helps us iterate really fast which other languages might slow us down yeah that's true and does it also apply to the ANN algorithms that you mentioned like pqlite is it also python?
I don't know if we are for instance I think we are also using some bindings for HLW so you are using probably C++ version of HNSW binding to python right?
 yes that's for sure but I don't know if some of for the HNSW you part yes for some other parts I don't know we are so maybe sites and I some of the code we are trying to optimize wherever we find yeah but but it sounds cool that you know if if we still continue kind of this comparison a little bit between ginae and vector databases right like vector databases if you pick them let's say viv8 is implemented in go what run is implemented in rust so these are compiling languages right so vespa is like Java plus some c I think C++ and but mostly Java so like nobody implements the vector search in pure python because it's it's very it's going to be very taxing on the latency you know no but the expensive operation we are not running so for instance then the nearest neighbor search we are doing we are based on Nampa operations which are optimized at Nampa level and the approximators neighbors I think most of the code most of the heavy lifting is done under c++ level from I'm just covering our our bindings yeah and I'm still curious about pq light like is it the c or is it python but I think we need to check the documentation yeah I'm curious because like I've actually invented a new algorithm in in NNSWRG but I haven't published it widely it's open source but I haven't like done the thorough benchmarking and what I faced is that you know like in python even though I optimized all parts of the algorithm I'm using pre allocation and Nampa it still runs out of memory runs out of memory as in it leaks memory and it doesn't explain like python virtual machine doesn't tell you where like you don't have tools okay there are some tools but they're not useful like no you're showing a little Java so yeah and I've been like a little bit like desperate and I've been thinking okay should I now move into RAST GO territory which might be a little bit more dangerous like even though I do have some experience in c++ but you know like do I want to go there now like python is much more comfortable the I think depends on the later you are working with and it's so I think that by offering python APIs in the field of machine learning will attract and will make everybody much easier to use then if you get the API rights the API is right you might then bind it to whatever of your favorite languages but I think getting the comfortable API for that developer to use and to love using is one of the key first steps so do you invest a lot into building these APIs can you give an example of like some API within jena that kind of makes the workflow easier for so for instance we are trying to improve a lot in this document so documents are central logic and documents are raised that these are the two core members of our family in the ecosystem so we are spending a lot of time on making them easy to use for instance with this fluent pattern we are trying to invest a lot of time on finding the best way the more python way to work on it yeah so it's a constant evolution yeah yeah of course but it's like API is exactly that layer which is essentially like facing the customer right and you don't know the scenarios they will use it in and sometimes they might kind of surprise you or they might say okay I found some workaround for your like missing parts but then you think okay I didn't think about it right the API layer is a fantastic way of talking to your client through like API contracting away right yeah and it's a quite a big challenge I would say to have the right balance between ease of use and flexibility so what belongs there and what doesn't belong there because there's always a risk to put too much functionality in one same thing and make it very powerful but make it a nightmare to use yeah so in these balance I think there is the key to what is your choice when you have to choose let's say it's a balance of flexibility or like flexibility or what did you say the ease of use right ease of use I think we're now I'm now sending to go for the ease of use because for instance with these open source I repeat that open source teaches you well I think at some point we did a nearly down to well the APIs and it was a little bit complex to use you could do a lot of things but at the end maybe not everybody was doing so I think ease of use for the first century barrier is the most important thing yeah and I mean also like it's interesting you know like if you have a real API let's say deployed somewhere and it's a published contract and people are sending queries there then you know actually which endpoints which features have been used which are not which options are completely ignored even though you put them in the docs right but how do you go about this in the open source code like somebody download your code they use it somewhere you don't know how so how do you collect this analytics from them do you just send like call out messages hey guys what do you use what you don't right now we are trying to keep attention on who is using guys what and when people ask us we try to get the most information out of them not information on the business of how they use it how they feel so right now the community is the only source of information we have that's the open source what how do you talk to them like do you like send like messages saying hey guys can you vote about keeping this feature and removing that one or not exactly like this but would you see if people that are more engaged or more or less engaged people that are more finding it more easier or less or having more difficulties with your with your solution so it's and sometimes we have a development relations teams that try to get also feedback from from the community in many terms so this is a global effort but but in the end you have you have a say right like no matter what they ask you have a say is that right well I mean sometimes you cannot please the community to all the extent I don't know we have to keep a roadmap for instance people my one you to build something that is eman related but maybe not so significant for search solutions this is quite a confusion I think also there so beyond beyond search like where can I use Gina what kind of other use cases have you seen beyond like kind of similarity search let's say so since we are building these abstractions it is quite easy for a year to use these abstractions for building any classification model or anything as I think you really did you could even deploy something and use Gina to easily deploy and scale and use your segmenter and object segmenter model but this is this is something that you could do but Gina is born and will will be working to implement new research solutions so you could still use this but might not be the best tool for it so we are not born for that but you could do but we can see that we are done we you can do this because for instance classification or segmenting can object can be part of your pipeline but in theory we are born to support search application yeah yeah so like or or something that is or search applications or something that you can frame as a search application right now for instance a question answering system that you can frame as a part where you will do something I research or spare search and then you have some really more extracts more information from something so anything that falls into this domain you can do it yeah I guess you can also like based on the research and some practice happening in data augmentation based on retrieving you can also formulate data augmentation as a process of search in principle right so the output will be your augmented data but you you use search in search in in yes actually that might be but search can be so many problems can be framed into search I don't know at the end is like vectors are somehow like the truth not like the semantic information so how we don't understand exactly why but is encoded there right so just by clustering them together somehow we have some understanding so so many things to confirm with I also wanted to ask a little bit like closer to the similarity search itself you know let's say I built a traditional kind of texts search engine okay and I'm moving away from BM25 which is like probably majority of this market today so I'm thinking okay what are these cool kids doing maybe I should try it out plug in some bird model and but then in my UI I'm also showing snippets and it's very easy to show snippets when it's a keyboard search right so what should I do or what can I do with model like bird and genie AI to show snippets or something that will resemble snippets to the users maybe you can also change your information so that you check where the attention is put in your model or somehow but yeah I think also there is a thing that we are framed and we have been grown into this keyword search that it's so interpretable and so easy to use and even so so easy to hack so how no you know you as a user know how to drive your leg search if you don't find it right but okay this word might find you here and I think since these models are kind of black box for many of us I think in this kind of sense this interpretability is one of main challenges and I think one of the main focus that research should go yeah but I mean you you call it out as an interpretability but like for the users and for me let's say I'm a product manager I don't care about is it bird model or is it BM25 I used to see snippets I want to see them now so like what should they do yeah but the point is in the 25 I can give you a snippet because I know why I have this solution here is well it correlates and but where the information that I want is there maybe for instance in dinner one of the main building blocks that we have is our document is our recursive structure because most of the things for instance if you find a search if you search a document a text or document you might need to break into paragraphs into tags and so on so maybe what you can do is you do the vector search at the variety level is at sentence level but then the results might be shown at paragraph or at sentence level so you can highlight very easily the sentence that really drove the search to this page that we know I use case yeah I remember actually I don't if you know the block was at Selman Run like Sujit Pal it's he's doing a lot of blogging in the area of like you know here is the problem how do I solve it and and then he look quite usually he goes into deep learning or like trying out some vector search maybe or or not and I remember like he was saying that to solve this snippeting problem how he he would do it because he comes from traditional search and I do in a way and like he said okay you can kind of build like if I remember correctly if you can do like almost like a dictionary right so let's say you take a word you can embed it take a word you can embed it like you can embed a dictionary right now when you found that document you can kind of from embedding so you can map back to the words if they happen to be close enough like geometrically you can find close enough words so you can kind of try to say okay maybe these keywords are representative of this text but I'm not 100% sure but at least you try so you go backwards like reverse engineering from the embeddings it's interesting though you may need to go through all the pain of limitizing kind of these kind of stuff that you may have saved by going through semantic search and now you are back to it so like straight-offs but yeah it might be a good yeah limitization is another thing but like I think there was this paper from I believe Google about byte level kind of training right so they don't care if it's like lemma or if it's like suffix or prefix they just go byte level they don't go sub word level they go byte level and then with byte level you can essentially kind of like okay now I can compute the distance again right okay how close is this to this dictionary word or not but then again from there in order to produce a snippet that will look like natural language you will have to use some kind of model like gpt or like in general generate the the sentence and at that point it might actually go completely different direction from your text right start like hallucinating or write a news item that doesn't exist so yeah well maybe you can use these extractive models were from a sentence given a context but nature all these all these top literature is yeah yeah but I mean like attention the mention what you mentioned the tension probably can be used here right so like you you can ask the model okay what did you pay attention to when you yes when you did the matching but but still it's not some people as you say like you can say interpretability but on that hand it's kind of like when you go specifically to that product case you need that snippet or you need that kind of context of the match all like if you said mathematics and it picked algebra like why did it pick algebra at least can you explain because here it's more or less obvious but in a specific domain it might not be right yes like what do we do I don't know maybe you are not using the right to I don't know maybe we are obsessed on using the degree for everything but I think the these two walls of keyword what we call tradition search and this neural search I think they can be combined to power things to the next level I think they need to be enemies and there is good and bad team both sides do you have any thoughts how you would combine for instance in any solution you can have solution I don't know maybe you can get results based on both on both sides and then at our ranking step consider what is best is this a complex is this a complex query maybe I'm looking more for some semantically reached solution did this guy just this is user just send a couple of keywords is it semantically written off no this user might be expecting keyword based feedback yeah yeah that's true well like you couldn't even like go as simple as kind of giving that control to users right so So everybody knows that it's keyword.
They first want to go with what they know, what works or may not work. And then only if they are not satisfied enough, then they optimize for equal. They might go into explorative mode that's put on the similarity search. Yeah, I mean, that might be quite viable. So it's interesting.
The problem is that keyword search, well, as far as search, it might have not a good future for image-based search or any other mobility related search. Yeah, exactly.
Like the moment you go beyond text, right? What do you do? That's a big power, I think, and the big future that neural search has ahead. That is, there is where not any traditional search solution, I think will keep up.
So if I want to build like a multimodal search, can I be some executor from the marketplace and plug it into Gina today and do it? Yeah, I don't know. Yeah, I think we have some, for instance, but you can use clip that clip. You can use clip to encode.
I think there is audio, you can text, or there is image and text. And it performs very well. We have wrapped it in one of these executors and hat modules. And you can use these clip models to your close model search faces.
It's quite efficient without the match-faint tuning to search for images given text and the other way around. It's good to present. Yeah, that sounds cool.
But I was thinking, if I want to combine like speech, text, and image, then I need to probably come up with some meta model of that, right?
There is some research in this area where it is not that like modalities are treated differently and encoded separately, but where they are considered together.
Even there is some research where there is multiplicity and some context switch. So they move the vector. So that's also possible to get the latest research, wrap it into one of these models and deploy it in production. But this is not so easy.
For us, we didn't focus on building these front scratch, but we're also looking to having these top-notch researchers into building these modules.
So like, in that case, would you prefer like communities to help out to bring in the model, or are you helping to do that? Right now, we are driving this direction to offer this to the community. I think that our dream as an open source is to have the community flourish and be alive upon itself.
So the future should be community driven? Yeah, because in the end, community might also know kind of... Well, when this grows big, you know, community will be kind of helping each other. Yes.
Like some of these things will become what you may call commodity to some extent, right? Or at least the way you integrate might become commodity and the use cases might become commodity and there will be new use cases which are untapped, but I think community can definitely help out each other.
What we might need to focus on to make these models easier to use or easier to find if we have a marketplace where everything... maybe we need to help the community on finding what they need in every time. Yeah, yeah.
Content wise, hopefully there is a time where community is the main contributor there.
Was there something else in China that we should know about as users, some cool feature or some system that you think doesn't exist in competitors, something at all is cool? I don't know right now about competitors. So I think what I like the most is the easy... the easy views and the time saving.
That's what if you go out to our economy and you try to build from zero to the ployding coordinate as an neural solution and image solution, I think you will all enjoy the asiness. Yeah. Yeah, so it's like kind of well-oiled machine.
But can I also bring it up on my laptop? Yes, you can try on your laptop everything. So the point is you may not be able to index so many images, but you can get the first feeling with your laptop.
Yeah, I mean, if I want to build a demo to impress my manager, you usually use my laptop, right? Like, that's maybe one way. G9 is really for that. Yeah, that's pretty cool. And I think also it's nice that you said it's Python friendly.
So it opens doors to so many things, especially on hacking face. It's pretty much all Python, right? So I need to pick some models like it in and do a need to containerize it maybe or figure out isolation and so on, like just plug it in and start using it.
I think that's also a great boost to productivity and actually kind of implementing the use case rather than focusing on some mundane components and parts and processes, right? Yeah, and even these modules that we have, they are already containerized for you.
So we have on our end, we build a container for you so that you can build it in an isolated way with all your dependencies and stuff. Yeah, yeah. Sounds great. I mean, I think we now have pretty good understanding of Gina. Of course, we didn't read the docs yet, but it's sounds promising.
So I hope some of our listeners and audience will take it out. I wanted to go more into this kind of philosophical level. Like, what drives you in this space? Like you said that you've been working in web scale search as well before, right? And like some other search and engineering in general.
So what drives you here now in this area when you join Gina? And why are you joined Gina? So I joined Gina, especially as I was in this tradition, search space I was working on training ranking models. So what drove me more is to enable this search system, this search experiences beyond text.
I was super curious about how we can, it's impressive to me how the same framework of getting something that extracts meaningful vectors with semantic information can be used for images, for video, for audio, for anything.
These frameworks, I think, it has a lot of features, because it's quite, and also how the different research areas from different modelities interact with each other, I don't know.
I don't know, the conversion neural network appeared, even some text classification used to do these, then appeared the transformer right now, the computer vision community is getting in love with transformers. These fact and forth, I think, it's impressive.
But also, if you think of the magic of getting this vector and having so much meaning there, it's quite amazing. Yeah, it's true. It's very powerful.
The sheer fact that you don't need to build a synonym dictionary if you go full text, right? Like it just tells you that mathematics is close to algebra, or you throw data at it, and it's unsupervised, right?
It just tells you, hey, I've trained it up, like now, okay, I can tell you what's close to each other geometrically.
It also has the mathematical beauty there, right? Geometric closeness rather than some obscure train shopstruck sparse closeness. It's quite elegant, I would say. Yeah.
You have tracked all these knowledge, all these, and you have this simple thing that you can imagine in your head as a three-dispose, and that is as simple as algebra from, I don't know, which grade, but quite simple. Yeah, I think in simplicity, there is a lot of beauty.
Yeah, it's very easy to explain to your granny. Like, I'm doing this, you know, like it's a three-dispose kind of, there are points, and I'm just looking the closest one.
I expect to have something that puts things close to each other that makes sense together, that is what we expect from this black box. Exactly, exactly.
And then the question of scale, like if you go to 10,000,000,000,000,000,000,000, then, okay, can you trade some of that closeness precision, and kind of get past the speed? So, yeah, it's very interesting.
I mean, does it interest you more like on deep learning side, or on mathematics side, or engineering side, like, or maybe some other side? In every side, from mathematics, I enjoy a lot the beauty of it. Sometimes, it's too obscure for me, but I really like it.
And understand, deep learning, I like it. Although, I feel that some of the research doesn't seem to be so innovative, and maybe we should spend more time checking other paths. Okay. Deep learning. Which other paths, like you? I don't know, you said that. Could be honest, I don't know.
I just feel that there's so much literature that I cannot keep up with. And then from the engineering side, I think it's cool. It is just space, I think I also can provide more value, depending on some times concepts are to abstract from.
Yeah, like for me, I want to call out, but you'll point on, is deep learning the only way. Like, for example, one scary thing is that these models are becoming more and more parameterized. So you have like 100 to billions of parameters, maybe billion, trillion.
Like, how many more can you have? Zillion parameters in there. But first of all, it's impractical. So if you take that model, you try to plug it in. It doesn't plug in because it's too expensive. And also, you might not have that much data in the first place.
So why should you care? Like, web scale search engines probably will. But like you, as a researcher, in let's say a startup, you don't know if you need that much.
You need to solve that specific thing, right? So it will look really strange to bring this huge microscope and like, GBT model in and say, this is what we need to use. And then like, the whole budget goes into paying that model, or whatever, you know, like, it's in practical.
So that direction by itself, like, I think it's a little bit like doled. Or like, I don't know, like, how you feel about it. Yeah, it's a race where I had another layer and get more parameters in A-Win.
And I think, I mean, but it feels that the first step to move away from this is to really understand how things are learned and why are learned the way they are. I don't know any match. You have these models to bring back to the image. Where the filters are learned more or less.
You have some idea or where the model is looking. But maybe to put more research on slow down, let's slow down these race and let's understand. And maybe we find a way to make it more sustainable for everyone.
Yeah, because I remember like, when I was doing my PhD in machine translation, it was using like statistical models, like Moses and statistical machine translation. And so it would suffer from things like out of vocabulary and how do I bring syntax in and what not.
But then like when deep learning came, like all of a sudden you see that it translates much, much better. And you think, wow, probably they solved it now, right? The claim from 50s that we will solve machine translation probably now is delivered that the promise. But then you notice it's fluent.
But it's kind of like, I don't want to use the word stupid, but it just doesn't get it, right? Like it makes wolf subject an object easily. It may go and hallucinate about something that doesn't exist there.
Or it actually goes and translates into like single letters all of a sudden, you know, or repeating engrams. Or like you see that it didn't exactly solve it, right? You would have tried your life to a certain system yet.
Yeah, and then you kind of come back and like, okay, and I used to do it like in a rule-based approach. So I could understand the syntax of the sentence and then semantics, all of each like nod in the tree.
And then when I translate, I use some semantic like function and it's all well-defined in the axonomy of semantic functions and so on. Like, okay, now I go back to deep learning, do you have anything like that? No, it's like just space.
Maybe there should be a way to combine, I don't know, we have built, I think as humans, we have built this complex way of talking to each other, know, with semantics, I mean multiple languages and stuff. And there is no way that all these language can go back to this deep learning world world.
It seems country-to-if, if at least. Yeah, yeah. So you are certain that like maybe the voice of those who built alternative models to deep learning of alternative approach should be maybe louder.
Yeah, I think that we may suffer from the bias of the winner, no, I mean, maybe the first one who opens a door might not win the race because, but even if they show another way that the race might go, I think they might deserve more attention. Yeah. Yeah, this is quite deep.
Thanks for this white section. Like you think about it a lot, like kind of, okay, not to be biased, okay? Yes, there are challenges, but it might not be the only right approach and giving you experience as well. Like you can judge a little bit with your open eyes.
I think we should explore more and maybe not one focus of that. And probably explore with Gina, right? That's the goal. For sure. Yeah. So this is super great. Is there something you want to share? You already shared that the fine tuner is available.
So our listeners can go and check it out, right? Is there something else that like we need to be expecting? Chon. And early next year we should be releasing our 3.0 version. So the state unit for that. Yeah, absolutely. Comment. We will be moving fast in the next times. Yeah, this is fantastic.
I mean, thanks so much for all this information and detail on Gina and also like your ambition and kind of like your thinking here. I mean, it's really nice that you keep your open mind available to all of our listeners. Yeah, thanks so much. It was a pleasure to talk to you, John, today.
Thank you. Enjoy it too much. Yeah, thank you very much. Looking forward to 4.3.0. Yeah. Thank you. Cheers. Bye, bye. Bye.