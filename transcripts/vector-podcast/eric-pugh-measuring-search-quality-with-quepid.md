---
description: '<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=0s">00:00</a>
  Intro</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=21s">00:21</a>
  Guest Introduction: Eric Pugh</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=180s">03:00</a>
  Eric''s story in search and the evolution of search technology</p><p><a target="_blank"
  rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=447s">7:27</a>
  Quepid: Improving Search Relevancy</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=608s">10:08</a>
  When to use Quepid</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=893s">14:53</a>
  Flash back to Apache Solr 1.4 and the book (of which Eric is one author)</p><p><a
  target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=1069s">17:49</a>
  Quepid Demo and Future Enhancements</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=1437s">23:57</a>
  Real-Time Query Doc Pairs with WebSockets</p><p><a target="_blank" rel="noopener
  noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=1456s">24:16</a>
  Integrating Quepid with Search Engines</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=1557s">25:57</a>
  Introducing LLM-Based Judgments</p><p><a target="_blank" rel="noopener noreferrer
  nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=1685s">28:05</a>
  Scaling Up Judgments with AI</p><p><a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=1728s">28:48</a> Data Science
  Notebooks in Quepid</p><p><a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=2003s">33:23</a> Custom
  Scoring in Quepid</p><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=2363s">39:23</a>
  API and Developer Tools</p><p><a target="_blank" rel="noopener noreferrer nofollow"
  href="https://www.youtube.com/watch?v=1L7UjjPz5wM&amp;t=2537s">42:17</a> The Future
  of Search and Personal Reflections</p><p></p><p>Show notes:</p><p>- Hosted Quepid:
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://app.quepid.com/">https://app.quepid.com/</a></p><p>-
  Ragas: Evaluation framework for your Retrieval Augmented Generation (RAG) pipelines
  <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/explodinggradients">https://github.com/explodinggradients</a><a
  target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/redirect?event=video_description&amp;redir_token=QUFFLUhqbnBCSEIwRlFsd0RDQUdnbDFiRFF0d1dEUDRLZ3xBQ3Jtc0tuaGNDd2FpZzdITFdiRjNmbjJwanh1cnpuQ2VaWDZmZjZicXZjVTREZEVsdXhHd0x6WUZwREd1QXJRN2dtWXgtc1g4NDhSOU11ZXdpajVxb1hxZmpkSXdtNnhhbktYSTkwMmJjWmxPZFBjcTREa3NERQ&amp;q=https%3A%2F%2Fgithub.com%2Fexplodinggradients%2Fragas&amp;v=1L7UjjPz5wM">...</a></p><p>-
  Why Quepid: <a target="_blank" rel="noopener noreferrer nofollow" href="https://quepid.com/why-quepid/">https://quepid.com/why-quepid/</a></p><p>-
  Quepid on Github: <a target="_blank" rel="noopener noreferrer nofollow" href="https://github.com/o19s/quepid">https://github.com/o19s/quepid</a></p><p></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20240626_010626_075b8a8d662d3fbf1946ef06b8218efa.png
pub_date: Wed, 26 Jun 2024 13:42:56 GMT
title: Eric Pugh - Measuring Search Quality with Quepid
url: https://rss.com/podcasts/vector-podcast/1539938
---

Hello there, vector podcast season 3. In this season I made one simple promise. I will try to stick to 30 minute episodes. Let's see how well I will do it. It's not always easy, especially when you have guests like Eric Pugh that I'm really having a pleasure to talk to today.
I can say that we've been working together on Quepid, on ideation, on things. And I've learned a ton from you. Yeah, yeah, I'm super excited. So when did I come visit you? I think it was two years ago or some two years ago. I think so. It was pandemic, I guess.
Yeah, it was the very end of the pandemic. Right. I remember getting my, yeah, so it was still pandemic. It was still pandemic. Yeah, it's still pandemic, right? Because I had to get Quepid to say, yeah, so I was, yeah, I was like, I want to meet to meet you in person.
And I called you and said, I'm going to come to Helsinki and visit you. And I think you were like, why? I mean, we don't work together or say we worked on Quepid though. Quite a few evenings together, right? I think it was like nine o'clock your time, Helsinki time. Yes. Yes.
Who was it? And it was Friday. I remember vividly Friday. What else to do? So I went camping with my family. Can I screen share? Did you meet? Yes, yes. Of course. You can give me permissions. I went camping with my family.
And if you think back to my visit to you, you and your wife gave me a little gift. Yeah, give me a free share. I can only do you host. Let's do a host and you can screen share. Yeah, I can. All right. And so I just wanted to share off this, this cup. Oh, there it is.
So we've had that little wooden cup. I think it's a traditional finish drinking vessel when you're out in nature. And there it is with coffee.
And then I'm also showing off my metal ceramic metal enameled cup that I picked up at OpenSearchCon EU a couple weeks ago in Berlin that Zeta Alpha shared had some great conversations about search, relevancy and measurement with them.
So we took these two cups on our family camping trip the other week. I wanted to show those off to you. This is lovely. This is lovely. And I'm glad you're putting this in good news. Yep. It goes with us. So fantastic. Yes. Yes, where we start. First of all, hello. Welcome. Welcome. Yeah.
Thank you very much for having me. It's long overdue. And usually we start with a little bit of a background. Obviously, people can go. I think you even have a Wikipedia page about you. I think so. I don't know. That is a lot of people. Right. That is a lot of people to get to a Wikipedia page.
I don't know that I'm quite there yet. Yes. So my name's Eric Pugh. Been doing search for about, I don't know. We're like getting 15 years. And I was there for when a search was like first, oh, you have your own search engine. It was very exotic. And there was nothing open source.
It was all commercial. And then cut my teeth in search going through the big data time period. Right. When, as Grant Ingersoll said once, search is the UI to big data. And it was all about data.
Can we handle and how do we store it and scale up our search engines? And that was great and kind of led into the machine learning time period. We're really at that point, it was like, OK, we have lots of data. We can now search it.
What does it mean? What are people looking for? It wasn't enough to have fast search with 10 blue links.
It was all of a sudden became really important to be like, am I giving my users what they want or not? And machine learning and data science really kind of came along and helped us make those determinations.
So really, and that's when open source connections, the company I was one of the co-founders of, and I'm one of the leaders of really kind of focusing on the value side of search, relevancy.
Am I giving people what they're looking for? How do I drive more revenue in e-commerce? How do I help people use my SaaS products? Are they subscribed and renew their subscriptions? All of this, right? And yeah, machine learning was awesome. Data science was awesome.
Really got into a whole measurement thing. And that was kind of one of the products that I stored, Quepid, we know each other, came out of that time period because we said, why are we building custom tooling for every project, maybe we could share some things.
So, and then yeah, today it's really been exciting to see sort of generative AI come along and vectors.
And it's interesting because I still feel, you know, for a little while I was like, is search still gonna be a domain? And you know, search is totally changed, but it's still how people interact with systems, right?
Whether it's a spot and a retrieval augmented generation or a more traditional keyword search, using LLMs, using models, using vectors, still a search engine in the middle of it, mediating, moderating that conversation.
So really excited about what Gen AI has let us do. And I think my big takeaway right now is that historically search was fairly mediocre. You could make it a little better, you could make it a little worse, but it was always like people understood it was fairly explainable.
Why I'm really excited about measurement and understanding these days is because now with Gen AI, we have much better tools. We don't have to have mediocre search kind of better, kind of worse. Instead we can have amazing, accurate search results that really understand what you're looking for.
And you're like, yes, this is exactly what I wanted. But, whoops side of it is sometimes those search results are back shit crazy and you know, no idea why it came back with it and you made me lose trust.
And so now instead of all search results sort of being in the middle sort of, yeah, a little better, little worse, we're now really polarized. Sometimes they're amazing, sometimes they're terrible.
And we need to understand what that curve looks like and make sure that the amount of terrible is something that we're willing to deal with, right?
Terrible results, one in 10,000, one in 5,000, one in a million, depending on your domain, it may need to be one in a billion is a terrible, right, depending on what you're doing.
So exciting times, really exciting. Yeah, it's amazing, it's amazing story. And of course, I'm very pleased to also being able to pick up, keep it with you early on, where I tried to pioneer it two companies ago and I was leaving actually. But it was almost ready.
And then the next company I actually deployed it. And we, I remember we generated 70 G-RAT tickets just by looking at queries in Quepid because you know how it usually goes.
People develop software, other people check on it, other people are just project managing and things like this and no one really takes the lead on looking at the queries. And this is actually the most fun sometimes to look at queries and sort of you know investigate what's going on.
Do you even like these results? How do you feel about them? You know let alone setting up a team around it where some annotators can actually go and label with some domain expertise, you know, or maybe pretending to be users and things like this.
So it's an amazing system and we continue to use it today. Of course this was the first thing I pioneered at TomTom and it's still there. It's fantastic, that is wonderful. I mean, it's been great to see sort of the adoption of the product and then people have been using it for a long time.
So I'm going to show a query set today that is a thousand queries and maybe a thousand queries that have been judged 10 deep right by hand for three years. Almost four years.
This one organization that Nerry Information Network has been using Quepid for years and now they built up this massive body of ratings and they have tons of data and trend lines for what did search look like four years ago? What did it look like last year? What does it look like today?
It's really been exciting to see them.
They've just been using the little hosted Quepid app.Quepid.com and but it's worked for them. So a thousand queries definitely takes a long time to work your way through. But these days they're just kind of keeping an eye on what's changing, right? Barring a major algorithm change.
It's just sort of staying on top of it and keeping everything right. But yeah, so it's really exciting to see people using it. Yeah.
Definitely I'm having a little bit of thoughts about where does Quepid live in our generative AI future? Been playing a lot with tools like Ragus and some of the other ones, right? And it's interesting to see what tooling and where does Quepid do things? Well, where does it have challenges?
Where do we want to go with? So yeah, for sure.
And for those who don't know Quepid, I mean, I can give my short intro, but obviously feel free to augment. But like the way I see it is that it's basically instead of hearsay and sort of someone saying, your search doesn't work. And here is one anecdotal example.
What you can do is that, oh, vice versa, you could say I improved search.
And here is one anecdotal example where it really shines, right? Now what should we ship it? So basically I think Quepid really gives you the tooling and you can actually, even if you want, you can even do it in an unbiased way, as possible where you will do blind labeling in some sense, right?
So I've done it actually just recently.
And basically you allow your users, well, your domain experts actually, but maybe even developers to go label queries.
And it also has this sandbox where you can actually, well, you can plug in your own engine, but you can also plug in those standard engines like Elastic Search Solar, Open Search and others.
And I think you even added some vector search engines recently, right? Yeah, so we have a Vectara, which is a pure vector search engine. We've got out the only app.
And then Open Search, Elastic Search Solar, the Lucine Bay search engines, and then kind of exciting, you can also now plug in your own search API. And so you can just talk to any API, a restful Git post JSON sort of API, you can use Quepid as well. So that's been really good. Fantastic.
I love this YCupid. This is sort of the origin story Doug Turmbolt, who many of you may know, right, from his book Relevant Search. He created Quepid. And we're looking at like a decade ago at this point.
And it was because, you know, it was difficult to measure and improve search, right? Lots of spreadsheets going back, lots of conversations. You fix one thing, break another. And Doug and Arena were working together on a project. And it was literally, this is the origin story for Quepid.
So Quepid's all about making collaboration better, making your testing more accurate, and making things go faster, right? Because we need to iterate and experiment quickly, right?
The one thing I know is that the team that can experiment quickly and effectively is the team that's going to win out, right? It's not about specific technology choices or technical expertise.
It's experimentation. Can you do it quickly? So yeah, so Quepid.com has the advertising free hosted version, really excited. It sort of continues to be useful in today's world. Absolutely. And it's also open source, right? So you don't have to be buying anything, whatever.
It used to be a product though. It used to be generating revenue. Yeah, I mean, you told me. We're consulting soon. So yeah, we used to sell it. We used to sell it for $10,000 a year for an enterprise license. And we had customers and it was great.
But I think then we figured out we were making, I don't know, $80,000 a year, which sounds like a lot, but then investing $150,000 in salary, supporting it. And it was like, yeah, we're not a product company. And we are open source connections, having a commercial product just didn't fit naturally.
And since we're all about training our clients and empowering search teams, right? Doesn't necessarily feel empowering to be like, yes, we've empowered you, but you have to pay us money every month for this one product, right? Just felt more natural to have it as an open source project.
Yeah, absolutely. And it also fits your, well, how should I say, your professional line at UI commuter. You've seen solar commuter, right? Yeah, so I am a commuter, not active on Lucene, that's just a level of technical expertise. But I am a commuter on solar.
And then interesting as like an interesting personal professional development, I've been really gotten much more involved with the open search community over the whole year.
And so I'm now a, they call it a maintainer instead of commuter, but I am a maintainer for open search documentation, which has really been really a lot fun to work on. And we're talking about it maybe in another podcast, but contributing some new features to open search, the open source product.
So really excited about that. Sam. Actually, give me one second. I have one thing to confess, one second. I have to confess or share one personal bit that when I started in search, it was, of course, it was early. It was like 2003 about when I wrote my own search engine.
But when I started doing search in the industry, right? It was 2010. And it was a patch of solar. And when you Google a patch of solar, you would mostly find Java, Java dog. Yeah.
And maybe, and then I figured out there is also a mailing list was like, but is there a place where I can read about solar besides Vicky pages because Vicky pages were not kind of complete in a way? Yep. Yep. I was like, and I found this, this book. Oh my gosh. One point four. Yeah.
A prior server data. Yes. Yes. Yes. And I've read it covered to cover. I have to say it because because I had one challenging task. I had to build what I suggest and that I would suggest had to abide to certain rules.
And I was like, oh my god, how will I do it? In the moment I did it, it was also slow. So I had to figure out on our data, on our version of model of data. Right. Oh my god, this was so exciting. I was like going back and forth between the book. And then a bit of googling and then trying things.
Ah, yes. Fantastic. Wow. Thanks for doing this. So you also the author. You also the author. Yeah. Yeah. Yeah. Yeah. Yeah. So we did that book. We did it. We did a second version of it for updated solar. But that was quite a few years ago.
I am kind of curious what's going to happen with technical books. Right. I mean, in the solar community, we got the ref guide, which is, I think, pretty darn good considering how it's written. I do sort of wonder what the future of technical books will be with open source communities.
And what do we do? So maybe like cookbooks, you know, like that you have specific cases and like, how would you come about building these things and maybe real data so people can actually try things, right? Yeah. Yeah. Yeah. I mean, it has gotten a lot easier to publish on the web, right? Yeah.
Have something. But yeah, what, you know, I think a lot of people write a book sort of as a writer passage as well. Right. So it's a book, a little different thing writing a book for an open source project reference. Right. For sure. How to make them printable.
So you can say I wrote the book for this open source project. But we'll see. Yeah. That's exciting. But you also wanted to show something. Let's demo. I'd love to. Yeah.
So we touched briefly on Cuban, right? And so I'm going to go ahead and tell you about the first one of the stewards of the project. And historically for those of you who views, who've used Quepid in the past, the way it has worked is I'll just, I'll just bring up my local host. Here we go. Right.
So one of the things that we've added in the not in the recent. This is the development version. So user with realistic activity and Quepid is who I've pulled up and I got a couple of cases here. But you know, in Quepid, it works well. I'm going to bring up a case, right? Here's a case.
I'm going to search for milk. I did a query for milk. This is using sort of a random data set here. It's backed by a solar search engine. You can see right there. There's my search engine. And Quepid works great for a.
Relatively small number of queries up to hundreds, right? And one of the things that we found is that. That this interface works well, especially if a search engine super fast and responsive. But because this is a rich single page JavaScript application.
It's making queries in real time to a search engine. If you have a thousand queries, like the people I mentioned before, takes like 15, 20 minutes, but as you wide load up and all the queries to be run.
And we know that lots of people want to run more queries, 5,000, right? When people ask how many queries should I be measuring? I'm like, well, start out with what you can. If that's 25 and 50, that's better than zero. Think about 200, maybe 300, maybe 1,000, 5,000.
Right? And then above 5,000 that sort of only for the most sophisticated teams. But Quepid kind of tops out at maybe a thousand queries. And so we've been doing a lot of work to think about how do we support larger data sets, right? Larger query sets.
And what's been really fun is to work on introducing background processing, right? Instead of everything being limited by the request, response cycle of your web browser. What if we can run some background jobs? And so I'm just going to show really quick. I'm going to go and bring up all the books.
And I've got an import feature. So we have exported a book book export 39. It's a 62 megabyte JSON file. So 62 megabytes. And I'm going to go ahead and click upload. And now in Quepid, what we're starting to do is we can take large files, JSON files predominantly. And we storm in the background.
And we kick off a process, a background job. And there you can see, right? There we are loading a whole bunch of queries, right? And these are all sort of scientific queries, some very complex ones and simpler ones.
And you can see it's going to take a while because this had what 28,000 query dock pairs, right? So that are being loaded along with their judgments. So, but what sort of fun with the new background jobs and using web sockets.
We're also able to push up updates to you as background jobs are happening inside Quepid. So right here, there we are. And we are loading a whole bunch of data. Now. Yes, it would be nice if it was a parquet file, not a MySQL database that we were using.
So we'll have to think about some of those things. But this is starting to open up the door to moving larger data sets and being really comfortable with that sort of 5000 queries, 50,000 query dock pairs kind of data.
Not going to manage the 100,000 queries or quarter million documents those data sets. So I think this is a great thing to do. And I think this is a great thing to do. I think this is a great thing to do. And I think this is a great thing to do. And I think this is a great thing to do.
But we're at least scaling it up to get a broader set. The other thing that I'm also excited about is we're getting closer to being able to run these analytics on a regular basis, right? Now that we have some background processing, we could think about every night.
So these little charts here that you see that are sort of showing some basic scoring information. You could start using this to monitor it over time. Instead of having to roll your own dashboarding tools. Yeah. So. So that's something I'm really excited about. I'm also going to point out to two PR.
So GitHub.com. 19 sqbid is the open source project. And a couple of pull requests that are in progress, but looking to land them soon. Right. Here is this pull request 976. Imagine if we could run thousands of queries, nightly and Quepid.
Now that we've got background jobs working and communicating with the user, right, of state. This will be coming pretty soon. Pretty soon in open source time, which means. I don't know. We'll see next few months. It's on a bag. People helping and testing. So this one's super exciting.
Let's go back and see how we're doing. Yep. We're doing. So there we go. We're up to 4968, where a doc pairs as we kind of count down. Yeah. Yeah. This is all through the magic of Web sockets, which has been really cool to see.
And as you are loading this here, you also executing it against the search engine. Or are you here because we had all static data. Yes, static data. A book represents the query doc pairs with all of the data. Whereas the case is what we do the real time querying. Now that we have this one working.
Once we have this PR, then you'll be able to run a background job in Kupid. With a similar counter, maybe it up here next to one of your cases that says we're running queries, 5000 queries. And this is our progress for the number of the error out.
But of course, for listeners to understand, like what takes time is basically, of course, also inserting this data into Kupid's database is like my sequel. Right. And ready. I guess, or have you stopped using a ready. So I'm not sure. So we're actually, so we're using my sequels or database.
However, what manages this communication Web sockets is all in red. So as the bat and it's our background jobs and our front end jobs and our web browsers keep track of each other is through red. Yeah, so if I actually, so if you, you know, I'm running local hosts, you won't see it.
But if everybody who is connected, who has permissions for this book, everybody would be seeing these messages. Yeah, yeah. So it's kind of broadcasting to everyone who has access. Exactly. Exactly. So that's something I'm really, really excited about.
The other thing that I'm really excited to be is LLM based judgments, right? So you kind of started out this conversation about using Kupid with human judges annotators, right, and gathering quite quality data. But as we all know, human judges is expensive. Not every organization can do it.
Um, my colleague Scott Stoltz last year did some interesting work playing around with chat GPT when it first came out to evaluate, is this query and this document.
And then we've been working with Moody's on their BG solution and using what we've been calling judge Judy and LLM to evaluate what that lets us do is. And then we're using a small set of human judges to validate our LLM judge judge Judy.
And if we have good correlation, right, our inner rate of reliability looks good. You know, flights, Kappa, Cohen's all those metrics look good. Then this gives us confidence to go ahead and scale up the judgments, right, using an LLM.
Today, that is a bunch of pandas notebooks and kind of custom code. However, the other pull requests that I'm really excited about, right, is this meat judge Judy, she is your AI powered subject matter expert. Right.
And so in the not too distant future, you will be able to let me go ahead and bring up this case, right, here we have one person who's been the judge. But soon you'll have a second column next to it judge Judy, right, using whatever prompt you've typed in, right, or provided is judging.
So that's the other big how do we scale up, Quepid and make it relevant in our gen AI world, right, those are sort of the two big things. This is fantastic. This is really fantastic. Yeah. Wow.
I hope this PRs will land really soon, especially the LM one, right, because this allows people to really quickly hit the ground running and start labeling.
Actually, someone will label in a way, but exactly exactly the trick is having the right props, right, and having the right set of positive examples and negative examples, right.
But one of the things that I were working on, so Quepid, right, ships with a set of data science notebooks, they need a little bit more work. See if this comes up in my diversion, I don't ship that. So I'm going to switch to the production Quepid. Yeah, no worries. And notebooks.
Who? They're loading up. So in this example's folder, we're actually shipping a couple of notebooks for you to use, flash capa, jacquard and RBO comparison, multirator analysis, right. These notebooks here, you can directly use with your Quepid book of judgments to evaluate how we're doing overall.
And so this can let you take your human judgments, understand how good or bad they are. And then when you bring the LLM power judge in compare the LLM judge to what your human judges were doing and feel some confidence.
So I'm really excited to be shipping these because I think it's going to lower the barrier to getting judgments. And that's something that a lot of search teams are like, I would love to use cute, but I would love to do this.
But I can't do any of this until I have judgments and I don't know where to get them or I don't have the domain experts that I need, right.
And you know, search oriented organizations often have that figured out, but a lot of other teams are like, we just see a search engine that works what you know reasonably well and we don't have that. So we got a lower the barrier to getting judgments in judgments and I'm excited about this.
This is fantastic, but I can also add from my personal experience, you know, that yes, you're absolutely right that there is this sometimes there is even a friction, right.
The search engineer says, no, I don't want to label I'm a search engineer, I'm developing the algorithm, but they will get so many more insights, so much more insights if they actually label.
And in our teams, you know, if you have, I don't know, 10 people and if each will label 10 queries, you will have 100 queries labeled.
So of course, if you don't go for overlapping and stuff like that, but if you go, then yeah, it's another story, but you know, and then all of us, all of a sudden get all this insights, right.
Now, now the LLM thing can actually help you scale this right and then of course all this prompting and in label studio, by the way, they have released a maybe something to think about a capability where an agent will learn from user feedback, right.
So let's say they label and then so LLM will label make make some mistakes and then the main expert will correct them and so it takes it in as a feedback and then it becomes better over time.
So it's basically you can kind of support it's like you're not copilot, someone said, seed prop stain in previous episode said, confident. So you kind of like give these things you collaborate in a way, right. So that would be this is fantastic direction. Yeah.
So I mean, this is definitely very much around that more narrow relevance judging versus generic labeling way label studio is right. But there's definitely room for inspiration from both label studio. I've been looking at more as well as ragas and how it's doing some of the new metrics.
Yeah, it's interesting. So yeah, exactly. What I love about Quepid is that I can really connect it to the live search engine. I mean, not necessarily in production can be some development version of it.
And I can start labeling and queering and as you said, search is the interface to big data, right. So Quepid becomes interface to your search, which is the interface to your big data and all the unknowns there.
And when this terrible that one looks to OK, that one looks OK, that one looks terrible, right. We can immediately start building. Yeah, maybe that one's OK, that one doesn't look it right we can immediately start building some sort of understanding right now.
So quick little binary one right we're going to start building that and get it get a sense of what our score is going to be exactly exactly. And that score is also customizable. We've done some little implementations in like JavaScript looking like language, right. I think it's JavaScript.
Yeah, but you just come in here and you take your score right there is so here's classic NBC G 10, but you can change it. So like one recently, we wanted to know in this score right here, we're being you know penalized because soy is returning zero results. And so it's giving us a zero.
So it's bringing down our average precision. But what if you wanted to know that it was supposed to be zero results zero results is actually the right. Yes, yes, yes. So that one we actually went in and said we added an option a per query option should be ZSR.
Right, and we set that option and then in our custom score. If it said should be ZSR is true and there were zero results then we gave it a one right because it's working the way and vice versa.
We had other situations where yeah, if we started returning results for soy, that would have been worse search right and so yeah that was a great use of a custom score. Yeah, fantastic. We're all about that because it was a great use sound. Yeah, yeah, yeah, exactly.
Also where Quepid helped us is sometimes you don't speak that language. So it could be Korean language that you don't speak but you need to move and on one occasion we've sent a Quepid just to our Korean native speakers in the company and they've labeled and they told us how it looks so. That work.
So the other thing right so here we are we are happily loading up all these but I'll go ahead and click judge right. So this is sort of an older approach to rating a zero a one to 10 wouldn't do that now but that's what we've been saying.
So there you can see a here is the human radar interface now I don't know what is a good rating or not but here you can know the rates and documents kind of taking this from this is a recent add on which is if you're if you can't read it why right.
I am a vet in there I am not a vet and don't understand the science. Yeah in this square. Right and so I'll skip judge in that and that and that that's been that's been helpful in just cranking out your human judgments so.
Yeah go got just a couple of judgments mostly Jeff I yeah Jeff Scott 2500 I've got four in here and I mark one is unreadable I can reset that which should throw it back in the pool right maybe we have a conversation about why it was unreadable and then throw it back in the.
Almost there we are almost there right the background job is wonderful but it doesn't necessarily mean it's any faster right now I know I know at least watch it and watch the countdown so.
Fantastic demo can you tell a bit more about the tech side of things we didn't mention sequel my sequel already is. So if someone wants to jump in and start you know going and and sort of what is the level of effort they need to go through.
It's a little bit of a challenge right so most of us so this is a Ruby on rails web app right like it is a full stack web app and this is all just standard Ruby on rails the app is.
 It's been upgraded over the years to be with the latest standard and so if you do rails development everything's going to feel very comfortable obviously a lot of us in the search or information retrieval world don't have that expertise and that that's just a challenge so one thing I will say is that if you join or ask questions on relevance slack pound Quepid happy to answer those questions the core application that you play with in here.
So it's an old angular one works great no problems but it's an angular one app and because it's an open source project not a commercial product we sort of stayed away from attempting the big rewrite to update it to react or name your thing lots of examples it seems to work fine animals.
So Quepid angular one app for all of this and then outside of this this is all just a standard rails application lots of model view controller type screens that you can see right here all standard rails my SQL database redis for sort of the communication layer.
And it's all built using Docker so if you want to so the read me has way too much developer centric set up right but if you have Docker then you run bin setup Docker yeah that will set you up with the development environment literally what I was just showing is the inside of Docker.
And then you fire it up locally with bin Docker server and that runs it locally so there's a lot of docs in here for all the different parts that can be a little overwhelming I think we have to rework some of this documentation but it's all there.
Now a couple of things I'll show off we actually have an API now so right here you have a you come here and you generate your personal access token like that and just for fun by and and this curl command will show you your user so we have authentication API.
And we're slowly working on documenting all of those API API API API. I'm doing well.
So there we go API API slash we're slowly documenting all the APIs and so one of the things that I encourage people right is maybe Quepid doesn't do everything you need to do and so you're building some scripts outside of it or in some notebooks but you can use Quepid as your shared source of truth.
So maybe you have a case that represents your golden set of queries right you in your notebook can go and grab all the query so and so we're adding sort of more and more documentation on all of these different API so that's fantastic yeah.
So make it a little bit easier for people to understand so I can look at this here's case for but I can also look at it like this. That's a Jason right.
This should give me back my Jason right it's all my Jason data right there's all my different scores etc so if Quepid provides a value but doesn't do everything you need to do you can read him right from it.
So I'm going to do a lot of export name pork functions as well so yeah so it's fantastic and yeah so it's loaded it's loaded. Like there we go 29,000 or 87 query document pairs and 29,291 judgments right. So there is all preserved so.
There you go this is fantastic thanks for the demo Eric I learned because I wasn't keeping up as closely I think we also are writing an outdated version of Quepid so I will ask the team to to upgrade obviously because.
Yeah we should yeah yeah the release cadence is fairly fast so make sure your deployment model is pretty simplistic and automated so it's keep up yeah exactly.
This is fantastic i'm sure we can talk more about your other projects and we will save it for another episode yeah but I was also thinking I like to ask this question and now I get the chance yeah.
 Why the the question of why I call it or the the motivational what keeps you up and at night so to say why you are still in search Eric you've spent so many years do you think it's still unsolved or what what keeps you going in in this topic yeah so what I love about search is it it kind of reinvents itself every.
 I mean seven years five to seven years it sort of reinvents itself every seven years right I sort of started out with saying at one time it was exciting just to have an open source search engine right in a world of big expensive commercial search engines and then it was really exciting to get into big data from the search perspective.
becoming a data scientist right I mean I I I pretend to be a data scientist I pretend to be a machine learning guy right through search right so it reinvented itself and now i'm a prompt engineer and generative AI person through search and so I love that the field reinvents itself.
 But also certain long standing principles around measurement experimentation appear to remain relevant even though it reinvents itself every seven years right and it's been really, really exciting like I like that what i'm doing now is not what I was doing seven years ago and I suspect I won't be doing it in another seven years.
And that I like making things happen I like solving problems and search remains sort of the way people interact with technology systems right I am really intrigued or looking forward to when.
Search isn't just I ask a question get a response but I ask a question get a response then I have another conversation and the search engine understands that we actually have we we talk about search as a conversation but.
We don't normally do that we just pretend it's a one shot kind of thing I look forward to that side of things and then what are the new use cases we're going to enable right i'm going with my family to Spain for three weeks.
 In July got my plane tickets booked I got not great flights but cheap flights imagine that there was a search engine out there that knew what my plane flights were knew what my wife's personal tolerances are and if it was constantly shopping for a cheaper flight and actually cancel current flight and gave bought the new one and you know just let me know by the way I saved you.
 Another 400 bucks for your family of four or I found a better flight or there was an upgrade right like wouldn't be amazing if once you kind of gave it the parameters is doing that and I suspect that's going to kind of look like a search experience right it's going to be a query with a bunch of parameters.
 It understands what my preferences and tolerances and risks are right and that's going to be a really interesting thing to measure and I think it'll be really powerful so so excited about that future I suspect that's the next thing that we get to once we get through kind of the current generative AI stuff.
That's a beautiful answer thanks so much really I've learned a lot today I'm sure we'll repeat this let's do it I know you have another topic to talk about from your conference talk and another project you're working on and I'm sure keep it keep it continues to be.
 Really relevant to what we do it's it's a it's a toolbox right it's it's it's all in your toolbox or maybe it's a toolbox of full of tools but I think it's it's fantastic one to have to really complete your search journey because if you are only writing code and you're never looking at queries you're never labeling you never here would you know how how does it feel like you know using this change and you will not get far so please use it.
I mean of course you can set up something you know in the kitchen with Excel but the Microsoft Excel whatever Google spreadsheets but maybe that's not scalable enough and not repeatable and yeah why waste time if they're already cool tools like keep it open source really excited about this.
That's great that's great yeah the scaling up is super great super exciting so yeah it will be interesting to make sure that keep it remains true to what it does and doesn't try to become all things to all people we'll see what happens.
Absolutely yes and you you as a listener have a chance to contribute it's open source exactly exactly thanks so much Eric I really enjoyed it. Thank you bye bye cheers.