---
description: '<p>Order your Milvus t-shirt / hoodie! <a href="https://milvus.typeform.com/to/IrnLAgui"
  rel="noopener noreferrer nofollow">https://milvus.typeform.com/to/IrnLAgui</a> Thanks
  Filip for arranging.</p><p>Show notes: </p><p>- Milvus DB: <a href="https://milvus.io/"
  rel="noopener noreferrer nofollow">https://milvus.io/</a> </p><p>- Not All Vector
  Databases Are Made Equal: <a href="https://towardsdatascience.com/milvus..." rel="noopener
  noreferrer nofollow">https://towardsdatascience.com/milvus...</a> </p><p>- Milvus
  talk at Haystack: <a href="https://www.youtube.com/watch?v=MLSMs..." rel="noopener
  noreferrer nofollow">https://www.youtube.com/watch?v=MLSMs...</a> </p><p>- BEIR:
  A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models
  <a href="https://arxiv.org/abs/2104.08663" rel="noopener noreferrer nofollow">https://arxiv.org/abs/2104.08663</a>
  </p><p>- End-to-End Environmental Sound Classification using a 1D Convolutional
  Neural Network: <a href="https://arxiv.org/abs/1904.08990" rel="noopener noreferrer
  nofollow">https://arxiv.org/abs/1904.08990</a> </p><p>- What BERT is not: Lessons
  from a new suite of psycholinguistic diagnostics for language models <a href="https://arxiv.org/abs/1907.13528"
  rel="noopener noreferrer nofollow">https://arxiv.org/abs/1907.13528</a> </p><p>-
  NVIDIA Triton Inference Server: <a href="https://developer.nvidia.com/nvidia-t..."
  rel="noopener noreferrer nofollow">https://developer.nvidia.com/nvidia-t...</a>
  </p><p>- Towhee -- ML / Embedding pipeline making steps before Milvus easier: <a
  href="https://github.com/towhee-io/towhee" rel="noopener noreferrer nofollow">https://github.com/towhee-io/towhee</a>
  </p><p>- Being at the leading edge: <a href="http://paulgraham.com/startupideas.html"
  rel="noopener noreferrer nofollow">http://paulgraham.com/startupideas.html</a></p>'
image_url: https://media.rss.com/vector-podcast/20211223_011243_83a6aa11fa9886f4212eedc43a2501c3.jpg
pub_date: Thu, 23 Dec 2021 13:28:43 GMT
title: Filip Haltmayer (Data Engineer, Ziliz) on Milvus vector database and working
  with clients
url: https://rss.com/podcasts/vector-podcast/347470
---

All right, vector podcast, episode three. And today we have Philip Altmeier, data engineer as Zillitz, who works a lot with users. And the company is building the vector search database called Milbus. Hey Philip. Nice to meet you. Yeah, I got it. Data engineer as Zillitz, pretty much me.
Yeah, awesome, awesome. Nice to meet you as well. And thanks for joining the show. And yeah, it's usually I would like to start with you introducing yourself to our audience, what's your background and how you ended up working for Zillitz. Sounds good.
Yes, so you got my name already Philip Altmeier. A graduated from UC Santa Cruz with a BS in computer science in 2020. So right start enduring COVID. And then out of college, I really wanted to kind of get into the startup scene.
And I was doing a lot of things, machine learning, taking a lot of classes, doing projects. And when I was going to look for a job, I realized anything machine learning related, you have to have a PhD. You have to be doing masters PhD, extra work.
You're not getting out of, you're not getting into the field out of college. So the next step was like, okay, what's kind of new and growing in that field? Somewhere where there isn't already so much this set knowledge. And that's where vector search came in.
And then Zillitz, I found them and did the whole process. And I really thought I fed it fit in. And that's where that took off. So that's kind of how I got to where I am right now.
But pretty much yeah, I throw a shout out of college, getting in the whole field and figuring everything out on how it all works. Oh yeah, that's cool. And can you tell me a bit more? I've been also doing some tech stuff for a few years here and there.
But like data engineer and you work with users. How exactly that looks like? Yeah, so data engineer gets thrown around a lot at a lot of companies. Right now, so for me, how it works for me, data engineer falls into kind of just user success and more also pre sale style of things.
So how to use our tech, creating new use cases like we have a bootcamp where we show examples of how to use Milvus. That's kind of what we're doing. We're talking to the customers that are trying to learn how they can implement in their system, what problems they're having.
So we're the ones that are kind of front facing in the company. And then as a data engineer, I've also worked with a lot on the cloud deployment, figuring that, optimizing that worked on some development aspects as well. But it's kind of a lot of hats.
So it's like startup data engineer, at least here, it's pretty much just whatever needs help with or whatever needs work. You kind of get put on that. So it's a cool opportunity to try a lot of it and get to meet a lot of customers and cool people in all different parts of the field.
Yeah, and you also learn to interact with users because they bring a different perspective over things. They probably don't focus as much on the integrated, but they need to solve something. Right? Oh yeah, it's definitely that. So it's figuring out, seeing their use cases is always crazy.
See how much data they're dealing with these cool ideas of what they're trying to do and seeing how we can make it work. And usually it all goes well. We can figure out solutions. We work together. We kind of keep relationships. I think it's really cool. And sometimes it doesn't work.
Sometimes we need to put more things into Milvus. So we kind of keep the communication line open and we kind of figure out more things to put in, kind of get their input on what we're working on and go from there. But it's a lot of a back and forth conversation with users and customers.
Yeah, for sure. It's kind of like, first you need to learn what is it that they're trying to do, right? And before, like you suggest any solution because it takes a lot of time.
Do you feel the same way when you talk to them? Like instead of kind of jumping in and like solving, you kind of try to figure it out. Or you do something else. You do you have a different approach.
So I think I go the way of like, yeah, kind of get all the info first because everyone's use cases different. None of them are ever the same. And then kind of come back to the team, kind of discuss it.
See, do we have any else that's been doing it sort of similar? Do we have a solution? Because sometimes we've had to do hacky solutions with our like our previous version.
There were things that just weren't up to par and you couldn't really change it that much because it was like kind of on an old style of doing it and it didn't work. So you kind of do some hacky solution for them. Some way to trick it into working it out work in production.
But then we take notes of that and kind of later on put it into like, so like when we're going to new version, okay, we got to think about this. We got to improve on this. And yeah, but it all starts with kind of figuring out what they're doing, getting the whole picture.
And sometimes like there's always conversations where they can't really say everything because a lot of these places that are these companies are, there's a reason that they're like, they got to be secretive because it's a new field and they may have a really good new idea.
But it's kind of like extracting as much as we can without like crossing those bounds, getting that in for the not a lot of tell and seeing if it can help them with what we got. But it's like a big team effort. It's not just me.
So I talked to them, bring it back and then we all work together to solve it. Yeah, yeah, for sure, for sure. And I, your users kind of aware of that you are helping them with vector search.
Do they even care? So it's fifth, there may be not fifth, like I would say 70% of the people we talk to are aware from it. They come to us with help.
So they know what they're kind of getting into and they know what they need vector search for and they know like what they're doing and why they're doing it. But sometimes we also get people that are, hey, like I want to find similar images.
And there it's like, we have like the simple like tutorials that kind of deal with it, but they want to know more about it. So there is some explaining of what vector search is.
What vectors are sometimes that some like it's understandable, like not everyone goes studies machine learning and knows what like vectors are math as well.
But I would say 70% of the time they get it and they know what they're getting into, but 30% of the time it's also just like a whole new world and they don't really like we explain it, but it's not like a can't explain it all in one day. There's a lot of stuff that goes behind it.
Sure you might touch on vectors one day, but then you have to get into the algorithms next day and then it's sort of like keeping that relationship and answering questions whenever they come up. Oh yeah, oh yeah, for sure.
And are you using like most of the time you're using Milders, right? Like as part of your user engagement or do you like how does it look like?
So you bring the database and you say you know it can solve a bunch of different use cases, but you know we also need to vectorize your data or maybe they bring the vectors.
How does it look like? So they they usually bring the vectors themselves. We're currently going working on as well as we're building something a little bit above Milders for actually getting the vectors, but that's the little in the working progress and it should be releasing soon.
But for now it's always we have like our examples, we have a bootcamp where we pull like the basic like the basic pipeline. It's always around Milders, but for like images we have a resident 50 and we kind of show them how it goes in that process.
So that's for the 30% we kind of go over like one pipeline. It's like a small file because it's just three steps encode it and bet it and then search it.
But yeah, so most of the time they already have their embeddings though with those bigger companies who knows who know what vector search is, who knows what they're getting into. They already have their 512 dimensions 10 million vectors. They know what they're getting into.
So they just want to see okay how many how fast can you do it? What are the bottlenecks? Where can we like what do we need to scale out if we're going to scale? So it's like again 70 30 the 30% usually you need to go over the actual embeddings as well. So just like a quick neural that lesson.
Yeah, yeah. Or maybe like it's in their culture in the company to kind of like dive deeper into what they are doing, right? Yeah.
And maybe they think that they can kind of take it over and then kind of run with it as they learned, right? But then you said 70% are kind of like, you know, here is my problem.
Can you solve it? Is that exactly like does this work? Can we have a handle this? And that's how usually like 70% again, but they have their own neural and that's sometimes they don't want to tell us like what neural nets are using or how exactly their data is.
But they give us okay, this many dimensions, this many vectors and this many read request, write request, will it work? And then we kind of go from there and see if we can solve it. Yeah, yeah, sounds cool. But can you actually tell me what is vector search and what is Mildos? Okay, sure.
So yeah, vector search. Pretty much a way to search vectors and check over vectors as well. Yeah, I'll go over. So numbers and vectors, you have numbers easily comparable. You can store them in relational databases. Yeah, like the greater than equal to less than.
So to actually index these and search quickly through them, you can do things like Beatriz. This is a very efficient and very fast way of searching for a value.
Vectors on the other hand, you don't really have this kind of like comparison, direct comparison, you have similarity metrics, which is a math equation where you kind of find out how far they diverge. But it doesn't tell you, okay, this element is diverging this much.
It's like a lump sum for every value in the vector combined. This is how different the entire thing is. And that makes indexing a little bit more difficult because you kind of start relying on more approximate algorithms.
So this is what approximate nearest neighbor search, which is pretty much all of vector search. It's this library of algorithms. And there you can do clustering and then you can do graph based, tree based. So the big, the big names are right now for inverted file we have face.
That's the library for its clustering based on centroids. And then you store values in the inverted file and you search through that. There's tree based, which is spotifies annoy what they're using for their music recommendations.
And that's just building trees and splitting all your data by hyperplanes and then going left or right. And then we have graph based, which is H and SW. I think is the biggest one right now. And they're doing pretty much graph start with a very sparse, a very like empty graph on the top layer.
And then you find the closest to one point, let's say, and then you drop down a lower where it gets more dense and you keep dropping and dropping.
And then there's a locality based sensitive hashing, which instead of with normal hash algorithms, you avoid collisions with locality sensitive hashing, you try to get collisions. If you get collisions, that means that they're close together.
And then one thing I kind of forgot to go over is like the data types that this brings. So there is structured data, which is those numbers strings, those things that can be easily compared to. And then there's unstructured data.
And this is pretty much these images, videos, medical data, some things that computers can't easily understand. And then with unstructured data, you throw them through a neural net and you get those vectors that we previously talked about.
And then relational data or with a structured data, you can just take the data itself because it can be easily compared. It's already known to a computer. It can understand them.
And then there's in between, which is semi structured, semi structured is things like emails where you have structured to it. Like you have the body, the header, the sending address, those are all like every email has that, but the data inside is unstructured.
This is kind of where you use a mix of both. But yeah, vector search gets a little complicated, but the main way to think about it, you have unstructured data, your computer does not understand whatsoever. You can have two images, a pixel apart.
And half the time if your algorithm is not good, your computer will think there are two exactly different pictures. It won't get it. So you take that unstructured data, you throw a through a neural net, you get vectors, and what vectors use those previous algorithms to find things that are similar.
And that's how you can quickly search through it. Right, right. And I mean, so you mentioned these several algorithms there, which is I agree, I agree, I read this paper as well. This is cool.
But like just to satisfy my curiosity, where would you put the product quantization methods, you know, which is also implemented in FICE and maybe some, some where else to.
Like, is this like a fundamentally different approach compared to LSH graph, trees, or is this something else in your book? So with FICE with this quantization, I think I just find that to be part of the graph based. I looked a bit into it.
This kind of went a little deeper because I didn't really work on that much, but I did up and look into it. But it's pretty much just simplifying it for clustering is the way I saw it, where I would classify that for clustering.
You kind of want to, you need something to kind of simplify and speed it up. So that's where in FICE you have the quantized base. You have the flats, which aren't, you have the SQH, which is quantized based.
And there's a few more throughout the names, but it's just a way of speeding up that already used one. I'm not sure how well it will work with other algorithms. So like using that quantization and then trying it on a noise, you quantize everything and then you start doing the splits.
It might speed things up, but yeah, it's a little bit outside of where I see what I know. But if it works with FICE, I believe it could work with the other locations. It's just, I don't think it's fantastic yet. Yeah, I mean, I agree.
And I mean, there are a number of approaches where they combine things. If you take the paper on disk and then from Microsoft, from I think Bing team, they combine H&SW with product quantization. And they also have clustering. So it's kind of three phase algorithm.
The first cluster, the points, they get the centroids. Then they kind of quantize them, I guess, kind of lose some precision on the vectors so that you can actually load them in memory.
And then like from there, they build the, so for the clusters, they build the H&SW, the graph kind of layout, right? For each kind of like shard, you could say, for cluster. And then they basically kind of, it's a, it's a few steps kind of approach.
So your query comes in, it basically goes through kind of this quantization. You find the closest, you know, centroids. And then you go and kind of searching them. And then you read rank the results based on the disk.
So from disk, you read the non-quantized versions of vectors, right? So that you can actually give, get the precision. So I mean, what I'm trying to say basically is that you can combine these algorithms in ways. Yeah. Yeah.
Depending on your use case, basically, like if you try to optimize for memory or speed or something like that. Yeah. Yeah. And so if we go back to, before we go into Milbus, like if we go back to use cases, you mentioned there are like a number of things.
Let's say you can encode almost any object and you gave an example, really good one about email, right? So email on one hand, everyone knows what it is. On the other hand, it has unstructured kind of parts to it.
And if you compare text, let's say versus audio or video, do you think that you can equally apply vector search? Of course you can, but I'm asking in terms of quality that you will get.
Or do you need to go like extra mile, you know, in audio, extra mile and video compared to text? There are so many models. Honestly, that's a good question. I think that's where the neural nets come in and that's where they're important.
How they kind of the black box doesn't how it kind of sorts everything out, but I believe in text, I feel like first, I feel like there's been a lot more work and a lot more kind of people have been looking into them for now, everyone's kind of switching to that for product recommendation.
There's a lot more money in that area. So I think there are a lot more advances in those neural nets. But I think the underlying to text, the way I personally see it, this isn't like scientific factor is I feel like there's a lot more underlying in language.
I feel like there's a lot more rules underlying connections that I would think a neural net would find compared to an image. And then with those underlying values and kind of the underlying language, you'll have an easier time kind of grouping things together with a neural net.
And then if you can easily more easily group things together, the more easily you can search it pretty much. You can make these clusters a lot more accurate if things are going to already be near each other, and it's easier to find.
With images on the other hand, I feel like there's not as much of a background connection in everything. Again, all personal take, but I feel like sure an image might have the same object, but there's no like real underlying thing linking those objects. Yeah, there's the shape.
But in languages, you have a lot more than just a shape of an object. So that's kind of where I think text does have a better time. But in reality, the way I want to, when we look at our systems and everything like when we try, it always ends up being very close to each other.
Maybe like up until now, it's already approximate. So no one's really been hurt that much by half a percent of accuracy. Up until like everyone understands it's still kind of a new feel. It's kind of growing in that these methods are all approximate. Like you're never going to get a perfect end.
It's really up to the testing with your neural net to see which embeddings and optimizing your neural net and then throwing because these algorithms aren't, it's like for the approximate year's neighbor, these algorithms aren't really learning.
Sure, there is some learning with the quantization based ones, where it is kind of making its own quantization. I know for face, it does it. But again, it's like it's like a, it's an algorithm that kind of goes step by step and there's not too much randomness.
Sure Spotify does random splits in their annoy. But yeah, so I kind of have to optimize your neural net to really get the best performance. But there are some values that you can mess around with the actual approximate nearest neighbor search.
But those don't play as big of a deal, I believe, as what you're doing with your neural net. Yeah, that's interesting.
So, if I actually take a step back a little bit, can you tell me and our listeners why we cannot do exact KNN, exact KNN nearest neighbors? Why do we need to do approximate? What stops us from doing exact? So with exact, okay, so first you can get exact, but that's just going to be brute force.
Maybe all these libraries, maybe not all of them, but most of them do have a brute force search. And then you haven't solved anything. Then you can just use relational database, throw in your numbers and go by each column, look through each one, see which one's closest.
Yeah, so you get approximate, that's where you get your speed up. And then with approximate, because you're doing clustering, you assume most things are going to be embedded.
Your neural net, like if you have a neural net, your embedding layer, your vector is probably like you hope that it's going to find similarities. Like if you have two items that are very similar, your hope that their distance is not going to be far. This isn't always the case.
A neural net, if you have a photo of a car and a photo of a car with a bike in the background in might for some reason, folks on the bike, we don't really know what's going on. There's research into seeing what's actually going on behind the scenes in the box.
But yeah, these two might pop out with two completely different values. They might be in completely different clusters, even though they kind of should be similar. So that's where this, it can kind of go wrong.
And then, see, yeah, you search the wrong cluster, and then you'll miss that, even though I was supposed to be a good choice. But then there's also the aspect of not searching through everything. You want to speed things up.
You search through the top 10 matches, let's say for inverted file list, which is centroid base, the clustering. You look at the top 10 centroid. If you look at the top 10 centroid, and you find those files in there, yeah, they're going to be similar.
But there might be the 11th centroid, might be a very similar one. They're all by just a tiny bit less. And then inside it, it might have the perfect answer.
So there is like all of this approximation where you only look at top X numbers, and then also combine with you only look at, you only make so many clusters, you make X clusters. There's always going to be outliers out of bounds. So that's where you kind of get that loss.
Because for the similarity search in this, leading on this, like, will similarity search take over everything? It won't really, because sometimes you need perfect results. And similarity search is kind of useless there.
It's going to end up being brute force, and then with brute force, any algorithm really works. You're going to be looking through every single value. Yeah.
So it's like complexity wise, it becomes like big all of n, where n could be like 1 billion, right? Yeah, when you're in 1 billion, there's no problem solved anymore if you look through everything. Yeah. Yeah. Yeah.
That's why you need to go approximate, but it's not like approximate to the level of losing like tens of percent in, in, uh, percent. Yeah. It's usually, I would say it's usually like around three to the three percent.
If you're doing like a very reasonable, like speed versus a recall, um, you balance it out of it, that's where you can change the values in the actual algorithm. But if you keep it kind of balanced, usually 97% the average of mostly what I've been seeing. So it's pretty strong.
And this is like where, yeah, it's approximate when you're dealing with billions of data, you don't really, like, yeah, finding the exact for some use it use cases is very useful, but usually in the billion scale data range, you're okay with just getting a few that are very close. Yeah. Yeah.
And I mean, I've, um, so when I published a blog post about like all the vector databases, I will make sure to link it in the notes. Um, and, and Mildbus was there as well. You know, and I can use somebody said that they have been actually using no SQL database for genome related project.
And so what, what the guy said that he did is that he actually can pre computed the nearest neighbors for each individual entry. And then he stored it as, as individual items in the no SQL database.
And so as query came in, he basically kind of went and kind of asked for each item, okay, what's your neighbors, right? And, and then he said on, on, on small scale, this worked fine, but he wouldn't necessarily use this on the kind of next level, right? Yeah.
And so can you tell me more like how Mildbus is done? What is, what is it as a product, let's say, uh, and, and what's included inside? What can I get as a user? Yeah, sure.
So, um, yeah, Mildbus, we kind of built it as a database first, similar research second, where everyone's collecting a bunch of data, a bunch of vectors, everyone's hoarding all their data and they have, they're making their neural nets, they're all getting embeddings, but then like, what's next?
You need to do something with that data.
So that's where against similar research. Yeah, what we're doing is building up a database system. So right now with version 2.0, we're really working on making a cloud, uh, native, something scalable, something fast and something easy to use.
So we, you can think of it pretty much as a MySQL database and just for vectors. And then in that regard, you have the cred operations, you have sharding, you have all of this, all these operations and we're kind of building up that for vector itself.
And then later on, we're going to be building up other parts that branch off to kind of make those vectors. So we're kind of, it's this core to our entire pipeline for dealing with similarity search. And yeah, that's kind of what it is.
In terms of like the actions you can do with it, you can do storing, you can updating, as I said, partitioning, sharding, we're adding scalar filtering in right now. It's with INS, but I think this month. So in the next week, I believe we're going to be having strings for a scalar filtering.
What scalar filtering is, is being able to filter results in a fast way. So instead of searching through everything and then filtering out these certain things, you kind of apply the filter first or apply the filter during the search to speed everything up and also get more accurate results.
So with, let's say you have a vector and then there's a filter that says, glass is equal true. You can look for every single vector that has glasses equal true. And that's very useful and something that everyone's been looking for. But yeah, it's a database first.
And then for the actual searching, we employ all these libraries that we produced and mentioned, us, annoy, phase, hnsw, all these guys to build these indexes. And then you can select whichever one you want. You can use multiple.
Sometimes some will work better for images or if you're neural nets working in some way, it might work better with this one. So you can store multiple of these indexes, decide, test pretty easily, and mess around with it. And once you're done, you select what you want and you call it a day.
And you search and you get results. Yeah. Actually, when I was watching one presentation by your colleagues at Haystack, we will make sure to link this as well. Like this got my eye besides, you know, the horizontal scaling that other databases as well have.
Well, maybe not all of them, but some most of them. But, you know, one thing that caught my eye was that I can indexes, you said, the data using different index layouts, essentially different algorithms that you alluded to earlier.
And then I can somehow test and kind of figure out which one works better. Is that right? Yeah, pretty much so we do have benchmarking algorithms, but you can also benchmark yourself as well.
But the way is you can build up also every single index has its own parameters and you can just constantly build up more. You can like build up 10 of parameters change this way or 10 of just completely different indexes.
And then you perform a search with the same vector for each index because when you search, you can select which index you want to use. So you can just take that search, throw it in through every single index, see the results.
And then if you have a baseline data where you already have it labeled and you know what results you should be getting from a brute force. So when we do these benchmarks, it's always compared to brute force because brute force will give you the exact answers.
And from there, you can kind of see, okay, how many hits did I get, how many did I miss, and see what your recall rate is.
And then you can also time these things as well because some parameters, if you make 10,000 clusters within your data, that's going to take a bit to search if you want to search through every single one.
So you time it and then you can kind of get this ratio of speech performance or we usually say like speech recall. But yeah, so you can build up all of them then go from there kind of if one doesn't work, you can just delete it, it'll do it in the background, build a new one.
You can be doing these searches and everything concurrently because back to go, the indexes can be built at the same time as you're searching and doing all these other things because we have workers for queries, for building indexes and for inserting data.
So it's all kind of in the background and kind of gets dealt for you. Yeah, that's cool. And I mean, so you mentioned the technical part, you know, like different products they might have some SLA, let's say, you know, how quick it is, queit per second, P99, whatever.
But like what about the semantic part? Like you mentioned that there is like a ground truth that you can compare to always, right?
 But like what about the other kind of side of things, let's say for people who are like, let's say product managers, they're not very technical, they will not look into this metrics, but they still would like to get a way of understanding, you know, what's the kind of impact on the semantic part of things, right?
For instance, you're comparing, you know, inverted index versus vector search, right?
So with the semantic part, we don't really deal with that as much because we're assuming that your semantics are done well by the, by the neural net because this is where it kind of goes, you compare everything to brute force.
If your brute force shows that this is the correct response or this like, or this wording or this wording or this wording or the top three results, those are mathematically the closest, most similar to your input. So that's where you kind of compared to that.
If those aren't close, that means that there's an error a step above because your neural net is not finding the connections correctly.
 So that's kind of how we compare to the base, which is just the flat index of brute force and we kind of pull out and we see if you're hitting the right responses, like if that's sort of what we deal with, not the actual, because the semantics come from the same from the neural net and to find that issue is more above us like in the whole stack, that makes sense.
Yeah, yeah, for sure. So basically what you're saying is that, you know, if I take, if I fix the model, right, so the model is fixed.
And I pick, let's say, different algorithms for indexing as well as let's say, different even distances, right? In some cases, I can maybe choose different distances, right?
Although maybe you can tell me if I'm wrong here, because if I trained the model for a specific distance, maybe I cannot easily pick another distance during test, is that right? So a distance, what do you mean by selecting a distance?
Because it's all based on closest, like we will rank it closest to furthest and then it's only like top end results.
Yeah, I guess what I meant is the distance metric itself. So it could be to harming, you know, and yeah, do you correct all those? Yeah. So comparing across this different distance metrics, that one is kind of you have to look at your data.
We don't have, because yeah, if you use different distance metrics, then your flat line is going to be different. But yeah, that's one where if you're going to compare across indexes, you have to keep them the same distance metric.
Swapping them out will make some big changes, I think, because if you go from L1 to L2, or maybe not L1 to L2, there's a cosine to Euclidean. It switches up things up a bit where in some cases, one of the distances might, a higher values better, in some cases, the lower values better.
So there's no real direct comparison. They're still going to usually rank in the same order. But yeah, for figuring out which one you want to use there, it's kind of give or taking off. I actually have to look at the results for that one.
There's no real like mathematical way to kind of compare some antics to distance and kind of get the relationship, if that makes sense. Yeah, yeah, for sure. For sure, it's more like an experimentation needed there, right? Yeah. Exactly.
And also, like, actually, I just remembered when you've been kind of describing these different distance metrics. I remember a paper, I think it's called Beer. So it was comparing different methods to do the re-ranking step, right? Like dense retrieval and some other methods I forgot already.
But they actually found out that if you have documents, let's say text documents, the cosine similarity will favor shorter documents given the tie versus dot product will favor longer documents. And this is by design of the formula. The cosine similarity is basically mapping it to the unit sphere.
The dot product is there is nothing to kind of normalize on.
So it basically just takes all the components of your vector and just basically says, okay, here is the volume and just the lowest one wins, right? And that can actually impact the user experience, right? Like if I have a database, let's say of news versus some deep research, right?
So deep research is thousands of pages and news is couple of pages, maybe even just couple of paragraphs.
Yeah. So if my hits are just in a paragraph in the news and also in a paragraph in the longer document with cosine, I'll get the news. I will not get the deep research, right? See what I'm saying? No, that makes sense. Yeah.
So yeah, that's I think one where you you have to kind of test it out and see what you want because I have some people searching they might want to use some people searching they might want the scientific paper. And that's one where you look at history, I guess.
For thinking about this, let's say Google is doing it. You look at the user's history of how they search if they're searching for scientific stuff or if they're always looking at you maybe swap the index and to a different distance metrics. But yeah, I haven't thought of that too much. Not way.
That's really interesting. Oh, yeah, I need to check out that paper. Yeah, for sure. I'll send you the link and I'll make sure to also include in the notes, maybe for those of us who are interested in reading papers. And yeah, so that's awesome.
So you guys basically will for a database where users can mix and match the way they want, right? And then you help them to kind of do you guide the users in the process of doing this?
So if they come to us for help, we usually kind of we have some articles where we mess around with the indexes and different parameters and we kind of have like a graph, let's say speed performance, recall that kind of stuff where it's kind of preliminary.
We kind of hope that they kind of learn it on their own because like we can only help so many people. So yeah, we do help out. We point to the right directions. And if it's like a really interesting use case or a really big use case, we'll kind of mess around with it ourselves and try to help out.
But we also just hope that people mess around and then post the results. They see that and then like we kind of the more data we get, the more like it helps a lot with when people kind of share what they're doing.
We're trying to share as much as everything kind of get people into this, get those words spread and that's pretty much open source. Like a big deal of open source is kind of getting this out there.
Like competition's good, new innovations good, just get this like vector similarity search, getting people interested in it. Yeah. Yeah. And your website has so many use cases covered like I was looking at audio search. That was interesting.
Like you basically walk through, you know, selecting a library, how I will encode the song and I have an idea to try it out on a few songs that I have like MP3s. Yeah.
And what I was particularly interested is like, okay, is there a way to separate like the singer voice from the musical instrument from like, I don't know, the style of this song and so on. Yeah.
And it's a really cool one because this is one of the things that kind of looked into a lot and did some talks on not like really detailed, but it always popped up.
So for all these like recommendation systems, like what's I think I have never looked at like their deep everything's behind closed doors.
But it's like spotifies recommendation and then when you're doing like the shazam, I think was shazam for the audio recognizing is, yeah, they separate the background music and the vocals and they pretty much discard vocals.
This music searching is based just in the background and there were some techniques. So I think there are for separating the audio, there is like one D neural nets that kind of go in the line and there's like times used based neural nets. But another one that was audio inversion.
So it would help when you had the background where you didn't vert it or where you had the vocals to get the audio out. But a lot of it was working on that of pulling out the background music is the big step. And then performing the neural net on that to get the embedding.
So that's how you avoid if you're recommending songs, you would cover songs and you kind of avoid it for you can easily filter out cover songs because they're going to have the exact same background. The vocals will be different.
And then with these recommendation system, another cool thing is you don't want the perfect similar like with your result search result, you don't want the exact same research. It's like you don't want the top 10 closest.
You might want like the last 10 out of 100 that are close because you want something similar but not really exact of the same. But yeah, audio inversion and one D neural nets and a few others.
I don't remember that on the top of my head, but it's a hard problem to solve of getting the vocals out without having like separated files already.
And it's like an exciting topic because like, you know, like there are so many examples on the web how you can index text, you know, how you can not index text and do something else with text and more text, right?
But it's like in frequent that I come across some image search or audio or even for that matter video, you know, I haven't seen any blog posts on the video.
I don't know if you guys have it. So video, yeah, that's that one gets a little difficult in terms of like video you can, but also how you're going to sort everything out because when you're doing dealing with videos, everything is framed by frame.
So then it's how to do you take every frame and sort of group it together into one sort of like ID and then if any frame matches, you kind of point to that, it gets a little difficult with video.
It's not too bad if you're doing let's say live tracking in a video like, like to say there's a soccer player and you pull out the most similar player that looks like him, you can get a name for him to track him. So it knows his name.
That's kind of similar if you're doing live tracking, but if you're looking for things within a video like you look for a single person an entire video, then it kind of gets difficult of it takes time.
You either go through all of you index all of them or you pull out a few key frames, but not too many people I'm going to be honest are doing video yet.
I think there is a little bit of lack right now to be honest, I think images are the most used for us like everyone because images, I think, is the easiest even compared to text because the text you have, some of these neural networks where the transformer networks are a little bit hard to use.
You always have like, yeah, you have like in Python you have sentence transformed the easiest one where you just input the string, but the other ones kind of required to add tags in the string and do these things which not everyone understands.
With images, it's just import some ResNet 50, which torch makes it really simple. So the image put the image in the ResNet and then literally you get your embedding vector you can directly pipe it.
So it's like, it's a very simple one and it gets good results that are pretty interesting and you can do a lot with images and I don't think enough people are doing it yet for like shopping things. Everyone's still relying on text but let's say you upload an image or a shoe, you find that shoe.
I think everyone will enjoy that a lot more. Yeah. So it says like very concrete application in business, right? And e-commerce is a very big area. So yeah, that makes total sense.
It's not like many users are like, oh, I remember that scene in the movie, can I find it expressing it in words? Yeah, it won't work.
Maybe you can like say the actor and then like some description of the scene, but then you already have to know the actor, which personally, I don't know any actor names. So yeah, exactly. It doesn't work for me.
And it and it defeats the purpose of search, right? Because actually like early on when I was kind of just entering this field many years ago, I was like, so search, it's like I need to know what to look for, right?
So I'm typing the keywords, telling the search engine what I'm looking for, but I don't know what I'm looking for.
Yeah, you're already doing the job for it. Yeah. Yeah, like if you're searching for the keywords, that means you really need to search for the engine anymore if you're doing all of this job. Yeah. And I really loved one competition that Yandex did. Like they actually stopped doing it in its epiti.
So the competition was like this, they give you a question. And basically you compete with other people, all right? So they give you a question, but it's not like what is the color of submarine in the in the song of Beatles, right? It's like you first need to answer the first part of the question.
Then you get like kind of another puzzle, another question kind of, you know, the puzzle gets solved and you get the full question and so on. And like it's multi-layered process. So basically they're telling you that a cool search engine would be doing that.
So you could ask like a very convoluted question and it would kind of figure everything out. I feel like you might know more, isn't there the the aspect of I remember taking an MLP class.
It was always you could only get to two degrees or was it like the third degree would always like if you have a question and then like based on that answer, you have another like part of like the next part of the question is placed on the first one.
I think they were only being able to do the second degree like a question after the question. And like getting that third part, it would always fail. Like it wouldn't be able to do the connection all the way back. Yeah, yeah. They stopped doing that. Seems like a really good conversation.
And it was also based on a lot of associations, something that computers may or may not be doing good. You know, like if you know prologue, the programming language, like it basically has the associations kind of built in as first class function.
You type something like, you know, orange fruit and then you type somewhere fruit and it says orange. If you type orange, it says fruit, right? It remembered that mapping, right? And then you can use this associative kind of programming in a bunch of places kind of building AI.
But I haven't been programming prologue. I was just it was part of one course. But you know, like the questions that they asked at the Yandex competition, they were also like, you know, something like, you know, who met this lady when he was a student blah, blah, blah, blah.
And you're like, I already lost the train, the train of thought in this question. So I spent like, I don't know, maybe one minute just figuring out what is being asked. And then you're like decomposing this problem into multiple problems and you start from the first one.
And then they are solving it. The next one and time is running. It's like five minutes of question, if I remember correctly. And it was fantastic competition. You know, like it's like, I don't think search engines are still kind of on that level. I don't think they are yet on that level. So yeah.
They'll probably get there. It's those neural nets. What they're doing with them is it's going to get there. Yeah. And we have those giant networks that they're now creating like Nvidia release that nega something GPT.
What is it three right now? Was it yeah, GPT three is the one that they're not releasing. No, it's going to get there one layer another.
So does it make you excited like to try these models in real life? What do you think they kind of make too far still, too far from real life? What do you think it makes me excited? And I think they're doing really well.
It's just that this whole trend has kind of been going to like not user friendly. No one can run any of these models and need like nine a 100s like $100,000 worth of computing who has that like other than those places that are doing that what GPT three took.
I don't know how many billions and billions parameters. No one can run that unless you're like at some super big company. It's like my opinion is what's the point like you can you can always throw more and more hard door at it and you can always get like 0.001 percent closer and closer.
And that's kind of like this whole thing of why like this area of research is on. It's kind of the new thing like we've already kind of maxed our selves out on neural nets. I personally believe unless there's some huge architecture changes that inspire some really interesting stuff.
I don't see neural nets changing as much and then I feel like we can do more with the next step of the nearest neighbor search in this approximate nearest neighbor vector similarity search. And I think that's where we can make some more new head games until we max this out. But yeah, I don't know.
I'm excited to see where it goes. It's just I hope it's going to be something I can try myself and not need to spend $400 on Amazon for hours worth of calculations. Yeah. So maybe somebody needs to work on compressing this model.
So sort of compressing the compute power they actually, you know, require. But now the thing is now it's like everyone's interested when they say, oh yeah, we use like 40 million dollars with the compute power. Everyone thinks that's cool. That's going to get some news.
And people are going to be interested in it. It's when it's bigger, but there is I don't know. I remember when I was studying all this, there was a lot of those things in regards to sparse neural nets and that was kind of going to be the future of compressing all this down.
Fortunately, I haven't really kept up on it too much, but hopefully they do make moves because again, I don't see throwing more and more hardware as innovation compared to actually making it efficient. Yeah. That's my take. People are doing it so there's there's reason to do it. Yeah, for sure.
It's like I guess the the hope of researchers is to essentially kind of emulate human brain, right? But like, I think human brain has like a hundred million neurons, so even more, right? I don't know. Like, yeah, it has a bunch of neurons and then connections that we have is. Yeah.
Yeah, we're not, I don't think hardware wise we're going to be close yet. Unless, I don't know, I know they're doing a lot of research in this area, but I definitely don't think the neuron or, yeah, I don't know. This is the way I estimate that. Yeah, yeah, yeah.
But like, just to close on that thought also that human brain does not kind of use the energy of a server farm is just like one electric lamp or whatever. Yeah, even less efficiency right there. And that efficiency. A couple of kilos and that's it, right? That's the device.
And then we can do all of this. Yeah, there is a long way to go. Yeah, yeah. And exciting though. Yeah, yeah, absolutely. But actually, today I've learned from my colleague Arnitalman. I will make sure to also ask him to give me the link to this paper.
The paper said that if you take a model like bird, bird will not be able to distinguish the negations. Have you seen such research, which is very amazing. Like for the bird power, not to be able to distinguish negations, that could be like deal breaker in some cases.
Even though it is very, bird is very kind of powerful model and probably Google is using it like for a few percent of their searches. But to know that it doesn't know to distinguish between negated or non negated phrase, that's interesting.
And that brings more questions in where some languages have double negations, where matters, where it doesn't matter. And then that's like where it's like a double negations in some languages are used quite a bit and they really change the meaning.
So that's where it's like, what do we do there? But I wonder why. I guess I wonder if that's something we know why or if that's something just a black box of mystery does something there.
Yeah, because if it was a rule based system, you could claim that, hey, I have the rules here, right? And I've managed to encode negations. I know what they are. And maybe you run out of all possible combinations, then you add another one and another one.
But in bird, you don't do that, right? You mask the text and you train it. And that's it, right? Yeah, you didn't tell what is negation. You didn't tell what. And you can also argue, yes, we also in our human brain, we don't have syntax, right? But there are some studies like that.
Like, for instance, when kids learn to speak the language, they don't know what is negation, right? They don't know what is the syntactic structure or pronoun or whatever. They just speak, right? And so we probably don't use syntax in our brain either.
Like we use some semantic grams, I don't know, something like that. Yeah. It's an exciting topic. So, and if we go back to Milbus, so you guys basically essentially have built support for a number of indexing algorithms, as you said, right? Yeah.
And can I, as a user, also plug in my own method? So we're currently working for those plans. Right now, it's kind of blocked off. Or like there's a bunch of changes you have to make deeper in.
But we are working towards doing something along those lines where we'll have a system where you can kind of bring it in. But we're also trying to add like Google scan and we're working on that disk and it's like we're already like working on putting all the main ones in.
But right now, you can't really do your own. And there's also another question that comes up a bunch of the time.
Is it using your own distance metric?
And that's one, unfortunately, you kind of lose, like you can do your own disks metrics, metric, but it would require you to only use flat because all these algorithms are kind of, the reason they're efficient is because they can do some of these distance metrics.
And like, let's say with quantization, like it kind of plugs and plays together nicely. When you try changing those things, everything breaks and you kind of have to revert back to doing a flat-based system.
But yeah, those are the, yeah, for now, we're trying to add in all of the most famous, or not most famous, but just state of the art near-snaper searching algorithms.
And then later on, hopefully, we can kind of make this thing where you can kind of code it out yourself and kind of plug it in and make it work. And also, Milbus, like, pays a lot of attention to scalability.
So for instance, like horizontal scaling, you know, is probably vertical, right? So, but for instance, one thing that I've been thinking about is that in the whole infrastructure and the pipeline of a search engine, one of the bottlenecks is actually getting the data, right?
So the data comes in, let's say in raw format, I don't know, news items, images, what have you.
Now you need to compute the embeddings, right? At some good, good rate, right? So kind of throughput. Yeah.
So do you guys have any work done in this area or kind of recommendations for the users?
So recommendations right now are what we've been using is when we recommend is like having a server, like there's in videos, try to in there's a few other ones of inference-based servers, and you scale those up themselves.
We are currently working on that, like we're calling it OE. And it's kind of the ML pipeline scalable that goes, that's focuses on embeddings. So like kind of doing all these things about embeddings, everything embeddings, and making pipelines that can scale in multiple machines and multiple GPUs.
Still in the work of progress, it's pretty early stage. That's what I'm currently also working on. That's going to be that, like there are people that don't know that step and that don't know what the best process is, and we're going to be, it's also open source.
So it's going to be the step ahead of millivis. And then we're going to, as it progresses, kind of interlink it with millivis, kind of make an easy plug-in play together. But for now, it's all about kind of scaling up inference servers. Luckily, you can scale it pretty easily.
When it comes to videos, where frame order matters, it's a little different. But yes, we kind of, for now, with millivis are only that storage and search part. Everything above it is up to the user. Yeah.
And millivis, do I only store the vectors, or can I also store the input object? Right now, no input object. And that was kind of like, it slows things down a lot.
And that's where sort of a no-skill database or something like that would work a lot better for those quick retrievals, where you need exact. So we, for now, store the ints, and then we're going to store strings. And then later on, we're going to add more and more types that we can store with it.
And another thing, we're hoping to be able to also index on strings on indexes. So you are on ints. So for now, we don't store the objects. So in the future, when we have string, you can link the file path.
Because that's usually what most, when you store an object, you're just storing the file path. But yeah, object storage is not part currently on in the millivis server. You just get an ID and a vector. Yeah, yeah.
And then you go back and kind of link it to the other days, where those objects are stored in case you need to display them or something like that. So in that sense, you guys are like a pure vector database, like you store. Exactly.
Literally the vectors plus, I guess, the scholar values that they can filter on, right? Yeah. Yeah. Exactly. Oh, that's awesome. I think that covers a lot of use cases, isn't it? Doesn't it? Yeah. Yeah. And I was also thinking like, so millivis is open source.
And it's one of my favorite also questions. You know, like, can you speak more why, why millivis is open source? What do you get from it being open source? So I think the biggest thing right now is with open source, like, you need open source to kind of get this idea out. So Dr.
search, if you close source it, you don't really know what's going on. Nothing like something like you don't really get the info that you want.
And that doesn't spark competition because if you're not in there already, you need to find out everything yourself, kind of do it all to just catch up in that that's sort of going to take a long time.
So with open source, it kind of promotes this competition innovation because everyone can see what we're doing. You can see all these algorithms, you can learn how a vector search works. And then people can kind of branch off and do their own.
Sure it's competition, but the only way you're going to get more users familiar with this and knowing about vector search is just get as many people doing it and get as many people trying their own routes, get as many people building up their own systems and just kind of get it out there.
That's like what we want. And I think like that's the biggest way you can do it. If you open source, everyone can see what's going on, learn from it and just go ahead.
And then also with open source, you kind of get feedback from everyone from all different areas, from all different like, you can be a student working on a project who has some great idea. Like he's not some company.
So if you're like sometimes close source, if it's not bringing money in or something like that, no one really really listened to that small student and his idea. So where he might not be able to use it.
So it's just about getting more perspectives on it and getting more input and kind of making it accessible to everyone and sparking that competition innovation. Yeah, actually you brought a very interesting topic. I didn't think about it that way to be honest.
 And now that you said it, it's very logical that it may as well be a competition between some users because they are using the same tech and they have different use cases or maybe the same use case, but they are competing like to get that last sort of percent of precision out or whatever you're doing.
But also at the same time, you know, like when you look at open source projects, like I don't know Apache Software Foundation for that sake, you know, when you go there and you ask a question, you, first of all, you don't have to say the company that you work for, right?
Or maybe you are that student that you said.
And, and you know, you just focus on the matter, right? You focus on what is it you're asking about? And then if somebody is so curious, even if they're competing over the same thing, they might kind of casually share something, right?
I mean, that's what I've seen in the in the mailing lists a lot.
Like users just some of the other users, they just come in and say, Hey, why are you doing this? You know, did you consider something else? And you're focusing so much on solving a specific problem? Yeah, I think it's just, yeah, it's kind of in like with competition, there's innovation.
And then with innovation, you get more people interested. And I think that's kind of like what neural nets did is started out. I don't think everyone was using it. Everyone was just using some brute force tech search of keyword matching.
And then as people learned about it more, there's open source systems. And I think a lot of these neural nets, if you're going to be making neural nets, you're going to be doing research on them. You're going to be posting those papers. Everyone's going to see it filled on top of it.
And we'll just explode. I think now that's happening with these Bert models with Huggins face all of this. It's just exploding and more people are looking into it. And it's just it's better for everything at this point. So that's kind of why we do it.
There's a bit of my opinion company motto, but that's kind of, yeah, the reason. Yeah, it's kind of like a compound effect of multiple inputs. And then everyone essentially has the same goal is to serve the users the best. Or maybe solve that specific problem they're solving.
Maybe even for themselves. But yeah, I mean, that's very interesting. And how do you, so basically you have Slack where I can go and ask my question. Like how do you kind of balance your time between kind of doing the actual work and helping the community? So that's a hard one.
Right now with Slack is it's people that come to us. So because this area hasn't blown up so much, it's still manageable. I'm thinking the future when you get to like levels of these other open source projects where they have like 20,000 people and they're slack all like posting questions.
Right now it's pretty manageable and you can kind of keep on top of it. And yeah, Slack, we made like a discourse. We kind of made a lot of like preliminary like areas that you could talk to us. And then yeah, there's GitHub issues all that.
But it's also there's another aspect of like kind of splitting up the problems. Because if you open up a Slack, people might post their technical problems there. Or like there's something that might be worth being a GitHub issue.
The people that are looking at the Slack majority of the time are more like user success style, not like full-blown R&D deep engineers. So that's where the, that's where I think the balance comes out where the problem is of like what belongs on Slack, what belongs on GitHub as an issue.
But for now, all of it's easily solvable because it's a steady inflow that we can manage. And we have enough people looking at it. But we'll see in the future that's going to be another problem to deal with in the interested to see how we do it.
Yeah, it's like both catastrophic success, right? Exactly. It may happen. But hopefully it will be manageable in your case.
And so you can, you can as said kind of cater to that community as well as actually keep solving the and keeping your roadmap under control because you also need to keep, you know, innovating in this space, right? Yeah, I'm glad it's working for you.
And I've been also slacking a bit kind of, the slacking is not the right word, but slacking with big ads. So just kind of, I so immediately answers to my questions and they've been like a long thread.
Why Docker doesn't work, can you try these? Can you try that? And it's also like, you know, it's like a first impression you get about the database or about the Nopus source product. Like how soon you get an answer? Yeah, and you definitely try this sometimes.
Also like you're working on one problem and you have another problem that's completely separate. It's like it's a big system. So jumping around between, but then it's also okay. Let me find someone to answer that for you. So you go internally look for someone.
Hey, can you answer this? But hopefully it's working. I think, I think we're pretty quick on our responses. Maybe like overnight, it's sometimes difficult to sleeping in everything. But we try to get responses whenever we can. Yeah, some people are like in China, I guess. So like, I don't know.
It was like five hours difference with my time zone and sometimes. With yours, yes, probably five. Mine is 14. That's like you ask one question for a couple of days, right? Yeah, it gets interesting with the technical like very deep questions, because then I have to kind of bridge the gap of time.
Try to find solutions on my own to why it's going wrong. But then also once five o'clock hits for me, I can pull in the external knowledge from the other team. But it's fun. This needs to be solved with vector search. I don't need an exact answer just in the approximate, but faster.
Oh, yeah, we were working on that. We're trying to apply it to like a chatbot for all the problems that you have. It's been working okay, but we're working on it. Trying to get more questions, kind of build up a data. So that's the issue with everything. Just building up that data said.
Yeah, absolutely. So that it will make sense for the chatbot to kind of, because chatbot wouldn't create answers. Well, unless it's some generative model. Yeah, the GBT3 for our questions. Like a story out of it. Yeah, but then it might be hallucinating as well.
In some cases, it's okay though, right? Yeah, one of the ten is the correct answer. The other ones are all just like burn your computer. Yeah, if you want to have fun with, you know, you don't need an exact answer. You just, okay, hey buddy, how are you doing? Yeah, so yeah, that's fantastic.
So I was lovely moving to to why section, even though I didn't say all the sections, but we kind of mixed what and how together in many ways. And you handle it really, really well.
You know, the why the why question that I really like to ask, everyone on this show is kind of what motivates you to be part of vector search development today? I think for me the biggest thing, I want to over a few times is everyone storing all this data. And like it's so like it's a huge amount.
I like all these companies. And then just the next step, I want to see what we can do with it. Vector search is one who knows. Maybe vector search might not be it.
But in that chase for figuring out vector search for like perfecting it, something you might pop out and kind of ride this wave of what's next. And that's kind of like why I really like vector search right now. I get to learn about all these things.
I still get to like throw my ideas into it and still kind of have them matter. Like the previous like the way that's past already, it's kind of gone to the point where you really need to have this deep, deep knowledge to actually be able to innovate.
You do also with vector search and all of these things, but it's a little bit more fresh. So more that sort of makes sense. Like I like to I want to ride that wave of freshness and kind of the next step of dealing with these huge data amounts. Yeah, that's that's that's amazing.
And also I think I've read somewhere on on to either one of the founders of Y-combinator. He said like it was an essay. He said like when you are on the bleeding edge of doing something, then you you automatically become the expert in that field.
And if something works for you, you know, the rest of the market will probably try to copy. If it didn't work, then probably everyone else didn't figure it out because you are the bleeding edge expert, right? You are right there.
And if you will figure if you will figure out something very interesting that will be kind of revolutionary in some way, then you will be first to possibly capture the value, right? And so you work for that goal.
On one hand, as you said, it motivates you to unlock the, you know, the silo database is kind of of data, unstructured and structured data. On the other hand, you said maybe it won't be vector search, maybe it will be something else because you are in that experimental mode, right? Yeah.
Whereas you can easily quickly transition and kind of keep that knowledge, keep it going and keep it running. Yeah, that's pretty much for me why I'm doing this. It's been really fun so far. Also startups. I like it. I like the multi-hat. Kind of just do it. Try it by fire and just get it done.
Yeah, yeah. With the money where you mouth, well, how do you say it in English? With the money where you mouth is? With the money where you mouth is? Yeah, like something along those lines.
Yeah, but I mean, you basically, instead of just kind of blogging or saying how cool it is, you actually go and try to apply it to some real use case, right? Exactly.
And if I may ask you, like, do you think that something kind of tactically or strategically is missing right now in the vector search space? Like maybe on the lines of how we explain it or maybe there are some untapped use cases or something else that comes to you.
I think I think the big thing right now is I may be wrong on this one. I kind of might be explaining it weirdly, but like having a standard, like we don't really have a standard for any of this yet. And there's a bunch of things kind of popping up and everyone's going to be scared to move away.
So like, I feel like the last to search. I don't know too much about the history and like what's going on, but like, everyone's a bunch of people have built up their system on that and it's kind of been a standard for doing that text-based keyword searching and that kind of stuff.
And then when we say, oh yeah, do word embedding. So we'll make everything improve. Do this, do this. But it's like, there's no standard in any of like, we're doing vector database. Some of the people doing vector search with database attached.
We're like, everyone's kind of just doing some like, there's no big thing there that keep people kind of try to make it similar. So yeah, there's no standard, which I think is kind of an issue. And it's going to hurt everyone in the long run because there's no standard.
People won't be as excited to try it out because there's too many options. Why switch is too much of a pain. I don't know if that kind of made sense, but that's sort of what I'm seeing as an issue right now. But it'll probably solve at some point. I think naturally that happens.
I guess explaining it could have seen from my previous explanation of trying to explain a vector search. It was all over the place. But it kind of gets hard to it's a step. It's a jump and um, yeah, not everyone will know like similarity like cosine distances.
Like you need to be sort of involved with machine learning. I think the best way of around that is just making full pipelines for people where you just put an image in you get your result and then go from there and then from there on they can start messing around with it.
But uh, in time, everyone I think will have that and everyone's working for that.
Yeah, I think what you said makes a lot of sense and thanks for bringing up this topic, you know, standardization because um, on one hand it basically points us to think that this field is still fragmented, right? I've, I've blogged about it.
I had six databases and then one evening I get a comment on the blog that hey, we are the new key on the blog. Can you add us? That's the seventh database, right? So how many more there are? Probably, yeah, dense. I don't know. Oh yeah, always popping up. But it's like it's good for innovation.
It's just like we're competing against ourselves. Like we're competing in everything but no one else really cares. Like we can all compete against each other but the people that are actually going to use are going to be like look at this mess. Why would we go into that area? Yeah.
And that there was actually, you know, if you know the relevancy and matching slack, I don't know if you're on it, it's like a community of all search in 3DS consultants. I think I am. Yeah. Yeah. Yeah. It's it's it's fantastic place. I'll also make sure to link it in the notes.
And there was like one very interesting piece on like actually touching on what you just said, you know, there was like a heated discussion on like how should we call it pre-filtering, single-stage filtering, something filtering.
And you know, like when you invented that, you go to your users and you say, yeah, today we released a single-stage pre-filter after filter filtering. So please use it, right? And then some other company comes and says, no, we invented another one.
It's called after pre-filtering single-stage with double sub-stages, you know, I'm just making it up obviously. Exactly what you mean. I've exaggerating, right? And then that's what you said.
Eventually it will hurt the users because they will say, oh no, no, no, I have that single-stage filter after filtering. I will not go and switch it to another one, right? Yeah. No, exactly then. It's just but I think it's it's all young. I'm like kind of new to this whole field of how things work.
But I think it's natural for these young someone like everyone's going to race at the top and see. And you just got to do what we got to do. But it's interesting how it's all going to play out if there is going to be more communication between everyone if there's not. I don't know.
It's might be a little bit above my what I'm doing. But we'll see future awaits with all of this. But I still feel like in the end of the day, you really need to focus on the users, right? You're not focusing on inventing a new turnbook or a new dictionary for vector search.
Eventually it will be published, by the way. I'm sure there will be so many terms. It will be published. But like we need to work to that. Yeah. No, I agree. 100%. So hey Philip, like it's been really great discussion.
I was thinking like, would you like to announce something to the users of Milbus or maybe those who are not yet using Milbus, but they would like to try it out. Yeah. So we have Hector profess get involved. And it's a pretty easy one to check out and see how our system works.
And we are releasing a general release candidate. So pretty much are kind of tried and true. Milbus 2.0 coming month. And you also mentioned that other system toy, you said, what is it about? And when will the yeah, Toby is a ML pipeline software kind of simplifying mainly for embeddings.
And it's all about embeddings and kind of making these for you. So it's pipeline system. Everyone can operate and everyone can upload their solutions if they want to download them.
And yeah, still in the working progress, but look out for it because I think it's going to help in a lot of these areas. Yeah, that's super cool. That sounds very exciting, you know, to kind of take this package and kind of plug things in and try it out for real on the real day. Exactly.
That's fantastic. Thanks for doing this. We'll make sure to also kind of mention this in the show notes or link if by the time it's there. Yeah, awesome.
Thanks, Philip, so much for your time for going so deep with me on on even philosophy behind neural networks and then sharing your ideas and thoughts. Thanks so much.
And I hope we can make another episode at some point down the road if you're open to it, especially as the company materials and the product materials. And you get more use cases. So I'm looking forward for more blog posts as well. Awesome. Yeah. Yeah, thanks for having me.
It was a really fun discussion. Thanks so much, Philip. Bye bye. Bye.