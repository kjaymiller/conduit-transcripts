---
description: '<p>Update: ZIR.AI has relaunched as Vectara: <a href="https://vectara.com/">https://vectara.com/</a></p><p>Topics:</p><p>00:00
  Intro</p><p>00:54 Amin’s background at Google Research and affinity to NLP and vector
  search field</p><p>05:28 Main focus areas of ZIR.AI in neural search</p><p>07:26
  Does the company offer neural network training to clients? Other support provided
  with ranking and document format conversions</p><p>08:51 Usage of open source vs
  developing own tech</p><p>10:17 The core of ZIR.AI product</p><p>14:36 API support,
  communication protocols and P95/P99 SLAs, dedicated pools of encoders</p><p>17:13
  Speeding up single node / single customer throughput and challenge of productionizing
  off the shelf models, like BERT</p><p>23:01 Distilling transformer models and why
  it can be out of reach of smaller companies</p><p>25:07 Techniques for data augmentation
  from Amin’s and Dmitry’s practice (key search team: margin loss)</p><p>30:03 Vector
  search algorithms used in ZIR.AI and the need for boolean logic in company’s client
  base</p><p>33:51 Dynamics of open source in vector search space and cloud players:
  Google, Amazon, Microsoft</p><p>36:03 Implementing a multilingual search with BM25
  vs neural search and impact on business</p><p>38:56 Is vector search a hype similar
  to big data few years ago? Prediction for vector search algorithms influence relations
  databases</p><p>43:09 Is there a need to combine BM25 with neural search? Ideas
  from Amin and features offered in ZIR.AI product</p><p>51:31 Increasing the robustness
  of search — or simply making it to work</p><p>55:10 How will Search Engineer profession
  change with neural search in the game?</p><p>Get a $100 discount (first month free)
  for a 50mb plan, using the code VectorPodcast (no lock-in, you can cancel any time):
  <a href="https://zir-ai.com/signup/user">https://zir-ai.com/signup/user</a></p>'
image_url: https://media.rss.com/vector-podcast/20220216_040237_4d74468969220e3376998953833bb185.jpg
pub_date: Wed, 16 Feb 2022 16:14:37 GMT
title: Amin Ahmad - CTO, Vectara - Algolia / Elasticsearch-like search product on
  neural search principles
url: https://rss.com/podcasts/vector-podcast/393967
---

Hello, vector podcast is here and today we're going to be talking with Amin Ahmed, co-founder and CEO of the company called ZIR AI.
I'm really, really excited to talk to Amin because basically he's innovating in this space, his company is innovating in this space of bringing vector search to practice and also making it usable. Hey, I mean, how are you? I'm doing fine. Thank you. Thanks for having me. Awesome.
Thanks for joining. And I know it's almost like festive times, so it's probably quite a packed schedule for you otherwise as well. So yeah, I was thinking let's traditionally start with the introduction.
Like, can you please tell me a bit of background before ZIR AI and OZIR AI is a startup and you're rolling at ZIR AI? Yes, sure. Me and my co-founder, we started ZIR AI in 2020. Before that, we were both working at Google. I had been there since 2010.
I worked in Google Research, focused on NLP and language understanding with machine learning. Prior to that, I had worked many other places in the industry. So I've been in the industry about 24 or 25 years now.
And around 2017, the team that I was working on in Google Research actually became known for Gmail Smart Reply. If you remember that feature. Yeah, that's an excellent feature. The moment I saw it, it was like, wow, that's fantastic. Yeah. Yeah, and it was impressive.
And I would say maybe it was a very practical application of NLP that went, that was deployed on a very large scale. So that was the research group that I was a part of. It was under Rakers, while that was developed in collaboration with some others.
Anyway, around that time, I became very interested in using neural networks for more general purpose information retrieval. And I specifically formulated this question answering over a large corpus. And at the time, I mean, Bert, when it was released a year later, changed this idea.
But at the time, a lot of people would approach a machine learning problem from scratch. It would take a completely uninitialized neural network and then try to train it. And when the models get big and deep, mostly you don't have enough data for your task.
And it also, you know, that doesn't jive very well with if you think about how humans approach a task.
If you ask me to answer a question or to read a message from a medical textbook, I may not be a doctor, but my understanding of the English language will allow me to get some of the information content from that passage.
So in the same way, I was thinking that if a neural network is truly understanding language in the way that people do, it should have this property. And it should be possible to train a general purpose neural network that without fine tuning in a specific domain can also work reasonably well.
So I set out to build this thing. And that was my research program in 2017. And we were actually able to launch the first iteration of that model in a product called Google Talk to Books. So to, and I'm saying this to my knowledge, I would love if someone corrected me in the comments section here.
This is Google Talk to Books is the first large scale end-to-end demonstration of a neural information retrieval system. So it is a search over a corpus of around 200,000 books from the Google Books corpus. But it's done entirely with vector search. And I'm not aware of anything before that.
So the neural network is very important here. I, not part of the team that conceived this idea and I was not actively working on it. They had a neural network which wasn't producing good enough results.
And we put in this more general purpose question answering your network and the results dramatically improved. This was basically the first rollout.
But then what I observed over the subsequent years was that I was able to take exactly the same neural network and apply it in at least six different products within Google. And this is what convinced me of the business value of what had been demonstrated here.
This could actually improve metrics and products used by millions of people. And so this was essentially the genesis of the idea of the ZRAI.
We started me in my co-founder in 2020 and the objective is to provide something like elastic search or algolia, except using the principles of neural information retrieval. So as you know, elastic search and algolia are based on the BM25 algorithm fundamentally.
So yeah, so that's what we've been doing for the last two years. Yeah, this is fantastic.
I mean, it's fantastic also that you bring your experience from such a large company innovating in search, right? Over to, you know, the rest of the world essentially, right? So that I believe your goal is to apply this with as many clients as possible.
And are you focusing mostly on NLP at the moment, natural language processing? Yeah, so well, we're focused from a customer's perspective. We provide a tech search solution.
Now, one of the beauties of embedding based techniques is that in your network, you can go beyond text and you can embed images, video and other types of media into a common embedding space. So that is where this company will eventually go.
But my roots are in NLP and I think that tech search by itself is a large area that takes an effort to do well. So that's where we're focused initially. Yeah, that makes total sense.
But as you said, you know, vector search is not kind of constrained by the application as long as you can embed it, right?
And plus all these multimodal scenarios where you can combine, let's say, your camera pointed at something and then you're talking to it and then you can kind of get some textual matches and suggestions, right? So that could be a very rich experience.
Right, right. And that particular application is actually achievable now, even in an all text platform, if you feed the transcripts in. And these neural network approaches tend to work especially well with natural speech, both as query input.
So this is why they're often used in technologies like Assistant or Alexa. Because people, when they speak, it's obviously much different than when you're typing keywords in a search box with your keyboard. But then also when searching over natural language text like transcripts. Yeah, absolutely.
And when you say neural networks, you know, some of them, let's say, vector database providers and vendors on the market, they give you sort of this machinery. You can plug in some models. They also have some models available, let's say, from hugging face.
In your case, in case of ZRI, are you innovating in this space of creating this neural networks for your clients? Yes, we are approaching the problem holistically. So we're, you know, the vector database is one critical component of a neural information retrieval system.
But there's other pieces, for instance, like the re-ranking piece or the neural network that produces the embeddings. And all of these need to work in coordination and tandem. Ideally, when they do, you can squeeze a lot more performance out of this system.
So yes, our focus is on, we even handle data ingestion. It's not a big area of focus. But the reality is that you have to make your experiences as easy as possible for widespread adoption, I think. So we allow our customers to just shovel in, you know, PDF documents and all kinds of other formats.
We perform the text extraction. We perform the segmentation of the document. And we actually do the encoding with the neural network, build the vector database and then handle the serving as well. Yeah, so it sounds like an all-around solution.
And I mean, it's very typical, you know, in some sense kind of to bring some algorithm or some idea to the market, but like it doesn't have any connectors. Okay, how do I feed data into it? Or maybe there is like a simple demo. And yeah, nothing beyond that.
But it sounds like you are taking the kind of all-around approach.
And have you been looking to implement everything yourself or are you also kind of reusing some of the open source pipelines, you know, like for example, for embedding or for document conversions and so on? Yeah, we are using open source as much as we can and where we think it makes sense.
So for instance, for content extraction, there's a Pashitika, which is a very good framework. But then there are certain document types for which there are better alternatives out there. And, you know, we've had certain customers for which PDF extraction, for instance, was a priority.
And we discovered some shortfalls with Tick-N-We went and we researched and found there's better alternatives out there. And so we've got those implemented. But we didn't write a PDF extractor from scratch, obviously. That's too much for a two-man company to do.
So yeah, we're trying to really combine the best of breed in every area and create a cohesive system that just works out of the box quite well for a broad range of use cases. Oh yeah, that's awesome.
And it's also great to hear that you reuse open source software, you know, at least initially or maybe you can find two minutes, so to say. But yeah, I mean, also that's amazing because you can quickly kind of build your product and focus on the goal.
Yeah, and now that we approached this more kind of closely, can you actually describe what is your product today? So as a client, what can I get? What can I, what kind of support you also provide? But first, can you start with the product itself? Yes.
So to describe it abstractly, and then I'll explain very concretely what I mean. I would say that we're a cloud platform as a service for text retrieval or text search. So the way it looks is we have two main APIs, one for indexing content and the other for running queries on the content.
So an organization would come and they would index a large amount of content. They might index periodically or incrementally as well over time.
And this would accrete in an index and then subsequently they would come and they would run generally natural language text queries against that corpus and we would return the best matches. So what we actually provide and how that looks on our platform.
So we, you essentially, you know, you come and you sign up just the way you would sign up for an AWS account, you're dropped into an admin console. Everything you can do in the admin console can be done through APIs. We're basically focused on again, a platform.
So we're accessible through GRPC and rest. The platform, the console is basically to allow you to, you know, point and click and quickly experiment and discover the value of the system.
Because our vision was that within, within 15 to 30 minutes, someone from an organization should be able to come, drop their documents into the system and determine whether or not it's even going to meet their needs.
And then if it does, they can consult the documentation and learn how to use the APIs and get a proper integration going. So we organize collections of documents into what are called corpora. So one corpus is essentially, it's a customer defined entity.
It groups related documents that they want to search together as a unit. We allow, you know, the customer to define any number of corpora, there's limits depending on the account type. And then you can essentially drag and drop the documents into the web browser, into the corpus upload.
It'll be, there's about a seven minute latency. And then you can start running queries. And when you run, we have a hosted UI that makes it easy to see the results kind of on the spot in the browser.
But when you run queries through our interface, you also have our, our, our API is, you also have the ability to run one query against multiple corpora and merge the results.
So we also support the ability to attach metadata as your indexing content, attach metadata that then is returned to you in the search results. So that would allow you to, to join to let's say another system on your end. But those are, those are some of the features that we provide. Yeah.
So it sounds like it's a self service system, right? And so if I was a client of yours, I could like get a subscription trial subscription maybe then upload my document corpus.
How big a corpus could I upload on a trial? Do you have any limitation there at this point? So our general trial has been 15 megabytes of text. And I'll explain what that translates to. I was, I was, I was just working with another customer.
And they had about one gigabyte of PDFs that we put into a corpus. And then that turned out to be about 48 megabytes of text. So the, the billing is by the actual extracted textual content. So 15 megabytes is actually a decent data set, several hundred documents you can imagine.
So, but we have, we have plans that go much larger and we have customers that are indexing far more data. Yeah, yeah, sounds great. And then what happens next? So let's say I'm happy. I want to move forward.
Now you said that there are APIs that I can start kind of introducing inside my prototype or my existing back end. Is that right? Yeah, that's right. So we, we support primarily we promote a gRPC interface because it's high performance low latency. We also do have a rest interface.
We have fully authenticated APIs. So we use OAuth 2.0 that standard. So you would give credentials to your servers and they would use those credentials to establish an authenticated session with the platform and then run, run queries for you at a very high rate. We scale horizontally.
We can go up to hundreds of QPS, though we haven't had a customer that's needed such a high rate, but we're capable of that. Yeah, yeah.
And you also mentioned that you maintain certain like SLA guarantees like P99 latency can you speak a bit about that and also like how much of that accounts for client need versus what you are building for the future. And this is a good question.
So in terms of client need, we really haven't had any client that's required anything better than 200 milliseconds. Now there's a potential client that we're working with. They're not yet acclaimed.
They're looking for more like 50 to 60 milliseconds because essentially the look up into our system is only one part of their overall request handling process. So they have a much tighter budget.
In practice, what we're seeing on our platform for our customers today aggregated over all queries is a P99 of around 130 milliseconds. Our P50 is about 60 milliseconds. And this has been sufficient for our customers.
For customers that have tighter requirements, we actually have many different ways to address it. So actually the main latency is not from the vector database. The vector database is generally quite fast. It's the neural network that has to do the text encoding. That's the bottleneck.
So we have the ability to set up dedicated pools of encoders, neural networks that do this encoding of four customers. So we scale and we're cost efficient by sharing the pool across all customers. But for customers that have very stringent needs, we can set up dedicated pools for them.
But even when you go, let's say single customer, single node, maybe GPU node, there are still theoretical boundary to how fast it can be.
Let's say if I take an off-the-shelf birth model, and if I throw 768 dimensions, what's going to happen? How can I fine tune it on the speed size? Yeah, well, let me address two things that you said there.
So the off-the-shelf birth model is a very common approach that many companies are trying to productionize NLP. They use it because birth has a phenomenal accuracy. You fine tune it with a little bit of data. And everyone always hits the same problem that is very difficult to productionize.
And even at a place like Google, they didn't productionize birth. They had to distill birth and productionize it. And distillation requires a lot of expertise. It's out of the reach, I think, of most companies.
So as good as the results look in a staging environment, that's not really a practical to productionize that.
And that comes back to the original point that we tried to make the right choices where if we were deploying birth, either it would be enormously expensive for us because we'd have to be using GPU instances or TPU instances, or we would have very high latencies.
So we have a model that produces similar performance, but it runs much faster. It's still transformer-based.
Coming to your second point, I think your main question, your original question, was actually what's the theoretical limit of performance that we can achieve in terms of are you asking from a latency perspective? Yeah, a latency. So I'll say this.
When it comes to the vector database, you probably know this better than I do. If it's indexed and quantized correctly on our last stuff, even running on CPUs, you can get down to three, four milliseconds of latency.
It depends on so many trade-offs, like how much recolor you will decircify and other things like that. What are the dimensions of the vector? But I think that we found that to be quite feasible for our system. We don't do 768 dimensions.
Our neural nets produce a little bit less, but still it's comparable. It's not that far off. In terms of the neural network, I would say that transformers are required for proper language understanding.
One of the things I didn't mention about our system was I think that we were basically one of the first teams back in 2017 to incorporate transformers production architecture. This was my colleagues, Noah Constant. He was actually one of our colleagues.
Previously being in our team was on the original transformer paper. He was in Google Brain at that time doing that research. We wanted to productionize a plan to a model. Noah basically spent a couple of months, took that research level code and got it to production quality.
Talk to books is actually being powered by a very early transformer based model. We saw an enormous performance jump in our metrics, doing nothing other than switching to transformers. I've never seen such a big jump in any... Our metrics, we were looking at F1. Our F1 jumped from 30% to 38%.
Just by switching to transformers. Not changing the training data or the evaluation objective, just making this one change in the architecture of the neural network. I would consider that's an absolute requirement.
I would also say that I'm not very familiar with the economics of GPU scaling because it's generally kind of expensive. Our neural networks are actually designed to run reasonably well on CPUs. There's also these tips like obviously Google's got the TPU, but Amazon has Inferencia.
We're still kind of experimenting with what we can do with latency there.
I think that you can count on about 20 to 30 milliseconds of latency at the low end from coming from the encoding process unless you start moving to GPU or something and then you might be able to do maybe 5 to 10 milliseconds.
If you put that all together, it seems to me realistically you can shoot for 30 to 40 milliseconds would be pretty aggressive in terms of what you can get at the lower bound. And maybe for many companies out there, this will be okay.
As long as they don't run web scale type of deployment, maybe they can scale per region or per zone or whatever it is that makes sense to them. I think sounds like 30 to 40 milliseconds could be quite an okay speed. We're talking about latency there.
I think that's a perfectly acceptable speed even for web search or something. That's literally the blink of an eye, 40 milliseconds. I think the other thing to note is that these solutions are very horizontally scalable.
In terms of serving any given throughput, you just scale the neural network and code or pools and you can replicate the vector database if using FIAS for instance you start up replicas. You can basically get almost unlimited throughput.
It just depends on how much money you have to throw at the problem. So if you need 500 QPS, bring up more hardware. If you need 5000 QPS, you can bring up more hardware and do it. Yeah, absolutely.
I also wanted to tap into what you said that distilling bird would be beyond reach for many companies. Can you open up a little bit and also can you share with our audience what do you mean by distilling? Maybe some of our subscribers don't know that.
So in the nutshell, and also why do you think that it's so hard to do?
Okay, well, so what distillation of a neural network refers to is taking a very large neural network and neural network with a lot of parameters, it's called billions of parameters, which is very accurate but cannot reasonably be run on a production workload.
And training a much smaller model that captures as much of the performance of the original model as possible, but fitting inside the engineering parameters of your production system. So able to for instance run an inference within 50 milliseconds.
So the way that distillation normally happens is you use the parent model is called the teacher model and you do a large scale labeling of data. And essentially the student model, the small model that you're training needs to learn to make the same predictions.
And interestingly, it gets as much bang for the buck in terms of training from learning to make the correct predictions as it does from learning to, you know, assign probabilities to the incorrect predictions.
So the reason I'm saying that distillation is difficult is there's, I think it approaches to it, it's still a fairly open research topic. There's a lot of active research.
I haven't looked in the last couple of years as possible that there might be frameworks out there now that make this much easier.
But certainly while I was at Google in 2018, 1920 time frame distillation was generally a topic that was tackled by entire teams working over a quarter or two, at least for the most serious production systems. That's how it was used to go. Yeah.
And definitely when it comes to collecting data as you rightly not just, you know, it's not something you can easily scale unless you have some clever technique for data augmentation.
And even then, like for text, as I was eluding in previous podcasts, you know, like if you have like a London is the capital of Great Britain, you cannot put any random city there in that specific sentence, right? Right. Right. Right. Yeah, you need to have certain control.
But there are still ways to, for example, use retrieval itself to augment your data set, right? For example, if you need more entities, you can find them through retrieval, maybe even through vector search, by the way. I don't know if somebody experimented with that already.
But there are other techniques like kind of producing these negative examples and as you alluded to, right? So you need to have as many negative also as many positive so that your model is balanced, right? And that goes to a general model training topic, which is a year to your point. Yes.
And I think that's one of the key to producing a neural retriever that can outperform VM25 in every workload. So that's an excellent point. Yeah. And also, I just reminded me of one challenge that we've been solving in my team actually earlier with building a job search engine system.
And when you evaluate the performance, let's say precision, when it kind of, we call the miss recall, so how frequently it missed triggers to query, shouldn't have actually triggered. like the basic challenge there is, okay, I have this job queries, which I can mind from certain sources.
But then you can, as negative examples, you can pick everything else, right? But that everything else doesn't actually count because just to give you an example, let's say when I say find full-time job in London, right? So that's just a typical query.
You are really interested to find that slightly negative example, which says, let's say, working hours of some office, right? Which is not about job search anymore. It's about points of interest search, maybe.
And so you really want to have those examples to see, okay, does your model, you know, is able to differentiate between them?
And I guess checklist paper is another example where they go like beyond, you know, imaginary in a way that saying, okay, you can actually fulfill this criteria and you can actually check your model on various, various aspects.
Right, right, right.
And is that something that you like, how did you go about addressing that in your research?
I mean, you know, what we did is that actually, if you look, it was like one of the early, early papers, you know, the reason I like reading papers is because you can bring some ideas from one paper to some other domain.
And so the paper was about sentiment analysis where one of the challenge was back then when it was dictionary-based systems, you know, how do I expand my positive dictionary? How do I expand my negative dictionary?
And what they propose there is that you can use a retrieval system where you say, okay, you take an instance from a positive dictionary, let's say it's good, okay?
And then you search with a pattern where you say good and and then a blank and you just let your search engine tell you what good is occurring with in the sentences or text, right?
And the same for the bad one, then they run some clustering on it so that you can actually pick more representative items from your data set.
And in principle, you could apply a similar technique with the job queries, right? And we didn't go that far, but we actually did try to use our own search engine to essentially, you know, augment.
 One of their potential techniques that might help their short of introducing hard negatives, it's easier than introducing hard negatives is to add like what they call a margin loss, which is to essentially just say that the separation in the score that the neural network assign the positive example versus the negative examples has to be large.
So you sign some lambda and it has to be essentially you handicap the scores of the positive examples by that lambda and it forces the neural network to introduce more separation. And so sometimes that can be helpful even if you haven't generated hard negatives. Yeah, yeah, absolutely.
Maybe we can also cite some papers in this podcast, you know, like especially you mentioned some papers and I will try to find this sentiment analysis paper, although I think it's probably like five, six years old or maybe even older.
But I mean, this idea is still live forward, I think, and like we shouldn't forget about them. Right.
And if we go back to your product, so basically, like you said that you also kind of look at using some of the existing algorithms in vector search, can you name them? Or is this some kind of secret? Or are you customizing them as well? So for the vector search piece specifically.
Yeah, so I think we can say that we know we at our core, we do take advantage of phase or fires. I'm not exactly right. I pronounce that from nobody. Nobody knows. I think everyone says their own way.
In my opinion, it's just an excellently designed system with a team that's actively maintaining it and there are obviously experts in that field. One of the features that customers have requested from us is the ability to mix in predicate predicates and traditional Boolean logic.
So you might have this corpus of documents and they all have this, every document has this metadata, which is the date it was published. And then you might want to say, okay, give me the most relevant matches for the query, but only from documents published in 2021.
So this is like a very crisp selection criteria in the selects a subset of the corpus. So this is actually something that we have not launched yet, but we've been actively working on and will probably launch in Q1. I believe I'm going recently added the support.
Google Vertex matching engine I think is a recent offering. They also claim to have this support. It's important. Many of our customers have asked for the same thing. So we've started from a fires, but we have been customizing it. Yeah, yeah, sounds good.
So basically some other companies called symbolic filtering and like that's what I think you refer to, right? So I can have certain categorical variables. So to say in my data and I can filter by it, right? Exactly, right.
Yes, I think when you love fires of face doesn't have this functionality as far as I know. And so essentially you'll have to kind of extend it.
And do you plan to keep it to yourself, which is perfectly fine? Or are you also able to contribute it back to the fires open source project? So I think what I've noticed about the authors of fires is that they want to keep the product very focused on being a first class vector engine.
And these are essentially augmentations that they're not interested in pulling in. And I think they would see it as scope creep, which is probably fair. That said, would we contribute it as open source? We could still contribute it back as open source.
In fact, down the line, we could potentially make our entire stack open source.
I think some of the abusiveness of that, say, over the guards to elastic and how it's worked, where you have these very large companies that essentially contribute very little, but they take advantage of their ability to launch platforms as a service like Amazon can. That's kind of scared us.
So I think in the short term, we're not doing that, but that's certainly something we could plan on doing in the longer term. Yeah. Yeah.
And I mean, of course, the dynamics of open sources kind of not necessarily solved, especially as you've brought up this example with elastic, right? And they're kind of battle between elastic and Amazon. But like for some companies, it still works.
As a starter, you know, you can enter this community. You start building the community around you. And so they bring back ideas. They feed in new use cases to you.
And maybe they even implement some features, right? And is this something that you've been thinking as well along these lines? Well, I definitely see your point. I definitely see your point. And you know, at the same time, we also do have some competition in the space.
We're still in the early days, but 2021 in particular, saw the launch of several competitors. And even Microsoft is in the mix now, Microsoft semantic search. I think it's still in beta. Amazon launch Kendra 2020.
I think that they probably get the credit for launching the first platform as a service, Neural Information Retrieval system.
So in both of the cases, both of those systems, by the way, I think that they actually are fundamentally based on a VM25 search, followed by re-ranking with the neural network. This is what I've gathered from their own product marketing material, which is still a neural search.
It just has a difference out of pros and cons versus like straight retrieval from a vector database. So for instance, just to give you one quick example, a multilingual search, VM25 is not going to work for a multilingual search.
You have queries coming in different languages, documents in different languages. VM25 won't work there, nor will a re-rank on a VM25 results approach work over there because the VM25 has to bring something back to it to re-rank it.
Well, in the case of our system, you can check on some of the demos. We can actually embed across languages into a shared embedding space. So you can search across languages. That's something which you need a vector database for. Yeah, exactly.
So you go multilingual on the first stage of retrieving the data dates. And I think this multilingual search in general, I think it has so much potential. I don't know if Google is using it already to some extent, but even a smaller scale instead of configuring, let's say, solar.
We keep mentioning last search a lot. They didn't pay for this podcast. But I'm just saying, let's say Apache solar, you see, right? So you'll have to go a long, long, long way to achieve it. But now, Lucine released H&SW in 9.0 version.
And so in principle, you could embed your documents using multilingual model and retrieve them in the same way. So do you see huge potential for the market, for the multilinguality? No.
There have been some studies that showed that when eBay introduced automatic translator tools, there was a significant increase. It was a few, I think, you know, a few percentage points of increase in commerce on their platform, which translated to hundreds and hundreds of millions of dollars.
So the, you know, the advancements that have been made in machine translation and now, and she like cross-lingual retrieval, will serve to further break down barriers to commerce, at least, and in the way that's commercially very valuable.
But speaking more broadly, I think that what I will be very interested to see is how vector databases evolve and merge into traditional database technology or into systems like Lucine, like information retrieval systems.
Because at the moment, you know, you have FIAS, it's kind of a separate discreet entity.
But longer term, just, you know, conceptually, in a way, very low-dimensional vector database technology has already made its way into my SQL and Postgres with the spatial extensions that they've supported for many years. The the quadri algorithm for doing, you know, sublinear lookups on a map.
Those spatial extensions have been around for a while.
 You can easily imagine that in the future, once people start to understand how useful vector embeddings can be, and that's established, that you you'll have a, you know, columns of vector type in a relational database and be able to simply build an index and perform, you know, fast nearest neighbor searches straight from Postgres.
So I think that's an exciting future to contemplate and I see that eventually it will go there.
That sounds really interesting, like you do you do you think that vector searches in general is hype right now? Like the way big data was few years ago?
No, no, it's not hype because again, I saw neural information techniques backed by vector databases, making a big difference in many products at Google.
So I think, I think where it is right now is that there's a few big companies like the Fang type companies in Silicon Valley that have the expertise to take advantage of it. It's not being commoditized yet.
So it's definitely not hype, but it's got a few years to go before it enters the mainstream consciousness.
Yeah, for sure, but like to your point, like maybe at some point, vector search will become, let's say, part of Postgres or my SQL or whatever other like kind of traditional, so to say database, which is traditional is in its widely used.
And then Lucine already also introduced it, right? So Lucine now has H&SW.
You can go and argue to the point, okay?
Maybe Lucine index layout might not be kind of optimally designed for, you know, nearest neighbor retrieval because, because like if you look at five methods or H&SW, you know, like it's some graph method or it's a way to partition your space in the scene, you partition it by segments.
And that's kind of like given, right? Because it's designed for inverted index.
But again, on Twitter somewhere, I saw it read from one Lucine commeter who said, maybe this will by itself open up some new opportunities because you'll have a separate vector space index per segment, right? And maybe you can design some features around that.
So it sounds like you still see the potential for merging these technologies in the future and then bringing additional benefit. Well, I, yes, I can't really speak for Lucine. I haven't taken time to study that implementation.
How it was done is I think you know more about it than me, but I was seeing that eventually relational databases could and might, you know, implement indexes, vector indexes directly. I'm not sure that I can see any technical reason why that wouldn't be possible, basically.
And it could potentially be very, very useful as neural networks, you know, go more and more mainstream for embedding. Yeah, I mean, it sounds like one logical step forward.
Maybe it will not be kind of scalable as a pure vector database, but like on a small, like, amount of data, let's say, when my SQL or Oracle or other databases, they introduced full-text search, right? Initially, it wasn't there, right? Right.
What restricts you from, you know, introducing another field with embedding and actually running, running your vector retrieval there? Right. Yeah. Yeah, and I think it also, it comes down to this that, okay, FIIS is always going to give you, you know, the maximum performance.
So, you know, there's going to be some subset of engineering teams that need that performance and that's where they're going to go. But what about the mass market, you know, the Fortune 500 companies and things? And they're dealing with problems at such a scale where it's not necessary to go there.
And if it's just in the database, even if it's only giving me 80% of the total performance, that's good enough.
And in a way, that pragmatic trade-off is what's underlying ZERAIs existence, because people often ask, I can get better performance on my data set if I find tune, a bird model, and then distilled the bird model is like, yes, that's true.
We're aiming to give you a neural network and a full experience that will give you like 80% of the performance that you might be able to achieve, which is still better than you get just from a keyword search.
But the reality is, you know, how many companies have the budget to have NLP engineers and data science and squeeze out that extra performance? It's just not important in a lot of cases. Yeah, exactly.
And do you think that, you know, there is still a need to find a way to combine BM25 or whatever you have there, like the idea of Spark Search with the results from the nearest neighbor search? Like, have you been thinking about it?
Have you seen your clients kind of thinking about it or asking about it? There's a very interesting paper from Google, about two years ago, Dave Dobson, and I'm forgetting the other individuals.
It was specifically on this topic. You can obviously model a BM25 search as, you know, multiplication of sparse matrices. And so you can imagine your vectors essentially having a dense part produced by a neural network for instance, and then a very sparse tail or something.
And you actually want to perform about products in this. And how do you do it efficiently? And the paper was going into some fascinating techniques for how to do that well.
So your question was like, do you see these merging? And I think that, you know, I actually brought this up with the folks at Fires. Is this something on your roadmap? Is this something you're interested in? They said, no, we're not interested in this.
They're specifically focused on either sparse or dense, but not hybrid. But I think that it's going to come down to this. If the utility of this sparse hybrid can be shown, then the technology is going to follow and try to create efficient implementations of it.
I think that there are certainly classes of queries for which BM25 can't be beat. And the exact keyword matching is going to be the correct way to do it in the future. So then you can take a few different strategies.
You can either try to classify the query when it's received and then dispatch it to the correct backend, or you can dispatch it to a sparse and a dense index and then merge with a re-ranger.
Or you can do this like truly hybrid system where you're simultaneously doing the multiplication on the sparse and the dense pieces and producing a final list in one shot, not relying on a re-ranger. So it's still an open area of research. Yeah, exactly.
And two things, like I'm looking at it from the point of view of a customer. Let's say I already have BM25 platform, right? Base platform. And so I'm curious. So I'm curious to see what that research can bring me. And maybe I'm thinking about introducing this as an explorative search feature.
So because I'm not sure if it's going to fly for my documents or for my items in the database. So that's one potential to think about, okay, as you said, I can actually route this query to both sparse and dense retrieval and then maybe combine them in some linear formula even.
And I can give like a smaller score, lower score to a weight to the dense part and then higher to the sparse part because I still believe in sparse part. And that's how my users are expecting results to be there.
But then maybe I can surface some magic like Q&A, right? So they ask the question and I can give them the answer. And that might be really interesting. And the second point, there was a paper called Beer, B-E-I-R. I will make sure that all of the papers will be linked here in the show notes.
But that paper actually compared a dense retrieval versus BM25 on a number of tasks, right? So you can have a search, you can have a question answering and leaves goes on. And so what they showed is that BM25 is fairly competitive.
It actually is above dense retrieval methods like on zero short retrieval, right? So like you didn't fine tune this model. So you just took them off the shelf. Here is the task. Let's compare, right? BM25 is very stable. So just few models actually outperformed it.
And so in that sense, it sounds like BM25 is here to stay. What do you think? I agree with you. And again, this is where our scope is as a company is on building an end-to-end information retrieval pipeline, which means that okay, today we have a neural dense retrieval.
Because BM25 has been done, right? It's in Lucene. It's well understood how to implement it. Although there are some tricks to actually make BM25 work even better than off the shelf implementations.
But what we want to eventually get to is we could potentially build the BM25 and dense indexes for our customers. And then return, we're trying to just serve the best results possible. So for instance, you could take even sometimes even very simple heuristics work.
Single word queries often BM25 is how you want to serve them, not from a dense index. So if it's a single word query, okay, you're going to be on 25 search. If it's anything longer than one word run, then search. That's not a very principled approach.
I'm just pointing out that, you know, what's going on behind the scenes? That's the intelligence of the platform to provide. And we're not really restricted or married to a vector database or only a vector database, powering the search of this platform. Yeah, yeah, that makes sense.
So does that manifest in some way in your product that as a user can have the flexibility in how my search is processed, is it going to go the sparse route or is it going to go the the density tree will? No, we don't.
So at the moment, we are only doing dense retrieval because we feel like that's the interesting part. We can add that we can add that BM25 parts without a lot of difficulty in six months from now or something like that.
So, but we do provide a few different flavors of the dense retrieval because there's a few. There's question answering. So the user puts it or query answering. The user puts a query in and then you're trying to find good responses.
There's also another task which is semantic similarity, which is closely related, but it's like I make a statement and I just want to find similar statements. So my statement is not necessarily a question that I'm looking for an answer to. I just want to find semantically similar statements.
And then the other thing is question question similarity often comes up. It comes up usually in the not in.
Well, you've seen it in Google, for instance, when you type with query and then it says people also ask these questions and they get these similar questions, right? So there's use cases for question question similarity. And so we support all three of those modes of operation.
And we allow at query time our customers to specify which mode they're trying to run it. Yeah, yeah, that makes sense. That makes a lot of sense.
And of course, one thing that I keep thinking about is let's say when we introduce the sparse search, let's say BM25 and some customer comes in and it's not English language, it's something else, right? Then you need to bring in also the tokenization and other things from maybe from Lucene.
And of course, Lucene is a library in principle. It could be wrapped in a Docker image and you can do that job, right? But then the question is, can you easily marry it so that it is production grade between different platforms and languages? And it's surprising.
Lucene has come along with Solars, come along in terms of providing a good sense out of defaults out of the box in terms of stop wordless and stemming. But I have my daughter's school started using this like a product that manages communication between the school and the parents.
And that thing was clearly using you know, Lucene or solar elastic search. And they didn't have the stemming configured properly. And I didn't know as possible to misto, misconfigure that. So I was searching for vaccine and it couldn't find it because it was vaccination in the title over there.
So yeah, so with the neurosurche is kind of a little bit more bulletproof, you know, it's a bit more immune to these kinds of mistakes. And those misspelling very easily. Yeah, especially I think there is also a paper about I think it was from Google, you know, to train on byte level.
And so you will not be constrained by okay, the complexity of the language because you have like byte level definitions. And so in principle, your model should be robust to typos and spellings and so on. And some of them come from speech, right? So exactly, exactly. Yeah.
And it sounds like interesting. Like the example you brought up with your daughter's school like system, like it sounds like largely search is still broken.
It's like like the moment you go to some system which is let's say for public use, right? Like it's not necessarily designed for for finability there. It exists.
And you know, like Daniel Tankelang, I think he says like the funny part of search industry in general is that when search engine works, nobody will go and praise you. They just use it. When it doesn't work, they will blame you. So you always err on that.
How do you feel about that? Like, is this also the potential for your company to go and fix many of these broken use cases?
Well, that certainly that certainly actually our vision that we will make it very easy for SaaS companies to provide a much more Google-like search experience in their products.
So when it comes to web, say, let's into two categories. SaaS companies and website owners. When it comes to website owners, I think the search for websites is really used because and it becomes like a cyclical thing. It's really used companies that therefore don't invest any money in improving it.
It's really used because it's not good. And basically Google does enough a good enough job actually indexing well sites. So site owners have accepted that Google is going to be the front door into their into their website.
On the other hand, I think it's it is obviously dangerous for them too because you've had sites that essentially get obliterated when Google changes their quality guidelines and they drop off the front page and the traffic goes down by 95% suddenly and there's no way to recover from it.
So it would be good for us to be able to provide a good search experience on their websites, but I think they don't do it for the cost involved and they don't know how to.
And certainly, algolia and elastic are making that easier, particularly algolia, but there's still a lot of better that it could be made. Coming to SaaS companies, they're they're talking about data that's private.
The communications of the school to the parents are not on the web somewhere they can be indexed by Google. So I feel like what I've noticed in the last few years is that some sort of search feature is present in most of these products now.
But yes, it's usually not tuned, maybe not even set up correctly and it doesn't work well. And there's a lot of room for improvement. So I think these these neural search technologies let you, you know, really easily improve the quality.
Easily if you've got a set of simple APIs and that's what we provide, our APIs basically look like elastic or algolia's index documents and you never know there's a neural network running in the background at all. And it's not important.
Just the queries go in and the results come out, but these results are far, far better than what you would get from a keyword search. So, so I think there's a lot of scope, particularly for SaaS companies for for neural search. Yeah, yeah, absolutely.
I actually wanted to ask you just a question came to my mind. I've been reading the book about I think about relevant search. It's called by Dr. Enbull and other authors.
I might be not remembering exactly, but this book, you know, it goes chapter after chapter, Wade says, okay, let's just take it from the first principles. You have a search to ask, you have documents, you need to start with like tokenization.
And by the way, if you make a mistake, they will be not findable. And then you move one level up and then you start thinking, okay, what about the model, okay, TF IDF, BM25, what are the trade of Sonson? And so they teach you to become a search engineer and then they proceed to ranking and so on.
So forth. And my question is like, what do you think is going to be the change in the search engine profession going forward once neural search will hit the mass market? Because when I was a search engineer, like I looked at the scene and saw and I didn't question much.
I just went and like implemented some changes, some parsers, some plugins or modified the behavior of some algorithm, right, by extending that class by the way, the scene was not it was making a lot of classes final and in Java. And so I cannot actually extend them.
So I had to copy the entire like package and then rename all these classes. So there is no like namespace clash, but that's okay. No worries. At some point I was worried that I will probably reintroduce the scene all the way in my ID because I had to touch multiple parts.
But so I felt like I'm in control more or less, right? Not because it's not only because it's open source, but because I could read the code, I could talk to people, I could read books, I could read blogs, and I could experiment myself, right?
And that made me, I believe a search engineer in that company, even though the company's goal was not to build searches of service, we were building the product.
How do you happen? It thoughts around like how neural search will change the landscape of this job? Well, that's an excellent question. A few thoughts on that topic. Neural search is going to make it easier. It's going to require less expertise to put together high quality search experiences.
And furthermore, the advantage the companies like Google or Microsoft have from click data, it's still going to be there, but it's going to diminish. And I think that's actually why I'm biased here in misreading it. You see a lot of search engine companies starting up in the last year or two.
You've got Niva, Kagi, I think the head of Salesforce research has started his own engine. I've even heard someone. You don't have to be Apple. You don't have to be Apple. You don't have to be Apple. You don't have to be Apple. I'm rich or something. Yeah, exactly.
So maybe some rumors Apple might be trying to do something like that. And it's basically because the amount of effort it takes now I think has gone down significantly. So I think that that's going to be one of the effects of neural search.
And I also expect that just like, you know, Lucine has been around for a long time. I mean, maybe the early 2000s, 2000, 1999, I think when that catching started learning Java and as a side project project, he decided to implement Lucine.
And so he started the whole community and then Hadoop followed and so on and so forth. Yeah. Okay, because I remember from a time ago. So I think that in the same way, there will be an open source neural thing. And it might come under the cover of Lucine or it might be a separate Apache project.
And eventually, it's going to be the go to solution. So what companies like mine are doing right now is, you know, this technology is still pretty new. And we're feeling in the gap. And we're also providing like a completely hosted solution, which has some some value on its own.
But I think longer term, that's where I see things headed because, you know, we're getting into these very good general performance neural networks, systems like Bert that can just perform well on a wide range of tasks.
And then you have like, you know, T5 and now MT5 and you can go across like 100 different languages as well. So there will eventually be models that are good enough and someone's going to take the effort to distill them into something that runs well.
And, and you know, anybody in any organization will be able to download and use it the way that Lucine today. I think that's where things will be, but it might be, it might be five years before we reach that point. Yeah, yeah.
 And I mean, to take this thought forward from here, like, like maybe the profession, do you think the profession will change in such a way that instead of tweaking the index configuration to make your search kind of work better, like increase recall and, you know, not suffer from decreased precision, you will move more like into, okay, here is the problem.
And this of the shelf network doesn't work. I have to fine tune it. So you become a little bit more like a researcher. Yeah, so that's an excellent point.
I think one of the key components in these systems and that we have not built yet in our system, but it's in the, it's in the blueprints is some kind of a feedback mechanism.
You'll notice this in Kendra though, for instance, thumbs up thumbs down on the results, for instance, where you indicate what's good and what's bad. And then even with a small amount of that data, you can start to train a re-ranker.
And I think that in the presence of like the volumes of data that you get on an internal application, let's say you're going to get a few thousand items of feedback. Training a re-ranker is probably the most effective thing that you can do that data.
Whether it's a random for a free rank or you take a cross-attention on your network and you fine tune it, but you can significantly improve the search quality that way.
So I think that the machinery for doing all of that can also be part of the open source offering because it's very broadly applicable and can be used by basically anyone.
Because like you're saying, this is the problem that then comes up as like, I want to give feedback on this result so the system can improve itself. Yeah, yeah, absolutely.
So you kind of create the flywheel of success, right? So that you bring the data back and then the model retains and so on and so forth. But there is also there are also like interesting challenges like in your old network like catastrophic forgetting.
Like is this something that you've been thinking maybe back at Google or now with your clients, something that kind of you need to keep innovating or solve it some other way. Yeah, so I am familiar with the concept of catastrophic forgetting.
I honestly haven't studied it very much in the context of these large language models like Bert. Although in general the approach of taking a Bert type model and fine tuning seems to be working well.
But then you're essentially talking about taking after it's been fine tuned on one task and then fine tuning for different tasks and it loses its abilities on the first task. And yeah, I guess I don't know how much of an issue that's that's going to be in the context of information retrieval.
Yeah, I mean, another thing like if you are familiar with learning to rank for example, which may or may not involve in your own network, it may also be based on decision tree like Lambda Mart for example.
You know, when you receive a new batch of clicks or downloads or whatever events you have in the system and you retain that model, it will also forget what it knew about the previous state, right?
It's very natural and it probably is we can associate it with human life as well in some sense, although they say the older you get, the earlier memories you will actually remember, you might forget what happened yesterday, but you remember what happened like 50 years ago.
But like, yeah, that's probably noticing that one myself.
Yeah, me too, actually, because days go by and I'm like, okay, what's going on? But then you go, okay, when I was a kid, I remember something, but like neural networks are probably a little bit different or at least the present neural networks.
And so I think when you when you retrain the model, like you have to retrain otherwise, it will drift, right? I think Google also has a paper about that, like kind of checking the consistency of your machine learning pipeline and your model.
So it doesn't drift and just explode in the eyes of the front of the eyes of the user, right? So you have to keep retraining it. But then that also means that it will forget things. Maybe they were quite important. Maybe they are not high probability anymore, but they still are true.
But the network has forgotten about them. Right, right, right. Yeah, yeah, that makes sense. Yeah. Anyway, it was it was a great talking to you, but I still want to close off.
And before we go to some announcement for you, I'm thinking like I'm asking this question to different to all guests and different guests take it differently. But I really would like to hear your view on that question of why it's a little bit more philosophical.
Like, like in a way, like, you had a stable job at Google, a lot of challenge, a lot of data, a lot of user impact. Like, as you said, like Autorripe Live feature was enabled and to like millions and millions of users.
So then you then you decided to go to build your startup and that's a nice nice kind of way to experience like another side of things.
But why specifically neural search? What drives you to work on it? Well, what attracted me to I was initially attracted very much to the idea of automated reasoning. And then of course that comes in its current incarnation, its machine learning. And so I started to learn about that.
And I had this opportunity to work with the Ray Kurzweil. He joined Google, I think around 2012. I knew about him. He's a very inspirational figure. And he was specifically working on language understanding because he saw that as being very critical to advancement in artificial intelligence.
So, so you know, then beyond that, I would say those are my broad interests. But then I just worked in this area specifically for eight years. And I think I became quite good at what I was doing.
And then also saw that what I was doing post 2017 in particular with this neural network based retrieval had a lot of applicability to products. And you know, I think I think that being in a research team or research team has a different type of focus.
There's a lot of focus on on publishing papers and things but not necessarily a lot of interest or appetite for building platform. So in that way, maybe this wasn't really the right place to attempt that kind of work. But to me, I'm an engineer as well. So this is this is very interesting.
And I'm not sure if I'm answering your question, but that's some of my motivation. No, you do. I mean, essentially, I'm currently leading a search team. And yeah, you know, our KPIs is like, okay, how many papers you published, how many patents you can file.
But also when you start thinking, okay, what impact am I making? Right? There is not that much room to think about creating. Maybe you can create a vision, but you might not necessarily tie it in back to the day-to-day scenarios of users.
You have to be part of engineering probably to start delivering these things at which point you are no longer a researcher. So it sounds like you you managed to combine both of these engineering and research at ZRI. Yes, yes, it's kind of both of the passions together in one company.
And if we're successful and we can take it into the future, the research end of the program is something that I'd really like to ramp up a lot. Since we started, honestly, there's been more engineering and less research.
The training, the neural networks was at the early stage of the company and then we haven't revisited it since then. But I think 2022 is going to be first of all, it's going to be a big year for this industry.
Beyond Pinecon getting funding, I was recently looking at Gina AI, if you're familiar with them. Yeah, they I think raised $30 million. It was in TechCrunch. So the industry is starting to get some notice. And for us as well, we expect, we expect to really expand in 2022. Oh yeah, fantastic.
And I mean, one manager that I worked with used to say that you need to first build the plumbing. And that's your engineering work. Once you have the plumbing, you can stand on it and actually fix some other things, high level.
And that's where you will probably come back to training neural networks and actually nailing that use case for your customers. Sounds really exciting. This was really packed and so much thinking that you brought in and also some discoveries during this conversation. I really enjoyed it.
I'm just thinking, is there something you would like to announce from your product site, something that our listeners can try? And yeah. Well, thank you. Thank you for the opportunity.
I think what I would say is that if what we've been talking about is interesting and if someone would like to try it out, then we've created a special promo code. We're currently in a closed beta. So we're accepting customers, but kind of on a case by case basis.
But we've created a promo code for listeners of this podcast. I think I'm going to I'm sure the exact code with you. And then you can post it in the comments to the to the video.
But essentially would give give you a 50 megabyte account, which is much larger than our standard trial account, a by about a factor of three for free for one month. If you want to just try out the system that we've been talking about. This is fantastic. Thank you, Amin, for this opportunity.
I'm sure some listeners will will take take this into use and build some cool vector search applications for their products. That would be great. Yeah, it was a pleasure to talk to you. I hope we can talk at some point down the road as well. And I wish you all the best in the future.
In the next year, with your ambition and also with with reaching to clients and getting contracts. And and all the best to you on that front. It was my pleasure to talk to you and hopefully to see you next year as well. Thank you so much. It was good talking to you too. Thank you, Amin. Bye bye.