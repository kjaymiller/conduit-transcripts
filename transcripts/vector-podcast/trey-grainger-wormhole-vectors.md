---
description: '<p>This lightning session introduces a new idea in vector search - Wormhole
  vectors!</p><p>It has deep roots in physics and allows for transcending spaces of
  any nature: sparse, vector and behaviour (but could theoretically be any N-dimensional
  space).</p><p></p><p>Blog post on Medium: <a target="_blank" rel="noopener noreferrer
  nofollow" href="https://dmitry-kan.medium.com/novel-idea-in-vector-search-wormhole-vectors-6093910593b8">https://dmitry-kan.medium.com/novel-idea-in-vector-search-wormhole-vectors-6093910593b8</a></p><p></p><p>Session
  page on maven: <a target="_blank" rel="noopener noreferrer nofollow" href="https://maven.com/p/8c7de9/beyond-hybrid-search-with-wormhole-vectors?utm_campaign=NzI2NzIx&amp;utm_medium=ll_share_link&amp;utm_source=instructor">https://maven.com/p/8c7de9/beyond-hybrid-search-with-wormhole-vectors?utm_campaign=NzI2NzIx&amp;utm_medium=ll_share_link&amp;utm_source=instructor</a></p><p></p><p>To
  try the managed OpenSearch (multi-cloud, automatic backups, disaster recovery, vector
  search and more), go here: <a target="_blank" rel="noopener noreferrer nofollow"
  href="https://console.aiven.io/signup?utm_source=youtube&amp;utm_medium&amp;&amp;utm_content=vectorpodcast">https://console.aiven.io/signup?utm_source=youtube&amp;utm_medium&amp;&amp;utm_content=vectorpodcast</a></p><p></p><p>Get
  credits to use Aiven''s products (PG, Kafka, Valkey, OpenSearch, ClickHouse): <a
  target="_blank" rel="noopener noreferrer nofollow" href="https://aiven.io/startups">https://aiven.io/startups</a></p><p></p><p>Timecodes:</p><p>00:00
  Intro by Dmitry</p><p>01:48 Trey''s presentation</p><p>03:05 Walk to the AI-Powered
  Search course by Trey and Doug</p><p>07:07 Intro to vector spaces and embeddings</p><p>19:03
  Disjoint vector spaces and the need of hybrid search</p><p>23:11 Different modes
  of search</p><p>24:49 Wormhole vectors</p><p>47:49 Q&amp;A</p><p></p><p>What you''ll
  learn:</p><p></p><p>- What are "Wormhole Vectors"?</p><p>Learn how wormhole vectors
  work &amp; how to use them to traverse between disparate vector spaces for better
  hybrid search.</p><p>- Building a behavioral vector space from click stream data</p><p>Learn
  to generate behavioral embeddings to be integrated with dense/semantic and sparse/lexical
  vector queries.</p><p>- Traverse lexical, semantic, &amp; behavioral vectors spaces</p><p>Jump
  back and forth between multiple dense and sparse vector spaces in the same query</p><p>-
  Advanced hybrid search techniques (beyond fusion algorithms)</p><p>Hybrid search
  is more than mixing lexical + semantic search. See advanced techniques and where
  wormhole vectors fit in.</p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20251107_051156_724d3e0d493d36eed167f0604822b7e3.png
pub_date: Fri, 07 Nov 2025 05:58:00 GMT
title: Trey Grainger - Wormhole Vectors
url: https://rss.com/podcasts/vector-podcast/2314900
---

Alright, hello everyone, wherever you are, really, really happy to see all of you online. Welcome to the Beyond Hybrid Search with Warm Home Vectors. It's another idea that Tray is going to present today and we will have a discussion and all of you are welcome to ask questions as well. Yeah, cool.
I think we'll start with that. This is just a quick intro from me. I'm Dmitri Khan. I, most recently, I'm with Ivan. I joined as a product director leading the search domain. We offer managed open search so that you don't have your headaches setting it up and doing some DevOps.
And you can choose any cloud whatsoever, really. And then just go and run with that. And I'll share a couple of links later. I'm also a host of the vector podcast that I started, I think, four years ago. I already stopped counting. Maybe some of you have heard some of the episodes.
And yeah, it keeps going on and off, but I'm really excited to continue doing that. I've been in search for, I think, 16 years, maybe 20 years if I include academic experience or exposure. I've built search at startups, multinational technology giants.
I think what was the startup, for example, AlfaSense became, I think, a unicorn company by now. Yeah, I'm super excited to partner with three AI power search and support from vector podcasts looking forward to the session today. Over to you, Trey. Awesome. Thanks, Dmitri. Appreciate it.
I'm really excited to have Dmitri Khan more for the conversation part of this. He's been doing, doing the vector podcast and in the space for a long time. So I think it'd be useful to help him facilitate, get lots of questions and good discussions. So I'm Trey Granger.
I'm the lead author on the book AI Powered Search along with Doug Turnbull and Max Irwin. I'm the founder of Search Colonel company that does AI Powered Search consulting, technical advisor, open source connections.
Last year, I've been an adjunct professor at from university teaching computer science. My background, basically my entire career has been in search, particularly the intersection of data science, AI and search.
My last company, prior to search journal, I was the CTO of pre search, which is a decentralized web search engine prior to that. I was the chief algorithms officer at Lucidworks, a search company, as well as prior to that, their SVP of engineering.
I also had a search at career builder, prior to that. I also a decade ago, solar in action, but AI Powered Search is the focus of what I'm doing right now. The books got, you know, quite good reviews from folks. If you haven't checked it out, please check it out.
And this lightning lesson is one of a series leading up to an AI Powered Search course that Doug Turnbull and I are teaching starting two weeks from today. I heard of it.
It's kind of themed based upon the book, but we'll be going into a lot of new and emerging techniques that aren't in the book as well. Just to give you a sense, I'll spend like a minute on this, maybe two.
If you're curious, it's, you know, four solid weeks of material, the first week will sort of, you know, do a course intro, introduce the search relevance problem, talk about ranking those things.
We'll have a guest session from Eric Pugh from open source connections, talking about user behavior insights. For collecting click stream data and how to properly collect and process that are an accession will be on signals and reflected intelligence models.
Everything from signals boosting for popularized relevance to learning to rank for generalized relevance to collaborative filtering and matrix factorization for personalized relevance to knowledge graph learning to learn from user behaviors.
You know, terms, misspelling things like that about your domain. And then every week will have office hours where you can bring your hardest questions or we've got labs throughout the course as well. If you need help with those, we can help.
We the next week will dive into AI powered query modalities, things like buying coders and crossing coders talk about chunking, talk about late interaction models, hybrid search, multimodal search, all of those. Again, all of this has code and notebooks associated with it will be working through.
We have a guest lecture from Jenny from quadrant who will be talking about mixing sparse sentence representations with mini coil and then we'll dive after that into sort of hands on building ranking classifiers or learning to rank models.
And what is entailed in that we will of course then have office hours again the next week will dive deep into rag talk about rag. You know, sort of naive rag, agentech rag adaptive rag guard rails all the sorts of things you need to sort of understand to do to rag well.
We'll talk about agentech search towards the end of the course talk about interleaving strategies for rag will have Max Irwin our co author on a powered search to giving a guest lecture session after that will be.
Automating learning to rank and with click models and with active learning so we'll be diving into how to deal with biases in your data how to deal with exploration versus exploitation looking for results that may maybe don't show up in your normal search patterns and then.
 The sort of final two weeks will have a guest lecture from john handler from open search and AWS really talking about scaling vector search in production with lots of good experience from large scale open search clusters and Amazon servers and then we'll dive into optimizing I search for production everything from quantization re ranking strategies semantic caching.
 Running local models and then for our last session will dive deep into AI powered query understanding and agentech search focused on really interpreting understanding queries leveraging agents as part of that process and so if that's interesting to you there's a link and a QR code here anyone who attends today is eligible for 20% off the course.
 And so definitely check it out if you've been considering it there's two weeks left and of course even if you can't attend all the sessions everyone who's enrolled will have permanent access to all of the recordings all the code and all the course materials so you can use these going forward into the future if that's interesting to you so done with that now I'd like to get to our topic which is beyond hybrid search with wormhole vectors.
 So let me dive straight in and feel free if you have questions as Dmitry said post them in the comments Dmitry feel free to interrupt me at any point if there's something worth diving into otherwise i'm just going to keep going and kind of focus on conversation at the end so I want to start with some basic material on vectors and vector spaces to kind of set our expectations for where we're going with wormhole vectors to start vectors by definition mathematically or something that have direction and we're going to start with the end of the end of the session.
 And magnitude vectors if you think of them as sort of something that can go in any direction and vector space and you can add vectors together to generate a new vector you can average vectors together to find you know sort of an area that's in between them there's lots of mathematical operations that we can do on vectors but let's keep in mind that they have both the direction and a magnitude.
When we think of embeddings and embedding as a set of coordinates in vector space and a Hilbert space but in a semantic vector space into which we can map a concept so whereas.
vectors have dimensions and those dimensions sort of go in any direction when we talk about an embedding an embedding is actually a point in vector space so for example this point right here this series of floats for book or tree or what have you.
You can think of it as a vector originating from the origin at zero zero here and extending out to that point but fundamentally we think of an embedding as a coordinate. That is a point in vector space that corresponds with some semantic meaning.
 And search whenever we're dealing with embeddings we often have things like word or phrase embeddings where we take an individual word and leveraging a transformer model typically we will generate that series of floats that represents the meaning of that word given the context around it but we can also have sentence embeddings where we look at all of the words in the sentence and their contextual meaning.
 And generate an embedding that represents the meaning of the sentence we can have paragraph embeddings that sort of summarize the core ideas of that paragraph and the same thing with a document often in search will start with just a document embedding and when we take a query we generate an embedding and we do sort of a vector similarity between defined related documents that match the query but you can chunk documents up in any way and any number of vectors.
And actually we typically think of embeddings and vectors as having a relatively small number of dimensions we call these dense vectors where maybe there's 768 or you know 1024 some number of dimensions and we compress lots of data into a continuous space within those.
However, there's also the notion of sparse vectors and the best way to think of a sparse vector for purposes of our discussion today is to think of lexical search and to think of just you know when I'm trying to run a search for keywords.
Imagine you have a 1 million dimensional vector not 768 but a million dimensions and every single one of those dimensions corresponds to a term in your index where you've indexed all of your keywords and let's just assume that there's only a million terms in your index.
 If I wanted to represent latte as a query well let me not do latte let me do a doughnut if I want to represent doughnut as a query then I can represent that as a vector with a million zeros minus one and that there's a one in the column for doughnut indicating that this is a million dimensional vector with only one value represented and that's the whether the text doughnut appears within.
 Within this document or query and so that's a sparse vector where it's sparse because most of the data is not filled and I have mostly zeros but there's some ones and of course in this case if I had the search for cheese pizza that vector would have two ones in it one for cheese and one for pizza so it's a million dimensional vector with two ones in it.
 This is just as valid as a vector as a dense vector with only 768 dimensions but what we typically do when we start to move from lexical matching where we can match on you know those yes or no ones or zeros and an inverted index what we typically do when we move to doing semantic search is we focus on a much smaller number of dimensions and so conceptually as an embedding here what I have is eight dimensions you know each of these items.
 The items that I showed on the previous slide has dimensions indicating whether it's food whether it's a drink you know how much dairy it has is it bread is a caffeine sweet calories healthy etc so you can see apple juice now is not represented as it has the word apple and it has the word juice but it's represented as very much not food very much a drink no dairy no no bread no caffeine very high on sweet but not all the way up very high on calories and all the all the way up and in between but you know kind of sort of healthy but not really and then you have same thing cheese bread sticks very much food not a drink good bit of dairy very much bread no caffeine you get the idea these map in the attributes are the dimensions of these concepts over here by representing them in these eight dimensions and in search what we typically do is we represent documents and queries leveraging these vectors.
 And then we do some sort of vector similarity calculation in order to say how related similar things are so if I were to for example take the vector over here for cheese pizza and I were to then do a cosine similarity between that vector and every other vector I would see that cheese bread sticks have a very high similarity followed by cinnamon bread sticks followed by doughnut all the way down to water so these are essentially ranked based upon cheese pizza these are the cheesiest.
 This is the most breadiest unhealthiest non drinkiest things at the top you know this is still very bready very non drinky not very healthy here ranked all the way down to it's essentially opposite in this vector space which is water which is all the way on the other end of the spectrum same thing with green tea very similar to water cappuccino latte you know healthy no calories drink all the way down to a very unhealthy very not drink you get the idea.
Actually in a semantic vector space things span across these dimensions and they fit at different places along within the vector space that corresponds to the meaning of these attributes.
 Now when we deal with transformers which we get from all the LLM's today that we're leveraging for vector search these don't use explicit features like we have here food drink dairy bread etc they use latent features and latent just means sort of hidden or another way to put it is the dimensions don't correspond one to one with particular attributes it's combinations of those dimensions together that they give us our meaning and so to think of that visually if I were to create an embedding space and this is obviously flattened you know there could be thousands and thousands of dimensions or hundreds but in this vector space if these are all of the the embeddings that I have and I would search for the keyword or start for the phrase Darth Vader turn that into an embedding and match it you'll see that over here on the right I have a cluster of meaning associated with the search for Darth Vader.
Now there's some other points in various places but if I were to look at the items in this cluster I see pictures of Darth Vader which is what I would expect because the meaning of Darth Vader is essentially in this area of vector space.
Similarly if I were to search for puppy then this cluster of meaning right here corresponds with puppies and did I see pictures of puppies so the interesting question arises when I ask what happens if I were to find the midpoint between puppy and Darth Vader in this semantic vector space.
People have different intuitions about what actually happens here some people think it's you know I don't know what I would find in the middle but the answer is if this vector space is properly constructed so that the semantic meaning is represented i.e.
 the further away I get from this point the more I get away from dog the further away I get from this the more I get away from Darth Vader and vice versa then what I would expect to find if I sort of average those two a vector from here and a vector from here together is a puppy Darth Vader a cute puppy Darth Vader right here in the middle.
 And so for some people that makes intuitive sense but if you think about what a semantic vector space is doing where's representing meaning across a continuous spectrum you would expect to find this because I'm essentially finding what the thing that is the average sort of in between Darth Vader and puppy within the semantic vector space.
Now there's all sorts of reasons why this could not work depending upon how you've changed your model and if you know there's too much data being compressed into to a little space but conceptually this works.
So similarly if I were to do a embedding search for superhero flying versus superheroes flying this is very comparable to running a search for you know superhero flying sort of.
With the with the idea of a singular hero and then sort of tracking out the idea of a singular and adding in the idea of a plural again from here to heroes and then what happens over here is this is essentially the same vector but moved toward or in the direction of multiple versus singular.
What you see over here in fact is that while some of the images are the same I all in general i'm seeing more images of superheroes that are in groups of multiple superheroes and so to demonstrate this with a very like explicit concrete example.
 If I were to take this an embedding for this image which is a delorean from back to the future and I were to sort of describe it right this is a you know a sporty car with you know to door one door on either side and it's kind of boxy and it's got really cool lighting and so when I run that search for this embedding on other images I find other sporty cars obviously some delorean's in here but also just in general sporty cars with you know a door on either side and really cool lighting for the most part.
 However, what if I were to take an embedding for the query from the last slide superhero and I were to average that or pull it with this image embedding what would I get well in fact we have an example of this in the i powered search book when we're doing multi modal search if I take an embedding for superhero and embedding for this image what I in fact do get is this very first result as a sporty car with cool lighting with a superhero on top because that's what I would expect in this.
 So I would expect a semantic vector space to be in in between these things and for these other images again sporty cars single door but notice that in all of them there's a person and it just so happens that that person is the protagonist of their story so maybe those stories didn't have actual superheroes but these are the heroes of those stories.
We get the idea and I wanted to paint that conceptually just to talk about regions of vector space and what they represent and how you can use math on vectors to move between them and sort of combine concepts and find related things.
 So one problem now zooming back out to the topic of today one problem that we commonly come across and this is where hybrid search comes into play is that we have disjoint vector spaces in search and that leads to disjoint query paradigms what I mean by that is that we have a sparse lexical semantic space which is our inverted index right what I showed you earlier with the million dimensions and you know the keywords.
So we have a very small number of dimensions represent the dimensions that is a vector space it's just a very sparse one.
 Similarly we have dense vector spaces where most of our embeddings are that we get out of large language models where they're compact into a small number of dimensions but they're continuous because we have these two different query paradigms what often happens with vector search is we say hey I don't know how to combine this dense query on this embedding with this sparse query with these keywords.
So I'm just going to run them as separate searches and in fact that's what most sort of hybrid searches hybrid search implementations look like out of the box.
 So this is an example of rrf or the reciprocal rank fusion algorithm where I'm essentially taking a lexical query over here for the habit and I'm matching on a bunch of documents you'll see you know each of these has the word habit and it's somewhere either in the title or maybe in the description.
 But notice that while the first four results look pretty good the next these are the only results that had the word habit in them and then the rest of these results the good the bad and the ugly this just happens to match on the word the three times and then this next result happens to match on the lord of the ring so it's got the in it three times as well.
 It happened to give me a good result but it was purely coincidence because it doesn't have the word habit here and then I get the abyss and then the apartment again only matching on the word the so the lexical search found all the results that had the word habit in them but it completely missed a whole bunch of other potentially relevant results likewise my vector query over here for this embedding matched the habit here it matched a Harry Potter movie here you know similar concepts similar themes and.
 And similar kind of visual style lord of the rings, the rings rise of the guardians I guess is maybe kind of conceptually similar even though it's a cartoon the wailing I think this has a visually similar style but is a really bad match you get the idea so there's some really good results I get from the vector search some the dense vector search some really good results I get from this lexical or sparse vector search and then with hybrid search with reciprocal rank fusion we can essentially take.
 Each of those separate sets of results and combine them together in a way that weights things that both the lexical and the dense search found relevant it moves those at the top and then kind of gives us better results overall you can see that i've matched most of the results over here so it's better than either of the two lexical or dense vector search mechanisms individually however.
i'm still treating them as entirely separate things i'm i'm doing the lexical search i'm doing embedding search and then i'm combining them together.
 But in reality there's lots of ways to merge these different paradigms and even beyond just the embedding some getting from text you know we can we can get text embeddings so for example and we can do a texting coder to generate embedding for that we can take images and generate embeddings for that we can also take user behaviors.
 And generate behavioral based embeddings and combine those together and there's different ways to generate new vector spaces you can concatenate these together and you can do dimensionality reduction or you can stack them i'm not going to get into those today but the reality is we've got a lot of tools at our disposal to be able to query and get at data and related in different ways.
In fact what I described for hybrid searches second ago with rrf is just scratching the surface for what we can do with. Combining different paradigms and so this spectrum here on the left, this is you know token matching or sort of.
traditional you know the lexical search and you'll see that you know things like tf IDF will be matching those kinds of things fit over here we've also let me just check the yeah okay we've also got.
On the opposite of the spectrum this dense vector search and of course the rrf would fall in here in this sort of.
 hybrid sparse retrieval and dense vector search where we're running them independently and come in parallel and combining the results but there's also mechanisms where we could for example run sparse retrieval first and then re rank using dense embeddings or something like with mini coil which I mentioned Jenny from quadrant's going to come talk to us about an AI power search course you can actually run a sparse search.
 And have embeddings that are sort of adding additional semantic data to your lexical queries to be able to better leverage semantics as part of your sparse search there's play there's semantic knowledge graphs there's all these different techniques that we can use to get better search whether it's hybrid search or leveraging one of the techniques but I want to just like.
mention that there's lots of ways to deal with embeddings and to deal with sparse and dense vectors to combine combine them to improve query understanding and to improve recall and so one of the.
 Things that i'm experimenting with sort of like an emerging way to do this is something i'm calling wormhole vectors and the idea of wormhole vectors is that i've got these sort of different vector spaces i've got my sparse lexical vector space which we talked about i've got my dents semantic vector space.
And then I mentioned we can generate behavioral vector spaces which i'll show and just a little bit and so I want to walk through what this technique looks like.
And I do want to frame this talk as this is sort of like new and emerging i've got lots of experience doing some of this across different vector spaces but.
There's a lot of things that I so need to iron out in terms of best practices for doing this so treat this as something this emerging and something you can play with and I think the intuition will be really helpful.
But i'm you know if in preparation for the course and going forward i'm going to be doing a lot more in terms of concrete examples for this.
And so i don't want to get into quantum physics or you know this is in general, but you know wormholes if you're not familiar are essentially passages through space time.
You can think of it as you know the ability to you know go from one point in space to another point in space and essentially like hop there instantly. I could get into Einstein Rosen bridge is not that kind of stuff but don't really want to for purposes of today.
And what I do want to do though is talk about. I'll give you one second well i'll skip over this will maybe come to that in the q and a if Demetri is interested and talking about.
This notion of entanglement and how that relates to wormholes it might be interesting later but I don't this is about search not about quantum physics and physics in general so. This is what it means to generate a wormhole vector by practically so if if you want to generate a wormhole vector.
The there's a fundamental base reality of all these vector spaces meaning if I query with an embedding an intense vector space or I query with. I query with a lexical query over here where I query with ideas and user behavior over here.
All of those queries ultimately boil down to matching something and the something that they match is really critical to how we understand queries and how we understand relevance and what they boil down to is a document set.
So if you run an embedding search over here you find a point in vector space and if it's a dense space you typically do an approximate nearest neighbor algorithm or otherwise find the nearest neighbors to whatever point you're querying and those are your relevant documents.
Those documents form a set and you can cut off the threshold at any point to say these are the documents that matched but that set of documents collectively has some meaning that some some relationships within it that represent the meaning of that query.
Likewise if I do a keyword search I find a document set and the collection of those documents represents the meaning of that query at least as we've been able to represent it in that vector space same thing over here.
So the idea of a wormhole vector is if I want to query in one vector space and find a corresponding region in another vector space that you know shares this sort of same meaning semantically then what I'll do is I'll query in the current vector space for example within embedding here.
Actually let me start it here, I'll query in the like sparse like school vector space I will then find a relevant document set this is what search does it finds a document set and then I will derive a wormhole vector to the to a corresponding region of another vector space.
 So for example once I found a document set over here I will use that document set to generate what I'm calling a wormhole vector but to generate a vector that will allow me to query in the other vector space or hop or traverse to the other vector space instantly to a region that shares a similar semantic meaning to the region in the like school space.
 Then once I've found that vector for the other vector space I will run that query in the other vector space to traverse to that vector space and then I'll repeat as needed so I can actually hop back and forth between vector spaces to find and collect documents to try to better understand them and then to use that understanding to take those documents and return them for the full set of search results.
So I'm going to actually just show this visually for a second let me. Let me click here and restart this demo so imagine I have a sparse vector space over here on the left.
 The way this works is I send a query in this query finds a set of relevant documents that are in this vector space and what's it's found those documents it uses them to as a essentially a wormhole in the pauses for a second maybe I can't essentially wants to run that query I find the relevant documents which are the things close by in vector space I then use that to generate a vector and embedding that I'm going to run a search for over here in the dense space.
And once I run that search you'll notice that in this example it's not exactly where these documents are but it's very nearby meaning the sort of collection of these things together and what's understood semantically about the relationship maps to this point and vector space on the right.
And then that allows me to then find other things really surrounding it that represent a similar meaning.
 And this is a you know just looking at two vector spaces a sparse vector space and a dense vector space for keywords and then for embeddings but as I mentioned there's also this notion of the behavioral vector space so the same thing happens here I can run a query find relevant documents use those as my wormhole.
 And then I generate this wormhole vector to hop through the wormhole to the other side to find the region corresponding to that meaning and either of these other vector spaces so in this case I've done major expectation which is like the process you go through when you're doing collaborative filtering for recommendations then I would you know hop to the corresponding region over here so that's the general idea just kind of visually describing it.
And I go over here. Give me just one second. Give me one second slides up. All right. And then the next question is how do we actually create these wormhole vectors so to meet you if there's any questions feel free to enter up me at any point but okay. I'll keep going otherwise.
I think we have a couple questions but we'll defer the end sounds good. All right. So the question now is how do we create a wormhole vector and there's essentially two two types that I'm going to focus on right now one is the. Sorry, I lost this.
The first is if I'm trying to go to a dense vector space within beddings so this is very easy all have to do is pull the vectors or average the vectors of the top in documents so imagine I run a keyword search.
I find a set of documents I rank those and then I don't necessarily need to take the entire document set I could but if I want to just take the top in documents to get a sort more sort of semantically relevant or you know just.
Just let's just say relevant set corresponding to that keyword query then I generate a new embedding in the dense space that is just an average of those.
If you go back to the Darth Vader example from earlier where the puppy Darth Vader is in the middle it was sort of a combination of the meaning of Darth Vader and a meeting of puppy.
 I think of this as taking a bunch of documents that each have their own meaning and when I pull them together I'm creating an embedding that has the average of the meaning and if I assume my documents that I queried on the lexical side have some sense of a shared meaning within them and I take the top documents from that then that shared meaning I can hop over to the dense space find and then find other things that have similar meaning even if they don't match the keywords.
 Likewise I can go the other direction if I'm in my embedding space my by dense space I can run a search find the top in most related embeddings by cosine similar to what have you and then conceptually it seems more difficult to then hop over to the sparse space how do you generate a sparse vector but there's a technique called semi acknowledge graphs which I'll kind of walk you through which allows you to do this.
 So zooming back out I mentioned pulling the vectors of the K and in documents all you need to do again I query an electrical space get the top K documents get the embeddings of those documents and average them together this is the simple way to do that just using Numpi for the semantic knowledge graph approach same thing I get the top K documents in the current vector space and then I do a semantic knowledge graph traversal to derive a sparse lexical query that best represents those documents.
 So functionally if you think of language I just talk about semantic knowledge graphs for a second and show you the structure of natural language you can think of it as a graph of relationships right we've got prefixes and suffixes and you know those mapped to terms those mapped to terms sequences and documents.
But once you get documents and we've got these terms across documents you can just think of this as a giant graph of relationships and so I can take you know individual words.
 In this case trade his he they all refer to the same entity I can take other things and if I think of this as a graph then in fact you can leverage your inverted index as a graph and you can traverse it to find these relationships and so and typical search engine so like any of the Lucy and engines for example you have.
 An inverted index which is something that maps terms to sets of documents and then you've got usually a forward index and you know open search elastic search solar any Lucy and engine this is going to be your your doc values but essentially I can take any term and map it to a set of documents so if I can take any term or sorry any document map it to a set of terms so if I can take any term and map it to a document set.
 And I can take any document and map it to a set of terms then that's a graph and I can traverse back and forth across this so for example if I have the skill of Java and the skill field and I've got a set of documents that has the keyword Java you can think of the set of documents is representing the keyword Java and then similarly you know there's sort of linked to other documents you'll notice that there's no documents that link both the skill of Java and the skill of hibernate.
 And so in a set theory view it looks like this notice that this set doesn't intersect with these and from a graph theory view the same underlying indexes look like this where I have a graph where I've got the skill of Java with a has related skill edge to the skill of scholar and it's skill hibernate and then oncology is completely disconnected from this graph and all I'm doing is leveraging my inverted index my sparse representation to traverse across these relationships.
 This is very useful for things like disambiguation where I can take a keyword like server I can traverse through documents to find you know what are the top semantically related categories for example DevOps and travel and then within each of those categories I can traverse to other keywords and find which are the most semantically related keywords to server and the DevOps category for example I get terms like doctor and genetics Jenkins get words and travel I get things like tip restaurant bill.
 So all of this just leverages and inverted index there's no embeddings what whatsoever this is all just leveraging the sparse semantic space but why this matters for modeling intent is if I have a query like barbeque near haystack over here I can generate a sparse vector representing the meaning of barbecue by looking at the index and seeing what's related to it.
So in this context what I'm able to find is that barbeque is related to things like ribs and brisket and pork and the category of restaurant IE I can generate a sparse lexical vector like this purely from the semantics the things that are semantically nearby in my sparse vector space to barbecue.
Also if you look at the query over on the right barbeque grill what I'm able to do is generate a sparse vector that is barbecue or grill or propane or charcoal notice that this vector is now different because it's contextualized based upon grill being in this query.
So now my query becomes category about to our clients and then this is the list of words that better represents the meaning of barbecue again no embeddings no transformer models no LLMs involved here. This is purely leveraging my sparse lexical space in the semantics within it.
And so this is some example source code from the app how research book for traversing semantic knowledge graphs. But the idea here with the wormhole vectors is that I can take a query in any vector space. So for example if I take a lexical query here.
 I can easily take you know lasagna or drive through what have you and I can generate these representations over here by taking lasagna finding a doc set that matches that keyword and then from that doc set finding these other relationships for example lasagna can be described as Italian with keywords like lasagna Alfredo pasta and Italian.
And then Korean barbecue can be represented as category of Korean with terms like Korean bon chaan sorry lawn et cetera fast food gets things like McDonald's and window. So this is purely leveraging and I've been doing this for years and it works very, very well.
But this is purely leveraging the inverted index in this document set. The idea with the wormhole vectors is not just to stay within a single vector space but to be able to go across vector spaces.
So similarly I should be able to take an embedding that finds a region in semantic vector space and a dense space find the nearby things which ultimately just translate to a doc set.
And then from that doc set I can use the same technique to say what are the things that are related within these documents and generate you know the similar kinds of outputs over here.
 You can also think of this if taking away all the wormhole vector terminology you can think of this is just a way to make your embeddings more explainable right I've got an embedding I go to a dense vector space I find documents and then from that set of documents I'm now driving a lexical vector which is readable.
And then I'm describing what's happening there and of course I can then turn around and take that and query my sparse space to match other things that have the terms but maybe didn't match in the dense space so that's the general idea.
And there's one kind of last thing I wanted to cover briefly which is this notion of behavioral embedding spaces because I've mentioned it multiple times and I have a feeling a lot of people aren't super familiar and so let me click here.
The general idea and I'll be very quick through this will spend more time in the AI power search course diving into all of this but the very high level intuition is that when users to interact with your documents right they they run queries they click on the data.
 These they click on things they like them add to cart purchase those are user behavioral signals and if you've got a sufficient amount of traffic you want to be collecting those and leveraging them to build reflected intelligence algorithms so one of the types I mentioned several earlier signals boosting collaborative filtering and matrix factorization learning to rank and knowledge graph learning but specifically on collaborative filtering which is mostly focused on personalized search.
 So I'm a personalized search or understanding user behavior to generate better personalized results we typically leverage collaborative filtering which is now for them for doing recommendations so I start with you know particular item or particular user and I recommend other items based upon that item or user.
 So it typically looks like right somebody run searches or purchases things like apple and Macbook and then these are the items they interact with you know iPads and Macbook errors things like that and then for that user we can generate this list of recommendations based upon running this call out collaborative filtering algorithm in this case.
 I want to briefly mention again that with typical content based embeddings I mentioned latent features earlier typically you have items and there's these densely packed dimensions that represents different features collectively you know like this particular feature might have a strong correlation with size this one I have a strong correlation with color this one might have a strong correlation with you know is this kind of like a computer but they.
 Those meaning spread across many different of these dimensions similarly whenever we're doing collaborative filtering these also rely on latent features or latent dimensions so for example if I have a bunch of users my first user likes these three movies my next user likes these three movies my third user likes these three my next user likes these three and my last user likes these three.
 You can kind of visually see here you know these are sort of you know there's some similarity here there's some similarity here your brains probably picking out what it is but if I were to map these conceptually I would say that users one two and three tended to like movies that were about superheroes made by Marvel studios and occasionally Warner Brothers they're all action movies and they're not suitable for small children.
 Whereas users four and five all like animated movies all of them are suitable for small children and all of them are made by Disney and Pixar a collaborative filtering algorithm sort of discovers these relationships and recommends based upon them because they exist in the underlying documents even though we don't have them modeled out explicitly and the way this works with collaborative filtering as we do matrix factorization so we start with a user item.
Matrix where here's my list of users and here's my items and then these are sort of the the amount to which they like those items we can derive this based upon just their querying click patterns.
 The intermediate step for collaborative filtering is matrix factorization which is taking this underlying user item interaction matrix and trying to break it into two different matrices this user feature matrix and this item feature matrix and the idea is that if I can generate a set of latent values associated with this user across some number of dimensions I'm only showing three here visually because it's a PowerPoint slide but you know there be more.
 And if I have you know the same latent dimensions over here for the items when I multiply a particular user and their sort of their particular values associated with these latent dimensions with the movie that I'm pulling apart how much of this belongs to the movie and how much of this belongs to the user in terms of the way I'm going to do it.
 So this is a user embedding and this is an item embedding what that means is that and this is just how it works to do collaborative filtering actually generate recommendations for for particular items not particularly useful for today but what I can do is I can generate these latent embeddings and these essentially allow me to create a behavioral embedding space for my items.
So I've done that I can add these behavioral embeddings onto documents just like I do with content based embeddings or whether it's images or text or what have you and then leverage those as a behavioral space.
So we do this commonly with you know personalized search for example we'll go through this in the course but if I have a person who previously searched for Hello Kitty plus toy GE electric razor GE bright white light bulbs Samsung stainless steel refrigerator.
 I can take a normal query key required for microwave which just returns random microwaves if I use these vectors and properly with no guard rails I might do things like blur the lines between categories most people if they've searched for a Samsung stainless steel refrigerator the best results here would be a Samsung stainless steel microwave but if you do this wrong the sort of naive approach is you know I might end up with a Hello Kitty microwave or a you know Panasonic microwave.
Not Panasonic but I they might end up with things that don't exactly match all of the preferences when the category again for another day but this is how behavioral vector space would typically be used.
But ultimately there's a lot of ticks of tips and tricks you can use to do AI powered search to combine all of these different techniques that you might use to run searches and to query understanding a relevance and sort of integrate one whole vectors in various places.
 So there's lots of different query paradigm to experiment with to merge using wormhole vectors but that's the general idea I wanted to kind of introduce today to get the discussion going about going from thinking of these vector spaces is entirely sort of orthogonal we have to query them separately or maybe I could even like query them in the same query but I'm filtering on them separately to trying to actually pull out the semantic understanding from one vector space and use that to craft.
A sort of wormhole or hopping off point to another vector space to sort of continue to explore leveraging a different query paradigm so that's pretty much it for the talk for today. Dima Dima, I don't know if you want to start to dive in some questions.
I know some people will have to hop off at the top of the hour because this is scheduled for an hour but I'm also happy to just kind of keep going with questions a little bit after if it makes sense and people can drop off when they want but let's maybe dive into some discussion.
Yeah, we have a bunch of questions thanks to a bunch. This is fantastic topic. I just recently traveled to Texas from Finland and it took me like 12 hours. I wish there was a wormhole, you know jump through points so I could just end up there much quicker. We have so many questions, man.
So I'll defer my questions off and I'll just jump. There is one logistic question from Arthjune. I hope I pronounce you name correctly. I'm sorry. How is the course different from the AI part search book and then later is this topic wormhole vectors covered in the book? Awesome.
Okay, so I would say there's material wise there's probably like a about a 40% overlap. Like the book is a good solid foundation for how to think about AI powered search. Obviously we go through all the mental models and lots of code examples.
So a lot of the labs and a lot of the code for the course will come from the book. However, there's a lot of new topics and things that we just like, you know, we couldn't write a thousand page book.
And so there's a lot of things we just couldn't get to because we had to start from the beginning and frame it.
So things like late interaction models, things like a gentick search that aren't in the book that like late interaction models are reference, but we just couldn't get into depth that are more modern and interesting ways to solve problems.
Things like many coil, which I mentioned, those things are will be in the course and unique to the course and we'll have guest speakers who are experts in those things. So I would say the course doesn't expect you to have read the book or to understand the fundamentals in the book will cover those.
But we won't cover everything in the book and we'll also cover a lot of things that aren't in the book and go in deeper depth. And so I would say, you know, if you've read the book, the course is still going to be like really valuable.
And even if you can't make all the sessions again, the videos and all the materials, you know, available for you forever. So, but so you don't have to have read the book to take the course, but if you have read the book, the course is still going to be massively useful. Yeah.
So the two implement each other. And by the way, I own the book and it's amazing read, you know, in silency. And then the course is a different way of, you know, engaging with the material like a dynamic way.
Well, I didn't answer the last part, which is, will wormhole vectors be covered? They will definitely be covered more so as like the techniques and strategies for how to hop back and forth between.
So some of it's actually in the book, the semantic knowledge graph stuff is already in the book, but the, yeah, we'll, we'll definitely talk about wormhole vectors explicitly and have some more specific examples people can play with. Yeah, awesome.
And I do want to mention this is like experimental and emerging and there's, there's some things that I glossed over today in terms of, you know, hopping to a particular point versus trying to hop to like a region and have more of a shape that we could chat about as well.
But, but yeah, there's some, some, there's still some things I'm doing to kind of better understand it fine to know. Yeah, awesome. I'm trying to speed up, but there's a question from Claudio.
What are the latent features is basically where you switched from sort of explicit feature metrics to like latent features. Maybe I can take it quickly. It's basically in an LLEM.
So if you deal with an encoder model, where you generate embeddings, basically these are like internal representations that the model learns and they're like compressed. They're like abstract way of, you know, dealing with patterns or relationships and your data.
It's not exactly that, you know, directly that black and white, but the thing is that I like on the conceptual level, they're like internal weights that the model learns. Then there is a question from Julian, very concrete one.
Can you give an exact, concrete example of how to compute the wormhole vector from sparse to dense space. Yeah, so I had a slide. It's sparse to dense is the easy one, which is, let me, let me go back to the slide. One second almost there. Here we go.
So to go from sparse to dense, think of it this way, you've got a bunch of documents in your index and you generate embeddings for those documents. That's how your dense space is constructed, right? Those embeddings on the documents.
If you query for the documents using keywords in your sparse space, then you're still matching that set of documents and all of those documents have the embeddings on them. So all you do is run a keyword search on your documents.
Take the top end documents are the most relevant, right? They hopefully semantically represent the concept, the best. And then you take those embeddings off of them and you literally adverse them together. The code for that is on the screen right here.
And that you just generate this pooled embedding.
It's that notion of Darth Vader versus puppy and finding the puppy Darth Vader in the middle, right? If someone were to run a keyword search and it's sort of is easy to think of this with a single keyword, but let's go back to my, what it, let's go back to cheese pizza, right?
Like if I search for pizza, I'm going to match a bunch of pizzas.
If I search for maybe cheese pizzas back as all pizza has cheese, let's do cinnamon bread sticks, right? If I search for bread, I'm going to find bread, you know, documents have the word bread. If I search for cinnamon, I find documents with cinnamon.
If I search for sticks, I find documents with sticks sticks by itself isn't really what I'm looking for.
But if I do cinnamon bread sticks, then I'm finding all of the documents that have those terms together, which likely are cinnamon breadsticks or have the notion of cinnamon breadsticks or talking about cinnamon breadsticks.
 So if I take all of those documents, the most relevant ones, and I generate, and I average their embeddings together and go over to the dense space where I land should be where the concept of cinnamon breadsticks is and things nearby, which may not have the word cinnamon bread or sticks in them should come back.
I might get, you know, certain kinds of donuts and certain kind that might get like a churro or something like that. So that's how it works. But this is the math here. That's actually the easiest way is to go from sparse to dense. The dense to sparse requires a semantic knowledge graph or similar.
Awesome. I hope this answers your question, Julianne. If not feel free to unmute and ask full of questions. Otherwise, I'll jump to the next one from Ursula.
Do we build the inverted index and the forward index to build the knowledge graph using just some document chunks? Do we need a much bigger document base to make it work? That's a good question.
 So the best way to, for a semantic knowledge graph to work the best, you need to have overlaps of terms that across documents, meaning if I take something like stack exchange, where there's a bunch of topics being talked about, you'll have lots of people who use the same words together and the same documents.
When that happens, you can easily find sets of terms that overlap commonly and use the semantic knowledge graph to generate semantic understanding and relationships based upon those co occurrences. All the math for that's in the AI powered search book, but that's when it works the best.
Something like Wikipedia is actually even though it's commonly used for like every data science project. It's actually really bad for semantic knowledge graphs because every Wikipedia document tends to be about a particular topic.
And other than common terminology, you tend to not have a lot of overlap across documents because they're all focused on one idea. So for semantic knowledge graph to work well, you typically are going to want to have overlap across your documents.
What that means is that if you chunk your document so small that you only have like a couple of words or sentences or something like that, you lose a lot of that context. I mean, in general, when you chunk, you lose context, that's the problem with chunking with most forms of chunking.
And so you have to be careful not to chunk too much, but the end versus also true. If you only have 100 documents in every single one of them is a thousand pages long, well, there's way too much overlap and everything is related at that point.
So I would say it's no different than just how you would typically segment your documents for any search problem, right? You need to be granular enough to be useful, but not broad enough to, you know, kind of be too general.
And now the logistical question from our unit whether we will share slides. Yes, absolutely. Yeah, the video for this everybody who signed up will get and probably like 48 hours, you'll get emailed a copy of the video. So you can refer back to it.
And I'll also send an email with the slides probably shortly thereafter. Yeah, and I plan to publish this in the vector forecast as well. Yes, absolutely. Later. The next question is really cool from Claudia creating a warmhole vector that will move us from embedding space to sparse vector.
I understand the methodology, but the way back now, how do we aggregate a set of sparse vectors that represent documents in a way that will allow us to move us to the embedding space.
So in the words from the sparse like you showed tray, we have like millions of dimensions, right? How do we compact that right and don't lose anything and not introduce any noise when we're not way to the dense space. Yeah, so it's it's a really great question.
I answered it technically in terms of pulling, but let me add some color to it in terms of techniques. So the. There's a couple of things here.
One, whenever you're querying in an inverted index, there's typically a kind of Boolean matching phase and then there's a ranking phase, meaning if you had 10 million documents in your index, you're not going to return a ranked list of 10 million documents.
So I'm going to probably return the documents that have the specific keywords you search for, which is going to be a much smaller document set. And so that's and you can do the same thing on the dense side with, you know, cutoffs on cosine similarity or something like that.
But step one is you start with a condensed document set that should represent generally the meaning of what you searched for using the keywords you searched for on the left school side.
However, because the idea of a wormhole vector is to find the best corresponding region in the other semantic space, it can often be useful to not take that entire document set either matching the query. But if you feel confident about your ranking, then you can take the top in documents.
So maybe you match, you know, 10,000 and maybe you only take the top 100 and say, hey, from the top 100, if you know your relevance ranking is good, then you're going to like use that to generate a more precise wormhole vector to the meaning of those top documents over to the dense space.
So, and that whether you go with the full matching document set or you go with the like the top in that's really a just practical matter of how confident you are on the ranking. If you're if you're really confident in your relevance, you should go with the more relevant documents.
And if you're not just take the whole document set and it should sort of average out, you know, the meaning. Another thing that we didn't really get into is that the strategy, the technique I was showing if I let me jump back to the final slide one second. So I jump back to here.
So the technique that I'm showing where I get my document set, pull my embedding together, that ultimately gives me a single embedding, which is a single point over here in my dense vector space. So the strategy is that different queries have different specificity.
So imagine this is like a job search engine. If I run a search for, you know, senior AI search engineer, action, culberts, signals boosting and, you know, collaborative filter. If I run that search, that's a very specific search.
Frankly, it probably doesn't match anybody, but if I ran that search, it would be a very small number of documents is very specific. However, and so in that case having a point kind of makes sense. However, if I ran a search for sales. So that's like a third of all jobs.
And for me to take the notion of sales, which is probably a giant region and this vector space with lots of nuance inside of it and to then turn that into just a point in the other vector space.
It's probably not going to work out super well because there's probably sales is probably distributed across that other vector space in a much larger region. And so, there's this notion of query specificity, which is also really useful.
So I would actually argue that the better way to do this technique is as part of your initial query when you're sort of finding the set of documents.
 If you can look, for example, at the embeddings and do just like a cosine similarity across the embeddings that you're pulling, you can, you can go from like a bunch of embeddings that are just pulled together into a point to actually saying what is the like relative size of the range of the co science within these embeddings.
And if it's a very large range, I understand that this is not a very specific query. It's a broad query. Therefore, when I go query in the dense space, I need to draw a larger radius or larger kind of shape around what I'm searching for.
So ideally you're actually searching for a shape and not just a point. But literally every vector search implementation I've seen at any company is searching on embeddings as points and just looking for the nearest things, not searching on shapes.
And so we don't even really have the query patterns and paradigms in place today to do that kind of a query. But I think that would be a further improvement on the paradigm here. Awesome. Yeah, Tim Allison says thank you. Thanks, Tim. The next question is from Julian.
Can you recommend any papers or the material to explore the topic further? So not really. So the one whole vector thing is something I kind of came up with. I will say, well, two things. One, semantic knowledge graphs.
I actually was the lead author on the original semantic knowledge graph paper back in like I don't know, 2016 or whatever was published. So this notion of being able to jump between spaces back to a. Sparse space. You could look at that paper.
If you want an actual research paper, I've also given lots of talks about it. It's in the AI powered search book. It'll be in the course. However, the notion of taking running a query and pulling vectors together. And even the notion of specific query specificity that co-signed similarity thing.
If you look at Daniel Tukalang, he actually did a lightning talk with us a week or two ago on query understanding. He actually talks about this notion of a bag of documents to represent a query. It's functionally the exact same thing. Right.
So if I run a query and think of the queries, meaning as being represented by the set of documents that match that query. Then to take that set of documents that holds that meaning and pull the embedding is to create an average embedding that represents that meaning and embedding space.
It's functionally the same thing that Daniel describes when he talks about bags of documents. So I would say look at Daniel's work. Look at the lightning talk he gave a week or two ago with us. And those are some good resources. And of course, the book and the course. Yeah, awesome.
Maybe at some point of paper as well. Right. Yeah, it's definitely possible. I need a lot of good. I need e-vows on how this actually does in practice. Yeah, absolutely. Are you not the same question? I'll skip that. Most of all, he's asking for a knife phone query cases.
So charges may also appear would be correct to take the average. Would it be correct to take the average of them? So good question. So, yeah, if you, so, Lexical queries work really well when you've got particular terms you're looking for, whether it's an ID or whether it's a keyword.
They don't work as well with like semantic meaning, whereas in a dent space, obviously you query on meaning, but if you try to search for product ID in a dent space, unless you've fine tuned it for that, it's going to do an awful job.
 And so, in the case of like searching for iPhone and getting iPhone cases, this somewhat gets back to what I said earlier about ideally, if you take the top in documents that are the most relevant and you limit to that, like if you're ranking algorithm can already sort of understand that when someone searches for iPhone that, you know, they mean, you know, an actual iPhone versus a case.
That's a, that's a better way to go versus just anything that matches the term. That being said, what you can do is you can, for example, in that case, search for iPhone, find the iPhone cases, along with iPhone, get that average vector.
And then there's still this region of, you know, along certain dimensions, it's associated with iPhone.
If you hopped over to the behavioral embedding space, what you're going to find is that, you know, hey, these cases are very highly correlated to these items that the iPhones that actually correspond to those cases.
So that might be a case where you would want to hop to the behavioral space and leverage what's there. There's also just to note, we've talked about taking entire queries and hopping from, you know, between spaces.
But there's also a line of thinking and practice here around using this for query understanding, not just ranking.
 And so you could, for example, split the query into individual keywords, you know, iPhone, like just the word iPhone on the, and you could also search on the dense space and you could try to take the individual pieces and find things related to them and then leverage that for query understanding to hop back and forth between spaces.
I, the answer is you still have the fundamental limitations of each space. But imagine if somebody searched for, you know, I want a phone that's really good at blah, blah, blah, that's made by Apple.
With product ID X, right? And imagine trying to search for that like an or a query on the submit on the, like school side, and you'll actually match that ID and probably have it come up at the very top. And then you can imagine searching for that embedding on the, on the dense space.
And you could imagine for each of those hopping back and forth and trying to see what documents are there a couple of times. So there's, look, sure, and answer there's a lot of different ways that you could leverage this technique to be hopping back and forth.
I'm not going to claim now that I've thought through every single one of them and there's lots of ways to do it.
But I think as an introduction to the topic and as a tool that you can add to your tool belt to be able to get explain ability and another vector space based upon what you found in the first vector space. I think this is a really cool technique.
And I just wanted to kind of present it and get feedback and enjoy this discussion. Thanks, Trey. We're quite over time. Thanks everyone for staying on and hopefully if you can still stay on, we can get to the bottom of the list.
Yeah, and by the way, to your answer, Trey, I think somewhere there is probably a notion of search result diversity as well. Right? So even if the user types iPhone, they only mean the phone, but they actually may mean something else.
Right? So I think that's really showing diverse results and then traversing to the other side with those diverse results could also make sense. Absolutely. Yeah.
Then Arjun is asking, is there, if I summarize the question, is there a cheaper way than using semantic knowledge graph? Maybe, you know, the fear is that the graph approach is computation expensive? Is there some cheap way to running an embedding model typically? But it just depends.
Yeah, I mean, there's other techniques. Like if you have a fine tune, blade model, for example, it can give you very comparable kind of semantic understanding on the sparse side. So with that is you have to fine tune it to your data.
And also one of the benefits of the semantic knowledge graph is that if you, I'm just going to quickly jump to the slide to show you this one. Let me do the one that's got keywords. Here we go.
There here with the semantic knowledge graph approach, you have the ability to not just represent the query with a bunch of terms with values, but you can actually use any field. So it's really useful to be able to describe it with a category of Korean and a bunch of terms here.
And maybe you've got other fields on your documents that are really useful for describing the document. And the text on me of some sort, the semantic knowledge graph gives you a lot richer ability to turn the set of documents into a fully expressive query. So yeah, there's other techniques.
You could look at displayed things like that, but nothing that's nearly as expressive. And these are the concepts you're going to cover in the course, right? Yeah, we'll cover it all in the course for those who are interested to learn more.
Sorry, not trying to make this talk just a big promo for the course, but this is really warm old vectors by themselves are really interesting topic, but yeah, obviously, I would love if you would join us in the course. It'll be fine.
The vibrato is asking, you know, what do you think about some of the reputable search sites like indeed on LinkedIn, where searching for a male engineer will bring your results like data engineers and, you know, whatever unrelated stuff, not directly related stuff.
And so the question is why search documents, not based on the entire user query, right? Only part of it. So I'm trying to understand the question in relation to the wormhole vector topic. Yeah, I think it's more, I think it's less directly related. I think it's more auring on the side of data bias.
Why this reputable search sites do not sort of utilize the semantic search, you know, one to one in a way. Yeah, I got you.
I mean, the reality is that most AI powered search algorithms, really all of them are used data and the data is biased, right? So like the reality is in the world, if you look at data engineering jobs, they are more statistically skewed towards males being in those jobs and females.
That doesn't mean that that doesn't mean anything in terms of who can do the job or who can't. It's just a reality that, you know, there tend to be more males and engineering and therefore the data is reflecting that it would be nice to be able to take those biases out.
And in fact, there's ways you can do that, but they're extra work. And so the out of the box algorithms that are typically employed don't necessarily try to tackle those biases.
 So yeah, I think it's I think it's valiant to, you know, try to, especially when you're dealing with things like people's livelihoods and, you know, careers and things like that, I think it's, it's a great exercise and something they should focus on, but it's like, unfortunately, kind of a reality of the underlying data that's being bubbled up, I think.
Yeah, my take is the data is biased. Yeah, I agree.
 I've built one job search engine like couple companies ago and my take is that probably these companies are trying to avoid, you know, these traps when you're super, super precise query, we will either lead to nothing or lead to just a couple of jobs on the screen because their business is to show you as many jobs as possible so that they can monetize that.
I'm not going to talk about maybe like business element as well, but I'm sure there are other like technical aspects of this, which we should note disregard. Sure.
And then from Arjun, and you experience how much do the following results differ first query against dance of vector space directly and the second query in sparse vector space and warm hole to dance vector space and finally get the dogs that are similar to the warm hole vector average vector.
Yeah, yeah, that's.
 I mean, at the end of the day, the, if you have a query that you run against your lexical space that matches mostly documents that are related to the query and then you hop over to the dance space, you're typically going to get a lot of overlap because the lexical space semantics are going to be very similar to the dense vector space semantics in terms of like the underlying meaning.
If you were to take the lexical space and I should mention you can actually use a one hole vector in the same vector space.
I kind of showed that with like taking a query like lasagna and then rewriting it with a more expanded out lexical query with a category of Italian and so you don't have to actually jump between different vector spaces.
You can even jump within the same vector space and I think that in this context, the more similar, the meaning of the underlying set of documents is matching each query, the more interesting you're going to be able to find missing links in the other vector space.
 If you have very orthogonal queries, like you can imagine on the lexical side searching for, you know, orange juice and Nintendo switch right like or you'll get nothing for that but aren't you or Nintendo switch will you basically end up with a document set that is really two separate documents sets right it like that there there's not a lot of overlap and if you hop over to the dense space and get the average of those.
 There's still going to be things that are like probably close to Nintendo switch and probably close to orange but orange juice but the more different those things are you might get some weird stuff in between because you're looking now looking across two different places any any Nintendo switch stuff that's orange or related to juice or something might show up but it's it's going to be weird and so this isn't like a magical silver bullet that solves every query understanding or every relevance problem it's just another tool in our toolkit.
To be able to better reason about the underlying documents and queries into you explain queries and another modality if you will another query modality yeah in other words what you're searching for should still kind of make sense yeah yeah it does it probably will return some useful results.
Tips a thank you thank you tips. Rostem. Rostem says you know the impact of of documents agmentation so basically what are the suggestions to improve that so that warm whole vectors would be useful.
Yeah I think I think it's common sense like the same way that you would chunk documents for doing rag you know I have to think of you.
 If the documents are too big then there's too much loss of specificity and too much context being blurred together and if the documents are too tiny then you you're losing context and they're they're too specific and not enough overlap so I think you know typical like whatever your domain is you know I mean if you've got giant PDFs that are books maybe break into chapters or possibly even sections of chapters of the large sections.
But yeah just like use common sense what's a reasonable size document that represents the meaning of something that is like sort of. It's called integral like like a whole like a whole thing a whole concept. Yeah yeah.
It depends on the domain but it's I would just say you know your common sense is probably going to take you far on that one. Yeah and like for long documents that are like a thousand pages for sure you want to do that and maybe the last question is from Arjun.
Can this idea of warm whole vectors give us more serendipitous results. Give us more what serendipitous results.
 Yeah absolutely so yeah think of just the behavioral space right if I run a query a keyword query and then I want to find other things that are related to this that don't match the terms and maybe don't even match the meaning but like user behavior is said that these things I should suggest I'm basically infusing recommendations then if I help over to the cement to the dense space.
Then I take my keywords and I'm finding other things that share meaning but don't necessarily have that keyword.
 I'm starting with dense and I hop over to the electrical side I'm making sure that I'm finding things with that meaning but I'm adding in keywords that were completely ignored by the dense space that was not necessarily a serendipitous that's just like fixing problems but I would say going from lexical to semantic more so we'll get you.
So the things that were dismissed but yeah for actual serendipitous you're probably the behavioral space is probably going to give you a lot more magic there. Alright awesome I think it's a wrap thanks so much everyone.
Thank you so much for the presentation for the idea and for pounding at the questions with such an immense speed. Thank you all for time. This was awesome awesome thanks to me to really appreciate you coming on this was awesome thanks everybody for joining.
And yeah the video and slides and everything will be coming out to you and I hope to see you soon. Thank you. See you soon. Bye bye. Bye.