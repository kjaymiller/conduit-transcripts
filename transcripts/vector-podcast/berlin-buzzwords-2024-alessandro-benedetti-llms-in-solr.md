---
description: '<p>Alessandro''s talk on Hybrid Search with Apache Solr Reciprocal Rank
  Fusion: <a target="_blank" rel="noopener noreferrer nofollow" href="https://www.youtube.com/watch?v=8x2cbT5CCEM&amp;list=PLq-odUc2x7i8jHpa6PHGzmxfAPEz-c-on&amp;index=5">https://www.youtube.com/watch?v=8x2cbT5CCEM&amp;list=PLq-odUc2x7i8jHpa6PHGzmxfAPEz-c-on&amp;index=5</a></p><p></p><p>00:00
  Intro</p><p>00:50 Alessandro''s take on the bbuzz''24 conference</p><p>01:25 What
  and value of hybrid search</p><p>04:55 Explainability of vector search results to
  users</p><p>09:27 Explainability of vector search results to search engineers</p><p>13:12
  State of hybrid search in Apache Solr</p><p>14:32 What''s in Reciprocal Rank Fusion
  beyond round-robin?</p><p>18:30 Open source for LLMs</p><p>22:48 How we should approach
  this issue in business and research</p><p>26:12 How to maintain the status of an
  open-source LLM / system </p><p>30:06 Prompt engineering (hope and determinism)</p><p>34:03
  DSpy</p><p>35:16 What''s next in Solr</p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20241107_011152_a59e71acc05fe03f850677d583f5111a.png
pub_date: Thu, 07 Nov 2024 13:59:44 GMT
title: Berlin Buzzwords 2024 - Alessandro Benedetti - LLMs in Solr
url: https://rss.com/podcasts/vector-podcast/1741381
---

All right, Dr. Podcast and here I have Alessandra Benedetti with me his second time on the podcast actually and exactly about same place we recorded two years ago. I remember on Berlin was works. Yeah, we were here. Yeah, I guess I got 22. It was.
It was by the way a lot no easier if you remember them now but was closing day that we like people. Yeah, but I think it's almost end of day as well here. First day of the conference and yeah, I wanted to chat with you.
How do you like the conference so far? So has been so far like a great conference. We've been seeing like many talks about the language modern integration with search. So that's the biggest new trend.
Vector based search is still quite a strong topic and in general with testing also like evaluation and explainability discussions around like vector based search or in general language models. And and my thoughts was about hybrid search. Hybrid search.
Yeah, so you work a lot on on solar right that's your kind of like playground and that's where you integrate things but also then I heard that like guys at Reddit are using the work that you've been doing also in solar. So that's amazing.
Tell me more a bit more about what is hybrid search right? How do you see it? What's the value? And and basically maybe what are the challenges that you needed to solve and you still see related to hybrid search?
So the first point and the reason I decided to start working a little bit more in hybrid search and contributing this even our rank vision to solar is because of the limitations of vector based search.
So vector based search of course introduces like the ability of closing the semantic gaps with light queries and documents with some some limitations right? So the explainability bar for example is an aspect I care a lot and it's just very difficult to explain vector based search results.
Yeah, we have I dimensional. So many many dimensions in the vectors and humans are not really good in managing many dimensions. We live in a three dimensional world and this even difficult for us to to understand life for dimensional life. Yeah, then we have like many elements in those vectors.
So each feature in the vector doesn't have like a meaning for the humans. So you have like 768 dimensions in your vectors and there's no single dimension that means something semantic. So it's just the output of some machiner model but we can interpret like what it is.
And we can interpret what would happen if that feature goes higher or lower. I mean does a higher value for that feature means higher relevance or not? You can't really do that with vector based search. So these kind of problems. Yeah, start to have like an input. Right.
So you have like your clients using vector based search. They are happy and then they are not and they want to explain for example, yeah, what happens. Yeah, and another limitation is keyword based matching. So by the vector based search try to solve the vocabulary in his best problem.
So if you have terms in your vocabulary that's different from the vocabulary used for queries. Yeah. At the same time, users are used to have keyword matching documents in their response.
So when you don't provide keyword matching document in their response, they're going to be like problems and questions. Yeah. Why do I see this now? And why I don't see for example this title. Oh yeah. So without any search, the idea is to mitigate those problems.
So mix up different query results sets. Potentially like vector based search results and traditional keyword based search results. Yeah. Get back one result set. Yeah. Let's try to combine both words. Interesting.
And if we kind of step forward from this, let's say we deployed hybrid search, so now it basically takes some similar documents from keyword hits and then another one from vector. You still get those documents that do not have keyword matches, right, from the vector space.
Do you know or maybe you have employed some ways of explaining to the user why they see them? So that's an interesting point actually at the discussion recently about how can we explain better by vector based search. So we mentioned already all the problems. We've explained the what can we do bet.
So there are other approaches that just cure dense vector based search such as learned sparse retrieval for example, where you learn query or document expansion term candidates based on learned models. So based on the probability you will expand your queries with additional terms.
So that's a little bit more explainable because at least you get back from the machine learning model alternative terms for the queries and the document. Yeah. It's still a first layer of explainability. So you have some that's like additional concepts. So it's easier to understand.
Still you have the probability assigned to their pair. So if it goes wrong, you may end up with unreasonable terms. So not perfect. A little bit better, maybe a little bit more explainable.
And then there are approaches such callvert where you encode your sentence, not just to just one vector back to a sequence of vectors. So multiple vectors, one pair for an action. And you do the same for your documents.
And then you you basically return results based on the similarity between not just a single query vector and the document vector, but multiple query vectors. So each query at each query vector, which is meant to be probably a term with the terms in the document.
So you may be able to highlight the terms in the document that are close to the terms in the query. Yeah. Also in this case, of course, it's just a first layer of explainability because then if this goes wrong, of course, again, you have sequences of vectors.
So you can get like a sort of itm up of what query terms match is like, more or less the document ones, but still not perfect. Yeah, sure.
Of course, it's kind of like maybe experimentation that is required, right? What works for you? What what what is the end product? But maybe one question is for me as a user, right? Let's say I'm using solar and you offer hybrid search now.
Are you already offering or will you consider at some point offering the capability to ingrain what you just said into let's say highlighter in solar that will it will actually build me the snippet regardless of the source of that document, whether it's keyboard or vector.
That's a very interesting question because there are in my opinion two layers of explainability for engineers. So we need to work on the engine and change the ranking, change the matching and user's equipment. Yeah.
So a user that just want to know why for example, what is there and for user's finability actually, my company, we design and develop the highlighter.
We call the neural highlighter that takes in input the wireless model and in the response, will I like the snippet for each result in documents, not based on let's say on match, but based on the question as a system powered by a level model. Yeah.
So in this way, you will be able to highlight part of the original document that are semantically close to the interesting place. Can you say the name again? What was the name? It's called the neural highlighter. Neural neural highlighter. So it's your proprietary product right now. Yes.
It's a lot of synths, right? Yes. Right now, yes. We may contribute it to a open source integer. I don't know. Right now is one of our products. But I mean, it's a feature that you, is it offered as a standalone component? It's a plugin. It's a plugin. So you ins it's a plugin to pull it.
It's a plugin. That's the value prop as well, right? It doesn't always need to be open. It's something I can plug in and exactly. It takes in input the wireless model. Yeah. It's a response point. So you can write.
So that will help to explain results to the users, right? And you also mentioned, right? It's thanks for doing this distinction, making this distinction that there is also explainability for the engineers that is also important.
So can you a bit explain what you mean? Explainability for the engineers because I care about it a lot of force. But I used to be an engineer full time. And I need to know how, like, how to do it, how to tweak something.
But also, can I explain to myself that what I tweaked is actually the right thing, right? So, you know, kind of the process of engineering the. Yes.
So in solar, for example, there is a debug component that give you the ability to engineer to expand the response with the information about how the score was calculated. So in solar, when you have a query and you have a result, a score is calculated for that result for that query.
And this core will impact the ranking. So, descending order, literally, right from the highest core to the lowest. And normally, this core is explained showing why you get that mathematical calculations from the term frequencies. Yeah.
The length of the document field, the average length of the field, the document, frequency, hour, error, a term was, for example, and so on and so forth. So long mathematical expression that are readable to the user and you can understand, okay, I was aiming for this field to impact the score.
Let's see, let's see, really impact the score. With better research right now, the only explanation that you get from an engineer perspective, literally is within the top K. So this document was within a top K with a cosine similarity between the query vector and the document vector.
That's not really helpful. It's just confirming what's you know already, right? I mean, yeah, it's in the top K, it was written.
So one of the ideas I was thinking of, because actually quite far from the implementation is to explain the reason document in the results set, showing examples of, so the language models used to return embedded, were fine tuned on sentence similarity.
So this means there were pair of sentences with similar meaning and pair of sentences with this similar meaning in a way that to learn how to encode this amount. So I think it could be very interesting if to explain the reason a document is being returned.
 Because of vector cells, you show like a snippet, say, because there are, there is this similar pairs of sentences and this is this similar sentence, in the way that then potentially the engineer can go back and realize, okay, let's take a look to the original training data, for example, did I cover the example well or maybe they are wrong?
So I see like, oh, these two sentences are shown as similar, but they are not so.
It's just an idea, you know, study it. Wow, that's very interesting, because as you said, it's very limiting today to just know that geometric search happened and this is the result. Yeah, that's amazing.
I mean, it's really interesting that with this work, you are not really just taking something and applying to implementation, right? Like, I mean, implementing a plugin, but you actually go into the space of exploring thing because it's not like everything is done, right?
And maybe in some companies, it has been done, but they are not open sourcing, right? And so you need to do the search, the search of the solution.
That's very interesting. So in terms of functionality today, hybrid search is already available in solar, right? Is it already released? And big portion? Yeah, so there are different ways I need search can be performed in solar. So right now, we saw 9.6.
There are ways of combining results from electrical search and vector-based search and then re-rank them, for example, using learning pranks. So you give like different ways to different factors. So for example, the vector-based core or the traditional core. Yeah.
What is coming next, which was the topic of my talk is the receiver rank user. So that's coming with solar 9.7. So I guess in a couple of months, we're going to release it. Nice. And that is a way of adding hybrid search that is independent on this core and just based on the ranking of the results.
So you mix the different rank lists. Yeah, they can be two, maybe more. Yeah. It's support more supported, not just two. And then you combine them based on the position of the documents in the different rank list. Yeah.
The higher the position in the ranking, the best the probability that the document is going to end up in a higher final result set. Yeah, yeah.
Actually, when I was maybe you can help me understand this, but when we were trying reciprocal rank fusion with another search engine, we actually found implementation. So we could kind of plug it in and Python code, very quickly.
But then when we looked at the code, one of my engineers said, this looks like round, raw, and algorithm essentially. There is nothing particularly peculiar about it or tunable about it, which probably is not true, but I'm not sure what's your take on this.
So it felt like you have two lists and you basically just take the starting from the top, you take like in order, you know, these documents and you combine a blend at least, right?
But if you wanted to pay attention to some signals from these documents, you know, based on their features or or maybe you wanted to introduce a logic on top of this, right?
So you want to say, let's say in the context of geographic search, I want to find in top three results, I want to see a super popular B.
O.I. and I know what popular means. Another second result could be, I don't know, the closest one or maybe vice versa, depends. And so on so forth. So I have some kind of rules in embed and then maybe it stops becoming RRE, already, right? But I still go going, taking a step backwards.
Did I explain it right? Or other some parameters and RRE that I could kind of be tuning a bit to have the different outcome? There's not much to tune to be honest. So you got it right.
It's not only around roaming, because what you do is basically you give a new score to the documents that are based on all the rankings of that document in the results list.
So it's not like in Perliving where, for example, you go with one document, you pick from one range of lists and then to the other list, you pick another and then you choose which one should I go next.
It's more about life, let's see this document how many times it appears in the ranking list and where it appears in the ranking list and let's build this new score. So the more you are in the top positions, the more likely you end up in the top position of the final result list.
Given that, you're absolutely right that if you want to be like more advanced ranking systems, potentially like with different phases, different steps, it makes complete sense to maybe build your original candidate sets with receiver or infusion.
And then you re-rank, for example, using learning to rank and many features where you can have like, again, maybe the vector distances one feature, the similarity we want to feel from a expert perspective, popularity, geographical distance and many other features.
And then you apply learning for example, so you train a machine learning model to identify these weights. It makes perfect sense in my opinion.
I believe receiver, rank, fusion and in general, like let's call them simple approaches with our research, because if you take a look to the algorithm of receiver or rank fusion, it's not the core, it's actually open and open algorithm from 2009.
But this opened the doors in my opinion to build your original candidate set and then potentially like, yeah, you re-rank it okay. Yeah, yeah, yeah, she's not random, it's okay, she's already some reasons to be there.
And of course, like in any case, those without saying that we do need to have some method of combining these completely disparate spaces of scores, right? Into one.
And that could be actually even like different search engines operating on keyword level because they output different scores, right? So maybe even potentially I'm thinking separate charts of your data that also have their own idea, right? Local idea. So, yeah, incomparable, right? Awesome.
We also, not related to this completely different topic, like there was also a keynote today about sort of what open source means, right?
And without, of course, criticizing, but some companies were mentioned on this context where they claim that the LLMs are open source, but when you look at the licenses, they are restrictive, they actually do not allow you to use them independently, right? And kind of go and serve your customers.
 But you also just mentioned what the code was started recording is that there are also cases where model can be open source, and it's kind of like more or less abiding the principles of open source spirit, but then contract, but then the data that it was trained on is not open source or the methods that were applied to the data are not open source, right?
So to me, it sounds so important to keep kind of declaring what open source is, what are the principles, right?
 And maybe this keynote also shed some light, but you also, it seems like this topic is also very close to you, and you are in the open source contributing a lot, you are the commuter, like can you can you share your vision on what is open source, what are the implications for how this field should be developing?
I think it's a huge problem, especially because nowadays open washing, which is like the practice of associating openness to something that potentially is not really fully open, is happening a lot.
Here's open source is cool, open source show like a good habit, so you're the good guys if you if you do open source.
So as you said, we are not going to make names of companies or association that claim, for example, they lar language model were open source, but lar language models are complex systems. So the outputs, the final light waves on the neural network is just one little part of the entire picture.
Those lar language models are normally pre-trained on huge quantities of data with a pre-training algorithm.
So the pre-training data and the pre-training code, is it open? Is it not? I mean, many times it's not, not only not, it's not open, it's not even known, what kind of data is just generic internet scale data.
What about the fine tuning them?
 So once you get the pre-training, which is the unsupervised part, where you just explore the web, that's pretty simple, then you want to fine tune for specific tasks, like sentence similarity or instruction following or, I don't know, summarization, any kind of task you want to use the and to do that normally using an additional training data set that is particularly designed for that fine tuning task.
And again, is that open? So do you communicate and then you make it available? And the code for fine tuning, do you make it available?
The output of the pre-training, do you make it available separately from the output of the fine, the documentation, any data that explains what is done, why you found it?
 So I've read like an interesting paper that I guess we can share, like, as a comment from a university, they were like comparing all these aspects for the MS models and how famous like open source of the MS models actually behaved on each of these columns and would be surprising how a small percentage of these like, you know, big layers are actually open sourcing everything.
So it's not just the license that as you said correctly, sometimes it's limiting, but literally like the components shirt, sometimes it's just the final which is, is it helpful?
I mean, in open source, you want to cooperate, you want to improve the code, like in normal code, you have access to everything and you can like improve, you can help the community.
If you just access the ways, you can use it, but can you, for example, improve it? Can you understand if it's fair? In the data you was there? Yes. Yeah, it's really difficult. Yeah.
And so, what do you think these discussions should start or maybe it's ongoing? Are you part of some discussion? And how does it impact business and maybe research? Right? Because there are different sides of this coin.
Many of these things emerge in the academia space, but then they move to create value on the business side, but it could also be vice versa.
So what do you think? What are we going to address this?
So I know that the open source initiative, which is a group of people that directly directly open source manifest, so I try to basically think to ways of the finding open source is they are working on a definition of open source for AI models.
We are going to see hopefully soon enough, a definition of what it means for a model to be open source. And that is going to be great, because at that point it's not a matter of like, I believe it's open, and I claim it's open. It's open for its notes.
And everything is covered by a license that is going to be open or not.
 In terms of like impolination between like the academia and the industry, I think probably that's the most, I mean, this period is so important to see like cross pollination, because there are like many models that for example are designed and contributed by the academia that must then be used by organizations and the other way around, because of course there are like a lot of money involved in training and free training and fine training on the algorithm of the models.
So many, I mean, only few actually organizations are able to do this. So they should try, I mean, ideally to make it as open as possible in a way that then universities can focus on small components and potentially help in some more. Yeah, yeah.
I mean, you know, pre-training and internet scale is incredibly expensive from energy perspective especially. So I hope, you know, we reach the point where everything is open enough for also smaller organizations and academia organizations to 12.
Yeah, it's very interesting because there is always going to be this kind of play between, okay, this big company has all the servers, they need to train the model.
So they can also decide how they will do it and not kind of disclose, but then maybe the question that we need to be disputing and sort of discussing is that they still don't have all the data to train on, right? Potentially.
Like there have been some cases mentioned in the keynote, you know, when some company, we will not name the company, it goes and trains it on some articles of famous publishing house, right? And now that publishing house is unhappy because they say you took our articles without us knowing this.
Now, it now it kind of evokes this question, okay, when I was reading this article, there was probably some license which said you can not, you can do this, but not this, maybe there is something hidden, right? But only now we started discussing these things, right?
And that's very interesting topic, but do you think that, you know, when the companies will be, let's say, they have open source to model and they have checked everything on that manifesto or on that contract.
 Do you think that there will be still a need for some maybe tooling or some process to kind of continuously maintain the status of this model as open source because it may well happen that, you know, either the company or research institute, they go and accidentally use some data that doesn't anymore confirm, confirm, like, comply with this contract, right?
First of all, without other lands, do you think such thing exists, would say, for Apache Solar or you see that no one will find a library that is not the license that it has to be, plugs it in and we do a release of you seeing a solar.
I think there is some checker, right? Yeah. So these applies to certain extent to code as well, right? So you are a contributor. When you sign basically the, let's say contract with the Apache Solar Foundation, you are sure that any kind of contribution you do is your own.
So there's not being COVID, for example, that was not COVID-rided and the sort of thing. It's genuinely created by you. It's genuinely created by you. So to certain extent, that would be a similar thing to potentially add some training data.
I think probably it's a little bit less likely that like in an existing large-language model, for example, someone would contribute a little more data. I mean, it's more likely that maybe you you would change a little bit the code, for example, responsible of fine tuning and it sort of things.
But still, I think there will be this layer of responsibility that wouldn't wait on the shoulders of the contributors because of course, you kind of have control on these single individuals.
And you need to have like this sort of layer where the no-profit, the Schopen source project protects itself from. Yeah.
Because I can imagine that again, it's probably putting it to extremes, but there could be eventually some tooling where you take the model and you introspect its behavior and you can make a guess on which data it was trained. Potentially. Or at least find some similarities with how it produces.
I mean, there been some attacks, so to say, right? So you can actually probe the model and see what it outputs, right? You can even break some models sometimes. That's true. So that's more like on the hacker side or the the bad hacker side. But I mean, there probably will be tooling.
Do you think it's possible that there will be tooling kind of checking the model and and making some hypothesis. And as you said, once caught, that organization will kind of lose its trust, right? So obviously, everyone wants to be kind of accountable and so on.
But then there could be a flip side of that that you can kind of accidentally assume that they did it, but that's not true, right? Now that becomes a very hard debate, right? So it's an area which I think deserves exploration and study.
And I believe that's being accountable of like the data you use and disclosing it, of course, is the first step. But then also validating that companies send the truth, for example, I think it's going to be important to build trust and to make sure that what you display is actually what happens.
Because we never know. It's very interesting. Was there some other topic you wanted to cover? I mean, are you also working on Raga or anything of that or evaluating the LLAM based search? We are working on many different integration with LLAM models. Retriol passage generation is one of it.
Nugro language parsing, for example, is another so moving from Nugro language to structured queries. Yeah. Probably the last thing we can discuss, the last topic we can discuss is prompt engineering. Yeah.
Briefly, because yes, it's this naming convention is something that really hurts me because it's not engineering at all in my opinion because you're just attempting to communicate with something and you don't know what to expect.
Because I've seen, I mean, I've seen tools today with people saying, you write this prompt and you hope you get this response. Yeah. You type this prompt and you ask, please give me the response. She is, to me, something that is, not scientific at all. It's not scientific. It's not scientific.
It's not science. You can't just be comfortable. Yeah, you can be comfortable. Yeah. So there's, in my opinion, just to give you short, a big margin of improvement there to interact with LLAM in a more program idea.
I want to specify it as with rules and get back a response that satisfies those rules. If I want to select an item from a list, I want to select an item from the list. I wonder LLAM is more than to be able to just select the item from the list.
I'm not 80% of the time, select the item on the list and 20% of the time, select the item and give me an explanation. I just wanted the item from the list. Yeah.
And right now, I've seen, and we will see the conference because I've seen in the agenda, there are a lot of many talks about trying to solve this problem. But right now, what I've seen as a possible solution is just like you post validates the response and you go back.
Like, okay, yeah, I asked for a specific JSON in the response. There are mistakes. It's like, it's not a possible JSON. I say, I go back to the LLAM and I say, this is not a possible JSON. Can you fix it? And again, and again, and again, which is not really something you want to go to production.
So in short, in my opinion, like using LLAM with models, program, I right now is full for approval concepts. But would I bring to production like out of the box like these sort of approaches? I want, because I wouldn't, you know, brings, I want to bring something that is deterministic. Yeah.
It does what I want to do 100% of the time. Sometimes. And I don't want to hope. It's a good thing. Well, I want to make it work.
Yeah, but it's also like I see it's very interesting topic, by the way, but I also see some level of contradiction that to like between non deterministic and hallucinating model essentially hallucinating by design because it keeps predicting the terms, right?
And some level of determinism as you just explained, right? But I guess, but I guess at the same time, someone might say that our life is not that deterministic, many moving parts and we still find a way to, I don't know, leave it and then build something, right? Yeah, there's something that moves.
I think, you know, it's the first, anyway, we are experiencing, in my opinion, at first, that in these new worlds, of AI big models. So I think it's fair. They were born to auto complete text, to generate text. And now we are trying to use them to do tasks. Yes. Which is okay.
We as humans use language to, to task. Yeah. So I just guess, and then we end up with programming. Yes. Computers, right? So let's play it in a little bit more that would be programmable. Yeah. I mean, it doesn't remind a bit.
I haven't explored it, but to mention GSPY, the packaging, probably you heard about it, right? Which replaces the prompt engineering with more programmable sort of way of doing it.
I still don't know how it works, but I know that some of the engineers in my team applied it quite successfully to generate some synthetic queries. So that was very interesting.
Have you played with it? Do you know? So my team, we've been playing with it for one of the bros of conceptual concepts for doing other language processing, for structure solar queries. And I think it's a nice first step.
Still is giving you like an in-direction between the prompt and the way to write a prompt. So you have like classes, the mimic, the programming language, but then ends up as prompt. Yeah. I see. You are not sure that you will get what you want. But it's a first attempt. Yeah. Yeah.
I think it's okay. I mean, we will improve that. It feels maybe like maybe first baby step in a way that it's not a work to the state that you mentioned. Yeah. That's not a conflict solution, but it's great. So what's next that you're working on? That you want to disclose? Yeah.
So first of all, I want you to bring and merge the hybrid search receiver rank fusion to solar, which is coming nine to seven. So I'm very close to the men. Awesome. For that. We are, we got some funding from the European Union to work on solar. So that's like that.
So we're going to be able to contribute more vector-based search capabilities, better integrations we've learned into rank, better integration with like inference and points to make it a little bit more transparent. That's it.
And still in the work like multivalued supports for vector-based search in solar. And there are like some pieces in losing to speed up and improve optimized vector-based search that are not yet in solar. And that's among my top priority. So this is in short. This is fantastic. This is fantastic.
And of course, it's all open source and you know, yeah, and then like anyone can join, but maybe we can also make a call out and say that, I mean, everyone that wants to contribute, you know, the more the merrier.
Yeah, I actually enjoy like even though I don't do solar or only seen today, I am still reading the main news. And and for the most part, I'm reading the lesson one. So sometimes I see your discussion as well and you where you say, actually, by the way, I'm working on this hybrid search.
I did this and this. So maybe it will influence you. And I also love the the culture where you do not really enforce or impose your solution. You just say just for you to know, maybe it will be useful. And someone says, yeah, awesome. I especially love that that discussion.
I forgot the particular topic, but I remember it was a recent one. It's a recent one. So keep up your great work. And it's always a pleasure to talk to you and it looks like it's a tradition that we started meeting in the same sort of early flood words. In the end now.
Yeah, so every two years, we see a lot of people. Yeah, there are a lot of people. So fantastic. Thank you so much, LeSando. And anyway, my reaction to the conference. Thank you. Thank you.