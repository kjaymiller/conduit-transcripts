---
description: '<p>Topics:</p><p>00:00 Introduction</p><p>01:21 Jo Kristian’s background
  in Search / Recommendations since 2001 in Fast Search &amp; Transfer (FAST)</p><p>03:16
  Nice words about Trondheim</p><p>04:37 Role of NTNU in supplying search talent and
  having roots in FAST </p><p>05:33 History of Vespa from keyword search</p><p>09:00
  Architecture of Vespa and programming language choice: C++ (content layer), Java
  (HTTP requests and search plugins) and Python (pyvespa)</p><p>13:45 How Python API
  enables evaluation of the latest ML models with Vespa and ONNX support</p><p>17:04
  Tensor data structure in Vespa and its use cases</p><p>22:23 Multi-stage ranking
  pipeline use cases with Vespa</p><p>24:37 Optimizing your ranker for top 1. Bonus:
  cool search course mentioned!</p><p>30:18 Fascination of Query Understanding, ways
  to implement and its role in search UX</p><p>33:34 You need to have investment to
  get great results in search</p><p>35:30 Game-changing vector search in Vespa and
  impact of MS Marco Passage Ranking</p><p>38:44 User aspect of vector search algorithms</p><p>43:19
  Approximate vs exact nearest neighbor search tradeoffs</p><p>47:58 Misconceptions
  in neural search</p><p>52:06 Ranking competitions, idea generation and BERT bi-encoder
  dream</p><p>56:19 Helping wider community through improving search over CORD-19
  dataset</p><p>58:13 Multimodal search is where vector search shines</p><p>1:01:14
  Power of building fully-fledged demos</p><p>1:04:47 How to combine vector search
  with sparse search: Reciprocal Rank Fusion</p><p>1:10:37 The philosophical WHY question:
  Jo Kristian’s drive in the search field</p><p>1:21:43 Announcement on the coming
  features from Vespa</p><p>- Jo Kristian’s Twitter: <a href="https://twitter.com/jobergum">https://twitter.com/jobergum</a></p><p>-
  Dmitry’s Twitter: <a href="https://twitter.com/DmitryKan">https://twitter.com/DmitryKan</a></p><p>For
  the Show Notes check: <a href="https://www.youtube.com/watch?v=UxEdoXtA9oM">https://www.youtube.com/watch?v=UxEdoXtA9oM</a></p>'
image_url: https://media.rss.com/vector-podcast/20220412_120408_e18078d3137041275301d6bf045caa0e.jpg
pub_date: Tue, 12 Apr 2022 12:29:08 GMT
title: Jo Bergum - Distinguished Engineer, Yahoo! Vespa - Journey of Vespa from Sparse
  into Neural Search
url: https://rss.com/podcasts/vector-podcast/452635
---

Everyone, Vector Podcast is here. I hope you have been waiting for another episode. And today I have a rock star with me. Joe Christian Bergum, a distinguished engineer with Yahoo. And he has been super vocal in the field of Vector Search.
And he has been also advocating for one of the famous Vector Search engines and actually like a platform. Shirley Jo can talk more about it called Vespa. Hey Joe, how are you doing? Hey Dmitry, I'm good, thanks. How are you? I'm great. Thank you very much for taking time to talk to me.
It's fantastic being here on your show. It's become so popular. Thank you for that introduction. I'm not sure if I'm a rock star. It's really interesting to be here. I really look forward for our conversation on Vector Search and maybe we'll touch on language models as well.
And they'll talk a little bit about Vespa and the technology in Vespa. I'm really excited. Yeah, I'm looking forward to that. And I mean, you are a rock star. I can hear you every way on Twitter and LinkedIn and blogging. And so what else? So this has been like this.
And I'm really glad to hear to talk to you here today. And so as a tradition, could you please introduce yourself however you want to know the detail you want and we'll take it from there? Yeah. Yeah, so my name is Joe Christian and I work for Yahoo. And I've been working for Yahoo's is 2007.
My current role in Yahoo is distinguished engineer and I work on the Vespa platform. And I've been working on Search and Recommendations since about 2001. When I joined a company here as an intern during my studies, a company called Fast Search and Transfer, an Norwegian company.
Back then they were doing web search with this web search engine called alldevab.com. So they started around 98 I think trying to compete with Google and so on. And then Yahoo came along and bought the web search division. The team here in Toronto. They also bought all the way star and so on.
So that was back in 2003. And in 2004, Vespa was born. So and I joined I actually worked in Fast in the enterprise search division for some time, three years. And then I joined Yahoo in 2007. And since then I'm been here working on search and Vespa in Yahoo. So that's my background.
I also hold a master degree in computer science from the Norwegian University here in Toronto. Oh yeah, that's great. Actually, by the way, I did visit Toronto Hame. Was it 2007 for an interview with one search company? Not fast. But yeah, it was a great, great visit. I mean, I love the city.
It's an amazing place. Yeah, it's an amazing place. And it's funny what you said about search and trial because it really has a special, maybe special even in Europe because we at one time we had both Google, Bing and Yahoo here in in in in trial line. So that was a fantastic time.
Google shut down their office here in trial line. And but now we have a Microsoft is here in in tronheim and also Yahoo as office here in in tronheim. So there's a lot of search technology competence here in tronheim.
This is amazing actually for for relatively small city, but I think Tronheim used to be a capital of Norway at some point in back in history. Yeah, in its on point, back way back in the Viking days. Exactly. So now all these Vikings are stopped going around with boats and harassing people.
Now we developed search technology now. Yeah, such a move. Wow. And I also understood that in tronheim, as you said, there is the university. Is it actually one of the talent supplies for this industry or engineering in general? Yeah, it is.
We have the largest technical university in Norway, C and in tronheim. So as an old kind of history and so it's definitely one of the reasons why the search companies evolved. And the fast search and transfer of the company was founded by people coming out of the university here.
So two point in the east week, very good swing and so these two reggae and they they came they actually started with FTP search bucket back in like the night the seven. So and that developed into this web search engine and then eventually this became a Westpaw in Yahoo. Oh yeah, yeah, sounds great.
So I can actually maybe touch on the backgrounds since I've mentioned now web search and you know how maybe not everybody has heard about Westpaw and so Westpaw actually we started developing Westpaw in 2004. So Yahoo said that you know we brought you into the company.
We want you to build a vertical search platform that we can use across our properties in Yahoo. So for example, Yahoo finance, Yahoo news, they need to have some kind of search engine.
So and they gave that task to ask you in trial and I'm so they started building Westpaw, the Westpaw platform using the routes and the technology from the web search and putting that into a package that the verticals could install and use.
And then over time this so basically starting with basic BM25 like search like keyword search and then gradually Westpaw added more features real time indexing, 10 source aggregations, grouping facets as well.
So it really developed over time and new requirements came in especially when we started Westpaw it was around search but in 2007, 2008 around that time Westpaw's also started to be used more of as a recommendation engine. So serving of recommendations. So when you go to finance, Yahoo.
com and there's a set of articles that are recommended to you the serving engine doing that is Westpaw. And then in 2017, Yahoo decided that we're going to open source Westpaw to the world.
So we open-sourced it using our Apache tool license and we still continue to actively, very actively develop on Westpaw and add new features and so on. So that's a kind of brief background. So Westpaw is not new.
It's really kind of it has in a very long history and I think that's also great thing and we can talk maybe a little bit about it because you know we need to develop software over time. There are a lot of changes you know in the infrastructure. There was no cloud, public cloud.
There were no Kubernetes and from 2004. I started in 2007, you know a high power content machine, content node machine would have maybe eight things of RAM. And it would have maybe maximum 1 gigabit per second network. And if we go fast forward, you know, and it will have spinning disks.
And now we have NVME SSD disks. We have nodes with four terabytes, potentially of memory, lots of CPU power. So there's like keeping up in improving the software and adopting it to the hardware and new hardware and so on. It's been really fun to watch.
I think we did a good job actually making Westpaw kind of modern from something that started in 2004. It turns like really an exciting journey and really like starting from when you would explain like you know small scale servers in the way all the way.
And the technology has changed so much right? The disks became faster I guess and you know the network has become faster.
And like I remember like in Silicon Valley, Citco, if you if you watched it, it like they had a case when they optimized one module in the system and the whole system went down because it's way too fast.
So it's like it sounds like you have done quite a bit of job to actually keep this shape of flow. And like if I understood correctly, technically speaking, Westpaw or portion of Westpaw is implemented in Java. And then portion in C or C++ and then you also have some Python.
And maybe you can talk more about the choice of languages and sort of culture that there isn't the team. But I'm also curious like around the same time. I've actually seen was also developing right quite quite fast.
Did you kind of look at what that team is doing which is like an open source project? Was there something to loan from? Yeah, so let me tackle the first questions around Westpaw and the kind of languages that be used. And there's a lot of things here to cover. So Westpaw is around 1.
7 million lines of code, the total Westpaw platform. And it's a roughly 50% is written in Java. And 50% is written in C++.
And why do we use two different languages and what are the trade-offs?
So in the Westpaw architecture, we made a clear distinction between what we call the cluster that holds the content where you actually index and invert the documents and you have all the data structures for fast searching in these data structures.
The content layer is written in C++ because you're managing a lot of data. You have the data that you need to have in memory and so on. So and it needs to be fairly efficient. And then on there what we call the stateless layer is the layer that actually interacts with user requests.
So user requests comes in. It's accepted by HSP server and there you do and that layer is written in Java. So you can also then deploy plugins. You can write your own searcher functions that can dispatch the query and get a reply. And you don't.
It's transparent from a given searcher if you have a 100 node cluster or if you have a single node cluster. So that's kind of hidden away when you deploy a plugin. So those languages have different trade-offs.
So it's a lot easier for people to write plugins using Java without shooting themselves in the foot using C++. So in the content layer in C++ we don't allow any kind of plugins. You can contribute or you can contribute to the open source but then it needs to be a kind of feature.
We don't allow you to embed a library or something into the content layer. So that's a trade-off. So then you mentioned Python. We have a Python, what we call pi-wespa which is language binding on top of the HDP API. So it's not of the core kind of westpides.
It's an API where we built around interacting with westpa, doing model evaluation and evaluating for example different retrieval and writing strategies. So that's the kind of language. And regarding your scene, Apache Lucene. So if I recall correctly, I think Apache Lucene started in 1998.
So around time. So there's a lot of inspiration of course and it's not that many ways you can build a search engine. So I'm losing pretty much, it's a really good library.
So yeah, definitely we look at what's happening in open source and they have a lot of admiration for the work and the committers of Apache Lucene. I mean, it's a great job that they've done and they'll be able to develop this over 20 years.
And the core difference is between westpun, Apache Lucene is that westpun is a full kind of engine. So it becomes more of like comparing westpun with elastic search or Apache Zoolar, which is kind of an engine on top. So there's no like westpun library which you can use.
You have to kind of buy the whole, you have to buy the whole platform. Yes, basically like a web server around it and all the components like the nodes and overseer and other architectural elements. Yeah, for sure.
And on the Python side, I'm also curious like with all the development of models and you know, hugging face and you can pretty much find a paper and then most likely there is a model already available in some shape and form.
And so the Python layer in westpun does it help you know newcomers to kind of easier experiment with these models in conjunction with westpun? We do hope so. And that was one of the goals for making py westpun.
So there are different kind of use cases where you if you have like a more of a low query volume, maybe you have 200,000 documents or something like that, you know, not really not really very low latency and so on.
Then you can use Python and do embeddings and you can play then it natively works with hugging face and all those libraries that are typically written in Python. And then you can use westpun, just purely HTTP based APIs and so on.
The other option, which is more involved, I have to say, and that is that you can take a transformer model, for example, and export it to one X format or on X, which is open neural network exchange format.
So that's a kind of open neural network format that multiple companies like Microsoft, I think also Facebook have rallied around, you know, this open format.
So you can take the transformer models from the hugging face library and then you can export it to on X and then you can import all next models into westpun for evaluation.
And westpun we integrate with on X runtime, which is open source library from Microsoft, which has a lot of different language findings, Python, C++, Java. So it's a really great library and we integrate with that.
So you don't use it directly, but we have like you can put the model here, westpun you can be use it and you can invoke it and so on.
 And those models and then you're kind of a trade off between, you know, getting to know westpun playing around with it and then, you know, maybe low QPS, but in the scenario where you have a really large scale, you want to do 100,000 per cent back and there's something like that, then you move it to on X and deploy it actually inside the westpun cluster, which has many benefits because then you don't transfer a lot of data over the network and so on, because network is still even, you know, within the data centers, maybe the network limitations have this sold so you can get 10 gigs or 25 gigs even, but going cross region, then latency is still concern and that's that's one thing that really fascinates me is that we're still sometimes, you know, the use cases are bottlenecked by the speed of the light, right?
So yeah, going from the east goes to the west coast and the US is easily 100 milliseconds.
So hasn't been yet canceled or sold so yeah, physics.
 Yeah, this is fantastic and and so and also like even before we go into this wonderful world of models and latest advancements like I'm still curious also to dig into the item that you mentioned like you when when you have been evolving westpun over time, you found a need to add something really interesting, some really interesting data structures like tensors you mentioned and like could you elaborate a bit more on how this need arise and also like, you know, what are the use cases, typical use cases for it today and also how accessible to an average user of westpun so to say.
Yeah, so I'll do a little bit of history on that. So the best for document model you write has a fixed kind of you have to have a defined schema in westpun.
So you have to define it for instance, you have a document type called document and it has a title, it has maybe some time stamp, it might be have an integer attribute.
 So there are different like normal document model, what you expect from kind of any any schema oriented database and we also had vectors so you can do early on that you can actually do brute force dot products as part of ranking because that was really popular among in in your you know for various ranking requirements you will multiply or sorry, you will perform multiple different dot products over the documents that you you're queried as retreat then in around 2013 2014 the researchers in your outside, you know, we really want to express these type of recommendation models where we can use the general concept of tensor so not just storing a vector in the document but even a matrix and they had some use cases around recommendation.
 So for instance in the in the in the document you can represent in the matrix so you can have multiple is this document popular in multiple different categories for example that you know this document is popular among people that are interested in use this is in the ones that they're interested in finance and so on.
 So it's a really like complex and complex like that you can actually have both the tensors in the in the document side but also the query side and then you can do during the ranking phase you can evaluate these kind of expressions so it's a really it's a really powerful the language and one example concrete example is we haven't touched on the language models and so on but for instance the callbert model which is contextualized late interaction overbert where you actually take the query is not represented as one vector but each of the terms in the queries represent the desivector and similar on the document side each of the document terms are represented as a vector and then at runtime you retrieve documents and then you rank them based on this maximum similarity function so it takes the vector of the first term and then it performs k dot products against the vectors of the document terms and then you you take the maximum of that score and then you do that for all of the terms and the final is the it's a sum so that was actually one of the things that I personally the tensors hadn't been that much used for search use cases but more around recommendation use cases when I when I when I saw callbert and I saw the maximum operator I was like this is just perfect fit for for the best potential it's a perfect use case so yeah yeah that's one example yeah awesome once you described like when you go like many models today is like okay embedded spas that you embed this paragraph whatever but if if you need to go world level that's like lots of data lots of computation right so how you would even do this sounds like tensors have found the use case there yeah and in in in in callbert what when we we represented callbert on less also we did a large sample application around the ms marker dataset the passage ranking dataset of mammoth marker so we made a sample app where you can combine these different retrieval and ranking strategies and but in our case we used callbert as a re ranking model and that's one of the really strength of of espice that we allow you to express really complex retrieval and ranking pipelines so that you do a query and then each of the nodes involved in the query they will do a local ranking or matching and then you could have a second face locally on each node and then when you have the kind of global view after you have done the scatter gather then you can do another re ranking face because then you have the global view so there are a lot of possibilities to kind of trade off between accuracy and cost them yeah yeah exactly and actually as you've been describing this I also realized that we've been recently discussing in one of the podcasts about multi-stage runker right so you could have either a sparse or dense retrieval but you can then later use your graph algorithm to kind of like re rank the items I think it was in the podcast with Yuri Markov the author of H&SW algorithm and so have you have you seen any use cases based on espice you know for multi-stage ranking pipeline?
 Definitely I mean so both the search internally in our we also see this outside from customers using last but they do multi-stage retrieval and ranking pipelines so there's basically the reason why you do it typically is that it's too expensive to evaluate the kind of final ranking model over all the documents right so you take some kind of approximation of that model and then you execute that as to kind of candidate the treaver and I think one of the we haven't talked about the vector search capabilities of VESPA yet but one of the beauties of VESPA is that we after we integrate it approximate nearest neighbor searches that you can do a combination when you actually do the matching and querying that you can combine it the regular sparse or keyword search with a vector search and then you re rank and it's kind of paradigm of having multiple stages you know you see that in the question answering pipelines as well right or you have retriever and then you have what they call a reader right so it basically finds some candidate passages from Wikipedia and then extracts in a reader but evaluating the reader which is a complex typically a transformer model where you input both the query and the document at the same time into the deep neural network it's very complex to actually evaluate that overall the potential passages and user types.
 It's like super intensive and I'm super curious to drill into this topic of like combining you know neural search with sparse search actually before that as you've been talking I've realized I'm actually now taking a search with machine learning course dot by grand ingressol Daniel thank you like it's a fantastic course I can highly recommend it it's super intense as well and I think yesterday grand mentioned that there are companies which you know that they really need to optimize only like top one or top two results and they have built models to optimize only that top one or top two which sounds like my mind blowing right and that was like something maybe this applies to web scale to some extent and one of my recent experiences is actually in web scale search engine we have a mobile screen and so we can only show top three results and the target is obviously to have a high CTR and so we've quickly noticed that if you do a sparse search without any logic on the query whatsoever CTR is very low so you have to do some tricks like query understanding and also trying to increase precision at the same time maintaining the diversity of search in some degree so you know basically it's very easy that with sparse search you hit just the tip of that iceberg and basically say okay I have three teacher jobs for you which is not that interesting because we don't know if the user is looking for teacher jobs right so so that's that's like have you seen cases like this I think these are really really challenging ones yeah but generally if you look at the results like if you ever rate on MS Markov for example and the official metric there which is the mean the reciprocal rank right so if you get the perfect the actual relevant document you're able to retrieve it and put it in position one that query gets a score of one right but if you put it in the second place it's gonna have a score of 0.
5 I think that's really good measure when you talk about mobile screen precision at 1 2 3 so that's really important but in the kind of retrieval not this stage retrieval and ranking pipeline it makes sense to spend more of the computational budget within the lakes SLA on those top K hits right so like in when you go to Google today and you do a search probably 100 million documents will be excluded in just a fraction of a millisecond right and then there are multiple stages and you can be sure that the kind of the the 10 last documents from the previous stages that the invest more time or computer computational resources on those hits yeah yeah exactly and also the good thing is that is that because that's when I talked about the West Architecture where you have this division between stateless which is doing the scatter gather and each of the local nodes is that basically in a search engine today you need to move move computations both to the data but there's a lot of talk about you know moving or separating compute from storage which is a huge thing right in the cloud but in search in search use cases with high triplet high document or you need to be able to do both you need to do fast computations across multiple nodes and then you transfer data like in last well each of the hits that you can include ranking features or so on that the subsequent phases can actually use for re-ranking and the good thing is like I know you've done a talk about diversity in search results and so on is that you need to have that global view you know in order to kind of optimize for for diversity and then you can kind of throw away a lot of the hits that you're not going to show because of kind of business constraints or diversity constraints and you don't need to invoke the heavy model for those hits yeah so yeah I think it's interesting for these kind of pipelines but one thing that is challenging regarding both the stage pipelines is that they interact with each other right and if you do if you have like a system for training your model retraining the model using statistical features what are users clicking on and so on the one of the features then you will have some biases towards the actual the ranking algorithm that is in place today because that's the model that is bringing interactions so you basically just retrain on the top hits and that was what we saw on Amazon as well as that when they started to improve the retriever so when it's not actually instead of having a BM25 like do BM25 and then rerank they had a mean reciprocal rank score of 0.
35 or something and now after changing into a dense retriever now we're talking about 0.
42 or something like that so by improving the improving the retriever right because the retriever kind of sets the upper bound you know because the rerank cannot really dream up you know the relevant hits if the retriever hasn't retrieved it right so that's an important point you know in the retriever and writing stages so exactly and I think we can gradually move into neural search and vector search but like you know it was one of the students question also yes then the same course how much you can actually solve with the rerunquer if your first stage retriever didn't even find what the user is looking for which means probably the query is not a match for this search engine you know let's say they're looking for a specific model of a phone but they don't even cell phones right so like and I think the response from Daniel Tankilang on this one was that actually you can implement a query understanding system which will understand the query as much as it can and if it knows that there are no such items in the database don't even bother searching for them and I think this was a really really clever advice on it and and and he said that system worked extremely well so like for user for user satisfaction to save their time right because in the end what we're doing is actually optimizing the user journey which then translates into business right so that was a fantastic example of how you can also talk such search problem yeah and that's one area where I wanted to because we're building all of these sample applications what you can build with VASPA and query understanding has been one of the topics that I wanted to build out to to demonstrate you know how to do that especially building it using a transformer model actually so you can have different ways of doing this but one way of doing it is to use it as a multi-label categorization problem so given a query here are the intents and their probability but what's stopping me from doing this is that we need to work on kind of open data sets now and there are very few query data sets in this way so one approximation is actually to train using the title so in the e-commerce set you can train based on the titles but then you need to have some kind of label on you know is this you can do it around categories so you have the title of the e-commerce listing and you have the category and the beauty of this is that you're actually mapping free text queries into kind of a fixed predefined vocabulary which is the categories and then you can actually eliminate zero hits the zero hits problem because you actually no longer retrieving based on on the free text queries but you're retrieving based on the most kind of interesting categories so yeah so these are really interesting you know topics and that's one thing that I find you know with search you know and why I love being in search is that there's there's there's there's a ton of things that you can you know build and you know there's query understanding you know there's facets there's retrieval and ranking dense bars you know and then you have the scale of it you know how to make it fast you know there's so many things you know they're just query understanding you know it's probably you know a full research topic you know on its own so there's so many things involved in search so that's yeah yeah it's like endless journey I agree it's like yeah it is you can also dive into an old piece out of things or you can stay with like scaling of search or query parsing whatever that you find passion and maybe your passion changes over time as well right so you did a bit of an LP then you move to query parsing then you move to maybe even scalability or whatever yeah I agree it's like a fascinating topic and also what fascinates me is that on the other side of things users are also not sleeping they're puzzling you all the time with new queries seasonal changes data changes as well because in mid-size a larger company it's usually work of multiple teams and you know or departments some departments looking after data some looking after ranking recommendation all the feature collection and what not you know something somewhere can sometimes go wrong and you need to prepare for it you need to interact you need to kind of build a system that is resilient and it's it's a fantastic fantastic space yeah it really is and there's so many methods and that's also one of the things you know people you know they want to build something that is great you know but even if you're using a passion to see know if you're using Westbar you know you need to have kind of some investment of actually getting great results and that the same thing if you're using like a vector search library or you know you need to have some kind of data pipeline for your documents and queries so there's you know I'm not a huge believer in you know none of these technologies really work that well you know out of the box you know it's it's such as definitely not a sole problem and even if you look at Google you know they're struggling as well you know there are many queries of question answering and so on that they totally get wrong you know and people want to build Google but they have like maybe two guys you know or or girls you know working on search you know you you you don't build a great search experience you know if you're by a team of two two people yeah yeah it's a huge investment and also like time investment not just like you need to hire a lot of smart people but you need to give them time to actually go through all these challenges and you know now that you've mentioned vector search I'm actually curious like when in Westbar journey you know did you first hear about vector search and actually what caught your eye and you know like sometimes even today when companies evaluate whether or not to take the neural search journey or stay with this part search journey it is not that obvious actually and and maybe you could share some advice there as well but maybe first if you could also do a historical deep dive there super exciting yeah it's it's so we've been using like dog products and so on for search but it's been it's been brute force right so been able to do brute force vector search in Westbar for a long time and then in 2018 bird came out and in January 2019 the researchers published really great results on them as Marco Pasadranking and then we like you know this is this bird model you know how can we use it you know is it you know there are a lot of things you know to get your head around you know what its bird is and how to use it and then we saw that there basically were two ways of of using it either as a representation model where you encode the query and the document independently and then you can build using a vector search library you can you build the index of your corpus and then you can retrieve pretty efficiently if you have La Crocs Med Search version so that actually was what motivated us in 2019 we started that work in summer August to actually have vector search and then also in term in Java there are a couple of image search use cases around hamming distance so they were pushing for that so there are multiple things and also our users it was open source by then so users were also asking for it you know can Westbar do vector search you know we see that we could represent vectors but it's not that cost efficient if we need to do brute force so then we start looking at you know it and we had all the kind of building pieces we had the tensor the document models representing floats and all these numeric fields and so so we had it wasn't a lot of work to get all the kind of pieces together but we had to implement the algorithm and we did we did a pretty I think like one month where we actually surveyed multiple algorithms for approximate vector search you know how could they fit into the Westbar model of doing things so that's the background of kind of why why vector search came to Westbar and I was really exciting when we started working on that because there were a lot of interest in it right so there were people wanted to work on that project so yeah of course because it's something like a bleeding edge and like a new but also like one of the podcasts I mentioned I think it was also with Yuri Malkov that I've I've had a friend who worked in essentially vector search but he was a mathematician himself right so I also viewed it as a pure mathematical concept and I was like yeah he's playing with some theoretical you know advancements and then he actually gave a talk at Google you know as well actually presenting this algorithm and the nearest neighbor search essentially and how to optimize it and even then I wasn't like essentially buying in and like okay it's still mathematics but then when I was reading H&SW paper I saw them citing his work I was like wow so now these paths have intersected so now this makes sense and you know usually it excites me when it's put into practice is that how you felt as well like was mathematics aspect of it like engaging for you or did you view it more like an engineering sort of yeah I'm definitely on the engineering side so I'm definitely on the engineering side so for example on transformers you know I don't care about the deep neural network architecture how these interacts you know I basically treat as a black box you know this is this is the box and you need a tokenizer for it okay what's the tokenizer what's saying people's output what can I use it for you know I'm not gonna do and they be I mean a lot of research actually study you know how can we build ultimate in neural network architecture so definitely know from for me that was not the math involved but we we have some people in our team with a heavy math background and you know they can teach me a little bit about what it's a proper distance metric and you know why why this one work and this one work so that that was really also a learning experience for for me to engage with such core team on this feature and a huge discussion we had you know who was you know one of my main point was that you know we need to be able to integrate for users when they want to use vector search they want to have filters they want to be able to express this in our query language so that you can combine the best of both worlds and and that took really some time you know to to get that right and that was really you know really fun to see that that you actually can write a query and say that hey give me documents that are near in vector space then filter on this attribute but at the same time also compute or retrieve based on the weekend query operator which you heard about weekend which is an optimization technique for doing spatial chival and that you could actually express that in the same query and I have to say that I was really proud of our effort when when it came out with that and and could be able to combine it and that's really if you look on the future side I think vector search it's been the biggest game changer for us was to actually integrate vector search because that's speared a lot of interest into VESPO yeah yeah and I can actually have people coming in you know yeah but I can imagine that vector search can still be useful in search as well as recommendation systems right yeah exactly so so and that's one of the things that you know you see that factorization machines dot products has been used for recommendation for a long time so you basically see the technology for search and recommendation use cases kind of merging into the kind of same same space technology space and for those type of use cases I think VESPO is really strong technology and but the interesting thing that I want to mention is that we have people coming in you know asking about VESPO thinking that it was a vector search database and then they realized hey you know there's keywords there's ranking there's a lot of other features here you know so that's been interesting for me you know you know I see vector search as a feature of VESPO in this whole kind of serving engine but you can use for search and recommendation not like I see vector search as a very important feature but it's like one feature of VESPO yeah I have to admit that part I probably played that role in bringing those users onto you through that blog post that I will of course mention and did mention multiple times and where I compare multiple you know now seven vector databases and I did put VESPO in that corner just to consider only the vector part but I knew that you guys over a lot more and actually still learn at some point hopefully we'll use VESPO in some project that I can actually evaluate but yeah you absolutely right that some of these systems are actually beyond just vector search and you know also the use cases like the way you view this right you should actually take a step back and ask yourself what is it that you are trying to build yeah I think it's really important and so when you look at vector search and we didn't so to clarify on the algorithm side after investigating an oil and several techniques we went for your emalco's H&SW algorithm so we implemented a version of that to be able to also handle filtering real-time updates and so on so but I think one discussion that is is not heard that often is that vector search when you introduce kind of H&SW or any technique you are losing some accuracy compared to the brute force right so for example a data set that is called SIFT one million documents you can do a single treaded route for search over those one million vectors in about a hundred milliseconds right but if you do approximate then some parameters of H&SW you might get down to 0.
1 milliseconds as well using a library right so it's a thousand times faster but by doing that you are losing some accuracy and that's kind of when I see blog posts about approximate vector search without mentioning the kind of trade-offs between recall and performance then I like you know you should include the recall numbers because there's really so it's really I think it's really important for many use cases right it might be that you need to do use a brute force because that kind of approximative error that you introduce is not acceptable right so we do have use cases in now that we actually use we don't have like large amount of documents that we actually use a brute force search and best but best best supports brute force search so yeah yeah okay so you can switch and that's the beauty is that since we support this you just say in the query time you can say approximate through your false and that means that you can take a query run it using a brute force and then you can compare the result for the brute force which is exact with the approximation then you can compute the overlap between those two and that's typically then what's used in the recall at k right so I did two blog posts on what I call billion scale vector search with with last one where I did deep dive I think into these kind of trade-offs because when you introduce approximate you also need to build these kind of index structures so in hnsw you need to build the graph right which is time and resource taking time you know I'm costing memory so there are all these kind of trade-offs and that's generally I mean generally for search a lot of trade-offs but especially around vector search I call it the jack of old trade-offs because there's so many things you know to consider you know memory usage this usage CPU and so on so yeah that love the term jack of old feed-offs yeah but it but it really is you know you really have so many trade-offs and some companies you know maybe you have lots of data but you don't have any real tripe it right in that case maybe disk a and n or things that basically using disk is is a good alternative because when you're buying servers in the cloud or renting servers in the cloud you pay when you want to have this amount of memory you get this amount of CPU right there comes in a relationship between the CPU and the memory and and so there are different trade-offs around you know what what's actually going to use it for you yeah exactly have you heard any other misconceptions about neural search at large you know when somebody comes and says hey I want to implement a question answering system you couldn't principle use sparse search techniques or like query understanding techniques you know to actually almost do it in the rule-based fashion but like neural search on the other hand is like you know new sexy stuff everyone's to try out so the question is like have you heard of any misconceptions or something that people think it's much easier than it is yeah that's that's I mean it's a fantastic question I think you know you can just sit back you know this is I'm relaxed for a few minutes because this is a topic that I really love um yeah so so so the first time when we if you look at semantic search especially around vector search if we semantic search might mean a lot but if you look at the kind of the typical that people use semantics search today is that you have this vector search right you have independent query embedding in the document embedding and so and if you base this if you take them pre-trained language model from hugging phase and you just pull that model and then you encode your queries using for instance the CLS token or the average over all tokens and the result that you will get from that is not going to compete at all with the VM25 right because that language model has not been it's only been learned learning how to do mask language model right so it's basically it's been trained on predicting the next word right so it's a deep neural network that's it's not been trained for that so it's basically like taking some deep neural network for my vacuum cleaner and put it into my car you know to try to try to try the car it's not been trained for that right so that was one of the things you know when we struggled as well when they looked at bird and the other people like oh that's so great and then we had the engine and we could like compare it with VM25 and then we did bird here and there was like these results if you look at the actual information retrieval benchmarks they're like the results are not good they're they're like really so then came the kind of you know realization I think that's actually happened around industry as well in 2020 when the DPR dense passage retriever paper came out from from a Facebook where they trained on natural questions the Google dataset they actually trained this dense retriever and the dense model using a contrastive loss and hard negative mining so they basically demonstrate you know how to actually train a dense retriever model and then we actually saw the results were much better than than VM25 in that but but it's a huge so that's one area where I think that people just using the pre-trained model might not work well especially if it's not been tuned for retrieval and even if you look at MS Marco which is the largest data set out there that you can train a model on if you train a model on MS Marco and then you apply that model into a different domain so on a different dataset it might not outcompete VM25 in fact it actually in many cases it is actually underperforms compared to VM25 so and that's why there's a lot of interest and especially recently is like you know if we combine this exact matching you know the actual user search for this phrase but we also have the vector representation you know how to combine that and that's that's actually two of my colleagues are right now working on the beer dataset to they open the PR to the to the dataset to include VASP as well and then we will demonstrate some methods for combining sparse and dense.
 Yeah that's awesome like I've read the beer paper after you referred it to me actually and it was quite eye-opening because it does compare not only sort of like search engine algorithms and approaches but also datasets and tasks right which different tasks like searching or answering questions may matter quite a lot and so it was quite an eye-opening that first of all VM25 is fairly competitive so it's not a loser not at all so like you should still consider using it like and actually maybe even keeping it as a strong baseline in everything you do and I know some companies by the way still use TFIDF so maybe they should also like first transition to VM25 and only then jump to neural search techniques are like a denser trivel and and I think you also mentioned that and I saw by the way that you have participated in various competitions on denser trivel and on ranking like can you can you elaborate a bit more like what drives your interest there because to me that sounds more like academic interest in a way right but of course you're also showcasing and probably bringing ideas back to that spa.
 Yeah so the motivation was actually around them as Marco passage ranking and where we actually could use this dataset and then our dream when we started to implement vector search was one thing and the other thing was you know how can we represent the re-ranking using bird in westbound so using the actual bird model inputting both the career and the document at the same time so that was one dream we had and but we were looking at the results and I think the first paper that we read it read that they they used maybe a day to with even with a GPU to actually perform 3600 queries right so it was not really you know how can we make this practical and then two years later we actually did did beat their benchmark and to end represented on westbound and we were doing it less than 100 milliseconds so on CPU right so but there being a lot of learning to get there but that was the motivation to kind of demonstrate that you can take this state of the art or close to state of the art with three-wheel and ranking pipeline from an open dataset which is how widely recognized and all the researchers are actually publishing papers around it you can actually take that model and use westbound and get those results you know so it was one way of demonstrating that you can actually then you can actually use these models with westbound and have it serve in your state so that was actually the motivation not on the kind of science side and so on but I have to say that I really would encourage everybody that works in search to look at some of these open datasets you know play with them you know maybe you have some ideas you know around search and how to do search and there's a lot of talks about boosting this phrasing you know how actually does that impact the results on kind of a dataset and I can really recommend the track COVID which is a dataset that was made at the start of the pandemic and it has about 50 queries and deep judgments for each of the queries and the collection is rather small so you can play with it on a single node and so on so that I will really encourage you know people in search to try out you know because then you get the feeling you know does it actually work does it actually you know compare it with BM25 what happens if I do phrase matching or do something clever you know so I think that's and and I'm really not a huge fan of anecdotal query examples to see these kind of commercial actors with this you know I'm searching for this on this magic results you know I'm more into you know demonstrating that actually Westpac can do this and it has the funding and actually on the real datasets.
 Yeah and I agree in the end of the day what matters is first of all can you apply this tag as you said in your real setting right in your domain then another thing that you mentioned just now you know the track COVID dataset so maybe as the result of your research you might also impact on the global situation right maybe somewhere locally maybe somebody will use your work to actually implement a better search system so I think that that's also a fantastic segue to you know the the context that you're doing and that's actually a very interesting point because we had at the start of the pandemic we built a cord 19 search interface that we published online so people who actually go and search this dataset and people they were I don't I don't recall the details but it's still online and people actually because of all open source so they forked it and then they started using Westpac and based on that and I think it's a much better shape that service right now than the the cord 19 search that we did so they actually built on that work so so that's that's great I love to put what I call sample applications you know how what can you build with with Westpac and and that's actually a lot of my time these days are spent on making these sample applications smooth and easy to to work with and especially we've been rather weak on the kind of UI putting together front dance and so on so that's actually some work that I'm doing right now to kind of build more of the product you know what can you build with it because people don't get really excited about looking at Jason I output you know to actually see some interactions faces facets you know out the completion and to actually build that whole experience you know for the for the product people it's like looking at the engine when you actually want to maybe look at the car right and then you get fascinated by how shiny and sort of sleek it is and then you're like I'm buying it yes yes I totally hear you there and like actually in these use cases you know there are other platforms you know in the neural search space also doing multiple demos have you been looking into the direction of multimodal search does that excite you do you think it's too much of a bling edge or niche use case or do you think it has potential because of the neural search crossing the boundary of text towards the image audio and so on yeah I think multimodal is really where recto search is shining so this is the area where you it really shines I have some doubts about out of the main like we discussed using a vector model for text search if you don't have any label training data and so on and adopted to your data using vector search alone for that I think is questionable but looking at this multimodal where you combine both a transformer model and a typical image model and you train that representation and from what I've seen from these models and we did a sample application on this as well using the clip embedding model from from open AI and looking at the results I I have to say that I'm really impressed by kind of just eyeballing I don't have any kind of I don't have any hard data sets or but it's really impressive you know what that model can can actually do so I definitely think that multimodal is it's very I don't think it's I don't think it's that far ahead I think because we have interest in representing clip in best from from actual questions I'm actually I'm seeing an email right now you know how to they want to help on their schema and definitely they want to use clip yeah so definitely I don't think it's that advanced at the moment and I think we'll see another thing that I'm working on right now is that I talked about our sample applications I want to build a new sample application that demonstrates in a UI in an e-commerce setting where you combine different kind of fussy matching exact matching vector search all in the same query and then you can have some sliders where you actually slide these you know how does the result change and they change in real time so I just need some help on the on the react front end because I'm not I'm not a great JavaScript programmer I have to admit so I need some help on that so yeah but I definitely think that multimodal vector search has a really has a huge number of use cases yeah I hope that amongst listeners of this podcast maybe there are some with front end skills and maybe since you're building this for open source you know that might be good use case as well to be contributing to this crazy journey but we have yeah I mean that would I mean definitely we do see more involvement and contributions from from the in the kind of community around VESPA so I think we build a lot of the last two years of the community side and people getting to know more about VESPA and actually starting to contribute back both on the sample applications and also documentation and also we're seeing our more involved in contributing to the code so definitely yeah so but I think it's from a product side it's really important for us to and also we have a commercial offering of VESPA where you actually have a hosted interface hosted solution multi-region and to I think it we want VESPA to be able to run fully fledged in your environment if you want to use it because it's open source it's our Pasha 20 if you want to use our cloud you are welcome to do that and to kind of have and the same kind of functionality and what we add in the cloud is CICD pipelines how to do multi-region failovers like in the US East US West you can have different so all this kind of top and take care of sort of take care of nodes failing and whatever you know the hole the kind of host the experience so and that's been an issue with our sample apps they have been like it has been some friction around you know how to deploy them locally how to deploy them to the cloud so I'm trying to kind of bring them together so that they they work in in multiple environments yeah that's a lot of sense and I guess it takes a lot of engineering effort to also kind of cover all these different use cases so sounds quite exciting and actually demoing the technology I think you know as you know other vector databases have got it and I think it's a such a low entry for especially for non-technical people or those who are in charge of businesses business units to actually make decisions and I think for them you know having a relevant demo is going to be quite a game changer because if they need to reason about your technology only through the eyes of engineers in their company then probably that's that's much longer path right yeah exactly and I want this experience to be as smooth as possible so that you can get started with the sample application run it locally get some data into it fire up your front and react and you can interact with it and if you're happy with it if you want to share with your friends you can upload it to the Westpac Cloud and then you can share to URL to your friends and that's a model that I really believe in that you can it's open source so you can actually run it locally and then you can take the cloud provider can actually take care of the hosting for you so that's and right now we actually we are providing like free trials so you don't you only need an email address for the Westpac Cloud you don't need a credit card or things like that so you can actually play it play with it and run with we can even leave a link where users can try out Westpac and subscribe so I think that will be quite beneficial and actually I was thinking like even though we a little bit drifted in our conversation away from better search you did mention the exciting space of combining you know better search with smart search and I wanted to take it from the angle of a non-technical user right so let's say they come to you and they say Joe can you actually enlighten me a little bit on how do I combine these things maybe I just want to deep my toe and vector search just to see what it cannot cannot do in my domain what would you recommend them to do assuming that they already have maybe like a smart search engine and maybe they are evaluating Westpac as one candidate yeah so I think the question is if you're using Westpac it's rather easy to do this because you you can express it in the query and then you write the right key profiles saying that you know this is how going to combine the sparse ranking single for example be on 25 with retrieval for others that are not using Westpac using for example elastic search and open source of our shesolar what we see is that they build a lot of infrastructure on top of these so they actually have the ranking layers outside of elastic search right so in that case is you could have kind of a vector search library running at the side of elastic search and then retrieve and then you need to you need to keep those two data stores in sync and then you can in parallel fetch okay give me elastic search your best results and vector search database give me your best results and then you can use a technique called reciprocal rank fusion where you basically merged results based on you know are they are they ranking you know it's the document found in both so that's that's a powerful technique of but you don't have to actually know anything about the distribution of ranking scores and so on so google is writing a lot about reciprocal rank fusion so it's interesting direction and that's one thing we know from Bing and from others from from both Bing and from bydo in in China is that they're doing this kind of mix mix retrieval with different systems for sparse signals and then signals but but then you have for the regular uses you have a lot of moving parts right you have different data stores make the manned ship and that's one of the things that we try to our advantage is that when you're using Westbyes that you know you you get these capabilities in the same engine you don't need to store the data in different stores and having consistency problems because of that yeah yeah so I will definitely if you're interested if you're sitting there today with open source or or elastic search and you don't want to invest in in in in the vast park you could try this batching the query and doing reciprocal rank fusion yeah yeah it could be like one way to actually introduce something from more like semantic search if you view it that way right so that's a great idea because I think there are multiple approaches to this and I think if you are within one search engine like say VESPA or elastic search open search a solar would have you then I think you could in principle experiment with like fusing you know the neural search result with sparse search using some kind of linear combination as you actually retrieve it right yeah so so so so you so you can actually use the linear combination but the great thing about this rank fusion is that you don't simply you don't look at the ranking scores so you basically just fuse them by the order of their returns so you don't have to know anything about the score distribution like EM25 it has basically unbounded it could be 25 it could be 100 to be 5 right so it's very difficult to to to combine that using a linear model because you have two signals you know and one is number is going to be like this and now it was going to be between 0 and 1 so reciprocal rank fusion is definitely you know interesting case actually this is super great point and hopefully we can provide some links to this because this technique because I think I heard this question multiple times that would you set exactly just now that the score space is completely different and they are not compatible with each other and so you have to find a way to still interleave them or merge them right so that would you set exactly makes sense that you don't pay attention to the score space you actually look at the order and you try your best to interleave them yeah that makes total sense yeah yeah there was actually a recent recent paper on because there has been more interest in that these dense models alone that they generalize not that well what you're using out of domain and one of the things that the Google researchers were doing and showed promising results was using this rank fusion and I've seen this rank fusion in a multiple Google papers so so it's very interesting the researchers they're really interested in reciprocal rank fusion so yeah sounds like a popular technique yes I mean time flies and I really enjoy talking it feels like we could record another podcast what do you think I'm talking multiple topics but I still really love to pick a brain on that philosophical question and kind of ask you what what keeps you so interested like you are a loudmouth behind West Pine general but you also offer a bunch of advice right like through your blogs through your public presentations and even sharing papers on Twitter at least for me was super helpful that I could you know quickly read the paper that you shared but what keeps you motivated and interested to stay in this field and also specifically you know maybe you think something is missing in the vector search space or in general in in search space that you would like to fix yeah so it's a great philosophical question I think I'm not that excited about vector search I see that as a technique so I'm more like excited about search because I think it's such a fascinating problem we touched on it before you know you have query categorization spelling you have so many different aspects of building a great search experience and also the scale thing is really appealing for me you know kind of passionate about you know how can we do this billion scale how can we make it fast you know what if we need 100,000 queries per second what if we need to update all the documents in real time and within 12 hours or one hour or you know where's the limits you know where is the cloud going you know this compute versus storage can we now move more computations out of the storage layer there are a lot of these exciting on the kind of system things but on them kind of more science things you know how to build a great search experience I think you need to have this kind of multiple techniques and we didn't touch on it but vector search is one thing sparse search is one other but GBDT models tree based models is really ruling the search or it's kind of the hammer of search because those models on tabular data statistical features you know they really show promising results and that's another thing that I think is great than about west based that you can combine these GBDT models newer metals in the same ranking functions I don't think that there's a one single silver bullet for retrieval I think there are multiple different singles like for instance let's let's do vector search if you only do vector search if Google only did vector search only on the text right it would basically you have a lot of duplicates on the web you have low quality content you know there's page rank there other factors you know it's not only vector search there's that kind of different techniques so and so that means also that there's a lot of things new things to learn you know how do you do caricaturization you know how do you how do you how do you actually determine which facets and then kind of navigation you're going to show to the user and like you touched on at the start you know if your user does a query and we don't have any good results you know should we just slow them some random results or should they say that hey you know I'm sorry but we don't have anything for your query so yeah that's really what motivates me is that it's such a fantastic problem if you're interested in scale and all these kind of things coming together so yeah yeah thanks for that it's deep and it's very wide and I think it's like limitless space and I hope also that newcomers feel it's kind of like a low bar entry especially and we didn't touch on this but especially with your work and open source you know the support like you can go and slack or whatever tool you're using to communicate with your users and actually listen and address their concerns questions and hopefully this opens more you know more possibilities for newcomers to enter yeah I love I love actually it's actually a weakness as well but I love answering questions you know you can see me answering questions on multiple slack spaces you know I love people you know asking questions about search so I really love that and I'm and what really gets me if someone is struggling with something you know how can I do this with West Park and I'll try to explain it you know you have to do this and that you know and then I like I go back you know at the program saying you know we need to fix this you know we need to make this more easy for people to use right so it's it's a both way thing and that's one thing that I learned in my career is that you know listen carefully to your users how they're using the product what are the pain points you know how to how does it feel to get started are able to progress so that's that's really also motivating and and honestly I think that some of the work that we've done using some of these smaller transformer models has been has an impact on the industry like I got contacted by a person here on Twitter the other day said that you know I saw your tweet about these smaller language models like not the birth base that people usually turn okay but this mini LM model which is a distilled 22 million parameters that actually did the demo that you can run in your browser and it said you know I saw your tweet and I went ahead and tried it for my domain which was classification of hate speech and then he like did a blog post on it and he mentioned me and I think that was really like interesting for me to see that you know that I could share something that some people could actually make use of even if it was outside of search show and I learned a lot from especially around the relevance is slack space that we are both in the open social connections slack space so a lot of discussion there rector search and we are you know sharing some blog posts and then I ask Greg from Pinecoin a tough question maybe you know and so I really love being there and discussing and I learn a lot from from other people like just devins from elastic search and so and I'm from you and especially around Berlin best search last year you did the AMA on the vector search and for me like one of the key moments was that Max Irvin your co-host of that he said you know what if the user types of phrase query you know actually quote marks I want to search for this exact phrase you know don't show me anything else give me that phrase you know and that's something that is really hard to do with vector search alone right because you basically map it into this vector space and we do the similarities and that was a key takeaway from me and that was a really eye-opener for me you know you need to be building out better examples of how actually to combine sparse and then single so yeah this is amazing and what I enjoyed what you said is that you keep your practitioner hat on so you don't just buy in easily into these new models or you don't stay on the field of okay I'm only an engineer I don't even know what machine learning is because I think the profession is slowly changing and it's like a blend of skills where today you need to succeed as a search engineer and maybe it shouldn't be called the search engineer anymore like I think it needs some new term but we will probably be stuck with it for the lack of a better term but eventually you will need more skills under your belt and I think of the work that you are doing is amazing in sharing this knowledge and that people can actually reproduce it and I think that's super super crucial for the progress yeah I mean thank you Dmitry that's that's really nice of you and you know that's that's I yeah I think it's it's it's actually true you know to share and I think that what you said you know building a search team today is really hard because especially since deep learning entered the search field right so now you need to know how to configure and do matching and boosting an elastic search and now you also need you know how do I train the dense vector model and you know how should I you know should I use birch they use birch large you know does it handle multilingual text does it handle spell correction you know they're always kind of different things you know so building a search team in 2022 it's not easy because you need the kind of a mixed NLP search you know there are a lot of different things and that's what I love about it you know and I talked about on Twitter and you know in a talk I did earlier as well that this neural paradigm shift has opened this kind of knowledge gap you know how to actually successfully apply these methods and also on the technology side that we try to bring you know with VESPA that you can kind of combine different techniques we don't have to throw away 50 or 300 years of the inverted index you know we don't need to throw that away you know it still has value it's going to have value probably forever you know so so we don't have to throw away so that's interesting but that's also what it's been fascinating and I've said numerous times that I don't think I've learned that much in my career that I've actually over the last three years because reading papers it's not being a big kind of interest of mine earlier it's been one of the system side engineering side but that's been a high-opener for me you know to kind of how to apply these techniques and yeah so and to learn you know and that's the great thing about open source is that you know we can share ways of doing things yeah yeah absolutely sharing is caring and so much comes back to you as you said you know you get mentioned somewhere and you feel like you didn't do it in vain but also you might learn something new like a new use case and I feel the same you know when when I blog or when some video is viewed by somebody and then somebody says thank you even just multiple months after I did it and you know it's just an amazing feeling it's like a sense of connection as well especially in these days when we don't maybe meet socially as we used to but that's a new actually evolutionary new way of connecting and I feel much more comfortable and enjoying this more detailed conversation so maybe these interactions on Twitter they they bring a lot more value and I think this is super super great is there is something that you would like to share on Vespa development or maybe something that users might anticipate and maybe you want to point them to some tutorial that they might you know take a look at yeah so I can give a few product updates of what's coming from Vespa we are coming gonna release some integrated dense models for Vespa so though you don't have to export so you can you can use these models off the shelf and then we allow you to tune the Korean coder so you have the document code is frozen but then you can tune the Korean coder and then show you how to combine these combining both dense as far so that's one thing that is coming out other thing is that we're taking some steps regarding for love QPS use cases because we designed Vespa you know to be kind of low single digit male seconds on multiple different use cases but not everybody needs that so we're introducing some new options for memory management so that we can actually run on service with less memory so that's I think that's gonna be a game changer for certain use cases that don't need high throughput low latency so that's two things and yeah that's I think that's more than enough you know and there will there will be some some blogs I think about our results on the beer beer benchmark yeah yeah that's and I'm gonna come out also with a blog post on a technique called span which is a paper for Microsoft so m2n represented that on Vespa so it's really interesting technique with the hybrid combination of HMSW and inverted file and you can represent this m2n in Vespa so I'm gonna do a part three of my blog post serial billion scale so that's something I'm looking forward to but right now I'm that kind of refracting a lot of the sample applications and so on to to get the experience more smoothly yeah yeah sounds fantastic looking forward to it and we'll make sure to link all the blog posts that you mentioned especially on billion scale vector search and other tutorials that you mentioned and this is fantastic thank you for doing this and keep doing this keep finding the energy I know it's stuck sometimes but I think it keeps you also awake and sort of like pushing yourself forward and I think the best way to use your brain is actually doing something that is useful be reading a paper or implementing code or blogging about it so this is fantastic thanks so much for your active contribution thank you thank you as well yeah um and I really enjoyed this conversation I really hope we can record at some point down the road as well if you will be open to it and I think we can cover a lot more topics as well but I wish you all the success in your endeavors and stay warm and excited about the field yeah I will I mean such an exciting field thank you very much Dmitry for hosting this and you know we'll talk later on thank you thank you and see you around bye bye you