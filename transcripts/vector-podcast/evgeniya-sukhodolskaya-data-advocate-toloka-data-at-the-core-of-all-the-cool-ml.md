---
description: '<p>Toloka’s support for Academia: grants and educator partnerships</p><p><a
  href="https://toloka.ai/collaboration-with-educators-form">https://toloka.ai/collaboration-with-educators-form</a></p><p><a
  href="https://toloka.ai/research-grants-form">https://toloka.ai/research-grants-form</a></p><p>These
  are pages leading to them:</p><p><a href="https://toloka.ai/academy/education-partnerships">https://toloka.ai/academy/education-partnerships</a></p><p><a
  href="https://toloka.ai/grants">https://toloka.ai/grants</a></p><p>Topics:</p><p>00:00
  Intro</p><p>01:25 Jenny’s path from graduating in ML to a Data Advocate role</p><p>07:50
  What goes into the labeling process with Toloka</p><p>11:27 How to prepare data
  for labeling and design tasks</p><p>16:01 Jenny’s take on why Relevancy needs more
  data in addition to clicks in Search</p><p>18:23 Dmitry plays the Devil’s Advocate
  for a moment</p><p>22:41 Implicit signals vs user behavior and offline A/B testing</p><p>26:54
  Dmitry goes back to advocating for good search practices</p><p>27:42 Flower search
  as a concrete example of labeling for relevancy</p><p>39:12 NDCG, ERR as ranking
  quality metrics</p><p>44:27 Cross-annotator agreement, perfect list for NDCG and
  Aggregations</p><p>47:17 On measuring and ensuring the quality of annotators with
  honeypots</p><p>54:48 Deep-dive into aggregations</p><p>59:55 Bias in data, SERP,
  labeling and A/B tests</p><p>1:16:10 Is unbiased data attainable?</p><p>1:23:20
  Announcements</p><p>This episode on YouTube: <a href="https://youtu.be/Xsw9vPFqGf4">https://youtu.be/Xsw9vPFqGf4</a></p><p>Podcast
  design: Saurabh Rai: <a href="https://twitter.com/srvbhr">https://twitter.com/srvbhr</a></p>'
image_url: https://media.rss.com/vector-podcast/ep_cover_20230128_100136_c691208feb13437a07aae0f929db756b.jpg
pub_date: Sat, 28 Jan 2023 10:19:56 GMT
title: Evgeniya Sukhodolskaya - Data Advocate, Toloka - Data at the core of all the
  cool ML
url: https://rss.com/podcasts/vector-podcast/799802
---

Hello there, vector podcast season 2. I hope that you were waiting for a new episode. And today I'm really happy about my guest and the topic, because in many ways we didn't cover it as much as I think and hope we can cover it today.
It's the topic of data, the role of data, while everyone is talking about sexy deep learning, chat, GPT, learning to rank new algorithms and so on. I still believe that we should not forget about where all these things begin and this is data.
And I'm happy to have and welcome Yvgenia Sukadolska, data advocate at the locker today with me. How are you doing? Thank you, Dima. Thank you. I am super happy to talk it. I'm very precarious. I'm very smooth. So I'm feeling like I'm just having a little chitchat before vacation.
Yes, exactly how it should be. And I'm really happy to have you here. We met at Haystack Conference and this was great. I saw so much excitement in you when you talked about, hey, but what about data? We should also talk about it. Don't forget it.
And I'm really excited to drill into this today with you. I think let's start as we usually start with your background and then we're all from there. Okay, perfect. Yeah, Haystack. I think I literally like formulated my passion that I want to talk about searching the means of the data.
So I'm feeling like today I'm getting a present for Christmas. So yeah, about me, I am this person, this type of a person which got his perfect position by chance because I never knew it existed.
Because I finished bachelor's in machine learning and I was like, okay, if I did so, I need to work like around it, you know, but at this point, everybody was, everybody is always hyping on something in machine learning, you know, so at that point, I mean, I finished in 2009.
So it was like a very big era of guns and like others. And everybody wanted to work with a computer vision and a dinner and so I thought, okay, maybe I need to start like working somewhere in the field and see what I will like. And by some chance, I started working as a software developer.
I don't know why just, you know, like out of the blue, your students, you're getting your first job. Then I kind of realized after some time that I'm doing more like business analytics, talking tasks and consulting through doing the development also and I like both.
And I was like, okay, so I don't want to be just a developer and I switched to a position which was called technical manager, which was like something in between analysts.
And also a person who like manages like some projects with people and also at that time, that was the first time I tried to do crowdsourcing like related to our projects because we were doing tasks about moderation.
And with moderation and actually you need a lot of data like labels and checked on the quality because it's a very hard task. And still was off because here I wasn't using enough machine learning and I was like, but I started it. Okay, then I switched to machine learning engineer.
I mean, it sounds like I'm hoping on and hoping off, but it was like some periods of time. It was amazing. I had an amazing had the I am still very grateful to him. He like, teach me much about machine learning in the production.
But at this point, I realized that I lost some of the knowledge that I like received back. So I applied for the master's program in the actually Munich. Now I'm studying the technical university of Minken on the also kind of machine learning program. And also I feel that I'm not speaking enough.
You see, so I'm like always like speaking enough, not the melon, not this enough. And at some points, my like, please, who I knew back then and they were working at that point in to local. They said, hey, Jenny, how about you will work with us as a data advocate.
And I was like, what is that? I'm going to Germany. I am not, I don't know what is that. And they're like, oh, that would be perfect for you because you can do machine learning and researching, but you also can speak.
 And I am so happy that it literally happened because last year and some time that I was like there working with the data, working for outsourcing, working with search because I also have like some past experiences when I was a machine learning engineer, I actually was working with a search recommendations a little bit.
So all of these combined in one perfect profession. So I would say I'm a very happy person. This is super great. And it sounds like you are in the warm waters of like what you really want to do at the same time. I think it's still a very demanding role and and then field.
And so you are basically still doing some ML, right, but you also advocate for data.
Can you expand a bit on what you do? Oh, yeah, it's actually like everything in a like all in one like, you know, this shampoos with conditioner and the shampoo in body wash and everything because I have some freedom at my position to choose what I want to like study now.
So, for example, I chose like going to the search conferences and talk about it because I had some experience. I really love the idea of comparing crowdsourcing and machine learning models in some particular tasks. For example, let's think about the adversarial attacks.
It's interesting how far we can expand with them, like detecting them by machine learning and detecting them for humans. And like these different comparisons where they crowd wins where they like manually where the machine learning wins.
It's a question which is in general interesting, especially now with chat GPT when everybody is like, oh my god, the ice conscious, OK, close everything, fire all the software engineers we are done.
 So it's super interesting to explore that and I am always like reading articles about this attending a talks about this also doing myself some talks plus of course I'm also participating in development of our company because to talk has started as a data labeling company, but now it's expanding much more in the means that we also started designing like a mouth tools on the top of it.
Because when you're having such a resource, you know, like human manual labeling in your basically in your, I don't know how to say in your basement, but it sounds creepy. Yeah, you can use it for transfer learning or for some other like interesting tasks.
And yeah, we expanded a lot and it's very interesting to a system this process and to come and to talk about this and also in the means of still manual lately in assistance to I am now developing currently. Yeah, this is fantastic.
And when somebody approaches to work on or like maybe you just create an account, I guess, and you start, you know, creating projects and so on.
I think many, many things go into the process like starting from the price, right, how economical it can be, right, do I have any control over this, but also in terms of, for example, the outcome, you know, the quality of labels that I will get.
How do you usually sort of structure the process, is there some general recipe that the lock up would offer to any user and maybe on top of that, you would offer some additional service, so to say right or advice to a company, is there something around that you could share with us.
 I would say I can talk no stop, but in general it's like this firstly, of course, when you're deciding that you need manual labeling for some reason, like some data sets, you need to understand that you actually need that because it doesn't need that for every task that they're existing just use data centric approach throwing data because nothing is tops up this that's not correct, of course.
You can be free using like open source data, you can be free using synthetic data because it's cheap, you're just generating it yourself. But sometimes in a lot of domains, you don't have enough available data for some specific domains.
And it's hard to gather it or generate it or sometimes you need a human creation over curation over the machine learning processes, for example, for monitoring or like with.
For example, it's like a hot topic i've seen such a thread in Twitter how people try to ask it for some like really, really dangerous stuff and check if it will have provided and it did so like you know we still need a human creation over like the data gathered by MLEI mechanisms.
So in this case, if you feel like you haven't need to gather a data set for your specific problem and you don't know where to start here is crowdsourcing platforms and for example into loka.
It is the platform which was created from engineers to engineers so it's not about like the only model with business where you're coming to us were consulting you and you're going away.
So we're actually super happy if you're like trying to deal with it yourself because we have an open API and it's more about mechanisms than speaking with manual labor. It's like literally about like handling the crowds with a mechanism sort of filtering and etc.
So usually to start you need to register and then we have huge tons of tutorials and education programs and also we have a community which like might in handles actually and we're bothered to discuss any problems or questions.
But I would say like we try to implement in the platform already simple steps that help you to do it pretty intuitively without studying much your first labeling project and set it up and let it run.
So there are like inbound instructions on every step there are like some little video or some little text instructions telling the good practices.
So we try to make it as simple like I actually saw it developing because when I started using to look at it wasn't any of this and now it's like impressive how they changed everything. Yeah, I can imagine.
 And so inside the log I mean if I consider to look as deep package product that I get access to you know inside it I presume you have the labeling editor or component whatever is it that you're calling it that I can flexibly load any data format right and also any vertical like from computer science to audio to text time serious maybe and so on.
What does it take for you know the way I imagine it they send my head is that let's say this is this is a team that hasn't had experience with labeling before but they realize the importance of it in their project. So they will not be professionals in this space.
What what do they need to think about when they prepare the data maybe quantity maybe you recommend them to start with smaller quantity how should they reason about format and should they first go and watch all the tutorials so can they somehow intuitively follow the UI.
 I would say like this like of course crowdsourcing a little bit like aligning it reminds me a little bit of training the machine learning model you need to spend some time tuning of course but yeah it's for the different data types that's firstly addressing your like comments it's for different data types it's for like all the text video etc and like can be used for multiple use cases like gathering data.
 Gathering data says for voice assistance and like for self driving cars or like as I hope we will still stick to the main name of the podcast and we will talk about search relevance with a human labeling but like yeah let's imagine your theme is on the record creating a project and they're realized we need human labeling but they never saw the platform.
I would say that the most important thing to consider is to start thinking in the midst of the composition of the task.
It's a key thing in the any crowdsourcing tasks that's not like it's pretty scalable so the amount is not a problem it's not that expensive you can set up reasonable prices and it will be pretty much cheap but the one thing that is very important if you make task too complicated.
Like as you would to for example having in house experts and you can ask them whatever and they will think of the rest here you're working with people which are not committed specifically to provide you something more than you asked for them.
 So tasks should be simple and they should be well defined so that is the thing that you need to a little bit train on thinking of how to decomposition task and for example like if you offer it to a person who is not in your area of the studies that you will be sure that he still can do it without special training maybe just reading the instruction or like completing some exam.
 And the rest is pretty much covered by the platform because now there are like specific mechanics which predefined your settings for you not making mistakes on like you know we money to the crowds or like doing the interface incorrectly because we try to implement as much testing in house as possible.
 And the interface they configure like the program where you configure the interface it is done pretty much into intuitive sense so you don't have to like learn JavaScript or HTML if anything it's done just in basic building blocks which can be like changed in places and group together in some nice looking interface.
 So I hope the most of the burden is just to start thinking differently it's like you know it's like with a programming sometimes there was a moment when I learned how scaling my life and I have to completely like reprogram my mind because it's such a different language in the means of programming you need to think differently the same is with crowdsource unique to think of the composing that is the most important.
Yeah that's exciting yeah a high school functional mathematical yeah it's so my god yes but probably beautiful too yeah this sounds great so it does sound like a self service in many ways and now that you called out search which is also very dear to my heart and I'm glad it is the same for you.
 So let's start with the basics you know like I have a search engine I have users I've got logs logs right so what I can do is that I can actually record for every search the position where the click happened right so what we returned what was the click and so I have plenty of data assuming I have plenty of users why do I need another data set can you convince me.
What am I missing.
Here I would say manual is much more about using not creating a new data set from the scratch but evaluating your abilities of ranking your queries in your search correctly because as far as I understand a lot of like there are like a lot of the ranking pretty sophisticated algorithms existing.
 The change in stay like starting from the simple like search like I don't know because I am a lot of people like document and a query and everything which was like some past time now people are creating vector databases and it's super sophisticated but still we have search we have recommendations we have like some order of queries which user expects to receive.
I mean user doesn't expect to receive some order but it user expects to see the right answer as like closer to him as possible not like searching for the five pages so for that specifically human evaluation on top of the implicit signals evaluation like clicking it's very crucial.
And I can try to elaborate on that how do you think like from your perspective like do we need if you like your creative search engine do you think we need also to make you and see the like ranking results or you think that clicks and buys if we're talking about equal is enough.
 Well if I play the interior to play the devil's advocate you know the data advocate I am the devil's advocate here in principle I already have users so they will tell me with the clicks they won't with clicks right so I might as well just measure you know the sort of plot this click through rate or something else and then see what's going on right so that will be my probably online metric.
But I guess when you when you talk about human labeling you infer that there is an importance in offline evaluation as well right.
Oh yeah I am like you know is asking this rhetorical question like we need it right right but actually I can give some motivation behind it actually it's a very interesting thing about what human clicks actually mean.
 We can return to it because recently I had this meet up about biases and it was also about like human clicking and one of the very interesting talks that we had at our meet up was about like position guys so the humans are just tend to click on something that they are offers because they're taught that the thing that offered like at the top positions are exactly what they need.
And they may be dissatisfied with that but they're just like you know follow the general way of how search engines and the com search engines work.
So technically online metrics they make a lot of sense of course because like by clicks and by bias you can predict the most of the behavior and it's pretty fast and it's automated.
 I mean like a testing by everybody knows that the one thing that it doesn't cover it's firstly explicit signals like you can you can't talk by the clicks and vice as the whole overall about the human satisfaction the satisfaction score because it's not like they are explicitly asked in general like do you like this do you like this search result maybe you wanted something else maybe you want it more maybe you wanted to be recommended something else.
 And yeah the other things that sometimes like if we're like doing some assumption about the like an A B testing for example that we change some interface and we're doing by some clicks and by some assumption sometimes we can by introducing new features pretty much hurt our product because it's happening in real life and user see the changes there like the ranking how it differs now with a new feature and they're getting super like.
 This satisfy the end humans they're not like you know for giving easily they see problems in your search engine they might say yeah i'm not using it again no thank you so like some reasons of why would apply lately be better you can experiment much more on it because like all you notice the feature in zoom that if i'm doing that it's actually noticing by the neural network and of course me.
That is amazing oh my god okay that yeah we're really in the like this is the time of the artificial everything so I got super amused i'm sorry.
 Yeah yeah so firstly with a plainly doing you can definitely experiment more because you can try different features without higher harm in the end to end users of your system of your engine and secondly you can check how they're satisfied what they do like actually explicitly ask them what do you think about this because when you're just the guessing their behavior by their like some implicit things.
Like where they are is nook and how much they click you can do much for mistakes because you know as it says we we can't get into another's human head but we can at least try to ask and then that probably will be closer to the reality.
 Oh yeah absolutely yeah I mean if we don't if we so what you're saying is that we if because usually in search engines in a way we skip that step of asking yeah we could integrate some give thumbs up or thumbs down approach but it might also be still rather implicit and not explaining everything we want to get.
 But basically like you know yeah I just remember that I was reading pretty much interesting articles recently about the recommendation systems and implementing them using them more about the behaviorism of the people like not just giving them the most popular result or the most desired by the similar by some features result like in the collaborative filtering.
 But sometimes we need to give humans the result that they are not thinking of what they it will make their like for example health or life better because you know this problem of recommender systems when you're like used to clicking on something at some point recommender system starts offering you the same pool of things you're kind of stuck in this and that's why like using more like sometimes and the offers of this paper they were suggesting that we need to sometimes.
 As humans explicitly did they like what they were recommended and do they understand why it was recommended to them and maybe they want to change track of the recommendations so that's why we shouldn't seem just online metrics but yeah also the second grand reason why apply metrics are good you can experiment without harm very fast because like online metrics they're usually taking like two weeks for something.
You need to wait for the statistical test to make some like results which are significant and here you can test it much more faster and you can perform even offline a testing by the applying manual label data which will cost less harm because the real users won't see your mistakes.
 Yeah I think this strikes a chord with me for sure so it's like you don't really because there is always a cost to pay when you go online that you actually deliberately potentially harming someone someone's experience to learn whether your contract is it called contract factual like your change in the algorithm is good a bad.
 Yeah it reminds me I am sorry I'm so totally up to the but it reminds me of the when I was working in moderation of the vertusments here it's very crucial to make mistake and line because they're like two different options otherwise you're like showing to the end to end users the advertisements with things which are like you aren't supposed to show like drugs.
I don't know some the funeries some yellow news something that is like dangerous or just like stupid on the other hand if you're not letting some like through the moderation some healthy content through your losing money of the companies which are like having a deal with you and your own.
 So here you need to be very cautious with any online experiments you pretty much doing everything online or offline and you need to very much monitor how you're watching learning algorithms doing the evaluation because environment changes a lot in like you know in the advertisement world new laws are incoming very fast people like people are input like they are impressively good at the end of the day.
 And the way the all of the boundaries when they need to fraud something so imagine every day there are like new existing algorithms of creating some advertisements which are passing the machine learning algorithm block blocking the fraud and everything so you need to adapt very fast and for that you and for a fine experimenting your course need applying data and applying labeling very much.
 Yeah I slowly start to wake up from my devil's advocate role so so I should stop being careless and not only rely on the data that I see in production because in a way it's like prime time for my product and I should be careful about it's not just deploying something once and it stays there forever and chat GPT takes care of everything.
But it actually something that I will need to evolve and this is where the crowdsourcing approach may help me to do more economical more less intrusive as well this is really good. Let's maybe try to make it a little bit more concrete right and let's emulate let's play this game.
Can you verbally visualize describe let's let's say I'm developing a I don't know flower search engine I don't want to say ecommerce I don't want to say something specific let's say I'm searching I'm offering flowers and I would like to search them. I will use her to search them.
 I guess can you propose sort of a framework of thought how I should approach the crowdsourcing so let's say what should I focus on can I choose a metric of line metric that you will recommend would you like to you know do you think that there is some specific thing that I could try to connect with my business goals like a metric that will be reflective of my business goals or would you start with something just something like I don't know in DC geo.
Whatever and go from there. It's a very very very long topic to discuss but less starts from somewhere so in my perspective like of course like you're doing your engine it's in some point of course you're implementing some like online labeling.
 So you can find a lot of things like online evaluation and like you have some somewhere to start and here you come into the position where you need to do some of line labeling so there is like these I would say like a circle which like in the parts of which you can like you can depict your your pipeline pipeline as a circle which goes in infinity and it has the several parts.
 When you're deciding like about like how you want to perform your ranking what is your ranking means what do you want to show the most what is your like how many positions people do see what do you want them to see the first what is relevant what is around and you're like selecting some end to end metric that you're going to use.
 There are like usually some popular metrics you notice the like and this is all like this this cumulative gain metrics is very popular and nice way to start there and there are like even like more simple ones just evaluating about the precision and recall of your position of the elements arising in your like ranking list resulting and there can be even more sophisticated approaches like.
 Like expected reciprocals around for example metrics if you heard of it it's like more cascade approach because you know that people are not clicking through after some certain position but think we're talking about flowers I would say it's like it's more about like image search simple one which has like some certain type of definitive answers and it's not like people are going to it's like with search in some items when you're like finding what you desire and then you are not.
 It's scrolling down maybe with flowers you just want to see so I would say I mean see them like download them or something so I would say this is pretty good at the beginning as a basis and then you can adopt this metrics based on what are you really interested in maybe you have some advertisements on some of the flowers for something.
 Then the next part after you define what are you want to do for example view like which metric you want to evaluate what do you want to see like what you want to compare you think about like what do you need for human labeling how do you need to sample data what will be the result how you need to aggregate it and how do you need to like use this information in your product.
 And then it comes like for example for in this G you usually need some ideal ranking to compare your ranking to so here comes exactly the crowdsourcing the manually because you can gather this ideal ranking from them and then do a comparison on the your real search engine answers so okay we define the goal we want an ideal ranking of flowers by the query and not one query because like I'm not going to do it.
 For example just one queries kind of I don't know super simple and you want to evaluate it in general so here comes the sampling and how you can approach sampling of your queries and the results of your search engine can be very different you can just try to sample the most popular flowers and queries but it's usually not the best approach just because like the most popular queries are usually very well handled and they are very simple.
 Because like when the people are marching in the in their disayers it means that it's not a very complicated thing so like what very like a huge tail of very rare queries which you also want to consider I guess in evaluation in ideal ranking so here comes like two techniques for example reserve works sampling or even like stratified sampling I would say I must recommend using stratified sampling adopted by like your own.
 So you can use the same method as the one you are using in the list of the situation in your own needs this one allows to like to very like shortly explain it without like the deep is just you have your own data the whole amount of the queries with their like how often they're asked how popular they are and you are doing like some bins of them based on the popularity and you try to sample equally from the each beam but these beans are different sized based on the general popularity.
 So we have the kind of like you're kind of modeling the distribution of the data in your engine by sampling like this so after you have this data samples you need to think how to present it to like manual labor what do you want to ask you like you want some ideal ranking yes and there is an option to give them like for example query and the like the first 10 or 20 results that your engine returns and it depends like how many results depends on the click through rate which you can like for example estimate my you have a data about how users click how far they click in your like length of your search results and you can estimate that after like I don't know 15th position it's not interesting usually to anyone so you cannot worry about it very much.
 But here like you see if you're giving the whole list to end user in crowdsourcing and saying okay rank needs from like the most relevant list relevant as I was saying before the composition is very important and this task of ranking is very hard because even like I am having like some degree like I have much worse and masters I think I am generally like a dukega person to some extent I am not sure you know but if somebody says to me okay this is the flower like this there is a 15 pictures can you please like rank them from the most suitable to the list suitable I would be like oh my god I can't do that because it's too much so there's like other approaches either like taking a specific item which returned by your system taking a query and answering are they like relevant or irrelevant together is it a matching or a matching pair it's much simpler it's very good understandable by the crowd but the problem is that here you can't kind of compare items with the same relevance because like it says like okay this relevant and this relevant and you're like okay what should I put on top this one or this one you can ask people to give like some percentage of relevancy from their head but still different people think differently so it's kind of hard very much to aggregate the results.
 The most nice approach I would say would be a pairwise comparisons so you're like giving a query you're giving two answers and you says okay what's what's what's you what's you better and then by this pairwise comparisons you can do a whole ranking then by aggregating this pairwise comparisons in the manner of the list with like from the most relevant part to the list relevant and of course If you're doing this pairwise comparisons honestly like how it's supposed to be it's n squared the amount of entities which is like a tons of entities so usually our suggestion is to do like more in the weak sort or like other sort manner with n log n so like doing a hard estimation of this pairwise comparisons sampling a little bit less but still you can like have in the end the pretty pretty good estimate it like ranking list.
 So you create this assignment you have this pairwise comparisons you have the results you can estimate the quality of results based by how like this particular user is good with this particular assignments and then you're aggregating it there are like some models then you can use for aggregating for example like mathematical models statistical models like for example, red literary or something we did it actually in our crowd kids it's a thing for pretty much we tried to do an open library in Python for every type of crowdsourcing annotations not only to local ones so you can implement it yourself for example take some library even hours and then you got your ideal ranking as you desired and you can compute the metrics like compared to your ideal ranking so how good your search engine returns like on big samples and these samples how good are the results of these flowers how relevant they are and then you see like what is the overall result it might be good or not very good if it not very good you for example can select some domains when you see the most mistakes and try to like ask the crowd in some separate projects the main wise like where are the mistakes exactly maybe you have problems with like defining the color of this flower or maybe you have problems with like good lightning on the photos and you can figure out what is exactly the problem and yeah you can use this manual labeling firstly for evaluating the metrics from time to time and to see how your search engine improves with like including new features and changing the search in algorithms and you can also train on this manually labeled data your ML models which perform ranking so I would say it like it works kind of like this.
 Yeah this is this is great it does sound like a very structured process what you explained but but I do want to drill into maybe couple of specifics so one is I believe in DCG is is definitely I think it's a browser that In principle if I was communicating this to some management in my team I could say that yesterday we were at 75% and today we are 76% so we are improving right and this is on a percent scale if I remove the letter and from this formula then this becomes like an absolute scale and there is no way to tell are we progressing or are we regressing.
 But at the same time again wearing my devil's advocate suit here for a moment and DCG has a problem that if let's say I have a scale of labels from zero to three right so zero one two three so zero meaning completely relevant result and three meaning completely relevant perfect result if I receive two ratings one with all all ones right so all ones and the other one is with all three so all ones it's kind of like a suboptimal result nothing better in the list but at the same time not perfect and the other one is absolutely perfect and DCG yields exactly same number because if only if we You rightly mentioned about optimal perfect ranking so if my perfect ranking equals in lens exactly the visible labeled area than the formal and DCG will yield 100% in both cases and this is kind of like a problem and you touched on this in that part where you say that we need to make sure to construct this perfect order of results right so how long it should be let's say if I show 10 flowers on the screen 10 bouquets whatever how long that perfectly should be 30 hundred is there any recommendation.
 I would say like as I mentioned before firstly you can use the metrics which is like expected reciprocal rank which is exactly talking about the moment when the user lose attention and after that you can make mistake but they are just not reaching it and for evaluation this like the moment of the termination of the interest you can exactly I think pre evaluated with like if you have some any data and pre-evaluated by the clicks so you can give like any item some weight by reach through general and then just predict in general how much like general user how many items your general users like look through before they're satisfied with the result and maybe over time this actually amount will be decreased because your ranking will be more perfect.
 But you also can try to emulate the same experiments with actually the cross-sourcing and just to see how like to give them some certain amount of objects why I'm talking about this actually because recently when we had this talk about biases the presenter for testing his hypothesis on the click through he created a project in soloka where he had like the query.
 And the recommendations which were like around the 20 or like 30 and he looked are there clicking through until some I mean they're also were like ordered like the search engine and he looked like how how far they're clicking through to check the hypothesis of the position bias so in general you can also try to test your hypothesis online with a click metrics and see how to like.
 Choose this position and then test it offline but one additional thing when we're talking about business we're in general also talking about budgets so of course the more you need to evaluate steel is the cost will rise just because you're like your offer it more data to crowd and crowd needs to like to do more assignments or it becoming more costly.
So I would say I would like estimate the amount that you need that you know that you need the amount of the click through and then maybe cut it based on your like general estimate the costs of mental labeling and try to align them little bit because still.
I would say the result might be not like 100% perfect in the means that people are reaching like farther and seeing the mistakes but it still will be a big improvement if you're catch a mistake in the top ranking like positions.
I think connected to this there is a notion of disagreement between annotators right so what is relevant for you might be completely relevant to me and I I want to see the for the same query I want to see the results in different order.
 I think one of the suggestions I've heard of how you could construct this perfect list is actually you can take and concatenate all of the rankings given by independent annotators for the same query and then resort them in the order that makes it perfect from the top to the bottom of course you will still have issues with ties right so if you have three three three's then how should you order them but.
But at least they will be visible on the screen so maybe that's fine.
Or maybe not who knows but at the same time you kind of like achieve this perfect list which incorporates the wisdom or the wishes of other people that have been in the same sort of group have you experiment something around these lines or do you think it's sensible to do is.
To experiment with the which part with the check in the form working with or with reordering with with with with constructing your perfect list right because for ndcg you need that perfect list to divide by right in the form.
So how we experiment with the length of this list you're asking me no in this case I think i'm actually describing that specific way of building it that you take a sub lists from different people that annotated the same query then you stack them together and then you sort them right.
 Or always it's yeah it's very interesting approach I would say that I myself never experienced such of the technique which sounds very interesting but we're usually just doing like I usually eat more like aggregation by the models which are not like concatenating but that taking into the account in the general the quality of the user in this ranking problem.
 And so when you're doing an aggregation you're just like more lean towards a user who are proficient in ranking in general so you trust him as an end to end good user of the search so for example when one person said all three and one said all once but I know that this three guy is in general good at this I will just take his one as an ideal labeling.
 Yeah this is this is the exciting part you're tapping into the topic of quality of annotators which is super super important at the same time you could teach the annotators if you have them in house but if you have them external you kind of do not have control over who gets what task so how exactly maybe the local or what kind of methodology should I apply to measure the quality of the data.
What are the components there.
 It's actually like I would say it's a very very like it's a very big system and the means that you need to not only measure quality but also like keep your projects and protect it from the fraud and from the people who specifically want to break quality not just they're like making a human mistakes but they're really really trying to scan with your data.
So there are like different techniques starting from the super simple ones like anti fraud ones which are like looking at how fast are you labeling.
Are you labeling with the same non human distribution of the labels like clicking only like one option until it just goes forever or like even sometimes it's shaking of how you're like how is your behaviors like in general with like different projects with a lot of data.
Different projects without not taking how your mouse works or something like this so this and also there are like of course general exams.
Checking your language proficiency checking your proficiency writing checking your proficiency in some other skills which are also building up some certain I will say port portrait of a good label or because if you're like able to and provide the good results in the some skills.
 Which are like around this problem like your good with this or this that means that you're in general won't be at least a broader and you have a chance to succeed in this tasks and then the main mechanism which is used in the most of the tasks where you have categories like classification or something and when we're working with a categorical tasks we know the ground truth some sort.
 Of course it doesn't happen with ranking because we're ranking we don't know like it's a very subjective manner what do you prefer this or this but with but you can of course actually create an obvious examples like very obvious so when you know like some ground truth you can hide you can shuffle in this.
And you can see the examples of the tasks with the answers which you know and you can like.
 Hiddenly shuffle them in between the assignments so people will complete them without noticing it because like it's hidden by the API and everything and by the percentage of the examples that they evaluated correctly you can kind of see me their skills because you know that like in general for this class they're giving the right answers.
 And the second technique which also works good for the more creative I would say or gathering assignments for example when you need to take a picture or when you need to do an assignment outdoors for example go and check there is like building on the some certain plate for like a map sub there you can do even more tricky thing and tell the crowd evaluate the other crowds.
 So you're creating a specific validation project with the other crowds or source and you're giving them the answers of the first crowds or source and you say okay guys now you need to evaluate doesn't look correct to you doesn't look like not a fraud and everything and there by this double evaluation you're actually sorting out all the problems.
Wow I have never heard of such method it's it's amazing I think more traditionally like maybe like 10 years ago in the project related to sentiment analysis we were talking about. Double annotation but at the same time so you give the same.
You know the same label and then you ask the human to whether they agree or not but twice and then you basically calculate the inter annotator agreement but what you just described is so brilliantly put and sort of invented in a way.
 Was this invented at the locker or have you seen this somewhere to be on I don't know if we did that I am super happy yeah it doesn't seem like rocket science yeah but it works yeah yeah yeah and also about yeah in inter annotator agreement also works especially in like some classification that's how we actually started creating this hidden assignments recently.
 Like as I told about them we are called and honey quotes or the golden assignment the one with a hidden tasks which you're like shuffle in the data and then evaluate the result the skills of people who are doing the some certain kinds of assignments and actually also we're saving these skills and sometimes you can access them because they're already on platform they call global skills you can just.
 Preselect on your project people who already succeed in moderation for example that actually helped me recently a lot because I didn't have to train the crowd for my very complex stuff so yeah but I stepped aside so before like when I was even working with to lock us on time ago you have to create this specific tasks yourself.
 Like this hidden you have to manually label them and that took some time and it was like kind of tiring because you're sitting here and you're creating like 100 like usually you need some certain amount of some sample of this task like at least 75% of the general like amount of the tasks on your platform on your project to evaluate how good the people are because you're just if you're like giving them 100 items to label and once you're asking like.
If it's correct on earth you can't evaluate if this person is good or bad it can be just the pure luck so labeling why yourself was kind of you know time consuming and said and recently we decided okay but we have.
 Crowd why do we are doing the drop of the crowd let's just create this hidden tasks by the other crowd and we can do this easily just using the interhuman agreement you're just giving them a task and you're pre selecting the crowd with the good skills in the past just in general so you trust them more and you throw for example 10 people on one tiny bit of a task and 10 people like legally without knowing what the others say and then like to have better than one you.
 Usually some certain amount of the strong agreement comes and you know that is the right answer and you can directly pick it and already shuffle it in the other project so you'll see yeah we're making the self working mechanisms like you know you just throw some data in your system yeah it's like self reinforcement or yeah I think this is amazing and and it also is surfacing I believe like it's.
Feature of the locker that you cannot get with let's say you set up an open source labeling tool that you can having a specific task like moderation or.
 I don't know sentiment whatever machine translation that you can actually ask and gather a group that will be proficient in that specific space so because otherwise you're going to be wasting cycles in potentially teaching people right yeah yeah I think this is something that now that we started to say in the beginning of the podcast the data is important but also humans that annotate.
It is important I important yeah absolutely this is great I still wanted to understand one building block you were talking about aggregation can you can you again sort of restate this what do you mean and what should I pay attention to as a as a user of such a platform.
 So for example like there are different ways of annotating data and sometimes you need like there can be different cases when you need aggregation so aggregation is like just imagine that you receive some raw results from the human annotators and then you need to aggregate them in some final answer that you will use for your model or for something it can be different cases when you need it for example as we were talking about the.
 The aggregation between humans on the some some task for example you have a task of labeling feature is that a cat or dog and like you decided that you want to like foreign a tater so it's and like three of them said it's a cat and the one said that it's a dog and you have less for answers and to understand that it's a cat you need to perform an aggregation so if it comes to classification tasks it's pretty easy to do.
 I mean you can do just major world or like major world to wake it by the skills of this people but then it comes for example for aggregating like images like for example you're doing a segmentation and you need to aggregate different answers about the segmentations here it's already harder because like doing like a major world piece of wise it's a little bit of a hard work you know.
 So for that usually there are like some models which are pre designed and used and studied in crowd science so aggregation for the aggregation of image and also aggregation of this pairwise comparisons that I was talking about because this is a specifically a hard task because you have this pair wise assignments and sometimes it's like a better than be be better than see but see better than a and you're having a cycle you don't know what to do so for that there are existing.
 So then a couple of models which are based for example noisy bread dietary which are based on the expectation maximization algorithm which assumes that flavors are actually by the skill know the ground truth of the answers and we're trying to estimate that to get as possible as close to that for a digmar with like a couple of iterations of this model and then the end it just gives you away the list of responses like for example if we're counting and.
DCD some other metric we just need a list where it says like item one is the best item 10 is the worst so the aggregations out of this all of the pairwise comparisons it will give you that list.
 You can implement these aggregations yourself and study them because like in crowd like we're not the one the first ones doing crowdsourcing action so they're like in crowd science they're like a lot of models presented and our research team actually also studying them and implementing them and I hope.
I'm not praising them too much but you can it's your it's your moment of okay our research team is created yeah but but yeah we for example for aggregation we just create a tool which can be used paired with a platform so you don't have to think much how does it work but if you want.
Right me and I can provide you with paper yeah absolutely and all the papers that you mentioned during this podcast I really would love to include a show notes as well because because I I see how how the listeners find the episodes educational and they.
Some of them spent a lot of time you know listening through and and and then you know reading the papers as well at least browsing through them. So Janet so much stuff you have shared so far and I think even those who are using open source tools like I don't know keep it a label studio.
 And others I'm sure they can learn from what you said but I also hope that they will improve their practices by typing into the talent behind the locker but there is one topic that I think keeps surfacing everywhere but also to some degree it becomes an overheated discussion around bias and data and how this can actually drive the inequalities in life and in the world and I.
I think by the virtue of us being in this space we should resist this as much as possible but I wanted to pick your your brain on what is bias for you and how you have seen or maybe discussed this in the email projects.
 I really like this topic because yeah I recently hosted a panel discussion about biases and a lot of hosting a panel discussions because you can come you can know zero about the subject you can ask the stupidest questions to the most awesome engineers in the field and your returning super educated so maybe I'll try to just to recreate what people from this panel discussion and early in set to me.
But as far from my understanding they are like not two types of biases but I would I consider them as a two types of biases.
 The one is I think all bias more related to the stuff which is like the things that we shouldn't be biased on but we are biased because of the historical data or like the mother unfair like like results of the history so it's like the bias of the skin color the bias of the gender the bias of some other and they are here and there in the set in the big models.
For example like even the dolly and GPT free and everything.
 Sadly scenes they are learning on the internet available data and the internet is a very toxic space sometimes especially like I still like the stories of the chat bots which were like learned on Twitter and then they are like so offensive that nobody can leave them out in the open like business communication word.
 So these models of course have this at the biases and that should be controlled very much and that's why we have like the fairness fairness topic in the eye and that's exactly like I actually I studied recently I love my masters because I'm revisiting all the topics in the ML so I'm feeling like when I'm talking about it I'm feeling like I'm literally coming from the academic background.
It's just the monsters and the fairness algorithms there like pretty much set up how you can evaluate how you can try to make your data list bias or just test it on the fairness but yeah still here.
Sadly and the second thing which and of course their approaches how to avoid it fully but sadly we're constructing new biases here and there so we're getting rid of the one and we're constructing you.
 And the second one they are more like behavioral biases maybe they're like last harmful in the general because I consider ethical biases being very harmful like when we're creating a systems related to jurisdiction or like to some other things these biases can be crucial and also by the way the same biases.
 Oh I can I remember the story about covid like with the covid when people tried it's not like that ethical bias but it's a bias and it was very crucial so when with covid people tried to at the beginning when it started and everybody was panicking so people started to think it maybe they can do something in the eye like some amount of which will help to recognize if the person has pneumonia like is it like caused by covid or not in the lungs.
And there already was data from China because it started earlier and there were like a lot of AI and the engineers working on that but the problem was that the data was biased and it wasn't cleaned and sorted out because people didn't have so much time I mean it was very like a crucial moment.
 So because of that models were working very biased and bad because they for example were predicting that if the person on the like the scan if the person is kind of in the position of lying she or he then they have covid but if it's in the standing position they don't have it just because the people for lying and taking photos they were just in the worst medical condition.
In general because like when you can't stand out that means that you're pretty you know so it was just a bias in the diet because it was a balance and that is the result of bias which you need to monitor in control what that's why you can't leave it in the open world.
 And yeah so the behavior biases it's more like about when like for example with the search engine I think I touched it the position bias it's when you're just trained to click on the like first three elements that you see because you're you're so well with information that you don't have like a power to go through the tens of pages and select what exactly do or there like some other biases.
 For example we know that one behavior thing that people have it's it's interesting thing it's called it's called choice over it's like when a recommendation systems people actually like in restaurants people prefer to see something with a bigger menu with a bigger recommendation because they think oh it's enriched it's nice I would love it but the more choice you have.
 The more cost you're spending of on decision your inner cost your evaluation cost at some point it becomes just not like not feasible not useful like you need less items to select better choice at some point you just lose attention and everything and that's another like thing which comes from our behavior and which biases a lot of instruments and which biases a lot of like models and which we need to take into account.
Or otherwise we won't be successful with implementing it yeah absolutely on this paralysis paralysis of choice would you think that reducing the number of options with bias our system in some way like strictly speaking do we actually introduce bias by reducing the number of options.
 Oh I'm pretty sure we do but like as I said sometimes you can use biases like not all of the biases I would say there that harmful sometimes you can just like try to use them for like having more good outcome of course I'm not talking about the ethical biases but with like for example with reducing the choice amount.
 Of course you're biased people towards like the what you offer for example it depends on what you offer in this limited choice and if you're like offering them the most popular of course they can be stuck in the pool of selecting the same items without changing their preferences which they would like to but in general for them it would be easier to select something that they really prefer by the whole characteristics.
 So even by using people here you're actually kind of helping them with the choice process so I would say it was a general recommendation like after 10 or 15 items as far as I recall your choice overload becomes too much so you just can't like you know I hate these restaurants when you have an immune soup she feeds Indian food Mexican food you're got oh my god I'm so hungry but I can't chew.
 Yeah it's like no focus and maybe no face of the restaurant in a way but at the same time I'm pretty sure there are customers who are like in haste and they don't have time to drill and understand what is this local cuisine here just give me that pizza or burger or whatever and I will flick through the menu right but I really wanted to relate to what you said and I think bias is not always negative and I think it's important.
 To understand that sometimes in circumstances circumstances it could be actually bringing positive impact and the example you gave is one of that right so but at the same time whatever I show on the screen in the search engine you already talked about it's a click bias right what I show in that order you know in majority of countries in the world will go top to bottom left to right and we will click and I will show you what I'm doing.
 In that order but at the same time anything that I say for example now I already bias you to think in that direction and if I choose another strategy and I start talking about snow or something else your mind will tune to completely different topic right and you will be biased again without realizing that I do this.
So the same actually will apply I think to the annotation and labeling project right so whatever I show in which order I show which questions I ask will bias the annotators to.
 Besides all other factors like if I overload with them with questions they will be tired and really not give me value but if I didn't do that the order of the tasks might bias them and many other items can you talk a bit more about that and also is there some silver bullet that can solve this with least improve.
Okay from from my position which is a very subjective opinion I would say bias is a very human thing so even creating like a big.
 The same shunt the perfect model trains on a purely human data in the amount that we have it now even if we increase the amount we can get rid of the biases which are we are so afraid of so we need to go to the some system with robots creating robots creating robots and then maybe we're get rid of our own biases because as you know human factor it's a really like a thing where just making mistakes sometimes just because we're.
Like not as perfect in the tentative so we're just biased as if but I would say with the human labeling you see you're doing product from bias humans to bias.
At least the thing that why I was talking about the Ecclicity like when you're predicting something by the their clicks in the online experiments you're introducing even more like you're introducing a third person in this chain developer who.
Does assumptions about others people bias is sometimes without knowing their culture or like as you said in search engines we read like on top to bottom from left to right but sometimes in some cultures they have different way of writing reading right to let or maybe they have like designing.
 There are different people which like different types of the search like based on age maybe sometimes some people are like seeing less or there are people with color blindness and they need some other results because like it's also depends on how do you see everything so when only one person like assumes especially developer like I was asking on the panel a question like should be or we all be psychologists and philosophers to create a systems because like when the development of the system is going to be a lot more difficult to do.
 The developer decides what to do it sometimes this person is not educated about like psychology of the human behavior and it might give some mistakes so that's why I think human labeling wins it's not like there are people are who are like psychologists and philosophers but you are giving the same task to the same crowd you can like do a perfilter in of the crowd.
 For example by the same targets audience that you're interested in by the language by the culture by the interests by like for example you have a comfort flowers like people who like flowers like work with flowers or ask them do you like flowers and then send them to this task and by this your at least like.
 Trying to model the same behavior with actual like people by this having the same distribution like maybe small like sub sample of the same distribution of people for your target users and you're not deciding for them yourself so I would say the best recommendation is to think about filtering your crowd for your assignments thinking of who you want to be satisfied by your product and asking the people related to that to do the evolution.
So I think that's the best recommendation to do the testing.
 Yeah, it's just one thought came to mind that in principle if we consider it you know annotation process is building some kind of mathematical function that we're trying to with which we're trying to fit into the reality then in principle we could have built a perfect annotation, you know composition project that would fit into the reality.
So I think that's the best recommendation to replicate the same biases that exist today and earn money right that would be kind of the wrong way to go. I hope companies do do doing that right.
I need I mean I need to say that even like I think even to log if I am not mistaken it actually uses as a support a little bit of machine learning labeling so we're learning on our crowd in each project and we're like providing some sublingling with the most.
Learned by the prices of the target auditory and tries to emulate the same behavior, but it's still not like the evil sentient AI robotics because it's mostly manually labeled but I need to know.
I need to mention that also humans who are like labeling assignments sometimes they are very educated and very smart and they are very willing to correct the system and actually when you want to correct the system you're becoming super talented.
So I saw some people creating some algorithms which are labeling the assignments for them and relating the human time of labeling human way of like moving the mouse human way of understanding instruction.
Recently I was asked how we're blocking this type of people but I'm saying by to bond this type of people there getting so close to actually label it that. Yeah, I'm sure we can learn learned from them because and you already touched on this topic that another big area of researchers how we can.
I believe it's called gamification or like you break the machine learning model by supplying certain sequence of actions and input such that it will unlock some doors or whatever right maybe you receive a loan that you are not supposed to and things like that.
Yeah, this is interesting and do you think I'm asking the same question as they see on your event. Do you think that unbiased data is attainable so there is a zero bias.
 The honest I don't think so I don't believe so like I might be incorrect and the experts said like different experts like who was sitting with me on the panel discussion and I of course I asked the same question because it's very interesting like is it only our thing the why we're in this loop of creating and fixing biases like you know like a cheap one but.
 I personally don't think so because like a bias is a very human thing we can try to get rid of one you're creating another but it's not bad it's not bad we just need to get rid of the actually like dangerous biases like ethic and other ones and with the rest we just need to understand how to deal with them and as humans we can recognize some biases which are harmful and that's good that's why for example we need.
 The manual evaluation of the AI systems which are trained now because they are having their very like nicely trained but they are producing biases and they can detect what they are doing so sometimes they can be harmful so that's why like from my perspective like big models alone still can be like used now even if there exists and they're like compressing us very much I need some on top verification.
Yeah exactly and this is where the human labeling comes in. I think the flip side or if I would flip around my question about getting the or your question rather.
 Of getting the completely unbiased said you could also say could we actually source the data said that that contains all the little biases or little diversities that exist in the world right or maybe not okay not in the world but in that domain of operation that you are you are in your business and maybe.
Formulating it that way gives me a lever to start thinking okay what is it that I'm missing in in the data and of course this is the most challenging question to know what I don't know what I'm missing right so it's equally hard but it's probably more in the trajectory of a massive the data set.
I would say that I actually heard some approaches which are working on that like specifically taking into account bias very biased data on the like in the domain and seeing how you're a reason will perceive it and actually catching the mistakes by it because yeah we we have.
We can account the bias data and there are like some guidelines how to notice biases in your data or models so here we can try to at least approach this ask from your perspective yeah.
Sounds great hey Jenny I really enjoy talking to you and I think we could talk forever on this topic but I really love asking the question of why which is.
 I'm tapping into your motivation you did say in the beginning that you know all the stars positioned correctly in a way and you got the the dream job but at the same time you still wake up every morning and you say okay what will I do today what drives me why drives me so what drives you to do what you do data advocate and a well.
 I would say I don't want to like start a story with reflecting on how I like I woke up one day and realized that my my heart belongs to a I and everything but I would say like a little little moments in my life when I had to write and say about can computers think when I was applying to university and I had to explain to my parents what is AI and why I'm doing it.
 When I compare the other like positions when I saw some questions which people in general like asking see in the Lee and GPT free when I visited some industrial conferences and compare them with the research and conferences and notice that people are fascinated by the models and their key textures when what when it comes to like taking it down to development and to actually helping people people.
Struggle with doing some simple things like not simple but basic things like providing the date not interesting there like they sound less interesting but they're actually very crucial like providing the right to cure in the Bay.
 By us monitoring it not just creating a proof of concept it bugged me so much because I see so many cool models ideas architectures around creating like insane applications but not always they're coming to production and not always there start like helping people and everything and I would say I really would love to I love to.
In general like seeing something start working like it's a very satisfaction or anything so I choose my like my path on say to approach people with talking about data and how can it help actually to train the models and make them closer to the production.
Make them closer to being actually here and working good and helping people out with this magnificent ideas created by researchers.
Oh yeah that sounds so cool very deep thank you for sharing this it's like I think in many ways it's like the dream maybe of creating that companion that can think the same way we think and it's not human.
Because we are could also as a humanity we do reproduce and so on but we also challenge ourselves and others in what is possible what is what are the limits of of our intelligence or you know are they need.
Yes and it's a very interesting task that are still waiting there to be solved and can be solved with the I think it's it's magnificent. I am waiting to see if somebody some model will finally win the Lovner prize and pass the cheering test so I'm working for chat GPT.
Maybe it will be fine tuned on something like flower search or something. Yeah yeah yeah on human labeling with for lock I am pretty sure exactly exactly.
And yeah and traditionally of course we will we will link everything we can link about the locker but if you were to announce something to the audience maybe how they can get closer to the platform you know start playing around.
Okay yeah okay I have three things that I really want to to announce firstly if you want to talk about just like do we need manual labeling do you need manual labeling do you need to do you need to transfer learning with crowdsourcing do you want to just use crowdsourcing and think about it.
And to join our community because we talk just in general they're about the mental stuff about the eye about crowdsourcing about that the century can all those entry approach and there we can.
concretely talk about like some topics which concern to you to your company or like to your business and just talk and also we have two initiatives for like education and for researchers if somebody is interested to check some hypothesis on crowdsourcing for example some.
 Christian air some ethical stuff some gathering of the data sets for your own to or like you want to create some education or like to teach a course over the crowdsourcing to your students we have two foreign applications where we can provide you some promulcals of using crowdsourcing for free and play around and maybe to start liking it as I do.
And I truly like it because like when you can gather at 12k data sets in one day start liking it. This is mind blowing. So yeah that's that's me that's it thank you very much. That's fun. That was magical thank you.
I'm sorry for being like a very talkative person but I'm always like this would be afraid of me.
 No it's it's your character it's your energy it's it's your experience and it's something that speaks up you know beside you controlling it I think it's it's it's important it's amazing and that's how it should be I think I really really enjoyed talking to you Jenny today I hope this is not the last time we can record another one and another one.
And know the best with your Christmas vacation or thank you and recharging and all the best with the local. Thank you very much I keep in. You can do it however you like actually we we approve.
Yeah thank you and I hope the audience got that magic tune as well and everyone will also have time to recharge during the Christmas and your break and we will continue from here thank you Jenny. Thank you bye bye.