---
description: '<p>Topics:</p><p>00:00 Kick-off by Judy Zhu</p><p>01:33 Introduction
  by Dmitry Kan and his bio!</p><p>03:03 Daniel’s background</p><p>04:46 “Science
  is the difference between instinct and strategy” </p><p>07:41 Search as a personal
  learning experience</p><p>11:53 Why do we need Machine Learning in Search, or can
  we use manually curated features?</p><p>16:47 Swimming up-stream from relevancy:
  query / content understanding and where to start?</p><p>23:49 Rule-based vs Machine
  Learning approaches to Query Understanding: Pareto principle</p><p>29:05 How content
  understanding can significantly improve your search engine experience</p><p>32:02
  Available datasets, tools and algorithms to train models for content understanding</p><p>38:20
  Daniel’s take on the role of vector search in modern search engine design as the
  path to language of users</p><p>45:17 Mystical question of WHY: what drives Daniel
  in the search space today</p><p>49:50 Announcements from Daniel</p><p>51:15 Questions
  from the audience</p><p>Show notes:</p><p>[What is Content Understanding?. Content
  understanding is the foundation… | by Daniel Tunkelang | Content Understanding |
  Medium](<a href="https://medium.com/content-understanding/what-is-content-understanding-4da20e925974)">https://medium.com/content-understanding/what-is-content-understanding-4da20e925974)</a></p><p>[Query
  Understanding: An Introduction | by Daniel Tunkelang | Query Understanding](<a href="https://queryunderstanding.com/introduction-c98740502103)">https://queryunderstanding.com/introduction-c98740502103)</a></p><p>Science
  as Strategy [YouTube](<a href="https://www.youtube.com/watch?v=dftt6Yqgnuw)">https://www.youtube.com/watch?v=dftt6Yqgnuw)</a></p><p>Search
  Fundamentals course - <a href="https://corise.com/course/search-fundamentals">https://corise.com/course/search-fundamentals</a></p><p>Search
  with ML course - <a href="https://corise.com/course/search-with-machine-learning">https://corise.com/course/search-with-machine-learning</a></p><p>Books:</p><p>Faceted
  Search, by Daniel Tunkelang: <a href="https://www.amazon.com/Synthesis-Lectures-Information-Concepts-Retrieval/dp/1598299999">https://www.amazon.com/Synthesis-Lectures-Information-Concepts-Retrieval/dp/1598299999</a></p><p>Modern
  Information Retrieval: The Concepts and Technology Behind Search, by Ricardo Baeza-Yates:
  <a href="https://www.amazon.com/Modern-Information-Retrieval-Concepts-Technology/dp/0321416910/ref=sr">https://www.amazon.com/Modern-Information-Retrieval-Concepts-Technology/dp/0321416910/ref=sr</a><em>1</em>1?qid=1653144684&amp;refinements=p_27%3ARicardo+Baeza-Yates&amp;s=books&amp;sr=1-1</p><p>Introduction
  to Information Retrieval, by Chris Manning: <a href="https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719/ref=sr">https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719/ref=sr</a><em>1</em>fkmr0_1?crid=2GIR19OTZ8QFJ&amp;keywords=chris+manning+information+retrieval&amp;qid=1653144967&amp;s=books&amp;sprefix=chris+manning+information+retrieval%2Cstripbooks-intl-ship%2C141&amp;sr=1-1-fkmr0</p><p>Query
  Understanding for Search Engines, by Yi Chang and Hongbo Deng: <a href="https://www.amazon.com/Understanding-Search-Engines-Information-Retrieval/dp/3030583333">https://www.amazon.com/Understanding-Search-Engines-Information-Retrieval/dp/3030583333</a></p>'
image_url: https://media.rss.com/vector-podcast/20220522_070529_71d7f3ebca3858a656066fb337b207c1.jpg
pub_date: Mon, 23 May 2022 13:00:19 GMT
title: Daniel Tunkelang - Leading Search Consultant - Leveraging ML for query and
  content understanding
url: https://rss.com/podcasts/vector-podcast/494873
---

We can get started. So I can kick us off even though you see are really the star of the show. So hi everyone. Welcome to our fireside chat with Dmitry Kan, Daniel Tunkelang. This fireside chat is on search and all the interesting topics that Dimeji and Daniel will talk about.
And it's a series that's hosted by Kauarais.
 Kauarais just do my plug here and we're a new education platform that transforms the way professionals build technical high demand skills through top industry leaders such as Daniel and collective peer learning such as Demetri, the format of our courses or pretty innovative because we mix live instructor sessions with real world projects and fireside chats like these with operators that are experts in their field.
And actually I see a couple of students from both the search class and from other classes are in the audience. So welcome back to you guys and welcome to everyone else here. So with that, I'll pass on to Demetri. Awesome. Thanks, Judy. And hello, everyone.
As they usually say, hey, they are vector podcasts is here. And today I have like a luminary guest, a mogul in search world, Daniel Tankele and beyond excited to be talking to him and discussing the, you know, favorite he's in mind topics in queer understanding and content understanding.
And traditionally, I will introduce myself for the first time on the podcast. And well, what I want to say is I have PhD in natural language processing. I work the machine translation back in the days. Currently in two roles, principal AI scientist with silo AI. It's a consulting gig.
And recently I entered the job as a senior product manager at the company called Tom Tom, which produces maps and map search and navigation. I have 16 years of experience in developing search engines for startups and multinational technology giants.
Most recently, I worked on multilingual web scale search. I also claim to be an expert in vector search engines. And I'm the host of vector podcast, which focuses on this tech, but also beyond that on search at large. I'm also blogging on medium.
And as I said, I'm beyond excited to be talking to Daniel today. And as a tradition, Daniel, could you please introduce yourself to me and our audience? Sure, do you make me? Thank you for that. I'm Daniel Tunkelang. And I've been working in search for, I guess, a little bit over two decades.
I started after completing my PhD, not anything to do with search information retrieval, but actually in network visualization.
I shortly ended up teaming up with a few folks to start a company called Indeka back in 1999 that ended up focusing on e-commerce search and to some degree enterprise search in general. Site search has there for 10 years of the chief scientist.
And then I went to Google where in fact, I worked on search in local search part of the maps search team as a tech lead moved ironically from the East Coast.
I've been living in New York to now to do to leave Google and join LinkedIn where I first ran the product data science team, but ended up coming back to my first bulb of search.
And it was at LinkedIn where I started a query understanding team and shifted my focus, which had really been more around faceted search and interaction to query understanding.
After leaving LinkedIn, I decided to go off on my own and for the past six or seven years, I think what I like to call a high-class consultant trying to bring the search to everybody who needs it, which turns out to be a lot of people.
And then last year, I discovered the wonderful folks at Co-Rise and started teaching these classes with my friend and colleague, Raddey herself. Fantastic. And I can add to that, the course being having been a student at your course. Fantastic course, I've learned a lot.
And yeah, I'm a happy owner of this certificate as well. So I can prove to future job employers that I have passed it. And actually, I watched one presentation you gave at the CIO Summit 10 years ago.
And one key phrase that I took away from it or suggestion, you said science is the difference between instinct and strategy. And I wanted to a little bit like ask you to talk to the role of science in every day search engine development and research.
Do you continue to view it that way 10 years forward? I do, it's funny because if you, anybody who watches that, that is probably the only recording of me wearing a suit on any sort of video is the science strategy talk.
And the when I was thinking at the time, you know, as a data scientist, a big part of my job was getting people to use the scientific method and there were whether that was a A B testing or having, you know, clear falseifiable hypotheses and so forth.
Now, it don't get me wrong, instincts matter a lot. For example, if you go to a search engine and you're not happy with what you're seeing, your instincts are probably right. There's probably something wrong.
But if you say, oh, I'm not seeing the results I like, I'm going to add a simple inventory. I'm going to turn up one of these knobs to see what I get. Then don't get me wrong, you'll sometimes get improvements, instincts are not useless.
But you won't have a way of being certain that you're getting improvements. And you may sometimes get improvements that happen to work in that particular moment at that particular time, but which you can't explain or sustain.
And so science is about using techniques like with other people might call randomize control trials, but we like to call A B tests.
 The science it poses a certain amount of discipline and it keeps you honest, which I do think is the difference between running on instincts that may or may not work and being able to pursue a strategy where you not only can see whether or how things work, you can measure this as well and repeat what you do.
So I still hold to that with regard to this. Yeah, this is fantastic. And I highly recommend also to watch that video, even though it was for high top executives, there is a lot of logical elements to it that you can apply in day-to-day work.
And yeah, I remember also one quote from the book called How Google Works that if we argue and we have data, let's look at that data. But if we go by opinions, let's go with mine. And it was written by Vice President of that area. So basically, he's a hippo or sort of top that letter.
So why not why not actually follow the hierarchy there? But yeah, I agree that if you have data, look at it if you don't try to collect it. Yeah. Yeah, I mean, indeed, I mean, data is what is the equalizer, but it's for those of us who are not CEOs, it's how we get things done. Yeah, absolutely.
By the way, I wanted to also say a couple of words on logistics. Please send your questions on the chat and we will handle them in the end of this session.
Yeah, and 10 years forward, I've read your message on LinkedIn where you said a little bit on sad tone, not everyone shares my passion for search. But I suspect that many would be more excited about search if they understood it better.
Was it just a moment of despair or was it a moment that you thought, okay, I need to approach it differently.
I can keep blogging about query understanding and content understanding, but how can I actually open the doors to the minds of new people, potentially students in this field? What was going through your mind when you wrote that? So I'm an off to this.
So I'm, you know, if two years of a pandemic and now the global crisis can get me out, I'm certainly not going to despair just because not enough people are excited about search.
 But I have seen that, you know, our technology industry tends to have certain kinds of fads and say, in fact, back in the 90s, everybody was excited about search for those of you all to have to remember, Google was not the first major search engine that we're using all of this, that we're using, gahoo and so forth.
And then after Google took on the scene, many people said, oh, search is done. Now, I happen to not be one of those people because I was at a startup, which actually also started in 1999 working on search. And they said, no, search isn't done at all.
I mean, we were trying to help e-commerce companies and we saw that there's a lot to do on search. Now you might think that 20 years later, search would finally be done.
But interestingly, there are still so many opportunities, in fact, using some of the latest developments in machine learning to do so. But what I've seen is that people don't necessarily gravitate to search as an exciting problem.
They're excited about voice recognition about what they perceive as AI in general, which they may see as question-answered, which at the heart of it has lots to do with search as well.
But they don't realize that, you know, that humble little search box in which they are typing and everything going on between it is still an extremely exciting area of development.
I think it's because it does look so simple that they don't imagine what can you do? Change the size of the search box, you know, change the font of what's actually going on between. You'll be behind that.
So my hope is that when people see the complexity involved and the way in which search is amenable to the very techniques that they are excited about, they'll then say, oh wow, this is great.
And then on the top of everything else, it's a place where I can have a huge about impact, a netable impact on the way that people interact in machines. So I know, no despair, just maybe sometimes a little bit of sadness that people don't share my excitement enough. Yeah, absolutely.
I mean, search is a fantastic field if you're not there, consider entering, or at least studying and evaluating, but it's very deep.
I remember actually myself like 20 years ago, still in the university, I was asking a friend of mine, how do the search engine works? And he was majoring in information retrieval back then. I knew nothing about the field. And he said, well, we we use inverted index.
That's how we represent the documents. But then I was still not satisfied. I asked him, hey, how can actually search engine know what I want to find when I don't know myself? Like, if I start typing something in the keyword box, it's like a chicken act problem.
It means that I know something already of the subject, right? But what if I know nothing about it? And so in my mind, I started hypothesizing that maybe we can build a search engine, which will kind of refine the query.
I didn't know how to do it, but I was just, you know, thinking in my mind that it's possible. And now so many years fast forward, we apply machine learning to search.
And what I wanted to ask you, why do you think we need machine learning in search today? Like, there are other ways to satisfy the user intent on sort of calculated, understand it. Then there are other things like established techniques with manual boosting and manual features that you can curate.
And many companies, I think, still do it. But like, what's your take on machine learning role in search today? So certainly when I was working on search back in 1999, I didn't give a no much machine learning.
I take a class that's highly theoretical, but I managed to help build search so clearly it's possible to do it without machine learning. And as you said, many people still are working with completely hand-to-systems.
I think that machine learning plays a few roles though in, I think what you could say is modernize in search, but what we're making it do things you really couldn't do before.
So for one thing, when you're doing all of these hand-to-tune boosts, right, which you're typically saying, oh, I'm going to have a bunch of factors that affect me. I'll change the weights on those. I'll see what can improve.
Effectively, you're performing an optimization problem, but you're doing it by hand, or you're saying, let me go a little bit in this direction, a little bit in that direction, let me see what it can do.
Well, the main technique that machine learning does is optimization only that it does so by formalizing the objective that you're optimizing for, and then using mathematical techniques, like variations of gradient descent, to look for the place that is optimal.
Well, it would be silly for you to do things by hand that there is an existing architecture to do that. But the other thing is that when you do things by hand, you're very unlikely to be able to move too many knobs by hand.
Three or four factors, you can handle a hundred factors, almost certainly not.
And that tends to be the big win of machine learning is that because of the win that it automates what you would otherwise do by hand, it allows you to do things at a much larger scale and yet keep your weights about you. And that's usually what people do when they're concerned about ranking.
But the other the other way to break through in machine learning today is that in areas like query and content understanding, machine learning often allows you to solve problems.
 You'd have been very unlikely to solve by hand to recognize when a query comes in that's in a particular category or that particular entities, people, brands and so forth are mentioned in that query or to figure out what a piece of content is about and get a representation that you can then use to inform what to be returned.
And that's an area where it's not new that you can use machine learning there, but the ability assistance to do so using the more modern AI techniques of word embedding to the like, have dramatically changed the quality of that.
And it's a breakthrough that I can only think of comparing to, you know, speech recognition has been around for a while. But if you use speech recognition systems in the 1980s or 1990s, you thought other very cute, but they'll never be useful.
And today we take for granted that they work well enough that people who had no other option could actually manage with them. And I would say that machine learning in search has now reached a point where it would be silly not to use it if you have the possibility of the data to do so.
Yeah, absolutely, especially if you sit on a pile of data, right? As they used to say in the age of big data. But of course, there are still niche areas where you, let's say you launch a startup, so you don't have clicks. Maybe you can measure clicks some way, but let's say you don't have clicks.
You don't have any user feedback yet. I think at that point, you could still apply machine learning, right? Like deep learning, hopefully we'll get there.
But before that, I think when people talk about ML in search context, they quite often mean, you know, machine learning based relevancy, you know, learning to rank, like you learn a function which will rank your documents.
But in a way, what can rank or find by itself, not much, if the data is not there, if it's not categorized. So what's your view on where machine learning can bring a lot of benefit upstream? And I think you touched on it like query understanding and content understanding.
Can you drill a little bit more into that, especially from the point of view, how you approach the task, where you start? Sure. Well, as you know, and anybody listening to this, is ready what I have to say. I like ranking, some of my best friends, but do ranking and even I do it occasionally.
But I think that ranking has been over emphasized in search in general and in machine learning, the power and search in particular.
So if we think of what ranking does, it says, but we have a search query, we have a potential result, and we score it with a function that will then determine the order which we present it, assuming that that result is as a candidate to be considered.
And if you go back to the original models of information retrieval, they act as if every document in your corpus could be scored. The only reason you don't do that is you can't forward to it's too expensive.
But that that's the, the gist of it, the scoring function on the query and document, then in some cases, even on the user. Now, that's a lot of input into a function. It's quite a different way that you might approach the problem is to say, I have a query.
I'm going to try to represent that query as useful as possible without looking at any documents first. Also, before I even see any queries, I have documents.
I'm going to try to represent them as well as possible before I see any queries, or at the very least before I see the particular query that someone's going to make. I might have something, I might use the history of queries to, you know, inform my approach.
So now we've factored out this original scoring problem that said, throw everything at a scoring function and said, no, no, no, we're first going to say, let's understand the query in a representation that distills it to its assets.
We have already understood the content, the documents, in a way that distills them to their assets. And now, when we even decide what to retrieve, we're going to use those representations that already have done some of the work for us. In the case of the documents, we did it offline.
In the case of the query, we have to wait till we see it unless it's a maybe ahead query we've seen before, but we do it once for the query, not once for every result.
And we can use that to then say, well, roughly speaking, if we have represented the query and the content in a similar space retrieval that is deciding what documents we should look at, is much more of a matching problem.
 In fact, if the space uses a similar schema, for example, if the query is mapped to a category or a set of categories, and the documents having categorized using the same set, we can say, well, we should probably retrieve documents from those categories, or we may have other structured data we can use that way.
And what is happening is that a lot of the work that ranking was doing, which was essentially trying to say, should I retrieve this document at all? Is this document relevant enough to the query that it should be in consideration?
This query-dependent aspect of ranking can be solved as saying, basically, once I have the query, and the content represented in the same space, do they know? Is that overlocked there?
 So we're basically changing the first order bits, the higher order bits of ranking into more of a classification problem, which we're experiencing is really, look, once we have the query and the content in the same space, figuring out if, you know, what is the content that matches the general gist of the query, should be an easier problem.
And then ranking is more, oh, well, a lot of things matched, but some are better than others. And that's, of course, the word, but she learning that, well, machine learning is how we get those representations.
It's how we turn the query into a more useful representation, how we turn the content into a more useful representation.
But it is, by treating those things in something of an acceleration, it allows us to be a lot more directed than we are with ranking, and in my experience, it's far better results. Yeah, I remember like the course, search with a meltot by you and grunting yourself.
You gave that brilliant example that stuck with me.
 I believe it was best by implementing, correct me if I'm wrong, implementing the query functionality, query understanding functionality where if you typed iPhone or some product that they didn't have at the moment, they would actually use query understanding to tell you that they don't have a product they would not even go in search.
And I mean, the example was this B&H photo with iPhones, but an example that's even more fun is with Netflix, where I don't have much inside as to the channels there that haven't been one of my clients. But the Netflix has perspective about limited catalog.
They don't, for example, get movies from folks like Disney that is quite protected of its catalog. But Netflix knows when you're looking for a Disney children's movie. And when you do that, rather than trying to simply match the text of your query, they show some of their children's movies.
So it's an example where you clearly split out the work of understanding, the searchers intent, and query understanding from simply retrieving and scoring results, because that improved representation.
They know you're looking for a children's movie, and they have children's movie is far more powerful than the traditional ways in which you might score a grand result. Yeah, I'm personally fascinated by the field of query understanding, having implemented it with my team in a web scale.
We worked on job search engine, as vertical search engine, no power by the web scale search engine. First of all, it was multilingual. Second is that you have to figure out this semantic subtleties when users type opening hours or working hours, whatever the way they phrase it in their language.
And that's not a query you want to execute in the job search. But if they set jobs in IT in London, that's okay. And you can use that and pass it through the filter. So query understanding kind of worked as a filter in a way.
But then it also, or a classify, you could say, right? But then it would also give us this reach semantics that we could apply in fields. Let's say if it's London as a city, you don't want to search that work just in the description. You can apply it in the field of the city on the document.
And I mean, this was like back then we applied rule-based approach.
And it worked fine, but it was very maybe conservative in a way, right? Especially for languages like Turkish, where they have the word ish, which is a, you know, overloaded, semantically overloaded word and used in different contexts. It may mean a bank card.
It may mean a job search and a number of other meanings.
But would you advocate for using machine learning and query understanding? I know, by the way, you wrote a brilliant series of blog posts on medium drilling into so many subtopics of query understanding, and especially like that you can actually utilize it in autocorrelate.
I was actually fascinated that you connected those two and I highly recommend everyone to take a look at that.
So what's your take on sort of rule-based versus machine learning? Would you start with rule-based? And then as you learn, go to machine learning, or would you start head-on with machine learning?
So I certainly see a lot of value of machine learning inquiry understanding for some of the reasons I was saying before.
But with that said, I think that there's often a sort of a burrito principle in 80, 20 in search problems. And when I go to people, especially folks in small organizations, I tell them, look, let's say for example you're trying to figure out would be, well, use your job of job search examples.
And so I spent a few years up like that and it's very close to my heart.
They knew on to know, well, somebody, for example, looking for a job title, or are they looking for, in say, lengthens case someone's name, or in the case of, say, language, maybe you're not sure what language they're searching in you're trying to do language identification.
You could start by looking at the most common queries that you see, and then just having people, your own employees, a hired crowd, what have you to say, look, can you just label these? I'm not going to label more than hundreds or maybe thousands of these queries that way.
At the hundreds of thousands it starts getting a bit silly. But you can do that and you can say, okay, maybe you have now handled 20, 30% of my traffic that way. It's not uncommon that in 10,000 queries you easily get to that. And you can see great.
Now that I've done that, now that I know, that this person is looking for a job title, that the language is Turkish, or what have you, what would I do with that? And I'm like, well, I'm going to have a particular search experience in mind.
If I know that it's going to be in jobs, I won't look in people, or I won't look in my content posts. If I know what language it is, I'm going to grab from that repository and so forth. And you can learn what you would do there. Now, this won't scale into the tail of your distribution.
But you can learn what happens with that sort of experience. And that's actually really important, because sometimes you don't know what people react to until you show it. There's a bit of a chicken in head in these things as to what is the quality of your data, but also what is the experience.
But once you've decided, okay, I am going to pursue this sort of experience. Frankly, without machine learning, you're never going to scale it.
You're not going to label everything in a rule-based approach to try to figure out what language something is in, or what category something is in simply isn't going to scale.
In the case, for example, language, it's not like you're going to just build dictionary, it's because you'll have cognates between the languages, or in the case of job titles.
By the time you get to Chief Vector Search Ninja, you're going to be in a bit of trouble as to recognize and bad as someone's title. So that's the point at which collecting training data becomes critical.
One of the nice things is if you've done some of the work by hands that can actually be how you bootstrap training data for these approaches, especially if you don't have our data position to do so using feedback from your own search application. Yeah, absolutely. A quick shout out to our audience.
I think it's, if I'm reading it right, just a second, Andre has a poll of how many people in this call are using hand-to-and-boost versus machine learning. I'm really interested to hear or read this opinion. Maybe you can say about it.
Yeah, and on the other hand, you've been advocating a lot on drilling into your content. And of course, some companies do this one way or another. But can you illuminate us on what you can do also on the content understanding side? Sure.
So if you think about it, if all you need was query understanding, you might be able to figure out exactly what the search or wants, but actually not be able to find it.
So content understanding is really what you're doing in order to represent content in your index and the best way to make it retrievable, it's horrible. So certainly, it's a great place to do things like categorization.
This is especially true to say if you have a marketplace or if you have a lot of unstructured content where you don't necessarily know what the content is about. It's also a good place to extract entities, terminology, even determined potentially the terminology that's used for representing it.
I mean, imagine it. For example, you have a collection of research papers. You can discover the useful words or phrases that tend to carry meaning.
You can relate them to one another by putting them in a vector space where the distance between the vectors that tells you how similar they are, you can cluster those.
So in general, doing things that involve either classification or essentially annotation recognizing entities or turns in those allows you to enrich the way you index the content.
You can also figure out when documents are similar to one another because when you have these vector representations, you can take the entire document or part of the document and do that.
And that can be useful for saying, oh, if you're interested in this document, you might be interested in these other ones or maybe you're interested in these other ones, but they're more recent.
And that allows you to combine what content is about with other factors like its recent seeds, popularity, other people that look at it.
And you see this often not just for search, but specifically for for being an engine for recommendations that are triggered from discovering something through an initial search. So all of these things basically make the content more retrievable, but also more exploreable. Yeah, absolutely.
I can also add to that in some settings, specifically in financial search, I'm happy to work at the company called AlphaSense.
You may end up in an institution when you cannot actually use the hints from the users, right? So for instance, like if you do a not a suggest and you extract themes from queries, you could actually do that. I believe Google does that to some extent.
But in financial setting, you cannot do this because banks will prohibit using their searches with their arrivals, right? You don't want to do that ever.
And so at that point, you do go deeper into content understanding and you start extracting stable themes and maybe over time you can also extract trends as they show up. And that might be one way to kind of combat the issue of not being able to use queries to influence your model.
But yeah, you might have another setting. I'm curious to hear in the audience as well, what kind of setting you guys have. And my next question would be on what I available data sets.
Let's say if I want to practice query understanding or content understanding at home in my lab, what are the available data sets, tools and algorithms that you can recommend that will allow us to train these models for both of these directions, query and content understanding?
So as those of you took the class, no, we've been using any commerce data set from Best Buy for a teaching.
It's a nice data set. It's a little bit old, but it has a virtue that it has a bunch of structured data queries and some click data as well. And that's proven useful. You can get that from Kaggle as they've made available freely.
And indeed, Kaggle, which is at this point, the subsidiary of Google, but Minkins independent brand is a great place to get data sets.
This one from Best Buy, I think is probably the best all around one for exploring the particularly query understanding, until that starts that content understanding. There are certainly other data sets available.
You can, for example, grab dumps of data from Wikipedia that are fascinating Wikipedia is perhaps the best overall data set in the world. But they're in mind that it's a bit sprawling and that they don't supply much in the way queries or feedback.
And you'll have to do a little bit of a work with that. There's a data set called MSMarco that's been very popular with essentially the deep learning crowd because it's an interesting place for doing question answering.
So I think a lot of the question becomes what is the problem that you want to work on?
And I would say for those of you who are already working in search and some capacity or at least have access to data, you should really consider trying to use your own data because usually the thing that is hardest to get in public data sets is user behavior.
For perfectly understandable reasons, companies are not eager to share what their users have done either because of the privacy constraints for their user or the competitive nature of that data.
So even if you're able to find catalog data, which you could, if it's structured, use to learn content understanding techniques. For query understanding, the most powerful thing you're going to use is a collection of queries and labels for what those queries mean.
But if you get a collection of data without even having what the queries are and let alone the labels, it's a little bit more difficult. Yeah, absolutely. And can you also share a bit on the tooling side or maybe algorithms? Sure.
So the, you know, for different problems, obviously call for different tools. On the ranking side, one of the most popular approaches that's still in use today is XG boost, which you can get online easily enough.
And it's also been integrated with, I think at this point, most of the major is certainly Lucine based, sort of solar, elastant, and so forth.
If you're interested in classifying text or doing unsupervised learning and untext, there, you know, these days, frankly, I would go directly to embedding based models.
And you can use tools like Burd or maybe the old school on the fan or fast text that you can get online and you can download those, you can install them on your laptop, you can even get pre-trained models for hundreds of languages that do so. And from that, it should be easy enough.
You can just walk through the tutorials where you take just a bunch of labeled text examples in the case of past text, it's an example of stack exchange cooking questions associated with it with their labels. And in half an hour, you find that you're actually doing content classification from this.
And in the course that we do this sort of thing with the best by data as well, it's amazingly easy to see that these sort of tools will start to give you reasonable looking answers.
 Now, getting for reasonable answers to answers that you're happy with and incorporating into a search experience can be the difference between an hour and a week or a month or but my hope is that by seeing how easy it is to get started with these, you get tempted enough that you say, great, but 80% isn't good enough.
I need to get myself to something I'd be willing to put in front of my customers. And to be fair, there's a little hard, more hard work to make that happen. Yeah, absolutely. Vector's search, by the way, is my favorite topic. I talk a lot about it. And the look as well.
And I'm super, super happy that you mentioned this now. And the gateway here to this topic is the connection with content understanding is one of the techniques called doc to query, essentially computes possible queries and then augments your document.
So you don't actually need to step in the unknown field of vector search trying to re-engineer your search engine. You can actually keep your search engine architecture as it is.
And you just basically augment your documents in the hope of increasing coverage and actually precision at the same time of future queries.
So what's your take on this on these techniques, emerging techniques, but also what's your take on on the role of vector search in general in the search engine design today? Sure. So if you think about it, the Dr.
Query approach is similar in spirit to saying, well, I'm going to just, I'll have a known set of fields that I would assign to the document in traditional inverted index or posting list. And indeed, the limitation of the older approaches is that they get a kind of force you to a limited vocabulary.
And now the query vocabulary is literally the language of users. So I think that's a great way to do things. And to handle also that documents have often a lot more variability than queries.
This is typically the only some of my people are going to do in a search box, but documents can be in all shapes and sizes.
So I'm certainly a fan of doing document enrichment that's query friendly or conversely, if you're going to do things on the query side, to think of a query is actually as a bag of documents.
I think even though there have been these explicit, what we call two tower approaches that try to sort of meet halfway, I think it's perfectly fine to say, well, we'll think of one of these things as more fundamental and not the second one, too. I think first off, I think it's great.
The idea that you can think of meaning as it comes, point at an eye-dimensional space, it explores things around it, even though in a way it's not a new idea, right?
People have been using vectors at least as far back as techniques like TFIDF where the bag of words representation of content was simply a vector in the space where every word was intervention.
I'm glad you've gotten a little bit smarter about that over the past few decades. And certainly what we can do now with embeddings is amazing. With that said, I think that sometimes people use vectors as too much of a sledgehammer.
For example, if I do a query on a site for cat, turning cats into a vector, and then turning all the documents into vectors, and then sorting them across my code sign, probably is overkill when how much information am I going to get out of a single token cat?
Figuring out whether something is a cat, as Google showed, may require a huge amount of machine learning, for example, with based on taking images.
But it's probably safe to say that at least at the query level, there's only so much new ones are going to get out of a one word query corresponding to an entity.
And if you a traditional approach where you might cure a vector based approach would say, well, I'm going to take the vector for my query cat. I'm going to take all of the vectors for my documents, which I vary the reason of cat and is implied.
I suppose in those vectors and sort by their distance, it probably makes sense to start by saying, maybe I could have actually, you know, either using the doctor query or more traditional methods annotated the documents in such a way that for the first pass, I could get the things here.
In the case of queries, and that simply, as only I've spoken in it, there may not be much I can do at that point in terms of use of vectors. Now, as the queries get longer, have more signal in them. This game changes completely. If I'm saying, I'm looking for a cat wearing a red bow tie.
Well, with a query like that, it's very unlikely that a traditional approach is going to be able to say, well, what do I do? I'm going to look for those words. Some of them, some other ones, you know, is a neck tie.
The same as a bow tie, you know, would a cat in a texito be better than just your typical cat pictures. And so, at that point, the game has changed because it's not a symbol binary question anymore. And the identity that is reduced to similarity make a huge difference.
And there, I think you lean much more heavily into things. Now, I'd say that it's still the case that doing it pure, you know, grab everything as a sort of a nearest neighbor's search in a vector space can be computationally challenging. And it can lead to you, you sort of unpredictable results.
So, most people today are still doing their first pass at least by using traditional methods. But I do know folks who are increasingly trying to use vectors from the get go, but just by using sort of course, or grade vectors or various techniques to make that first pass be quick enough.
So, I think we're going in that direction. I think that there's still a lot of value both computational efficiency and for end explainability in using, you know, traditional inverted indexing techniques where you can, especially for the early stages of retrieval.
But for either for getting these nuances or for say increasing your recall, but, you know, retrieving things that might might have lost otherwise. We're seeing increasingly the use of a vector search to get them. And, you know, we're doing this talk now in 2022.
I suspect in a few years the inverted, inverted index methods will become more and more confined to those cases where where the data is really just simple binary. I think I've always used this.
I think this kind of, there'll always be the certain head cases of it, but the use of vectors is only going to expand. Yeah, absolutely.
Oh, like, especially where I would say, inverted index will still be needed if you are looking for an exact phrase, like you don't want to say, hey, vectorize this and find the similar. No, I don't want similar. I want that exact thing.
And of course, there are other things that need to be improved in vector search, like, I don't know, bird model, according to one study. It doesn't recognize negations and it might be actually crucial for some search scenarios, or sentiment analysis.
And also another thing is that by now, at this point, the sparse search BM25 based methods is a very strong baseline when you compare these methods across datasets, across tasks. And so across domains. So I think the future is very bright on this direction, in this direction.
And I think a lot of folks are trying to combine this method. So I'm happy that you are looking at this as well. And I believe you will be teaching about this as well. We are quite close to the top of the hour. And I'm happy to see so many queries coming in.
But I'm going to ask you my favorite question, the question of why it's this kind of mystical, philosophical question. You are the most celebrated search engine professional today, one of the most, if not the most.
You've done everything there is to do in search, in my opinion, like when I look at your CV, you know, even you consulted Zoom through which we're doing this session. So that speaks volumes. And I just wanted to ask you what drives you to continue focusing on search engines.
And especially teaching about it. So search, if all of the problems that, you know, of all the things we do with technology, I believe is the one that puts us as human beings front and center.
So much of what you see, and specifically machine learning AI is being done to us feeds recommendations, advertisements, and search starts with people expressing what they want.
And I know, in my version of the future, I'm not a custodian, but I believe that the machines will help us, but they have to start with us expressing our intent. So that's an ace of my search is so exciting. And as for why I teach it, well, it comes back to what you asked in the beginning.
Now, I'm a despairing that there's nothing exciting about search. Not despairing, but I do think that the need for people to be building great, great search is not net by the supply of people who have learned about it.
And as much as I enjoy personally working as a consultant for companies, that's not exactly a scalable approach.
 So what I see is there's so many people out there who know enough that with a little bit of a push, some combination of the sort of the basics domain knowledge that we're teaching in our fundamentals class, but also the kinds of techniques and feedback are somewhat opinionated way of showing those techniques in the search with machine learning class that focuses on queer understanding, non-conset understanding.
That takes dense retrieval, vector retrieval, puts it in context, is just the nudge they need to get over this. You don't need to spend years and not everybody's going to get to do a PhD in information retrieval and machine translation.
But I think that today, if you are a software engineer, if you have a basic knowledge of coding, and you learn a few of these things, you can do wonders with the tool that's out there. And then from experience, you'll develop the rest of the sorts of skills that you need.
So I'm excited that I can be a part of enabling the next generation to just run circles around anything I ever got to do.
Now look back at the work we did in the early 2000s and it looks so naive, although I think we were working at least on the right problems, but without the machinery we have today.
And I just think, you know, in in in another 20 years, I look forward to looking back on the naivety of what we thought search was, you know, back at this point. Yeah, I'm ecstatic. This is very deep Daniels. Thanks so much.
And please keep doing what you're doing because I really, really enjoy reading your blogs. I need to also read your book, by the way, on FASITED search.
And before we move to the questions from the audience, is there any announcement you would like to make to our audience? Well, you know, for those who don't know, we're going to be teaching these two classes in June.
There's a search fundamentals class, which is a two-week class intended for people with no background in search. And this search with machine learning class that will start two weeks later. So people can take both a fact, focuses on search with machine learning.
So we're going to show you a query understanding, constant understanding, and vector retrieval. That will start the first course. We'll start at June 6th, the second on June 20th. And these classes are available to anyone in the world.
We make sure, granted eye, that we cover the time zones well and are available asynchronously. So I hope that some of you have already signed up. Some of you have taken the class before.
But what we experienced when we talked this class before was the incredible community, which did make sure it's indeed a part of. And we're excited to keep going. Absolutely. And I highly recommend you to take this course, or both of these are one of these.
And first hand experience, it was breathless run. And I was like, yeah, it's every single week. I need to hand in a project. And it's not just theory. And theory, by the way, is very deep. If you have time, go and read all the write-ups that Daniel and Grant have done on the course.
But also the actual act of coding, the actual thing that you see how it evolves in your hands. It's amazing. So awesome. Let's proceed to the questions from the audience. I will take the first question from the Q&A panel. We also have in the chat. So the first question is from Hemann Schu.
I'm a polygis of five meets pronounce your name. Hi, Daniel. Any specific book to get started with search with elements of machine learning? Thanks. Yeah, so I mean, the, I'll say this, there's a lot that's been written on, on learning to rank. I think Chris Mannings book discusses it.
I recorded by a Jesus book, might, I'll have been a little bit older. What I think you're going to be less likely to find to though, is that's on the query and content understanding. There is a book.
It's really a moral collection of survey essays on query understanding for search that was published. I forget if it was last year or the year before, a little bit expensive, but if you look at that out there, if you're more interested in things for free, my blog at queryunderstandy.
com is available. At least we'll give you a survey of the of the techniques there. But to be clear, it's very focused on query understanding as such. I'm writing a series of content understanding that unimaginatively content understanding.com that starts doing the same thing there.
But from the perspective of books, I would say that probably Chris Mannings information retrieval book would be a good place to start an information retrieval in general. And the query understanding collection of essays is frankly the best publisher resource you're going to get for that. Awesome.
Hope that answers your question. I'm sure the next one I'm going to take from the chat. It's from Chris. What are your recommendations for integrating information retrieval, retrieving documents with question answering, returning answers within a context? Yeah.
So question answering is really exciting, right? And that the this idea that you can get information instead of just the document.
 The if you think about the the way we've gotten there, a lot of it starts front of a mental passage retrieval where or even before that search snippets essentially, if you think about the way Google looked five to ten years ago, you would see that sometimes as you looked at your search results page, your answer was in the few words that were highlighted for the for the result.
But now it's more likely that you'll see that sentence extracted and put near the top. And I would say that a lot of question answering today feels a lot like passage retrieval. That is find that sentence.
 Although I would say that while before that tended to be retrieving a passage that contained essentially the exact words you'd use, maybe a little bit of variation for stemming or synonyms, nowadays it's more likely using a vector-based approach to be a sentence or passage that is similar in the vector space.
That's it for exciting. However, what people really want is that even though there is no sentence in the content that exactly answers your question, somehow the search engine will be able to not to be a search engine.
Answer engine and say, oh, I'm able to synthesize content from different places, understand your question and learn that. We are not there. I mean, of course, you can ask what's each the i, i, i, i plus one, and it will say zero, but that's cheating, right? That's really just doing this.
You can play with wall from alpha that is a more sophisticated version of trying to essentially parse what you ask into a question that it can then execute in a language. But I think doing that on general information, we are far away from it's exciting, but that's that will require a generation.
Absolutely. I agree on that. The next question from Q&A panel from Donnie. What roles do curated control for capillaries, terminally name, third-east exonomy, and so on, playing in practical approaches to query and content understanding in your experience? So the huge.
And I'm glad that you asked this on me. Basically, collecting these sorts of curated capillaries can be a great way to label the content and have targets for doing a machine learning labeling. So for example, you know what colors things come in, then those people tend to look for colors.
They're great. You can actually say great that I'm going to do to look this way. Or if I know what subjects people are interested in, those will be the subjects that they will content with and where it targets things for queries.
 So in a way, having these vocabularies changes what would otherwise be an unsturableized problem, saying, well, I'm hoping that I can get some representation of what this is about, what the query is about, and match them, which if I don't have vocabularies, I'll be somewhat unattainingated in how I do this.
Having control vocabularies can say, oh, those are the ways in which I will try to represent things.
Even if I'm using vector-based methods to get there, they give me some alignment on the space and having multiple such vocabularies might say, well, sometimes I might be interested in one aspect of this content that's sometimes in another, right?
So I might be interested in the color and I'd be in the prototype and I'd be in the material.
So I would say that having these vocabularies can make a big difference. And since you bring up the SORA, it's going to be helpful when there's a vocabulary gap between the way people ask things and the way things are represented to use a SORAist for query expansion.
You have to be careful because the SORA tends to wreak havoc with the context in which those words occur, but still they can be great for generating candidates for retrieving more results. Awesome.
I think we still have time even at the top of the hour for the last question from an anonymous attendee. So Daniel, you talked about query classification for the retrieval side of things, but that can be a slippery slope. If content isn't 100% correctly categorized, and often it's not.
Therefore, our recall would be negatively impacted by using query understanding as a hard filter. Any input on that? Absolutely. I mean, I was burned by this myself when I was helping a client with trying to target promoted search results.
And I said, oh, we should only put them in the right category, indeed, because there were some categorization errors. This had such a negative impact that I stormed into the room saying, close the test, we're losing money.
And it felt very embarrassed because it turned out that, indeed, this is a problem, the content wasn't classified well. Of course, the first thing I would say is invest on the content side because if you're able to classify queries, you probably can also invest in classified with the content.
And by the way, even if the content has been say, categorized in a way you're not allowed to override. Right. Maybe you're a marketplace or you're the content that you know, you don't own the categorization.
For example, on LinkedIn, if I decide to say I'm an attorney on LinkedIn, LinkedIn's not what you just automatically change, no, you're actually not your cat. But it can still classify me and say, you know, you kind of look like a software engineer.
And you can use inferred categories in your retrieval. There's no prohibition there. I'd also say that, you know, maybe the content isn't 100% categorized because there are some similar categories.
Well, you can always take query classification, you know, it's not not a hard and fast rule, but more like guidelines and say, well, return things in that category. But I'll also return things to say we're referring to that category.
And maybe you'll even return things that are in similar categories. Oh, in my way, if I'm not 100% sure on that category, even on the query side, maybe I'll take the top few categories that I thought were possible.
There are a lot of ways in which you can use what you've seen, what you've learned about the query and what you've learned about the content that's hints. And with all these things, it's a precision recall trade-off.
And you have to generally decide what's the cost of losing that recall versus what's the cost of having the annoyance of for precision. And it will depend. Absolutely. It's a journey. And I've enjoyed this conversation so much. I've learned new things. I will rewatch this podcast myself.
And thanks to everyone for asking your questions and thanks, Daniel, for for answering them. Brilliant as you usually do. Hopefully we covered all the questions from the chat and from the Q&A panel. And yeah, thank you so much.
And I think where you don't feel comfortable or you don't know yet, I highly recommend you to take the course or a course on search. And go from there, experiment. Be bold about what you do in your experiments. But just apply science and apply the knowledge from the moguls like Daniel and Grant.
So thank you very much, Daniel, for your time and for your wisdom today. Thank you, Jimic, you're so pleasure. Are ready. Thank you, everyone. Thanks.