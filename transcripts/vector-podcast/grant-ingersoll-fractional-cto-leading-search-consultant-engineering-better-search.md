---
description: '<p>Vector Podcast Live</p><p>Topics:</p><p>00:00 Kick-off introducing
  co:rise study platform</p><p>03:03 Grant’s background</p><p>04:58 Principle of 3
  C’s in the life of a CTO: Code, Conferences and Customers</p><p>07:16 Principle
  of 3 C’s in the Search Engine development: Content, Collaboration and Context</p><p>11:51
  Balance between manual tuning in pursuit to learn and Machine Learning</p><p>15:42
  How to nurture intuition in building search engine algorithms</p><p>18:51 How to
  change the approach of organizations to true experimentation</p><p>23:17 Where should
  one start in approaching the data (like click logs) for developing a search engine</p><p>29:36
  How to measure the success of your search engine </p><p>33:50 The role of manual
  query rating to improve search result relevancy</p><p>36:56 What are the available
  datasets, tools and algorithms, that allow us to build a search engine?</p><p>41:56
  Vector search and its role in broad search engine development and how the profession
  is shaping up</p><p>49:01 The magical question of WHY: what motivates Grant to stay
  in the space</p><p>52:09 Announcement from Grant: course discount code DGSEARCH10</p><p>54:55
  Questions from the audience</p><p>Show notes:</p><p>- Grant’s interview at Berlin
  Buzzwords 2016: <a href="https://www.youtube.com/watch?v=Y13gZM5EGdc">https://www.youtube.com/watch?v=Y13gZM5EGdc</a></p><p>-
  “BM25 is so Yesterday: Modern Techniques for Better Search”: <a href="https://www.youtube.com/watch?v=CRZfc9lj7Po">https://www.youtube.com/watch?v=CRZfc9lj7Po</a></p><p>-
  “Taming text” - book co-authored by Grant: <a href="https://www.manning.com/books/taming-text">https://www.manning.com/books/taming-text</a></p><p>-
  Search Fundamentals course - <a href="https://corise.com/course/search-fundamentals">https://corise.com/course/search-fundamentals</a></p><p>-
  Search with ML course - <a href="https://corise.com/course/search-with-machine-learning">https://corise.com/course/search-with-machine-learning</a></p><p>-
  Click Models for Web Search: <a href="https://github.com/markovi/PyClick">https://github.com/markovi/PyClick</a></p><p>-
  Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing, book
  by Ron Kohavi et al: <a href="https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical-ebook/dp/B0845Y3DJV">https://www.amazon.com/Trustworthy-Online-Controlled-Experiments-Practical-ebook/dp/B0845Y3DJV</a></p><p>-
  Quepid, open source tool and free service for query rating and relevancy tuning:
  <a href="https://quepid.com/">https://quepid.com/</a></p><p>- Grant’s talk in 2013
  where he discussed the need of a vector field in Lucene and Solr: <a href="https://www.youtube.com/watch?v=dCCqauwMWFE">https://www.youtube.com/watch?v=dCCqauwMWFE</a></p><p>-
  CLIP model for multimodal search: <a href="https://openai.com/blog/clip/">https://openai.com/blog/clip/</a></p><p>-
  Demo of multimodal search with CLIP: <a href="https://blog.muves.io/multilingual-and-multimodal-vector-search-with-hardware-acceleration-2091a825de78">https://blog.muves.io/multilingual-and-multimodal-vector-search-with-hardware-acceleration-2091a825de78</a></p><p>-
  Learning to Boost: <a href="https://www.youtube.com/watch?v=af1dyamySCs">https://www.youtube.com/watch?v=af1dyamySCs</a></p><p>-
  Dmitry’s Medium List on Vector Search: <a href="https://medium.com/@dmitry-kan/list/vector-search-e9b564d14274">https://medium.com/@dmitry-kan/list/vector-search-e9b564d14274</a></p>'
image_url: https://media.rss.com/vector-podcast/20220609_020607_0461c4544521e6be53134d28774b7c4a.jpg
pub_date: Thu, 09 Jun 2022 14:51:07 GMT
title: Grant Ingersoll - Fractional CTO, Leading Search Consultant - Engineering Better
  Search
url: https://rss.com/podcasts/vector-podcast/514832
---

Hello there, vector podcast is here. I'm Dmitry Khan and I'll be hosting this session. And just a few words on the logistics. Everyone in the audience feel free to submit your questions either through a Q&A panel or directly in the chat and we will try to handle as many questions as we can.
I'll save you words about core eyes. What's core eyes is a new education platform that transforms the way professionals build technical high demand skills through top industry instructors and collective peer learning.
And the format of their courses is innovative mixing live instructor sessions with real world projects and fireside chats like this one technique with operators who experts in their fields.
I will say a few words about myself as well, untraditionally on the podcast, but I think it becomes a tradition now second time. I said and Dmitry Khan I have a PhD in natural language processing. I've worked at company Alphasense helped to build the search stack.
I spent like a decade, you know, there. I've been a principal AI scientist at silo AI. It's a AI consulting gig focusing on a number of ML verticals. And recently I joined company Tom Tom as a senior product manager working on search. I've also been a contributor and user of Quepid.
It's a query rating tool go check it out. It's an open source tool. So overall I spent like 16 years in developing search engines for startups and multinational technology giants. I also happen to be hosting this podcast vector podcast go check it out. I'll share the link in a second.
And I'm also blogging on medium on on my findings in vector search. So you might hear me talking about vector search here and there. And today I'm super super excited to have Grant in your soul with me. I've known Grant since about 2011.
Not personally, but I've seen I've seen him on stage on you know Berlin buzzwords conference and Lucinda revolution. And he has been a long contributor and open source as well. Solary, Lucinda Mahoot and others. And very very effective presenter.
I just watched a few presentations as a homework for this session. There will be some questions from there. But hey Grant, let's start with an introduction from Indio own words.
Hey Dmitri and thank you so much for having me on the vector podcast and obviously props to co rise here as well for helping sponsor this. Both Daniel Tunkelang and I are on the co rise platform and really enjoying our time there. So real quick about myself. As you said, my name is Grant Ingersoll.
I guess these days a long standing user and contributor and committer. And generally somebody who participates in the search space, if you will, I think I wrote my first Lucine code back in 2004 or so. I guess that maybe makes me old.
As far as my background is I was one of the co founders of Lucidworks, which is one of the leading companies in the search space. I then left them in 2019 to become the chief technology officer at the Wikimedia Foundation.
You probably know them better as the nonprofit behind Wikipedia and Wikidata. So I was the CTO there for two years. And then in August or so of 2021, I took some time off. And then in January of 2022, I went on my own as a consultant and an instructor for co rise. So here we are now.
I am commonly doing work in what I would call fractional CTO land, which means I primarily help companies kind of get their technology stack in order, make decisions about technology, higher teams, upgrade teams, do all the things that a CTO would do. Often for small businesses and or startups.
And so that's really my background. Really happy to be here and looking forward to the podcast. Awesome. Great to have you, really grand. And also, you know, finally, I have a chance to ask some questions and chat to you in this cozy atmosphere as well. And I wanted to start with a question.
So I was watching a kind of short interview you gave. During Berlin buzzwords 2016, where you said how you split your time as then CTO of I believe, Lucid works. You said that you split your time between three C's, which is writing code, going to conferences and talking to customers.
Now that you're independent, is this how you spend your time or did you did you get some new letters of the alphabet? Yeah.
Yeah, and there's often in there as well, colleagues and co-workers, you know, especially in, you know, the CTO role is kind of a funny one, right? Depending on the company, it can mean a lot of different things. At some companies, CTOs are entirely outward facing.
It's effectively a sales role or a marketing role, right? You're out evangelizing the product, you're talking to customers, etc. In a startup, the CTO is often the primary engineer.
If you're a two person startup and you're just getting off the ground, you probably have the CTO title if you're the technical one in that startup, and you're probably writing all the code, right?
In other places, you're running your engineering team, and you may not be writing as much code, but you're responsible for the team.
I guess over my years, I've worn all of those hats. I've been out doing conferences and evangelizing. I've done a lot of sales work, especially later on at Lucidworks, I did a lot of sales work as the company evolved and grew.
When I was at Wikimedia, it was all pretty much internal running the technology team, making, you know, helping making technology decisions, all of those kinds of things. So I wouldn't necessarily say it's changed much.
I still do write some code, but not as much as as I used to, I guess, when I was a full-time engineer. But yeah, it still roughly falls into those categories.
Yeah, and I mean, like having been a student on your course, I've really enjoyed so much code that you've written to support this infrastructure of building the search engine. And I mean, you are still highly technical person, so I wouldn't discount that.
And I mean, this is something that is dear to my heart as well for me being an engineer, to talk to like-minded person. And in this segment year, in the same conference, 2017, you gave an excellent talk title, BM25, is so yesterday, modern techniques for better search.
And what's funny, and I'm going to share the link as well. But what's funny is that I don't know if you noticed it yourself, but you again have three C's in there. I wonder if you did it on purpose. What you have there as building blocks of this kind of journey of building a search engine.
So the first one is content. And you piggyback on solar capabilities, but in general, it could be any search engine out there with rules for content, like with boosting, manual boosting, you know, lending pages, and so on. The second C is collaboration.
So that's like the way you put it, it's collective intelligence to predict user behavior based on like historical aggregated data. And this is where I think recommenders come in, popularity, signals, and so on.
And last but not least, you have context, which is when you ask questions, who are you, where are you, you know, what I have you done previously.
And this is when you start doing market and user segmentation and venture into personalization and so on, would you say that you view the search engine journey and development the same way today, or have you have you changed your perspective? I really need to check and get a little more creative.
I think I'm using the letter C there too many times in a row, but I mean, I think a lot of that still stands pretty true.
Regardless of the engine you're using or whether you're using deep learning techniques or not, like, you know, at the end of the day, you're trying to match users to information that will help them make better decisions or be more informed, right.
And you know, these days, I would probably add in one more. I'm trying to think of how I could be witty and make it into another C, but you know, in working with Daniel Tongueleg on this class, one of the things that is just absolutely wowed me is the query understanding aspects of it.
And so maybe you could put that into the context category if you wanted. But, you know, realistically speaking that that work you can do, especially in large scale environments where you have a lot of queries, to really understand what users are asking or intending to ask when they put in a query.
So I would probably throw that in there if I didn't include that back then. And so like I said, maybe that's part of your content or your or your context is the actual query, a user or this, the set of queries that a user is asking.
You know, but I still think a lot of that stands at the conceptual level, right, is you have to have some, you know, if you think about it, this is the vector podcast, right.
All of this stuff we're building vectors and then essentially calculating this fancy version of a cosine similarity between them.
And at the end of the day, all of these techniques we're doing are effectively how can we shape those vectors so that things that are meant to be closer together, show up closer together and things that are not as related the cosine is further apart, right.
Like at the end of the day, like that math doesn't change, yet all these techniques, whether it's deep learning, et cetera, are all about creating those vectors and doing that calculation, right.
And so by understanding your content, you're shifting those vectors, you're transforming them in the space, you're adding synonyms, you're adding embeddings, all of those kinds of things, you're adding proper nouns, you're you're doing noun phrases, et cetera.
By understanding your context, you're able to ask better queries, right, which is shifting the query vector, right. And by using popularity, et cetera, you're also then shifting those vectors by essentially adding more weight to things that are more popular, right.
You know, so at the end of the day, like, yeah, I would say I'd still stand by that with the caveat is really bringing forward the query understanding aspect of it.
Yeah, I think query understanding, you put it brilliantly, it's like really an exciting space and we actually recorded a podcast as well with Daniel Tankilank, where he explained a lot of it, he also blogged a lot about it. So go check it out.
And like in that same presentation, like when you demoed the capabilities of Lucidworks platform, where you played a lot with different like ranking strategies, basically like you pre-trained some of them and you you were able to switch live, I felt like you you are a tinkerer as well.
You enjoy really going deep down into the what search engine can do and what you you can extract from the data.
And my question is, where do you see the balance between kind of like doing this in a more manual fashion, where you actually educate yourself, right, versus like throwing it to a machine learning model? Yeah, it's a great question.
 I mean, I think you know, obviously, and I see my former colleague and co-founder Eric Hatcher is on, I mean, he used to always say it depends and I'd say it depends here of course as well, which is, you know, I mean, there's there's some situations where you just you don't have enough data for machine learning, right?
So by default, you are going to be manually tuning the situation, right? You you see that a lot in enterprise systems, especially smaller enterprise systems or in niche applications where, you know, effectively search just needs to be good enough.
Maybe you're not monetizing search. And so you don't, you know, you just kind of need it to be reasonably good, right? It's a feature in a much broader set of features that users are going to engage with.
And so, you know, where and how you would use machine learning in those situations, you know, you may or may not.
 In the situations where you have lots and lots of data, lots of users, you're probably monetizing search, whether that's via e-commerce or or web search or ads or whatever, like, you know, I think machine learning makes a lot more sense there and and it's a lot easier to run these types of experiments that allow you to tinker not just with the hand-ranked models, which I think hand-ranking still has its place, right?
Because they help you form intuition about what is in your data, right?
And that intuition is really important even in a machine learning world because, you know, at the end of the day, even with machine learning, while you can still try out, you can try out a lot more features and approaches, you still have limited time, right?
 And so, you still have to have some intuition about what's going to work and I think there's no substitute for that intuition helping guide you into what matters, like, so for instance, in a learning to rank scenario where you're actually learning a ranking model, you still are often building up those systems using the features of your data.
 So you have to know what those features are and one of the nice things is like with tools like Lucine-based engines like OpenSearch or Solar or Elastic, I'm sure Vespa has the same kind of thing, you can go and play around with those, you can create your own function queries that allow you to roughly try out different formulas for ranking and then you can go and turn those things into machine learning models, right?
That learn a much more effective function than what you could come up with, right? So, I think even in this world of large data sets and machine learning, you're still going to have to build intuition, right? Yeah, absolutely.
And like in your own experience and in the experience of the teams that you supported, how do you nurture this intuition? Like, do you read books? Do you constantly experiment?
 And also like when it comes in, you know, to understanding fundamentals of search, let's say knowing how TFIDF formula composed or you have 25, what are the trade-offs versus sort of like going and actually experimenting and trying out things, you know, where do you see that balance as well for yourself maybe and also for the teams around you?
Yeah, I mean, I think everybody will have their own, you know, kind of depending on where you come from, right? Like if, you know, like if you have, if you've done deep academic work, you're probably going to have a lot more understanding of the math and the theoretical side of it.
And then you're going to have to develop the intuition of real world data, right? How messy it is, how clunky it is, how full of junk and spam, et cetera, right? Because a lot of times when you're dealing with academic data sets, they're pretty clean, right?
Relatively speaking, they still of course have their own set of garbage and nuances in them.
Whereas if you're an engineer and you're coming at it from like, hey, you know, often what I see with engineers is they come at it from a quantitative standpoint of like, I want to make sure this is scalable and reliable. So they're solving for the hardening of the system problem first.
And then they often will develop the the relevant side of it or the the understanding of the data second. Now again, broad generalizations there because, you know, folks have all kinds of different backgrounds.
But you know, so like as a leader in somebody who does, you know, manages people in this space, like I would often just work with you depending on what your background and understanding and intuition is.
And then, you know, try to help you complement whatever it is you're missing there, right? Like I think you have to have an understanding of how these engines work.
I've often seen folks who don't have an understanding of all the capabilities of these modern search engines recreate the wheel, right? Like they're reinventing the wheel because they they're coming from this first principles of the math that they learn at the academic level.
And then, but they don't necessarily know how that applies to real data in the real world. Whereas a lot of these, you know, modern search engines, because they are, they grew up in large scale, you know, publicly traded high volume spaces.
They've really been hardened on the engineering side and they really know how to deal with all the nuances of real world data, right?
And so, you by learning those kinds of things, you will be much more effective at the at bringing to bear your intuitions and understandings from whichever background that is.
I don't know if that makes sense or not. Yeah, no, absolutely. Yeah, actually in the same presentation, you also said like, you've seen cases where you you come into helper company and they they point you to sort of like a data, the ace almost of 10,000 rules.
And so you you said they have that in principle, you could just remove solar or whatever search engine you have and just use those rules to retrieve documents, right? But when you go and ask specific questions, what what what this rule does?
The answer that you you illustrated was well, it was created by Joy, you know, and he quit five years ago.
So he then said it makes sense. So we keep it. So how do you go about convincing the organization or teams to change their perception and sort of like become more flexible and move into this flywheel of experiments? Yeah, it's hard.
And again, I think, you know, I mean, you have to look at incentives and first principles there, right? Like, again, if you're in this boat of like searches, just a feature, there may or may not be any incentive.
But if you're in this boat of like, hey, search is a really critical aspect of what we do. Our users use it all the time. It's key to revenue. It's key to timeliness or it's, you know, people's lives are on the line, et cetera. You're going to invest in making sure searches as capable as possible.
Those folks usually don't take much convincing once you can show them a better way, right? They're often already frustrated by the sheer number of rules that they have.
And so one of the things that can often work in those situations, I think is, you know, you can start to just learn the, you know, a lot of these machine learning systems will actually learn the set of rules, right?
And so if you want, you can just start to learn the rules by the fact that you're gathering your queries and your click logs and you're looking at the engagements users are having with the system, with the rules in place.
And then over time, you know, that will learn it.
That the harder part often is getting that last part, which is true experimentation whereby they actually have a system in place for running multi-variant experiments or AB tests, right?
And they can actually try out different approaches and see which one wins and see which one's most effective and then go with that from, you know, until the next one beats it, right? That's a fair amount of engineering work to get in place.
It's also a fair amount of math to do in order to make sure it's appropriate. These days, there are systems and tools that allow you to do it, but if you want to homegrown it, you know, that can take a lot of work.
So getting people to be in that mindset, especially in environments or company cultures where like there's pride in being right, you know, you sometimes see that in a lot of companies where it's like whoever's the boss has to be right kind of situation.
Those types of companies are always going to struggle with experiment mindsets because, you know, they reward, quote unquote, being right as opposed to, quote unquote, you know, rewarding longer term growth and incremental improvements with the occasional failures, right?
So you really have to look at company culture first and potentially reset that and then build and bake in the the necessary engineering work to make experiments work.
Yeah, absolutely. I agree to that same thought that, you know, without failures, you cannot really breed the culture of creating cool new stuff because you basically cannot unleash yourself to go and mess with your code base, right? And do things and create new stuff.
So like, you need to be brave for sure. Well, as I think the front of mind Ted Dunning said, the cool thing about experimentation frameworks is you get to be wrong and that's okay, right? Like you're actually right by the fact that you're wrong.
You're because you're right in the long run, right? Yes. Even if any given experiment is flat or bad, right? But overall, you know, in the long run, you're going to win out because you're going to just, it's easier and easier for you to add in a new approach. Yeah, absolutely.
I think that Turnbull also said, like, you know, how you basically accumulate this bruises, right? So you're like, Oscar tissue as some other people say. So I think without doing things, you can't without failing as well, you can't learn. So I totally agree to that.
But still for those who are still learning, you know, and we are discussing, to some extent, the courses that you couldn't be teaching, you know, where do you start? Like, let's say you have some data, right? You have some click logs within your organization or maybe you found some data set.
Where do you start? How do you go about dissecting that data set? What do you do with it as next steps and what to avoid maybe and what good things to know to keep in mind?
Yeah, I mean, I think, you know, first off, I mean, a lot of companies aren't even at all that great at actually collecting and managing their query logs, right?
So if you're, if you've got a search engine up and running and you want to improve it, I mean, I think the first thing you have to start to do is again, it kind of goes back to this first principles.
Like, if I'm not measuring things that help me understand what users are doing, and that's the first step, right? Like, make sure you're able to process your query logs and capture things like session history and what users clicked on, what they saw.
A lot of companies will only measure what was clicked on, but they actually don't measure what was seen by the user or at least inferred to be seen by the user.
And that can be a big loss because a lot of these machine learning systems, you need to know what wasn't chosen just as much as you need to know what was chosen, right? So really make sure you've got the instrumentation of your system in place.
And guess what? A search engine is a great place to store all of that data as well, right? As elastic as proven out with their using search for logs and spawn as well, right? And so make sure you're captioning all that stuff. And then again, I think this is where your intuition starts to come in.
So whenever I get a new data set, a new set of click logs, I start to look at, well, what are my most popular queries? What are users asking today? What are they asking overall? What led to zero results?
How often are they rewriting their queries like they typed in a query and then they didn't like the results.
So they rewrote it. You know, all of these things are pretty easily discoverable in query logs, right? So just start digging in and building some intuition for those things.
 So for instance, one of the things when I was back at Lucidworks that we would do is what we call like head tail analysis or long tail analysis is another thing you see in the literature, you know, especially in the e-commerce world where you have this power law distribution where most people ask the same things over and over, but you often have a really long tail.
When you analyze the long tail in a lot of e-commerce situations, what you often find, for instance, is the long tail is actually pretty highly correlated to the head queries, right? And so developing that intuition of like, you know, why are these long tail queries working or not working?
That can then help you do much better at all of your queries, right? And so, you know, from those click logs, then you start to focus on, well, how do I improve my head or my torso queries, like the ones that are most common?
And then as you go on, then you can look at how do I handle long tail queries depending on how important they are to you? You know, and from from that click log, then you can start to build either, you know, in some cases, you still might make sense for you to have rules.
And then, and then you can also look at, you know, like again, like I would try to look at it the problem holistically, what's going to get me the most bang for my buck in terms of where I should spend my time, right?
So in the short run, rules are probably easier, but they're harder to maintain in the long run.
And of course, you can only manage so many rules on your own and, you know, even with several people, whereas machine learning may take more work up front, but in the long run is probably easier to maintain.
Although I do still wonder, you know, if we're going to run into the same kind of problems we have with rules with machine learning models where we have so many different models that are being applied and they're built by different teams and they're applied in different scenarios.
And, and next thing you know, you have a complexity problem on that front as well.
 But, you know, luckily, like with things like machine learning operations becoming more of a focus and people getting much more rigorous about how they deploy and manage models, I think most of those problems will be mitigated in one run, but it still goes back to the same core principles, which you need to have good housekeeping in order to be successful both with rules and with machine learning models.
I don't know if that that was kind of long wind. I don't know if that answered the question or not. It does, it does.
I mean, it gives the intuition, especially where you said the connection between, you know, that that was an insight actually to me, like the connection between head and tail that 50% of tail may correlate with your head. And that's amazing.
Like 50% of this super hard queries could be kind of, you know, removed from that complexity space, right? Which is, again, you know, your mileage may vary, right? Like it depends on your data set in Europe, but you know, like in e-commerce, right?
If if I phone 13 or whatever is the head query, there's probably a tail query that's, you know, silver 64 gigabyte iPhone 13 with case, right? Like that's probably a tail query or at least a torso query.
And once you have those types of realizations, you can start to link these up. And then the cool thing really is that then the things you know about the head can apply to those types of tail queries as well.
And so you're actually, you might be able to more effectively manage those tail queries, even without machine learning models. Yeah, absolutely. And just a quick reminder to our respected audience, feel free to send your questions.
Otherwise, I will ask all the questions myself, which, which of course I have, but, you know, I'm sure you guys have guys and girls. I'm sure you have some interesting cases. We do get a few questions already, but we will we'll answer them in the end of this session.
And couple coupling, you know, that process of sort of, you know, crafting the signals and training your model and deploying it and ML ops that you mentioned.
How do you when it comes to measurement, how do you measure? How do you make sure that, you know, what happens right now in production still makes sense that they don't need to do any hectic action about, you know, okay, pulling the model back or something like that.
What's your sense on on on that front? And like, maybe some measurements that you have deployed yourself and have been observing every single day and relying on it. And again, it depends on your what, you know, kind of what domain you work in.
But, you know, I mean, there's there's lots of literature on how to score and and, you know, test your model.
 So things like precision and recall where you're looking at what users are clicking on and whether they're finding the results, things like zero results or often one of the things that I find helpful is like what what you would call surprising results where documents are occurring fairly high up in the results, but they're not actually garnering the clicks that you would expect given that position.
So for instance, you know, I mean, many people in search understand that there's a position bias that's just built into all of us as humans. We we trust the machine. And so we click on the first one.
 Well, if you if you consistently see that a document is appearing at say number one or number two in the results, but it's getting way less clicks than say the six or seventh document, that might be an indication to you that that document isn't particularly relevant or for whatever reasons users aren't liking it.
So those kinds of more subtle metrics can also be informative.
 I think, you know, if you have a AB experiment, testing framework in place, obviously you can do all of your metrics around AB testing, you know, start with just giving a certain amount of traffic to your new approach and then ramping up as it meets your metrics, whatever that is or what, you know, your targets are if that's things like add the cards, etc.
You can ramp up those those types of tests as you as it proves out. There's obviously there's things you can do offline as well, like especially if you have enough query logs.
And if your index hasn't changed that much, but maybe just the approach you're taking has, then you can you can replay your logs, you can test out and you know, effectively simulate what users might click on in those scenarios.
And then of course there's the old fashioned just, you know, things like smell tests like do these results look better to me as an expert, you obviously have to be careful there or to a small cohort of experts, you know, like maybe your colleagues, etc. might spend some time scoring.
So all of these things, I think are techniques and measurements you can use to check to see whether results are, you know, good enough for you them to go into production.
I think there's I think Ronnie, Ronnie, co-hoved me, I forget the name of the book, but he has a really good book along with a co-author on online experimentation. It's probably these days the Bible of online experimentation. So I would encourage users to check that out.
And then, you know, there's there's lots of metrics that you can deploy, you know, that are pretty well standard and publicized. There's some quick googling should find those for people. Yeah, for sure.
Of course, I think you could measure some things like a DCG, which is offline, right? So like, but you do need like rated queries.
And as a contributor to Qbit, which is a query rating system, open source system, I'm curious to to hear your opinion on, you know, sort of on one hand, of course, you can always go and just check, sanity check, you know, smoke test, your, your, your runker.
But that's just maybe for engineers or product managers, like a smaller group versus when you go and try to understand the intent of queries at larger scale with this manual effort.
Have you seen, have you deployed such methods within organizations? What do you feel like doing this in the companies on more regular basis?
And I also know, as a shout out to what you did in the course, search with the mail course, like you did ask us to rate some queries and create a judgment, please, to get a feel of the process.
And I think that by itself is a great idea because it pushes you towards, you know, further understanding what is it that you're building for? So yeah.
 Yeah, I mean, I think, yeah, I mean, it makes, it makes a ton of sense to have, if you can afford to do offline evaluation using, you know, professional annotators, you know, like, I don't know how good mechanical Turk these days is, but like, you know, something like a mechanical Turk or like, I forget what crowd flour is called now or I know we've worked with a company called Appen in the past, like, there are these companies out there that will provide you with a large number of annotators who will run your queries and then rank them for you.
And of course, you can use that as well.
So again, like, you know, it often comes down to whether you're monetizing your search results and folks who do monetize their search results will typically pay for those kinds of things, especially once they reach really large scales, you know, like your, your Amazon's and the like.
Where and how much you can do that often comes down to budget and time, right?
So, you know, if you have the budget, I've seen companies do that, you know, maybe I don't know about weekly, there might be some that do that weekly at the really large scale, that gets really expensive quarterly or whenever there's a major update to the system, those kinds of things.
So by all means, I mean, I think anything you can do to get, you know, I think often in this space, we love to say, oh, well, this is the way you do it.
And the reality is, is like, you want a hybrid approach to most of these things, right? Because there's no one perfect way of, there's no one perfect model and there's no one perfect way of evaluating a model, right?
And so you need to blend these and build up a broader sense of what actually works, right? Yeah, absolutely.
 It's just like, I guess, I guess, general awareness, like that these systems and approaches exist and like when you feel stuck that you don't know, okay, you don't generate ideas where you can improve your search engine, you can go deeper and try to involve, you know, and the teachers, I believe, to help you understand.
And before we move further to some of higher level questions, I still wanted to ask you a little bit more detailed question on if somebody in the audience or listeners wants to try to build the kind of end-to-end search engine at home.
So what are the available datasets, tools and algorithms exist today that will allow you to build this and train relevancy models and all these building blocks in the search engine? Yeah, I mean, it's, you know, it's interesting.
I think in many ways we live in a golden age of of search engines, right? Like, there are several just top notch open source freely available search engines on the market.
There are a number of companies competing in this space, right? So, you know, picking an engine is almost like, hey, you know, it's a plethora of riches.
It's almost, it's like, you're, it's a challenge to pick one because there's so many good choices, right? And you're often like, what specific features or domains am I going to participate in? So, you know, it's obviously one like, choose a good engine.
And I think you really can't go wrong with any of the main ones. What, you know, it's the Lucine-based ones, Solar Elastic Search Open Search. I haven't played with Vespa myself, but, you know, I think that one's coming on strong as well.
You see a lot of interesting capabilities that are coming out of that. And then, you know, you have obviously the, the companies behind it. Of course, I'm co-founder of Lucidworks, and so still a big shout out and big fan there, because I think they're doing a lot of interesting things.
But you also see a number of other players in that space, both with deep learning or neural-based approaches, as well as blended or hybrid or traditional approaches. So, one, start with your engine. See what it's capable of.
And then on the data set front, it really kind of depends on what your, what domain you're in. But, you know, I'm a big fan. You know, I often start with public data sets, Trek TREC is a great place to get data sets across a large number of domains. You can also get queries.
So, whether you want to do web search or e-commerce or legal or enterprise or medical, like you can go to track and get a data set and start indexing that, playing around with it. These days also, it's just super easy to go crawl.
So, you know, get like scrappy or curl or WGET or whatever, or it's one of these crawlers and go crawl websites. And then you can start going from there. The query log side tends to be a little bit harder because companies don't like to release their queries.
But there are several data sets that do have some form of queries with them. They may not be enough for you to fully test all the features of an engine. So, in our class, we use a really old data set from Best Buy that has query logs. In it, well, query click logs.
But for instance, it doesn't tell you what was shown the user. It just tells you what they clicked on. And so, you can't actually build full models or effective models with that.
But it's actually a really good e-commerce data set because it has all of the problems of a data set that comes from a company. Namely, there's a lot of missing data in there. There's a lot of bad data. But there's also a lot of really good data.
And so, starting with those, and then I think, you know, you kind of just start to push the engine through its paces. Start with the tutorials, the basic features, and then see where you can go deeper.
Can you actually get Best In Class relevance measurement out of it? Can you get Best In Class speed performance out of it? And then just work your way through the engine. And these days, you can typically do that in, say, less than a week.
And that's really amazing, right? Especially when you combine that with all the great information out on the web, right? Like, you know, I think when I was getting started, it was, you know, you had to go and really dig in underneath the hood and kind of figure out a lot of those pieces these days.
It would take several weeks, if not months, you know, month or more to really feel like you understood an engine and where it went. And I think these days, it's just so much easier to do that, which is awesome. Yeah, absolutely. And I remember during the course we had to do it within a week.
So per project. So that was super exciting. And I think this would not be a vector podcast if I wouldn't ask you also on your opinion in vector search.
Like what's your feel for how it will augment the search engine experience on the user side as well as on the development side and connected to that. What do you think the search engine engineer profession is going to be like soon? And I think it's already shaping up in many ways.
Like the boundary between data scientists and the search engineer blend. Do you feel yourself like that? Do you think this is the direction we are going? Or do you think it's going to be like a form that will wear off? That's at some point. Yeah, I mean, it's, well, it's not going to wear off.
I mean, there's too much money and too much investment and too much better results. I will state upfront, I'm not an expert on these vector engines, right? Like I, it's kind of interesting.
Like they, I went back and look through some of my talks and I think I gave a talk in 2013 on what the Lucine and Solar community needed to do next. And one of the things was we need to add support for dense vectors. That was 2013. I think we just got dense vector support in solar.
Elastic maybe was there a little bit sooner, but roughly same time frame. There are plugins, of course, that have been around like the K&N plugins, things like that. Hey folks, like this stuff is here to stay.
I mean, the really interesting questions, you're starting to see these hybrid models where, like BM25 is still really good and really fast at that first pass retrieval.
It's kind of hard to beat in terms of the scale at which you can get a first pass rank, right? And then feeding it, those results into much deeper or more capable engines. I think that's been around for a while and academia has proven that out.
Clearly, like using embeddings and vectors for things like query understanding and content understanding and using tools like Burnt, etc. for enriching your understanding, your content, and then making those searchable. That's all, I think, well and good.
 I think the really interesting question will be is whether the vector engines can add all of the layers that the sparse approaches have, I don't know about perfected, but added over the years, you know, the fascinating, the aggregations, the spell checkings, the highlighting, all of those things that actually go into building a search application.
If the vector engines deliver all of those things and deliver better results, that's probably a no brainer, right? In the meantime, we have these hybrids because I think there nobody is delivering all of the capabilities.
The other things that's interesting with the dense vectors, right, is that you can start to map multimodal data types all into the same engine. So images and text and audio, etc. Right? And again, like I'm not an expert on this, but that's my understanding.
So then, so then you can query across spaces, if you will. Again, like I'm not using the right terminology here, but and that to me is often the, at least people talk about that like it's a holy grail. I'm not fully convinced people will actually search that way.
I still think that remains to seem because there's a lot of implications for the the user interface and the user experience is how you interact with that.
You know, like people have long talked about, oh, hey, I'm going to take a picture and then get my back, my search results, but like I don't every time I use those tools, I'm like, okay, that's nice, but it's still clunky from a user experience standpoint, right?
So, so like there's a lot of that work above and beyond just the core engine that has to be solved.
But clearly, there's a lot of money and effort going into it. And so like as a search engineer, you can't ignore as a data scientist, you can't ignore it. And so you've got to get up on how these are built.
I think all the major engines open source and private have some form of it at this point of blended models.
 Again, like, you know, if you're in a domain that you don't have enough data for these and may or may not work, although again, like one of the interesting things with these neural models, right, is you can often train on a general model and then just use a few examples from your domain to essentially tailor that general model to your environment, right?
Like I'm working on one of my clients is doing this in the NLP space right now.
We're using a general model around analyzing contracts and then we're applying domain specific things to it.
And it's really interesting how effective it is with very few examples, right? That's an NLP problem, not a search problem, but you know, so I think you're going to just continue to see that trend and grow and expand, right? So you've got to be on board with it. Yeah, absolutely.
And you can find of course more conversation on the podcast about this. But I think I agree with you that the multimodality aspect of vector search is quite exciting.
And where the data sits in images, for instance, that haven't been annotated yet, right? And so many images uploaded every single day in videos, you know, if the model is able to transcend the the domains so easily like clip model, for instance, built by OpenAI, it's not a perfect model.
Sometimes it fails, but sometimes it also uses you like, how could it figure out, you know, to work so reliably on my data that it hasn't seen before? That's amazing.
Well, and it goes back to your earlier question, which is like, you know, at the end of the day, folks like go evaluate it and see whether it works better for you.
And then like I said, even earlier, I mean, they're all just vectors and we're all just trying to calculate cosines between the user's query and and the vector. And so in some regards, like we're just building a better vector, right? It's just a better vector. It has more information encoded in it.
And so if I can query that more effectively, then why wouldn't you use it? Yeah, yeah, exactly. And of course, there are other subtopics there how to make it faster and so on, but I think eventually we will, hey, Google figured it out for 10% of the queries.
So I guess the rest of the world will catch up.
Before we continue to the questions from the audience of which we have at you, I do love asking, and if you can keep it a little bit short, because we are short on time, but I'm still super, super interested to hear your motivation to stay in this space.
You have tried so many things in your career, right? Looking at your LinkedIn profiles, just on and on experiences and fractional CTO and full-time CTO and an engineer and so on and book author.
What motivates you to stay in this space today and also go into education teaching? Yeah, I mean, it's funny.
 I think, well, even when I was at Wikimedia and I quote, unquote, left search, I mean, we still ran a very large search engine and I always enjoyed my conversations with a search team at Wikimedia just because they were, you know, it's such a high traffic website and search there, I think does something like 6,000 queries per second or something like that.
 So you know, in some ways, and this is reflecting back on my career, I mean, I think I fell in love with language and the way humans use language and find information back circa 1999 or so when I started at a small company called TextWise run by Liz Litty who is one of the pioneers in the natural language processing field and it just happened to have a search project that I started working on, right?
But to me, you know, at the end of the day, like, this space and this is why I went to Wikimedia.
 So I say, searches that necessarily the through line, even though it's often the main, it appears to be the through line in my career, the deeper through line, I think, is that I am fascinated by how we can leverage computers to help users make more informed, more capable, more aware decisions in their lives, whether that's purchasing online or political or governmental or whatever it is, like, I am fascinated by how we can help people make more informed decisions because I think that's the thing that lifts us out, right?
And so education then is a easy follow-on from that through line, right? Like, the more people I can help use these tools and also learn myself, the better off will I'll be, right? Like, we have to use these tools to, you know, to help us as humans get along better, etc.
be more informed, so on, so forth, right? So that's probably the through line of the career, right? Is this how do you help people find information and take action that makes us all better? Absolutely, this is very deep. Thanks so much.
I love asking this question because I'm super motivated to stay in the space, but I also love to see the facets and the motivation of other professionals like yourself that I'm looking up to. I really enjoyed this conversation.
Is there an announcement that you want to make in terms of the courses that you're going to be teaching soon? Yeah, that's great. I appreciate that, the Metri, and I know we'd have some user questions, and I'm happy to stay on a little bit longer as well, get those.
Yes, we actually, we have two classes coming up.
So one of the things we learned in the first run of search with machine learning is, you know, effectively we had one week of trying to get everybody on to the same page of how does open search work and what are the basics of search?
 And then we had three weeks of fairly intense machine learning in a search environment, and one of the things that happened in the class because we didn't have a lot of prerequisites is we had a really wide array of students of folks who were deep experts like yourself, as well as like totally new to this arena.
And what happened, I think, is that first week for the new people was like, hey, this is too much for me to get up to speed. And for the folks who had already done search, it was like, hey, I already know how to do all of this.
And so trying to, trying to go across that gap, I think we kind of ended up in this lukewarm area where nobody was quite satisfied.
So one of the things we did was we split out the new stuff into a two week class called search fundamentals, which covers all of the basic intuitions of search, whether it's deep learning based or a sparse learning based or sparse vector based, sorry.
And so we cover, you know, indexing querying, facetying, spell checking, auto-complete, kind of all the building blocks of a search application.
And then with the machine learning class, because we're dropping that beginner class week, we now have added in a neural retrieval dance retrieval into that as well. And so the search with fundamentals class starts next Monday, June 6th. You can still sign up. It's $200.
There's a code, DGSearch 10. And then search with machine learning is two weeks after that. And that's a four week class. Both are project intensive.
Every week, you're going to do a project, you're going to write code, you're going to interact with students, you're going to hear lectures, so on, so forth.
In many ways, I think it's modeled after a university style class where you, you know, every week you have homework, every week you have lectures, and so on, so forth. So yeah, please sign up. Yeah, that's awesome.
What I've personally enjoyed during the course, the search with the Mel four weeks course was the atmosphere. The atmosphere that was basically creating itself amongst the students and was over 100 people there on Slack helping each other. That was just amazing.
Somebody saved me like a ton of time by just sharing, you know, a recipe that I followed and quickly went through to some hurdle. And I learned, and I, of course, I knew some stuff.
Yes, I'm an expert in this field, but also you can put your expertise, you know, to a test when you, when you run so fast during the course and the support that you guys provided was amazing. So that's amazing. I've enjoyed this conversation so much.
Now we are moving to the questions from the audience. And I'll pick the, and feel free to ask questions, please. We still have a few minutes. The first question comes from Avynash, who is currently testing the approach of buying coder to find the similar sentence, top 10.
And later passing the top 10 sentence to a crossing coder model to find the most similar sentence in the top 10 using cosine similarity. Yeah, I guess he's asking for advice is this an appropriate method. This is where my expertise just is not. So Avynash, I will apologize.
I do not know enough here to give you advice.
I would probably ask first, like, what is the actual problem? Are you trying to solve? You know, so if you're trying to find similar sentences, then from my understanding of it, that my basic level understanding of what you're describing, it sounds like a reasonable, a reasonable approach.
But there are people who are much in probably Dmitry, you probably could answer this one better than I, but I have not played with or tried out those specific types of capabilities. So I don't have good advice there. I have worked in general on sentence similarity type problems.
It is always challenging. In fact, I have a my current company that I'm one of my fractional clients. We are doing sentence similarity or clause similarity types problems. And I think they are we are using similar modeling techniques, but I'm not doing the day to day modeling on that.
So I'm really just trusting the data scientists on that. Yeah, I can add to this that I happen to have given a community talk during the search with the mail course. And there I actually go explicitly into this by encoder and cross-ent coder.
So only one thing is that cross-ent coder is much more computationally intensive. And so you don't want to run it on a huge amount of sentences. And it looks like that's what you're doing. So that sounds sensible to me. I think I would pay more attention to testing your approach.
So make sure to reserve some part of your data set to test it. Careful. Yeah, this is the cool thing for me coming back in from Wikiland is I'm learning so much now too.
Like this is I've been digging my way through a lot of these things, but as you can see, this is why it's the gold age because there's so many approaches and they're often improving state of the art every week, right? Yeah, exactly. A lot of things is happening.
Another question I'm taking now from the chat, Carlos is asking, I'd like to know Grandsepinion inside about learning to boost. He gives also a link to a presentation at a high-stack high-stack conference. I don't know if you're familiar with this approach, Grant. Can you say anything? I am not.
I'd like to know learning to boost interesting. Another thing to go learn. Yeah, I think it was all kind of learning to rank. I think it's related, but I actually don't know myself like that much in detail, but that that presentation was great.
It looked like new thing, but at the same time kind of familiar. Basically, instead of learning to rank, you learn the boost values as far as I remember. It sounds interesting and reasonable.
Again, at the end of the day, how do we shape these vectors? I know that's a generic wave in your hands, but I would take this and go try it. I think most of these machine learning systems you're trying to learn weights that then shape the way that vector gets called.
If it works on your domain and it's fast enough and you can maintain it, then go for it. You don't need some experts blessing on it. It certainly sounds interesting. LTR certainly has its own challenges in terms of tweaking and tuning. I know I've struggled with that with LTR a lot.
I know I've struggled with hand-tuned boost a lot as well, so anything that helps do that I think would be good. Yeah, awesome. The next question comes from Nico, Hey, Nico, a former colleague from AlphaSense.
If you're hosting an information search engine which should catch new topics like COVID when it hit, how do you notice that your boosting model of vector embedding model does not recognize queries related to these new topics proactively?
Yeah, that's where I think the instrumentation of your system comes in, right? And the human and the loop on that instrumentation in the system, right?
I mean, I think nobody talks about it, but even at the really large successful search engines, there's still people who are reviewing where things are working and not working, right? And generally they're doing it at the experimentation level, but people still dig into queries.
What queries are underperforming? What documents are underperforming? I think there's tools, there's a lot of good tools out there for anomaly detection as well.
So recognizing when new queries are coming in is something like anomaly detection algorithms will work with, right?
You know, looking at your top queries, your trending queries, and then again, looking at those results, there are machine learning approaches to automatically identifying and alerting on those kinds of things, again, along the anomaly detection line.
But at the end of the day, you can always do that with people as well, right? And that's where humans maybe are better at still at recognizing some of those things. Yeah, and I think you also alluded to this somewhat.
I mean, this question is to me, it's like chicken-eyed problem, right? So if a new topic arises in the queries and also in the documents, but I haven't handled it yet before prior to this, then what can I do live?
So I think you said that try to measure things like if some top ranking documents are not clicked, then that's probably a signal of something is smoky there.
Go check it out. Another thing that I think I could recommend, maybe from my side, is you could try to cluster your queries actually.
And sometimes the funny thing is that queries are related in some way, right?
So like if it's a completely new cluster and usually dense retrieval helps a lot there, pre-trained models on your domain or maybe on some generic domain like news, they might still pick these things up and put them in the same basket, then ask some human annotators to go and check.
Instead of checking the whole multimillion log, you know, which would be super, super complicated. And you know, I agree.
And the nice thing about like, you know, especially, you know, these engines, you know, there is still the good old BM25 case where like at least the basic level keywords are going to match.
And so if a new term comes in for COVID and like it's in the documents, you'll at least probably get an exact match. You may not deal with the fuzzy matches all that well, but you know, like something's better than nothing. And then that allows you to start to iterate on it. Yeah, exactly.
So the next question from Q&A panel is from Chris for the search with ML course, which front-end framework are most students using for their projects? Front-end framework feels a little open-ended to me, but I mean, I can tell.
So one of the things we're doing in both classes is we try to work with a real data set-end with a real search application. For better or for worse, we chose not to use notebooks. Notebooks are great for a lot of things, but I don't know that they always show you how actual applications work.
So we actually build out a really simple application. The front-end is like tailwinds, CSS, and really simple flask serving layer for the APIs. And then we use open search for the search engine and things like fast text and a few other things for ML side of it.
You know, we use the learning to rank plugin for open search, trying to think if there's anything else in our stack. It's primarily Python, but I think if you were a Java user or any of the other languages where there's clients for open search, you would do just fine in the class.
You maybe just won't be able to use all of the Python capabilities that we have in the class. I hope that answers your question, Chris. The repositories are all at least the base level repositories are all available under my GitHub.
So you can just go to my GitHub, which excuse me is GSING, ERS, and put that in the chat. And then you can see the frameworks we use. Yeah, awesome.
And I can just, you know, you can pick these things up or you can, if you know Python, it's probably easy for you, but if you don't, you can pick this up. And the next question is from the chat from quasi, I hope you pronounce your name correctly.
As these days, most of these sort of approaches are based on transformers for anyone who wants to try out IR approach using transformers as a pet project. Does grant have any recommendations in terms of cloud services tools? I don't have any specific recommendations.
I know I've looked at there's several players. I was so for instance, I saw somebody in one of the IR communities that I was in with posted around, I think I don't know how he's pronounced about quadrant, I think QDR and T.
I know there's UVA, I know there's Pinecone, elastic, solar, and open search all have dense vector retrieval capabilities. I've been playing around just getting started with hugging face. I'm a little late to the hugging face game when it comes to these things.
I know a lot of people I talk to use colab to build and run these systems. And so I think you can probably get started. Again, like Demetri, you may have better tutorials. I know you've posted a bunch of stuff on medium amount, how to get started in this as have other people.
So I would start there, I guess, any one of those you probably won't do wrong with. And then for me, I always go back to, like I like to take a data set that I'm familiar with first rather than a technology that I'm unfamiliar with.
Whenever I'm learning something new, I start with something I'm familiar with and then try to apply that thing to the new technology as opposed to picking the technology first and then trying to, you know, kind of go back and forth between the tutorials that they provide.
But I always like to go back to a domain I'm familiar with because then I don't have to rebuild my intuition. Right. So for instance, I've never really done image search, but I've done e-commerce search all the time.
So it makes way more sense for me to try out transformers with e-commerce than it does with images just because I don't know the core intuition as much on the images as I do for e-commerce. So I would probably start that way first. Yeah, I agree.
And another thing, yeah, of course, Grant, you thank you, you mentioned, you know, my medium blog post, there are a lot more people blogging on this, but I have a specific collection on medium, 37 minutes by sheer reading time.
You can go through like basics like exact can and search all the way up to, you know, neural retrieval, which is approximate nearest neighbor search because you cannot do exact can and search at scale. It will just not not scale.
So you have to kind of go and cut some corners, so to say, but actually in a more mathematical sense, you create this algorithms that are beautifully handling this complexity for you. So go check it out.
I think the next and probably last question, but not least, is from a shish, is the search with a mail course right to step into if I'm looking to learn about semantic search and add the functionality to SQL or no SQL databases? That's an interesting question.
I guess I haven't thought about it in that sense.
I mean, I think, you know, I think a lot of the techniques we use in the ML class relate to semantic search and relate to like how can we get better relevance out of the engine? So semantic search being one of those types of capabilities, a kind of semantic search often is a pretty loaded phrase.
So depending on what you're trying to do there, as you should your mileage may vary. But we certainly cover things like classifying your content, classifying your queries. We do learning to rank.
We talk about synonym expansion query, you know, smarter queries, better filters, all of those kinds of things, I think fall can be loosely coupled into semantic search.
If you're talking more like you want to do like graph-based inferences or, you know, using things like wiki data or dbpedia or those kinds of things to infer relationships and do semantic search that way. We don't really cover those as much.
We do base off of open search, but I think the concepts apply in general. With the SQL and no SQL databases, like I know a lot of them have kind of baseline search functionality in them.
And so you would be able to apply some of the principles because a lot of the principles we do, you actually do either before indexing or before querying.
So those would certainly apply, you know, because at the end of the day, you're just using those things to then generate a better query or a better document to be stored in your engine. And so I don't see your reason why they went work in a no-SQL store or a SQL store.
It's just then how do you translate that into your query language, right? But we do use open search. All the examples are open search. You would have to do the work to leap to that, whatever it is your engine is doing. Yeah, absolutely. And the good thing is that open search does have a K&N plugin.
They call it K&N plugin, but it's actually approximate nearest neighbor search. And so it's off heap for those who care. So it's not inside Java, but it still allows you to get a feel of how neural search will influence your results at.
And you can also, you know, mix and match, sort of using more traditional VM25 with this. Awesome. This was the last question. Thanks so much to everyone who asked their questions live. And, you know, consider joining the course if you haven't yet.
And, Grant, thanks so much for this session and for answering the question and sharing your wisdom. I've enjoyed this conversation very much. Thank you. Thanks so much for having me, Dmitry, and keep up the great work. I love the podcast.
And it's awesome to see a search dedicated podcast out there. So congrats and good luck with that. Thank you so much. All right. Bye-bye. Bye, folks. Thanks, Dmitry. Thanks, Grant. All right.